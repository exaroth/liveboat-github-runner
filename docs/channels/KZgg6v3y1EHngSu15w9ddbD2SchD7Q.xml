<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News - Last 2 days</title><link>http://site-url-not-set.io/you-can-set-it-in-liveboat-config</link><description></description><item><title>Lifelong Learning: 88+ Resources I Don&apos;t Regret as a Senior Software Engineer</title><link>https://thetshaped.dev/p/lifelong-learning-88-plus-resources-i-do-not-regret-as-senior-software-engineer</link><author>/u/pepincho</author><category>dev</category><pubDate>Mon, 27 Jan 2025 05:15:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Internet is flooded with content, materials, and resources.Knowing the most useful newsletters, books, courses, creators, and tools is hard.In this article, I want to share some incredible resources that Iâ€™ve found valuable in my experience and day-to-day job.Each resource on this list taught me something new and helped me learn and grow as an engineer.As a Senior Software Engineer, I need to keep up with the industry trends, updates, libraries, tools, vulnerabilities, etc, so that I can apply this knowledge at my job.Here are most of the newsletters I read every week.I use these four newsletters to stay up-to-date with the Web and JavaScript world.Iâ€™d suggest avoiding reading all these newsletter at once because youâ€™ll feel overwhelmed.Depending on your current needs, priorities, and career aspirations, you might want to choose a few of them and come to the rest when needed.Books are a great way to learn and grow as an individual.We can learn from a lot of people even though theyâ€™re not in front of us.We can learn from their mistakes and see what lessons they have learned.However, Iâ€™ve found that if I read a book and donâ€™t apply my knowledge from it as soon as possible, the value from reading the book drops drastically.Think in advance how reading a particular book will help you in your day-to-day tasks, job, and personal life. If you canâ€™t apply the knowledge immediately, postpone reading it.Each book has taught me something or sparked a new idea and way of thinking.Itâ€™s not necessary to apply everything from each book but rather look for the things that most suits you at the moment and adapt them to your lifestyle.A great mistake I made in the past was to try to apply everything on 100%.The true wisdom comes when you find the 10-20% of the book to apply at the moment.At each phase of our lives we need different things, so think twice before applying anything directly. Be conscious.Donâ€™t try to read and follow everything at once.Think in advance how reading a particular book will help you in your day-to-day tasks, job, and personal life.If you canâ€™t apply the knowledge immediately, postpone reading the book or newsletter.As you might see, thereâ€™re no courses. My preferred way is to read and apply what I read immediately in my day-to-day job or side-projects.That's all for today. I hope this was helpful.What are the 1-2 resources youâ€™ve found life changing? Share them in the comments ðŸ‘€ ðŸ‘‡Become a better React Software Engineer. Join 17,400+ engineers who are improving their skills every week.I share daily practical tips to level up your skills and become a better engineer.Thank you for being a great supporter, reader, and for your help in growing to 17.6K+ subscribers this week ðŸ™You can also hit the like â¤ï¸ button at the bottom to help support me or share this with a friend. It helps me a lot! ðŸ™]]></content:encoded></item><item><title>If OpenSSL were a GUI (2022)</title><link>https://smallstep.com/blog/if-openssl-were-a-gui/</link><author>tambourine_man</author><category>dev</category><pubDate>Mon, 27 Jan 2025 04:34:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA["When something exceeds your ability to understand how it works,
it sort of becomes magical." - Jony Ive*This is incomplete. It covers about 80% of one corner of OpenSSL's functionality. The certificate policy options have a lot more knobs that I didn't include.Carl Tashian (Website, LinkedIn) is an engineer, writer, exec coach, and startup all-rounder. He's currently an Offroad Engineer at Smallstep. He co-founded and built the engineering team at Trove, and he wrote the code that opens your Zipcar. He lives in San Francisco with his wife Siobhan and he loves to play the modular synthesizer ðŸŽ›ï¸ðŸŽšï¸]]></content:encoded></item><item><title>Orbitiny Desktop Environment Released (Originally Announced as LQDE)</title><link>https://www.reddit.com/r/linux/comments/1iayzwm/orbitiny_desktop_environment_released_originally/</link><author>/u/sash-au</author><category>dev</category><pubDate>Mon, 27 Jan 2025 03:49:03 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Finally, after about a month after the original announcement (LQDE), the  has been released.Built from the ground up using  and coded in C++,  is a new, 100% portable, innovative and traditional but modern looking desktop environment for Linux.  because it has features not seen in any other desktop environment before while keeping traditional aspects of computing alive (desktop icons, menus etc).Portable because you can run it on any distro and on any live CD and that's because everything gets saved inside the directory that gets created when the archive is extracted (this can be changed so that the settings go to $HOME/.config/orbitiny).One of these innovative features is  but more on that later in this post.It comes with its own set of utilities and applications. It has a device manager which can disable / enable devices by right-clicking the device and selecting  and all that without black-listing the whole kernel module so it targets the selected device only and nothing more.It has its own fully featured and innovative file manager, a fully featured desktop panel with 18 plugins with full and natural Drag&Drop support, a dedicated search utility, one integrated with the file manager while the other is a stand-alone one, a , hot-plug detection with desktop notifications and more. is  a derivative of or based on any other project. It started with a blank / main window - the one that you'd create in Qt Creator when you start a new project.So what is so special and innovative in ? I don't know where to start, here are some of the features that sets it apart from other DEs (I've probably missed some). - On the blank area of the desktop, draw a gesture pattern (like in a web browser) but on the desktop to perform an action, like for example, launch a custom command or use one of the built-in supported actions available to choose from. Up to 12 gestures are supported for both left and right mouse buttons, 12 per button + additional configurations for middle clicks. Gestures are drawn on the blank area of the desktop and they work regardless whether icons are turned off or on. When a file is  or  to the clipboard, a little icon emblem with a "cut" or "copy" symbol is attached to the icon to indicate that the file is on the clipboard, either copied or cut. If the file is a directory, and contents of that directory change (like a file is created or deleted), an emblem is attached to let you know that the folder contents have changed. - Drag a text file over another text file to add the contents of the dragged file to the target file. - If there is ASCII content on the clipboard, right click the files and select "" and the content will be appended to the end of the file. Prepending is also available. If the selected file is a folder, the text content will be pasted into that folder and a file gets generated automatically. There is also image pasting. If the clipboard has an image, right click + paste will generate an image file. - Select a set of folders on the desktop and click "Paste" and the content from theclipboard will be pasted to all of the selected folders. Text content will also be pasted automatically by generating a unique file name and a file (works with images too).Custom Desktop Directories - Choose any folder and use it as a desktop directory. It doesn't have to be . - Each screen is a separate desktop so on one screen you can have one desktop with its own set of icons (by selecting a desktop directory of your choice) and on another screen, you can have another desktop with its own icon by selecting a different desktop folder. Of course, works with wallpapers too. So it's like two different computers running on two screensBeautiful and Non-Blocking Custom Context Menus. Non-blocking means your traditional shortcuts you have assigned in X11, will continue to work when a context menu is open, the shortcut won't get caught/blocked by it like it is the case with many other applications that use standard context menus. The context menus are custom made, not using the QMenu component. - Select several folders, right click and select Open Terminal and a new terminal will open for all of the selected folders.Built-in Run Drop-down Box (Combo Box) in the context menus allows you to run a command against the selectedfiles (highly experimental and new).Multi Profile Support on the Panel - Right click the edge button on the panel and create a new profile or select one of the previously created ones to get a new configuration / sets of applets. You can switch between profiles like you switch different TV channels.Full Drag&Support on the Panel - Drop any File/Folder from the Desktop or a File Manager or Drag and Re-arrange any applet, any icon on the panel. No special "Edit Mode" is required. Just grab the applet on the panel or a file from the desktop / file manager and drop it straight onto the panel and an icon for it gets created or the dragged one gets re-positioned. So to be clear: Launch Thunar, Nemo, Dolphin or whatever and drop any file / folder from it onto the panel, either on the Quick Launch or anywhere else and a file icon gets created. This, Drag&Drop Support . The panel can be resized, and placed on any corner of the screen by dragging its handle or you can put it on the middle of the screen if you wish, or turn it into a dock with auto-resizing, or a deskbar that takes the width or the height of the screen. It's highly configurable. I use it as a deskbar as I am used to it.A Comprehensive Start Menu / Application Launcher applet and again with full Drag&Drop support. You can re-arrange icons within the menu, from / in the menu, and there is designated area for a sidebar too on the menu which you can also attach / remove icons from and in to it. - Perform custom actions on the selected files. Commands can be edited in the configuration file. inside the right-click context menu. - click any edge on the desktop to launch a dashboard window that shows you running tasks + installed applications. Search/Filter is available. At the moment, the running applications only work with X11. - All the files needed to run along with the applications it comes with can be downloaded to a USB flash drive (or a folder) along with the settings so you can just take the whole folder with you and run it on any Linux computer and the settings will remain the same so the settings are also portable. support. All the components mentioned here support both WINE and DOSBOX. This means, if you drop a Windows or DOS exe onto the panel and click on it to launch it or double click it from the file manager or the desktop, its path will be handed over to either WINE or DOSBOX to run it. - Remember this? Well, if you double click on a MAFF file, it will extract it in the /tmp dir and launch it for you, same as if you are clicking an HTML file. - Some of the panel applets such as the launcher applet, quick launch and the drawer menu along with its items allow you to add two commands per launcher. One for left-click and another one for middle-click.Multi-content Search Support in File Manager - The file manager supports searching for content inside files but it also gives you an option to search for an additional word on the same line the match is found. - To increase / decrease the icon size, along the standard CTRL + Wheel to zoom in / out, you can also click and hold the right-hand mouse button and use the scroll wheel â€“ up/down.Double-Clicking a Blank Desktop Area Run a preset gesture or an individual command when the blank area of the desktop is clicked. Hold-Down Right-Hand Mouse Button and Double Click - Run a preset gesture or an individual commandRight now, it serves more like a desktop shell because it doesn't have a session manager and other utilities such as a power manager, screensaver, screen config etc but if I get enough motivation, I intend to develop that too.The application can run in  and  mode.To run in portable mode, make sure a file named ".portable_mode" (without the quotes) exists in  If running in , it will return the path to the folder/dir that contains all the files. if running in e, it will return  Returns the path to $BASE_DIR/shared directory. To make sure the package remains portable across live CDs and distros, save/download all yours files to the "shared" folder and then when assigning commands to launchers, do something like $SHARED_DIR/my_file To run in , make sure a file named "" (without the quotes) exists in .This desktop can be run on top of any other desktop, even GNOME, Elementary, KDE. When so, it draws its own desktop window, full screen covering the already running one. When run under iconless desktops, you will get icons (works on GNOME).Right-click the Desktop and go to "Environment & Workspace Settings" and then "Appearance" to adjust the content margins of the desktop. This is the left, top, right and bottom positions of where the icons start. It's in the "Content Margins & Spacing" section. This should be adjusted according to where the existing panels are positioned such as the GNOME menu bar or any other panels on the sides of the screen.Double-clicking the "Linux System" icon brings a "Disks & Partitions" menu. This behaviour will remain until I implement a proper and fancy "Computer" window. I have most (but not all) of the code already.Right-clicking "Linux System" brings up a menu with a set of system utilities whose paths need to be set in "Environment & Workspace Settings"->"Applications". Except for the "Device Manager" which I already have working (most of it), the rest of the utilities need implementation but as a work-around, you can enter a path to an external utility.Double-clicking the "Disks & Partitions" icon brings up a different, perhaps fancier "Disks & Partitions" menu so use the one you prefer. Right-clicking the "Disks & Partitions" icon will bring the same "Disk & Partitions" menu as the one that comes when double-clicking the "Linux System" icon.I have pre-prepared an existing "Custom Actions" menu for you to look at. Take a look at the examples, I think you will get the gist but if you don't, then just email me / ask me. "Right-click"->"Custom Actions"->"Edit Custom Actions".When holding the "Alt" key when double-clicking an icon, either on the desktop or the file manager or any of the panel applets that let you run commands, will force-run the command in a terminal window  there is a catch. This will NOT work if your window manager's accessibility key is set to "Alt". On my system, I have this accessibility key set to the Super key so it works fine. I will make this customizabe in the future.You will need to right-click the "Orbitiny" applications menu on the panel and go to "Commands" to set log out, reboot and power off commands. These will need to be matched with the ones used by the exisitng session manager.I have done it like this because I don't have a session manager yet. My next primary goal is to develop a session-manager so that you can select the DE from your display manager and run it. Right now, you can set "start-orbitiny" as a start-up application in your existing desktop environment settings and when so, it will start automatically.Wayland support, as far as I am aware, the window tasks and the systray are the only parts that don't work but it has not been tested fully. When testing, you should be testing it under the X11 display server rather than a Wayland compositor. Right now, I don't support any of the Wayland copmositors but I intend to add official Wayland support in the future.By default, middle-clicking an empty area on the desktop will bring the fancy looking "Disk & Partitions" menu. You can change this in "Environment & Workspace Settings"->"Advanced"->"Gestures"->"Middle Button Click".You can change gestures in "Environment & Workspace Settings"->"Advanced"->"Gestures"Some of the panel applets such as the launcher applet and the drawer menu along with its items allow you to add two commands per launcher. One for left-click and another one for middle-click.The code base is huge, some of it is very old and requires a re-write and some very new and I've most likely missed something and that would cause an error.Please don't get upset/disappointed if you encounter an error or something that's annoying, just let me know and I will fix it.Finally, if you are happy with what you see, please consider making a monetary donation. That would be very much appreciated and would motivate me to continue working on the project and release updates, add/improve features etc. Originally I built this DE for my personal use but I now decided to release it to the public.bce30f77bcdc42bdc9633095e4b97327Again, the code base is large and without a doubt something is broken so please report bugs / issues and I will try to fix it. Looking forward to your feedback.Something I forgot to add about the panel.In some VMs, pressing and holding keyboard keys simultaneously do not behave as expected and as such it is not an issue with this panel. Click on a panel handle or the edge button and move the bar to any of the 4 edges of the screen / monitor to dock the panel to that egde position of the screen. Click on a panel handle and then while holding CTRL, drag horizontally to resize the bar. Click on a panel handle and then while holding SHIFT, drag vertically to move the bar vertically. Click on a panel handle and then while holding ALT, drag horizontally to move the bar horizontally. Also, the Edge button at the end of the panel can act as a handle too.Click on a panel handle and then while holding CTRL, press the Up/Down keys on your keyboard to move the bar vertically by an inch at a time. Likewise, press the Left/Right keys on your keyboard to move the bar horizontally by an inch at a time. Hover over the panel and use the mouse wheel to scroll the panel Contents (when scrolling is enabled). Hover over the panel and then while holding CTRL, use the mouse wheel to resize the bar. Double clicking a panel handle will run a command. You can edit the command in Preferences. Middle clicking a panel handle will expand/collapse a panel. To copy the content of a tooltip, click the tooltip icon on the right. To stop this message from popping up, go to Preferences and uncheck "Show Drag Handles Tooltips" located in the "Other" tab. To get to Preferences, right click the panel and select Preferences from the popup menu.]]></content:encoded></item><item><title>Linux 6.14 To Switch From SHA1 To SHA512 For Module Signing By Default</title><link>https://www.phoronix.com/news/Linux-6.14-Modules</link><author>/u/unixbhaskar</author><category>dev</category><pubDate>Mon, 27 Jan 2025 02:57:59 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>Purelymail: Cheap, no-nonsense email</title><link>https://purelymail.com/</link><author>LorenDB</author><category>dev</category><pubDate>Mon, 27 Jan 2025 02:38:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Let's get straight to the point:We host your email address.We're IMAP and POP3 compatible, so we work with most mail apps.Or you can use our webmail, powered by Roundcube.No arbitrary limits. Have as many users and store as much mail as you want.Bring as many of your own domains as you want, or use one of ours. No extra charges.It's cheap. Really, really cheap.Let's say you only need one email address, and have 3 GB of data to store. For one year of use, this will cost:For most use cases, Purelymail is the cheapest option available.  if you need more than one user. All of the plans above charge per user, but we don't. Check out our pricing page for more information. (For an in-depth explanation of this particular comparison, see here.)What you get here is   mail. We're not trying to bamboozle you with glossy images, or sell you a lofty ideal. If we have a gimmick, it's that we offer a known service at a low price. What you do with it is up to you.An honest list of drawbacksBased on our experience, here are our weak points:Occasionally, obscure email servers will block emails sent through us. Usually this can be resolved within a day or two.We don't have a 24/7 support staff, although we do try to get back to any inquiries within a day.Some features other providers have, such as calendar syncing, are not yet implemented.We're not for sending any type of unsolicited or marketing emails (you should use a dedicated marketing platform anyway).Our UI can be a little unpolished and unglamorous, if that bothers you.That means there might be a few hiccups along the way. If you run across any problems you can always let us know, and we'll do our best to fix them.If you're interested in Purelymail but want to wait until we're out of beta, check out our mailing list.What about security and reliability?As for reliability, we're in beta, but our architecture has proven itself robust so far, our infrastructure runs on the highly reliable AWS cloud, and even if we do have an outage the email protocols are pretty forgiving. (See our status page for our historical issues- there aren't that many.)Do you sell user data or ads?What features do you support?Still can't decide if we're right for you?]]></content:encoded></item><item><title>Event driven restart of Pods?</title><link>https://www.reddit.com/r/kubernetes/comments/1iax3fa/event_driven_restart_of_pods/</link><author>/u/ButterscotchWeak1192</author><category>dev</category><pubDate>Mon, 27 Jan 2025 02:10:49 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Context: we have a particular Pod which likes to hang, for unknown to us reasons and conditions (it's external software, we can't modify, and logs don't show anything).The most accurate way to tell when it's happening is by checking a liveness probe. We have monitoring set up for particular URL and we can check for non 2xx status.This chart we talk about deploys  Pod as well as  Pods. Each is separate Deployment.The issue: when  Pod fails it's liveness probe, it gets restarted by k8s. But we also need to restart  nodes, because for some reason it looks like they lose connection in such way that they don't pick up work, and only restart helps. And  in this case .  Pod first, then .Restart in case of liveness probe restarts only affected Pod. Currently, to restart workers too, I installed KEDA in cluster and created ScaleJob object to trigger deployment restart. As trigger we use kube_pod_container_status_restarts_total Prometheus query:apiVersion: keda.sh/v1alpha1 kind: ScaledJob metadata: name: n8n-restart-job-scaler namespace: company spec: jobTargetRef: kind: Job name: n8n-worker-restart-job spec: jobTargetRef: template: spec: containers: - name: kubectl image: bitnami/kubectl:latest # imagePullPolicy: Always command: ["/bin/sh", "-c"] args: ["kubectl rollout restart deployment n8n-worker -n company"] backoffLimit: 4 pollingInterval: 15 # Check every 15 seconds (default: 30) successfulJobsHistoryLimit: 1 # How many completed jobs should be kept. failedJobsHistoryLimit: 1 # How many failed jobs should be kept. triggers: - type: prometheus metadata: serverAddress: https://<DOMAIN>.com/select/0/prometheus metricName: pod_liveness_failure threshold: "1" # Triggers when any liveness failure alert is active query: increase(kube_pod_container_status_restarts_total{pod=~"^n8n-[^worker].*$"}[1m]) > 0 This kind of works. I mean it succesfully triggers restarts. But: - in current setup it triggers multiple restarts when there was only single liveness probe failure. This extends downtime - depending on different settings for check time, there might be a slight delay between time of event, and time of triggeringI've been thinking about more event-driven workflow. So that when event in cluster happens, I can perform matching action. but I don't know what options would be most suitable for this task.What do you suggest here? Maybe you've had such problem? How would you deal with it?if something is unclear or I didn't provide something, ask below and I'll provide more info.]]></content:encoded></item><item><title>Marginalia â€“ A search engine that prioritizes non-commercial content</title><link>https://marginalia-search.com/</link><author>herbertl</author><category>dev</category><pubDate>Mon, 27 Jan 2025 01:39:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tips for detecting if our shell is running inside a virtual machine</title><link>https://distrowatch.com/weekly.php?issue=20250127#qa</link><author>/u/daemonpenguin</author><category>dev</category><pubDate>Mon, 27 Jan 2025 01:09:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How do popular Rust UI libraries compare? Iced vs Slint vs Egui</title><link>https://www.reddit.com/r/rust/comments/1iavpit/how_do_popular_rust_ui_libraries_compare_iced_vs/</link><author>/u/nikitarevenco</author><category>dev</category><pubDate>Mon, 27 Jan 2025 01:00:40 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[For creating a desktop application, I've come across 3 libraries which I'm thinking of using:Which one would you use and why? If you have another library in mind I would love to hear it.    submitted by    /u/nikitarevenco ]]></content:encoded></item><item><title>Openhaystack: Build &apos;AirTags&apos; â€“ track Bluetooth devices via Apple&apos;s network</title><link>https://github.com/seemoo-lab/openhaystack</link><author>thunderbong</author><category>dev</category><pubDate>Mon, 27 Jan 2025 00:11:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How did you go about &quot;Writing an interpreter in Go&quot; (The book)</title><link>https://www.reddit.com/r/golang/comments/1iau5os/how_did_you_go_about_writing_an_interpreter_in_go/</link><author>/u/PeachKnight96</author><category>dev</category><pubDate>Sun, 26 Jan 2025 23:46:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Did anyone here finish this book?The author says we should follow along the book and type the code in our machine but I really don't understand this way of teaching. What am I learning if I only type this into my machine?]]></content:encoded></item><item><title>Lessons in creating family photos that people want to keep (2018)</title><link>https://estherschindler.medium.com/the-old-family-photos-project-lessons-in-creating-family-photos-that-people-want-to-keep-ea3909129943</link><author>mooreds</author><category>dev</category><pubDate>Sun, 26 Jan 2025 23:13:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[As a consequence of scanning thousands of slides, I learned quite a bit about taking photos that capture a familyâ€™s life. Hereâ€™s a personal memoir, with a few lessons in taking memorable snapshots.My father was an avid amateur photographer. He loved to take pictures, he invested in expensive cameras, and Iâ€™ve plenty of vacation memories where he had one of those cameras in hand.But organizing the slides afterwards? Labeling them? No way. Pop threw the boxes of slides in big piles and said, â€œIâ€™ll sort them after I retire.â€ And, in preparation for his retirement, he put all those slides into five huge boxes â€” the kind youâ€™d use to ship vinyl records.Whereupon, three days after my father formally retired in 1988, he died in his sleep.The slides stayed with my mother. When she moved into assisted living, the boxes went into my sisterâ€™s garage. After mom died, three years ago, they came to me. The result was a huge project of scanning family slides â€” between 8,000 and 10,000 of them.The primary goal was to save the photos before the media deteriorated beyond hope. It was too late in some cases. I remember Pop telling me how much cheaper Ektachrome was (compared to Kodachrome), but many of those images were as ghost-like as a half-remembered dream. Memories fade even faster. Is that a photo of my second cousin Charlotte? Iâ€™ve no idea.For those who want practical lessons, herein you will find two categories:How to go about a family photo archive project (or at least how I did it) andPractical suggestions for taking photos that your family will treasure long after youâ€™re gone.This was an oddly spiritual process. We take pictures of the moments we think are valuable or important. So, in the photos he took, I saw my fatherâ€™s dreams, the things he thought were beautiful, his moments of pride. And in so doing, I gained more understanding of who my parents were. â€¦but Iâ€™ll leave that essay to another time.I have spoken with several people who have similar family photo archives, so let me begin by describing I went about the project.Before I began, I had an inexpensive Wolverine slide scanner but I knew a manual unit would not cut it. I bought a heavy-duty slide scanner to help me process the images. Itâ€™s a Canon CanoScan 9000F. I like it, in case youâ€™re shopping for an affordable unit; in particular, I do not loathe the built-in software, which sets it apart from other scanners Iâ€™ve used.The project, which took me about a year, became a background process. I could scan a box of slides while I was reading my daily morning e-mail, then clean up and share the images during moments of down-time (such as waiting for poky websites to load site statistics). Over a weekend I usually could get through five or six boxes of slides.Scanning a box of slides had several steps, each of which became a kind of emotional triage:I held up a slide (in front of a desk lamp) to identify it generally and decide if it was worth scanning. In other words: Do I care about this at all? Something out-of-focus easily could be thrown away. A picture of people I didnâ€™t care about (e.g. someone my folks met on a bus tour and never spoke with again) could be dumped, too. It soon became obvious that I didnâ€™t need to scan tourist photos; there are just-so-many pictures you need to see of the Tower of London (which looks the same today as it did in 1972 when my parents visited) or random sunsets over random mountains.If the slide looked interesting, I did a fast preview scan. For instance, if my father took three pictures â€œjust to be sureâ€ I could choose the best image; I could throw out the ones where my brother had his eyes closed. And I could eliminate the pictures that were inherently uninteresting, by which I mean it brought me no sense of nostalgia.By the time I scanned an image, I was pretty committed to keeping it and sharing it with my siblings. Sometimes, if an image was entertaining or meaningful, Iâ€™d share it among my friends on Facebook.From a box of 24â€“36 slides, I usually shared about 8 with my brother and sisters. By the end of the project, Iâ€™d shared 2,800 images with my siblings, and a few hundred on Facebook.I used iPhoto to clean up the images and sort them into a dedicated folder. While tools like PhotoShop certainly could do a better job (and were trotted out for a few special images), 98% were treated with iPhotoâ€™s crop, straighten, and the â€œEnhanceâ€ button. I also added dates and locations to the imagesâ€™ metadata.To share the images with my family, I uploaded photos to Flickr. Other photo sharing sites have far better user interfaces, but Flickr has two advantages: I can limit sharing to a set of people marked as friends-and-family, and viewers can comment on the images. Plus you can search images, if youâ€™re smart enough to add tags as you go. (Do.)Towards the end I also created private Facebook groups, which let me share with cousins as well as siblings, though its search capabilities are poor. Itâ€™s been useful for sharing those videos, though, and for encouraging conversations among my relatives.For general sharing online, I created an Old Family Photos album on Facebook. iPhoto makes it easy to share to an album (though, alas, not to a private group). Iâ€™ve been astonished by how many of my familyâ€™s history touches a chord. Donâ€™t be shy; but do keep your familyâ€™s privacy sensitivities in mind when you share.The earliest roll of slides was from my parentsâ€™ engagement party circa 1941, followed by their honeymoon snapshots in 1942. Thousands of slides record their lives all the way through the 1980s, with a Family Circle gathering held only two months before my fatherâ€™s death.Inside the big boxes were two shoeboxes with a hundred 8mm video movies, which went back to the 1920s but mainly record 1950s camping trips. (iMemories did a very good job at digitizing those.)Most images are from family vacations and special occasions, rather than â€œdaily life.â€ Earlier vacations (1950s and 1960s) are mostly camping trips; later pictures are from trips to Europe, particularly when money eased up after â€œthe kids left home.â€But more is visible than the campsites and Boy Scout trips. I saw a young coupleâ€™s struggles to cope with three young children (I was an afterthought); I watched their idealism diminish and exhaustion set in.I threw away many thousands of pictures. Some of them undoubtedly had meaning to my parents, but nobody alive cares about those photos. Yet I also came across special moments â€” and none of us need to have â€œbeen thereâ€ to appreciate them.In reviewing thousands of slides, I learned quite a bit about taking photos that capture a familyâ€™s life. Perhaps these lessons can help you, too, in considering which images to snap â€” on vacation or in daily life.Those â€œtitle slidesâ€ are meaningful after all. I remember rolling my eyes whenever my father would station me in front of a road sign or National Park entrance. Such pictures seemed really lame.As I reviewed the pictures, though, the title slides were priceless. In all those years, my parents went to dozens of beaches, gardens, and campsites in random mountain ranges. Other than the date on the slide (â€œSep 83â€) I have no way to identify which one it is. (Occasionally, thereâ€™s a scribbled note, like, â€œExplorer Tripâ€ or â€œLondon.â€ Um, thanks, Pop.)So I was always glad when I found a photo of us kids standing next to a â€œMystic Seaportâ€ sign or â€œUnderground toursâ€ (always looking put-upon and sullen, because we were told to â€œStand up straight! And smile â€” it might turn out good!â€).Labels matter. Even a few words helped me know when-and-where something happened: â€œ1955 Nova Scotiaâ€ or my grandfatherâ€™s name. One of the saddest experiences was looking at a family-gathering photo from the 50s with several people in it, and having no idea whoâ€™s in it. (Is that my great-aunt? Maybe my sister remembers? â€¦and too often she didnâ€™t.) If you inherit the photos, take the time to identify the people in them. Even if itâ€™s obvious to you that the picture is of cousin Janet who died in 1943, you canâ€™t assume that the next viewer will know.: Do take pictures that give the viewer a clue of where you are, and with whom.Kodak picture spots arenâ€™t memorable. Destination pictures surely remind the travelers of their experience. Iâ€™m sure that that picture of the beach in Portugal would have encouraged my father to say, â€œThelma, remember that night?â€ Thatâ€™s fine, for the people who participated. Iâ€™ve taken thousands of such photos myself.But if I wasnâ€™t there, the image brings me no nostalgia.The worst of these pictures are the touristy photos. My father took plenty of pictures of the Eiffel Tower on their trip in the 70s. But the tower doesnâ€™t look any different today, so I didnâ€™t bother to scan those photos. In fact, I dumped boxes without even looking at the contents, because thereâ€™s nothing in that experience that speaks to anyone but the participants.: Itâ€™s fine to take pictures that capture a moment for those who were present. But if  could have taken that photo, donâ€™t expect anyone to care.People pictures matter the most. Especially the non-staged ones. The formal pictures of special occasions, where we kids are lined up like weâ€™re in front of a firing squad, are not the ones that bind us.The best family photos are the ones where weâ€™re clowning around and laughing, or where weâ€™re doing something together, or a moment captured without the subject realizing it. The most precious are those where the family is putting up a pup tent, or using the water pump, or packing the car for a trip.In general, try to capture your family when they are actively doing something, ideally an entire process. Let it be a photo essay: â€œMom making Thanksgiving dinnerâ€ or â€œDaddy taking the kids to the petting zoo.â€ Donâ€™t choose only the â€œrevealâ€ moments such as Mom presenting the turkey to the table; include a picture of her hurriedly putting on lipstick before Grandpa arrives, or the kids conked out, asleep in the back seat, on the car trip home.A few exceptions: Nobody looks attractive or interesting while heâ€™s swimming. Few people look great sitting on a towel on the beach, wearing a bathing cap. Also donâ€™t take pictures of people eating dinner, even at a fancy dinner. And while itâ€™s no longer relevant, it was never a good idea to photograph exhausted travelers arriving at an airport gate.Include the photographer. I have few pictures of my father, because he was always the guy behind the camera. When he did ask someone to take a picture it was always posed, such as â€œMom and Pop standing in front of the Grand Canyon.â€: Photos that capture you â€œbeing thereâ€ â€” which means most selfies â€” rarely have meaning.  matters far more.Take photos of daily life. Iâ€™m stunned by the pictures my father  take. There isnâ€™t a single photo that represents what my parents did for a living. They werenâ€™t the type to attend company picnics, fine. But I found nothing indicating â€œtake your daughter to workâ€ or â€œMom typing up a reportâ€ or â€œthe building I worked inâ€ or â€œthe woman Mom commuted to work with for 10 years.â€ That would be more understandable if my parents disliked their jobs, but both of them were passionate about their careers.Take photos of people at rest. Even though I spent much of my childhood writing letters, there is only one photo of me with a pen in my hand â€” and that was taken by a friend at summer camp. Yet my friends and family all recall me with a book or pen within reach. My father never captured that essential part of who I was.Some of the absences may reflect their superstitions. There are zero photos of any woman who is visibly pregnant. Maybe that was considered bad luck; I donâ€™t know.: Donâ€™t limit photo-taking to special occasions.Take at least a short class in photography basics. Or read a basic book on the topic. As much as my father loved photography, he never got any kind of formal training. I spend a lot of editing time cropping images to take advantage of the simplest rule-of-thirds, for instance.Even if you arenâ€™t devoted to photography that much: Crop photos closely. My father took a lot of photos of â€œMom in front of a pretty vistaâ€ but in the long run I care more about Momâ€™s expression than the expanse of mountains in the background. Thanks to iPhoto I can zoom in, but a lot of detail is lost.: Take the best quality photos you can. Your grandchildren will appreciate it.]]></content:encoded></item><item><title>Kansas tuberculosis outbreak is America&apos;s largest recorded since the 1950s</title><link>https://www.cjonline.com/story/news/politics/government/2025/01/24/kansas-tuberculosis-outbreak-is-largest-in-recorded-history-in-u-s/77881467007/</link><author>toastedwedge</author><category>dev</category><pubDate>Sun, 26 Jan 2025 23:03:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[An ongoing tuberculosis outbreak in Kansas has become the largest in recorded history in the United States."Currently, Kansas has the largest outbreak that they've ever had in history," Ashley Goss, a deputy secretary at the Kansas Department of Health and Environment, told the Senate Public Health and Welfare Committee on Tuesday.As of Jan. 17, public health officials reported that they had documented 66 active cases and 79 latent infections in the Kansas City, Kansas, metro area since 2024. Most of the cases have been in Wyandotte County, with a handful in Johnson County.Jill Bronaugh, a KDHE spokesperson, confirmed Goss's statement afterward."The current KCK Metro TB outbreak is the largest documented outbreak in U.S. history, presently," Bronaugh said in a statement to The Capital-Journal. "This is mainly due to the rapid number of cases in the short amount of time. This outbreak is still ongoing, which means that there could be more cases. There are a few other states that currently have large outbreaks that are also ongoing."She noted that the Centers for Disease Control and Prevention started monitoring and reporting tuberculosis cases in the U.S. in the 1950s.Tuberculosis is caused by a bacterium that typically affects the lungs, according to KDHE. People with an active infection feel sick and can spread it to others, while people with a latent infection don't feel sick and can't spread it. Tuberculosis is spread person-to-person through the air when a person with an active infection coughs, speaks or sings. It is treatable with antibiotics.State public health officials say there is "very low risk to the general public.""Some of you are aware, we have and still have mobilized staff and resources addressing an unprecedented tuberculosis outbreak in one of our counties," Goss told lawmakers. "We are working collaboratively with CDC on that. CDC remains on the ground with us to support. That's not a negative. This is normal when there's something unprecedented or a large outbreak of any kind, they will come and lend resources to us to help get a stop to that. We are trending in the right direction right now."Goss said that when KDHE got involved with the Kansas City outbreak last summer, there were 65 active cases and roughly the same number of latent cases. She said the number is now down to about 32 active cases.For active patients, after 10 days of taking medications and having three sputum tests, they will generally no longer be able to transmit tuberculosis."They're no longer contagious," Goss said. "They can go about their lives, they don't have to stay away from people, and they can go back to work, do the things, as long as they continue to take their meds."The course of treatment is several months long for active and latent cases."We still have a couple of fairly large employers that are involved that we're working with on this," Goss said. "So we do expect to find more, but we're hoping the more that we find is latent TB not active, so that their lives are not disrupted and having to stay home from work. Because it is highly contagious."(This story was updated because an earlier version included an inaccuracy.)Jason Alatidd is a Statehouse reporter for The Topeka Capital-Journal. He can be reached by email at jalatidd@gannett.com. Follow him on XÂ .]]></content:encoded></item><item><title>k3s pods networking</title><link>https://www.reddit.com/r/kubernetes/comments/1iaszwq/k3s_pods_networking/</link><author>/u/crewman4</author><category>dev</category><pubDate>Sun, 26 Jan 2025 22:54:02 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[im not used to "onprem" k8s and am testing setting up an k3s in my homelab and i cant get it to work. ive been testing this on debian server and whatever i do, fresh installs and such, i cant enter a pod and wget an external internet site. all sites point to some IP (213.163.146.142:443)Non-authoritative answer:i can resolve dns , but thats hosted internally. everything else works from debian server and no firewalls active. ive been chatGPTing for hours but im stuck. ive rolled new servers and tested everything :P]]></content:encoded></item><item><title>MoxyProxy, an Atreugo(fasthttp) proxy with web interface</title><link>https://www.reddit.com/r/golang/comments/1iasbf8/moxyproxy_an_atreugofasthttp_proxy_with_web/</link><author>/u/ShotgunPayDay</author><category>dev</category><pubDate>Sun, 26 Jan 2025 22:23:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I wanted to make a proxy that was as easy as possible to configure with functionality that I've always wanted. Here is the list of features from the README:Built with Atreugo (fasthttp).Simple Web Interface (html/template, HTMX, Surreal, BulmaCSS), hijacks /moxyproxy routeACME autocert using TLS-ALPN-01. Activates upon setting DomainName and restarting application.Automatically upgrade http:// to https:// when DomainName is set.Wireguard Server automatic update/restart upon peer changes.Wireguard Peer config generation through HTTPS GET /moxyproxy/wg with dedicated Token and new Peer Name.Serve static assets from /moxyproxy/public and /moxyproxy/private (OAuth2 protected)Built in OAuth2 to block non-logged in users and send JSON user data to upstream servers using "moxyuser" header.User data is not stored in the proxy and is instead sent to client in compressed ZSTD then encrypted AES256/GCM cookie.Built in RateLimiter that will return 429 Too Many Requests on any request.Automatic IP banning (403 Forbidden) on 4xx Response which counts against the IPs LimiterBanCount for the day this includes responses from upstream servers: 401 Unauthorized 5x penalty400 Any other 400 1x penaltyMinimal configuration needed to get started.It's still a work in progress and currently being used for my website and services.Not production ready and there will probably be breaking changes. I still need to figure out a testing strategy and benchmarking to do fine tuning. I'd like to test HTTPS to HTTP with wireguard with a net/http server serving HTML which means using VPSs to test the round trip without it breaking the bank. Synthetic tests aren't going to show any real world performance.Screenshots are in the README.]]></content:encoded></item><item><title>Show HN: DeepSeek My User Agent</title><link>https://www.jasonthorsness.com/20</link><author>jasonthorsness</author><category>dev</category><pubDate>Sun, 26 Jan 2025 22:03:39 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[DeepSeek R1 is a new model and service that exposes
chain-of-thought to the user. You can use it live for free at
chat.deepseek.com, or via an API at
platform.deepseek.com that is currently significantly less
expensive than OpenAI o1. OR, simply click  to see what the model thinks about your user
agent, browser capabilities, and IP location headers. If you dare.]]></content:encoded></item><item><title>Learning Rust is like running a marathon â€” you need cardio!</title><link>https://www.reddit.com/r/rust/comments/1iaqh5i/learning_rust_is_like_running_a_marathon_you_need/</link><author>/u/orionwambert</author><category>dev</category><pubDate>Sun, 26 Jan 2025 21:09:23 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Iâ€™ve started learning Rust, and I have to admit, itâ€™s a serious challenge! After years of coding in more "conventional" languages (started with Java 7, then moved to JS/TS, Python, Dartâ€¦), I thought I was ready for anything. But Rust? Itâ€™s a whole different ball game! Between memory management, the heap, the stack, borrowing, ownership, and all these concepts that feel out of the ordinary, Iâ€™m feeling a bit overwhelmed. This is my second attempt to dive into it seriously, and I have to say, itâ€™s not as "friendly" as what Iâ€™m used to.Do any seasoned Rustaceans have tips to help me keep my head above water? Any resources, tricks, or even personal experiences to help me tame this beast?Iâ€™m determined to crack the Rust code, but a little boost would be much appreciated! ]]></content:encoded></item><item><title>Astronomers delete asteroid because it turned out to be Tesla Roadster</title><link>https://www.astronomy.com/science/astronomers-just-deleted-an-asteroid-because-it-turned-out-to-be-elon-musks-tesla-roadster/</link><author>geox</author><category>dev</category><pubDate>Sun, 26 Jan 2025 20:59:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[On Jan. 2, the Minor Planet Center at the Harvard-Smithsonian Center for Astrophysics in Cambridge, Massachusetts, announced the discovery of an unusual asteroid, designated 2018 CN41. First identified and submitted by a citizen scientist, the objectâ€™s orbit was notable: It came less than 150,000 miles (240,000 km) from Earth, closer than the orbit of the Moon. That qualified it as a near-Earth object (NEO) â€” one worth monitoring for its potential to someday slam into Earth.But less than 17 hours later, the Minor Planet Center (MPC) issued an editorial notice: It was deleting 2018 CN41 from its records because, it turned out, the object was not an asteroid.To be precise, it was Elon Muskâ€™s Tesla Roadster mounted to a Falcon Heavy upper stage, which boosted into orbit around the Sun on Feb. 6, 2018. The car â€” which had been owned and driven by Musk â€” was a test payload for the Falcon Heavyâ€™s first flight. At the time, it received a great deal of notoriety as the first production car to be flung into space, complete with a suited-up mannequin in the driverâ€™s seat named Starman.The case of mistaken identity was resolved swiftly in a collaboration between professional and amateur astronomers. But some astronomers say it is also emblematic of a growing issue: the lack of transparency from nations and companies operating craft in deep space, beyond the orbits used by most satellites. While objects in lower Earth orbits are tracked by the U.S. Space Force, deeper space remains an unregulated frontier.If left unchecked, astronomers say the growing number of untracked objects could hinder efforts to protect Earth from potentially hazardous asteroids. They could lead to wasted observing effort and â€” if sufficiently numerous â€” even throw off statistical analyses of the threat posted by near-Earth asteroids, said Center for Astrophysics (CfA) astrophysicist Jonathan McDowell in an email to . â€œWorst case, you spend a billion launching a space probe to study an asteroid and only realize itâ€™s not an asteroid when you get there,â€ he said.And it is a problem that is set to worsen as more nations and companies venture to the Moon and beyond.The Minor Planet Center â€” which operates under the auspices of the International Astronomical Union â€” is the globally accepted authority on handling observations and reports of new asteroids, comets, and other small bodies in the solar system. Its responsibilities include identifying, designating, and computing their orbits.It is also no stranger to spacecraft and discarded rocket stages masquerading as asteroids. In the 2000s, NASAâ€™s Wilkinson Microwave Anisotropy Probe (WMAP), stationed in deep space around a million miles (1.5 million kilometers) from Earth, made it multiple times onto the MPCâ€™s Near-Earth Object Confirmation Page (NEOCP), a list of NEOs pending confirmation. And in 2007, the MPC had to retire the asteroid designation 2007 VN84 when the object was discovered to be the Rosetta spacecraft â€” a high-profile European mission then performing a flyby of Earth en route to make the first ever landing on a comet.â€œThis incident, along with previous NEOCP postings of the WMAP spacecraft, highlights the deplorable state of availability of positional information on distant artificial objects,â€ the MPC fumed when it retracted 2007 VN84. â€œA single source for information on all distant artificial objects would be very desirable.â€That central repository has yet to manifest itself. And the rise in space launches coupled with advances in telescope surveys means the MPC is seeing an uptick in reports of artificial objects, said the centerâ€™s director, Matthew Payne, in an email.These include defunct craft and rocket boosters as well as operational space missions. Spacecraft that are swinging by Earth for a gravity assist (like Rosetta) to more distant locales are particularly prone to being misidentified as near-Earth asteroids. So are spacecraft stationed at the L2 Lagrange point of gravitational stability beyond the Moon, like WMAP.Over the course of 2020 through 2022, at least four spacecraft were added to the MPCâ€™s asteroid record books â€” and quickly deleted. They include the European-Japanese BepiColombo mission (in transit to Mercury), NASAâ€™s Lucy mission (headed to the Trojan asteroids in Jupiterâ€™s orbit), the Spektr-RG X-ray observatory at L2, and what is thought to be the Centaur upper rocket stage for the 1966 Surveyor 2 lunar probe.Closer to Earth, spacecraft are monitored and tracked with much more more scrutiny. Satellites in Earth orbit are regulated by national and international agencies, like the U.S. Federal Communications Commission. Companies also routinely publish orbit information for their own satellites, traditionally in a format known as two-line elements (TLEs). These data are collated by the U.S. Space Force, which also performs its own radar tracking observations and issues alerts to operators when two satellites are at risk of colliding so that they can take avoiding actions. Sharing positions and trajectories is generally in companiesâ€™ best interest as it protects their own assets from collisions and helps prevents destructive clouds of debris that could, in a worst-case scenario, render near-Earth space unusable.But the situation is different in deep space, which is filled with a growing fleet of spacecraft at the Moon, in orbit around the Sun, and at associated Lagrange points of gravitational stability. Because of the Tesla Roadsterâ€™s fame, it happens to be included in a database maintained by NASAâ€™s Jet Propulsion Lab called Horizons, which computes orbits for natural bodies in the solar system. But disclosing artificial bodiesâ€™ trajectories in deep space is not a standard industry practice.Deep space is â€œlargely unregulated,â€ McDowell told a special-session audience Jan. 14 at the American Astronomical Societyâ€™s (AAS) winter meeting in National Harbor, Maryland. â€œThereâ€™s no requirement to file some kind of public flight plan, no equivalent of the TLEs or the corporate data that we get for low-orbit satellites.â€McDowell has also been critical of the asteroid mining startup AstroForge, which plans to launch two probes this year, ridesharing on the Intuitive Machines IM-2 and IM-3 missions. The craft will visit a target asteroid, prospecting for valuable platinum group metals that the company hopes to one day mine. But in order to avoid giving competitors a chance to get there first, the company does not intend to disclose which asteroid it is going to. â€œThatâ€™s kind of not OK,â€ said McDowell dryly at the AAS meeting.Last September, the AAS raised the issue of deep-space transparency in a statement led by its Committee for the Protection of Astronomy and the Space Environment (of which McDowell is a member). It called on U.S. space operators â€” government agencies and non-governmental alike â€” to publicly report and update trajectories of deep-space objects. It also urged operators to place those data in a public repository like JPLâ€™s Horizons, echoing the call from the MPC 17 years earlier.AstroForge says it will be transparent about aspects of its target asteroid â€” other than its identity â€” including releasing images of it. The companyâ€™s co-founder and CEO Matt Gialich told  that Astroforge has not yet settled on a target asteroid because â€œas a ride share customer, we donâ€™t control our launch date.â€ He added, â€œJonathan McDowell is someone I respect, and I love the pushback. Itâ€™s what science is built on. I hope that images and information we deliver outweigh the perceived negatives in this case.â€At the time of publication, SpaceX had not responded to a query from .â€˜A rare confluence of factorsâ€™The Tesla Roadster mix-up came as something of a disappointment to the Turkish amateur astronomer, who asked to be identified as â€œG.â€ He hoped he had discovered a near-Earth asteroid, not a used car from 2010 with a few billion miles on it.He identified (the object briefly known as) 2018 CN41 with software he wrote in his spare time to parse through the MPCâ€™s public archive of observations of objects, which anyone can peruse in search of asteroids and other small solar system bodies. His code identified several candidate objects that could be traced through multiple observations from various telescopes around the world. 2018 CN41 was one of them. It had shown up in images taken by the Catalina Sky Survey at Steward Observatory near Tucson, Arizona, and the Pan-STARRS and ATLAS surveys in Hawaii, among others.After G. calculated an orbit to fit the observations, he saw that the object had a very small minimum orbital intersection distance (MOID) from Earth. In other words, its orbit came very close to Earthâ€™s, making it a potential near-Earth object. â€œI was ecstatic and submitted the identificationâ€ to the MPC, he told  in an email. The MPC accepted the submission and notified the astronomical community in what it calls an â€œelectronic circular,â€ a term of art that reflects the long legacy of observational tradition.But after seeing the objectâ€™s trajectory plotted in 3D on the MPCâ€™s website, he began to harbor doubts about its origins. He realized the orbit resembled that of a spacecraft traveling to Mars, using a Hohmanm transfer orbit, with the exception that it slightly overshoots Marsâ€™ orbit. (He credits, only half-jokingly, his time playing the spaceflight simulation video game Kerbal Space Program.)I first went to JPLâ€™s Small Body Database to quickly take a look at the Earth close approach dates and potential Mars close approach dates, to see if I could correlate those to a known interplanetary mission. I failed â€” the Falcon launch had never crossed my mind. I almost concluded it was an actual NEO and stopped looking, but I asked around on the Minor Planet Mailing List just to erase my final doubts. To my surprise, Jonathan McDowell quickly figured out it was the Falcon upper stage. Being slightly embarrassed that I might have caused unnecessary excitement (it WAS quite a low MOID), I quickly went to MPCâ€™s help desk and let them know the NEO I just submitted was a rocket stage.The MPC has multiple checks to flag artificial objects, said Payne, the center director, all of which broke down on the Tesla Roadster. â€œThis case highlights a rare confluence of factors,â€ he said.First, the MPC uses a routine called sat_id, written by Bill Gray and commonly used by the minor-planet community, to see if an observation of an object matches the position of a known satellite on the sky. The database of satellites it checks against is maintained by the research community of both professional and amateur astronomers.Payne noted that when the Tesla Roadster was originally launched in 2018, the community caught it and flagged it as an artificial object, and the MPC â€œcorrectly labeled it as such without assigning a minor planet designation.â€But when subsequent observations were archived by the MPC and later identified by G., sat_id failed to locate the Roadster, said Payne. And the object was not caught upon further review because unlike most satellites, it orbits the Sun and not Earth. In addition, it is an unusual Sun-centric orbit for a spacecraft. Because it was a test flight for the Falcon Heavy, there was no destination in particular; that is why its trajectory originates near Earth but overshoots Marsâ€™ orbit, as G. noted.Payne agreed that a central repository, â€œregularly updated by national and private space agencies, would significantly enhance the identification process.â€ Currently, he said, the MPC is collaborating with JPL on a system to better detect artificial objects that arenâ€™t in Earth orbit and filter them out of the MPCâ€™s observational database.Citizen science remains keyIn one sense, this case shows the scientific process at work. Mistakes are inevitable, but quick corrections mean science is working as it should.It also highlights the crucial role that amateur astronomers play in making discoveries â€” a role they have played for centuries, well before the term â€œcitizen scientistsâ€ came into vogue. â€œTheir involvement significantly improves the overall efficiency of object identification and contributes to the broader mission of the MPC,â€ said Payne.G. is able to see the bright side of what he calls â€œthe Tesla incident.â€â€œIâ€™m still sort of disappointed it wasnâ€™t a NEO, but it was an interesting experience to say the least,â€ he said. â€œAt the very least we managed to filter out some non-minor-planet observations from [the] MPC database.â€G. continues to hunt for small bodies in the solar system on his own and in citizen science projects like Come On! Impacting ASteroids (COIAS). Developed by a team of Japanese astronomers, COIAS allows anyone to scour observations taken by the Subaru Telescope on Maunakea in Hawaii for asteroids, comets, and trans-Neptunian objects and report their measurements to the MPC.Through COIAS, G. has been a co-discoverer of two named asteroids: 697402 Ao and 718492 Quro. The asteroids are named for one of the main characters and the author, respectively, of a slice-of-life manga named  (also adapted as an anime), about two high school friends who join their schoolâ€™s Earth sciences club and dream of discovering an asteroid. G. said that while he didnâ€™t know much about it before, he â€œloved people who were fans of the manga get crazy about it on social media.â€Recently on COIAS, G. came across a small, â€œbarely noticeableâ€ speck of light moving slowly across the sky. According to his measurements, it appears to be a small body in the outer solar system that crosses Neptuneâ€™s orbit. He identified the measurements and submitted them to the MPC. On Jan. 18, he posted about it on X, the social media platform now owned by Musk, noting that the objectâ€™s orbit takes it within half an astronomical unit â€” the average Earth-Sun distance â€” from Neptune. If confirmed, the object would be a member of a dynamically intriguing subset of trans-Neptunian objects, one that has recently been studied for clues to the whereabouts of the theorized Planet Nine.Of course, G. has his sights set on even rarer observational feats. In an email, he wrote: â€œIâ€™m thinking the holy grail could be a beautiful comet, an interstellar visitor, or an alien spacecraft like in [Arthur C.] Clarkeâ€™s book , heh ðŸ™‚ None of that might happen, but that wonâ€™t stop me from dreaming about it.â€œRealistically, at this point in time I will settle for anything thatâ€™s not a car.â€Editorâ€™s note (Jan. 25, 2025): At the request of the amateur astronomer who identified the object, the story has been updated to remove his first initials and last name.]]></content:encoded></item><item><title>Best Way to Collect Traces for Tempo</title><link>https://www.reddit.com/r/kubernetes/comments/1iaq420/best_way_to_collect_traces_for_tempo/</link><author>/u/Sule2626</author><category>dev</category><pubDate>Sun, 26 Jan 2025 20:56:17 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm currently using Prometheus, Grafana, and Loki in my stack, and I'm planning to integrate Tempo for distributed tracing. However, I'm still exploring the best way to collect traces efficiently.I've looked into Jaeger and OpenTelemetry: seems to require a relatively large infrastructure, which feels like overkill for my use case. looks promising, but it overlaps with some functionality I already have covered by Prometheus (metrics) and Loki (logs).Does anyone have recommendations or insights on the most efficient way to implement tracing with Tempo? I'm particularly interested in keeping the setup lightweight and complementary to my existing stack.]]></content:encoded></item><item><title>DeepSeek R1 API First Look: How This Open-Source Model Outperforms OpenAI</title><link>https://www.kaishira.com/2025/01/26/deepseek-r1-api-first-look-how-this-open-source-model-outperforms-openai/</link><author>/u/haberveriyo</author><category>dev</category><pubDate>Sun, 26 Jan 2025 20:13:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Terminal Trove - Discover CLI tools</title><link>https://terminaltrove.com/categories/</link><author>/u/Pollux_Mabuse</author><category>dev</category><pubDate>Sun, 26 Jan 2025 18:36:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[The $HOME of all things in the terminal]]></content:encoded></item><item><title>Has the behaviour of maxUnavailable and maxSurge for RollingUpdates changed since v1.21.9</title><link>https://www.reddit.com/r/kubernetes/comments/1iam71l/has_the_behaviour_of_maxunavailable_and_maxsurge/</link><author>/u/mrnadaara</author><category>dev</category><pubDate>Sun, 26 Jan 2025 18:31:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[We've deployed a new cluster with v1.30.7 and tried to deploy a deployment with a maxSurge of 1 and maxUnavailable with 0. The new pod remains stuck in Pending with the following reasons:0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.Changing maxUnavailable to 1 fixes it but I'm curious as to why it fails with the new version when it worked fine in the old version. It exceeds the replica count when doing a rolling update so it makes sense the pod wouldn't be scheduled until the old one is deleted, but since we've set the maxUnavailable to 0 the old pods are never deleted. This in theory shouldn't have worked in the old version as well. Am I misconstruing things here?]]></content:encoded></item><item><title>[Media] Introducing: yeehaw! A TUI Framework with Batteries Included</title><link>https://www.reddit.com/r/rust/comments/1ialadw/media_introducing_yeehaw_a_tui_framework_with/</link><author>/u/bogz314</author><category>dev</category><pubDate>Sun, 26 Jan 2025 17:58:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenJazz is a free, open-source version of the classic Jazz Jackrabbit games</title><link>https://alister.eu/jazz/oj/about.php</link><author>doener</author><category>dev</category><pubDate>Sun, 26 Jan 2025 17:40:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[OpenJazz is a free, open-source version of the classic Jazz Jackrabbitâ„¢ games.
     OpenJazz can be compiled on a wide range of operating systems, including Windows
     98/Me/XP and Linux.
     To play, you will need the files from one of the original games.
     With the demise of DOS-based operating systems, it became necessary
     to use emulators to play old DOS games.
     Jazz Jackrabbitâ„¢ deserves more - and would benefit greatly from new features.
     OpenJazz was started on the 23rd of August, 2005, by AJ Thomson.
     Academic pressures put the project on hold until late December 2005. The
     source code was released on the 25th, and the first version with a degree of playability
     was released on the 15th of January. Since then, a variety of ports have been released
     by other people.
     In 2009, a multiplayer version was released.]]></content:encoded></item><item><title>How long is a second in JS? | Why Some Are Longer Than Others</title><link>https://docs.timetime.in/blog/how-long-is-a-second-in-js</link><author>/u/iagolast</author><category>dev</category><pubDate>Sun, 26 Jan 2025 17:36:15 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[When you ask the question "How long is a second in JavaScript?", it seems like a straightforward query. However, the answer reveals layers of complexity intertwined with history, science, and the foundations of modern computing. Letâ€™s dive deep to understand how humanity has measured time and how it connects to JavaScriptâ€™s timekeeping.The Early Days of Timekeepingâ€‹Time measurement began with observing natural periodic phenomena. Early civilizations looked to the moon's phases and the apparent movement of the sun to divide time into manageable units. These observations gave rise to the concepts of days, months, and years.As human needs evolved, a more granular division of time became necessary. The ancient Egyptians and Babylonians divided the day into 24 hours, likely influenced by their base-12 numbering system. Later, hours were divided into 60 minutes and minutes into 60 seconds, creating the framework we still use today.While these early methods sufficed for centuries, they lacked precision. By the mid-20th century, advancements in technology demanded a more accurate measurement of time. The invention of the atomic clock in the 1950s marked a revolutionary step. These clocks measure time based on the vibrations of cesium atoms, offering unparalleled precision.As a result, the definition of a second was updated to:"The duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the cesium-133 atom."This definition formed the foundation for what we now call international atomic time (TAI)Universal Time is based on the Earth's rotation. Itâ€™s intuitive and tied to our everyday experience of day and night. However, the Earth's rotation is not perfectly consistent due to various factors such as gravitational interactions and geophysical processes. As a result, we could say that Universal Time is imprecise and variable.Atomic Time, derived from atomic clocks, is based on physical phenomena and is both precise and consistent. It serves as the backbone for modern timekeeping systems.In the 1970s, Coordinated Universal Time (UTC) was introduced to reconcile the discrepancies between Universal Time and Atomic Time. UTC aligns with Universal Time but incorporates adjustments, such as the addition or removal of leap seconds, to compensate for the Earth's irregular rotation.Leap seconds are occasional one-second adjustments added to UTC to keep it in sync with Universal Time. These adjustments ensure that UTC remains within 0.9 seconds of Universal Time. However, they introduce complications for systems that rely on precise time calculations.Hereâ€™s the improved article in English, incorporating the original tables:ECMAScript and POSIX Timeâ€‹This approach treats every day as having exactly  (24 hours), ignoring both leap seconds and astronomical variations in the Earth's rotation. While this simplifies calculations and suffices for most applications, it introduces limitations in scenarios where precise timekeeping is critical.Although POSIX and UTC appear similar, . Here are the key differences:UTC includes leap seconds to adjust official time to the Earth's rotation.POSIX does not count leap seconds, meaning that POSIX days always have 86,400 seconds, even when UTC days have 86,401 or 86,399 seconds due to a leap second.Simplified Interoperability:POSIX assumes all seconds are of equal duration. This makes calculating time differences straightforward but causes discrepancies when leap seconds occur.For example, during a leap second (e.g., ), POSIX simply ignores the additional second, making timestamps near the leap second ambiguous or inaccurate.Step Adjustment: The Traditional Solutionâ€‹Historically, systems that needed to account for leap seconds would use a technique called . This method can be described as just ignoring the leap second, causing the clock to jump forward or backward by one second at the exact moment of the leap second.Lets see a table to understand this:As you can see from the both TAI and UTC are strictly increasing, but POSIX (step) has a jump in time at the leap second. This means that 2 different UTC seconds are mapped to the same POSIX second. This can cause issues in some applications. The biggest problem is that some apps can go "back in time" when a leap second is added.Lets say that a payment is requested at , and approved during the next second at . Our logs will show something like:This is a problem because the payment was approved before it was requested. This is why most system use a different approach to handle leap seconds.Smearing: The Practical Solutionâ€‹In many modern systems implementing POSIX time, a technique called  is used to handle leap seconds. Instead of adding or removing a whole second, the effects of the leap second are distributed gradually over a longer period (typically 24 hours). This has several implications: First of all during the smear period, seconds are slightly longer or shorter than a standard second (measured in atomic time). This ensures a smooth transition instead of an abrupt jump in time. On the other hand compared to stepping the clock is strictly increasing, which is a requirement for many applications.Example: Smearing and Step Adjustment Tablesâ€‹In this table:  ignores the leap second entirely, treating  as if it doesn't exist while  gradually adjusts the clock, ensuring smoother transitions but at the cost of seconds having slightly different durations.This is how Google's NTP servers handle leap seconds. They smear the leap second over a 24-hour period, ensuring that the clock is always strictly increasing. More information can be found here.Implications for Applicationsâ€‹Why It Usually Doesnâ€™t Matterâ€‹For tasks like scheduling events, calculating ages, or displaying time in user-facing applications, JavaScript (and POSIX) time is precise enough and 99% of the problem will be derived from timezones.For applications requiring high precision, such as those in scientific research or financial systems, the discrepancies caused by leap seconds and smearing can lead to significant errors. Accurate timekeeping is crucial to ensure the reliability and accuracy of measurements and transactions. During a leap second, a single POSIX timestamp can refer to two different moments in UTC, creating ambiguities and potential errors in systems that demand precise time synchronization.]]></content:encoded></item><item><title>New Michigan Law Requires High Schools to Offer CS Classes</title><link>https://news.slashdot.org/story/25/01/26/1547204/new-michigan-law-requires-high-schools-to-offer-cs-classes?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><pubDate>Sun, 26 Jan 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[The state of Michigan will now require each public high school in the state to offer at least one computer science course to its students. "This bill aligns Michigan with a majority of the country," according to the state's announcement, which says the bill "advances technological literacy" and ensures their students "are well-equipped with the critical thinking skills necessary for success in the workforce." 
Slashdot reader theodp writes:



From the Michigan House Fiscal Agency Analysis: "Supporters of the bill say that increasing access to computer science courses for students in schools should be a priority of the state in order to ensure that students can compete for the types of jobs that have good pay and will be needed in the coming decades." 
That analysis goes on to report that testifying in favor of the bill were tech-giant backed nonprofit Code.org (Microsoft is a $30 million Code.org donor), Amazon and AWS (Amazon is a $30+ million Code.org donor), the tech-supported Computer Science Teachers Association (CSTA), and the lobbying organization TechNet, whose members include Amazon, Apple, Google, Meta, and OpenAI). 
It's not clear how many high schools in Michigan are already teaching CS courses, but this still raises a popular question for discussion. Should high schools be required to teach at least one CS course?]]></content:encoded></item><item><title>Improve Rust Compile Time by 108X</title><link>https://burn.dev/blog/improve-rust-compile-time-by-108x</link><author>/u/Handsome_AndGentle</author><category>dev</category><pubDate>Sun, 26 Jan 2025 17:28:48 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
Before you get too excited, the techniques used to reduce compilation
          time are not applicable to all Rust projects. However, I expect the
          learnings to be useful for any Rust developer who wants to improve
          their project's compilation time. Now that this is clarified, let's
          dive into the results.

We started with a compilation time of 108 seconds for the matmul
          benchmarks, which was reduced to only 1 second after all the
          optimizations. The most effective optimization was the element-type
          generics swap, where we instantiated generic functions with predefined
          "faked" element types to reduce the amount of LLVM code generated. The second optimization also had a major impact, further
          reducing the compilation time by nearly 3Ã—. This was achieved by using
          our comptime system instead of associated const generics to represent the
          matmul instruction sizes. Finally, the last optimizationâ€”also the simplestâ€”was
          to reduce the LLVM optimization level to zero, which is particularly useful
          for debug builds, such as tests.

Compilation times are measured using incremental compilation.

First, let me explain the situation that led me to investigate our
          compile time. During the last iteration of CubeCL, we refactored our matrix multiplication GPU kernel to work with
          many different configurations and element types. CubeCL is a dialect
          that lets you program GPUs for high-performance computing applications
          directly in Rust. The project supports multiple compiler backends,
          namely WebGPU, CUDA, ROCm, and Vulkan with more to come.

The refactoring of the matrix multiplication kernel was done to
          improve tensor cores utilization across many different precisions. In
          fact, each kernel instance works across 3 different element types: the
          global memory element type, the staging element type, and the
          accumulation element type. These are all different since we normally
          want to accumulate in a higher precision element type, as this is
          where numerical approximation is most sensitive. Also, the tensor
          cores don't work across all input precisions; if you have f32 inputs,
          you need to convert those to tf32 element types (staging) to call the
          tensor cores instructions. To add to the complexity, tensor cores
          instructions only work across fixed matrix shapes that also depend on
          the precisions. For instance, f16 staging matrices work across all
          shapes from (32, 8, 16), (16, 16, 16), (8, 32, 16). But tf32 only
          works on (16, 16, 8).

In our first refactoring, we represented the shape of the matrices
          supported by the instructions using const associated types, since this
          is the abstraction component that makes the most sense in this case.
          For the element types, we naturally used generic arguments for traits
          and functions - pretty much what any developer would do in this
          scenario. However, with all the possible combinations, we ended up
          with a compilation time of 1m48s using the cache.

Yes, you read that right: 1m48s just to rebuild the matmul benchmark
          if we change anything in the bench file.

For the purpose of this optimization, we only consider incremental
          compilation using cargo caching, since this is the most important one
          to speed up dev iteration time. Changing one configuration to test if
          an optimization worked took almost 2 minutes just to create the binary
          to execute a few matmuls.

Well, we need to understand that the Rust compiler is actually very
          fast. The slow parts are the linking and LLVM. The best way to improve
          compilation time is to reduce the amount of LLVM IR generated.

In our specific case, each combination of the matmul would generate a
          whole new function - this is what zero-cost abstraction means. There
          is no dynamic dispatch; every type change duplicates the code to
          improve performance at the cost of a bigger binary. Before all of our
          optimizations, the binary generated was 29M, and after we reduced it
          to 2.5M - a huge difference.

To reduce the amount of code generated, we had to use different Rust
          techniques to make our abstractions for the matmul components. In our
          case, we don't need zero-cost abstractions, since the code written in
          Rust for the matmul components actually generates the code that is
          used to compile at runtime a kernel that will be executed on the GPU.
          Only the GPU code needs to be fast; the JIT Rust code takes almost no
          time during runtime. Zero-cost abstraction would actually be optimal
          in a setting where we would perform ahead-of-time compilation of
          kernels.

Ever wonder why LibTorch or
          cuBLAS have executables that are
          GIGABYTES in size? Well, it's because all kernels for all precisions with
          all edge cases must be compiled to speed up runtime execution. This is
          necessary in a compute-heavy workload like deep learning.

However, CubeCL is different - it performs JIT compilation, therefore
          we don't need to compile all possible variations ahead of time before
          creating the binary: we can use dynamic abstractions instead! This is
          one of the two optimizations that we made for the matmul components.
          Instead of relying on const associated types, we leveraged the
          comptime system to dynamically have access to the instruction sizes
          during the compilation of a kernel at runtime. This is actually the
          second optimization that we made and helped us go from 14s compilation
          time to around 5s.

However, the biggest optimization was quite hard to pull off and is
          linked to the generic element types passed in each function. We still
          wanted to use zero-cost abstraction in this case, since passing around
          an enum listing what element type operations are on would be terrible
          in terms of developer experience. However, the hint to improve our
          compilation time was that when you write a function that will execute
          on the GPU, we have an attribute on top .

We want the code to look and feel like normal Rust, but the macro
          actually parses the Rust code written and generates another function,
          which we normally call the expand function, where the actual GPU IR is
          built for the function. That code will actually run during runtime,
          not the code that the user is writing. The element types generics are
          only used to convert the generic element type into the enum IR element
          type. In the expand functions, we also pass a context where all IR is
          tracked.

So the optimization was to pass a fake generic element type, called  instead of the actual element type like . When
          compiling a kernel, we first register the real element type in the
          context, using the const generic item to differentiate multiple
          element types if a function has multiple generic element types. Since
          we always call the expand function with the exact same generic items
          for all element types, we only generate one instance of that function,
          and the element types are fetched at runtime using the context.

The most tedious part was actually implementing that optimization
          while trying not to break our components. The biggest problem caused
          by that optimization is that we can't support generic dependencies
          between traits over the element type in launch functions of CubeCL.

It makes sense though - we don't want to recompile all the instruction
          types for all different precisions. Since our optimization is
          activated at the boundaries of CPU code and GPU code, where cube
          functions are identified as launchable, we need the generic trait to
          not have a dependency on the element type. They are going to be
          switched by our macro. We use generic associated types instead of
          traits with generic element types.

This is known as the family pattern, where a trait is describing a family of types.

Using this pattern, we can inject the family type at the boundaries of
          CPU and GPU code and instantiate the inner instruction type with the
          expand element type.

Migrating most of the components to the new pattern, we reduced
          compile time from 1m48s to about 14s.

It was a lot of work, and I don't expect all projects to face cases
          like this, but it was worth it! Now waiting for about 5 seconds after
          trying something in the code to see if performance is improved doesn't
          break the flow, but almost 2 minutes did.

We essentially leveraged the fact that CubeCL is a JIT compiler and
          not an AOT compiler, which is very appropriate for throughput-focused
          high-performance applications.
Playing with LLVM optimization settings
Since our benchmarks are compiled with optimization level set to 3, we
          could still improve the compilation time further to about 1s by
          reducing the optimization level to zero. Another 5X speedup that we
          can have by simply adjusting the LLVM optimization level.

We decided not to keep that optimization in production, since we want
          the benchmarks to have the same LLVM optimization level as user
          applications. However, we activated it for testing, since we often
          rerun tests to ensure we don't break correctness when implementing or
          optimizing kernels.
Not a Rust Compiler Issue
All of our optimizations actually created tons of code - we used proc
          macros, associated type generics, const generics, and tons of complex
          features from the Rust type system.

The Rust compiler is actually very fast; the slow part is really the
          linking and optimizing of the LLVM IR. If there's one thing to take
          from this post, it's that you shouldn't worry about using complex
          features of Rust, but make sure you don't generate huge binaries.
          Reducing the binary size will improve compile time even if you use
          complex methods to do so! "Less code compiles faster" is not exactly
          right. "Less generated code compiles faster" is what we have to keep
          in mind!
]]></content:encoded></item><item><title>Qwen2.5-1M: Deploy your own Qwen with context length up to 1M tokens</title><link>https://qwenlm.github.io/blog/qwen2.5-1m/</link><author>meetpateltech</author><category>dev</category><pubDate>Sun, 26 Jan 2025 17:24:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Hard numbers in the Wayland vs. X11 input latency discussion</title><link>https://mort.coffee/home/wayland-input-latency/</link><author>todsacerdoti</author><category>dev</category><pubDate>Sun, 26 Jan 2025 16:57:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Yesterday, I read this blog post:
Wayland Cursor Lag: Just give me a break already...,
where a Linux user discusses their frustration with input latency under Wayland.
They detailed their own subjective experience of the problem,
but failed to produce hard evidence.Even though I'm a very happy user of wayland overall,
I share the blog post's author's subjective impression that there's more
cursor lag under Wayland than under X11.
In my opinion, their experiment was limited by their phone camera's 90 FPS,
which really doesn't feel like it's enough to get conclusive numbers
when we're talking about differences which are probably on the order of
single screen refresh cycles.So I thought: hey, I have a phone with a 240 FPS camera mode,
I bet that's enough to get some conclusive results!I simply pointed my phone's camera at the screen and desk using my left hand,
made sure to get the mouse cursor, the mouse and my right hand in frame,
and recorded myself repeatedly flicking the mouse with my finger.
I recorded myself flicking it 16 times under Wayland,
logged out of the GNOME Wayland session and into a GNOME X11 session,
then did the same there.I then converted the two resulting video files into a series of JPEGs
using ffmpeg (ffmpeg -i <input file> %04d.jpg),
and counted from the first frame where I could see the mouse move
until the first frame where I could clearly see that the cursor had moved.I elected to include the start and end frame;
so if I saw the mouse move in frame 1045,
then I saw the cursor move in frame 1047,
I would count that as 3 frames.Here's an example which depicts a latency of 3 frames (requires JavaScript):Here's a frame from before the mouse has begun moving.The mouse still hasn't moved.Here, the mouse has just about started moving,
so I consider this "frame 1".
The cursor hasn't moved yet.The mouse moves a bit further, but the cursor still hasn't moved.
This is "frame 2".This is the frame where the cursor starts moving.
This is "frame 3", so I would note down this sequence
as taking 3 frames.Distro: Fedora Workstation 41GPU: AMD Radeon RX 7900XTMonitor: Gigabyte M32U (4k IPS @ 144.99, no DPI scaling)Mouse: Logitech G502 LightspeedCamera: iPhone 15 Pro, slo-mo 240 FPSThe main limitations of this experiment are:240 FPS still isn't  much. With my 144Hz screen, I have less than
two camera frames per screen refresh. This introduces some random variance.Pixels don't switch instantly, so there are ambiguous frames where the cursor
is  starting to become visible in its new location.
I decided to count the cursor as "having moved" when there is a clearly visible
cursor in a new location on the screen, even if the pixels haven't fully lit up.For some reason, the video recording from my phone contains some duplicate frames.
I don't know why this happens. I decided to interpret these duplicate frames
as a representation of a frame's worth of time passing, so I counted them as normal.All these factors introduce some uncertainty in the results.
However, they  affect Wayland and X11 equally, so with enough data,
it should all even out. Another caveat I should clearly point out is that there are
many other Wayland compositors out there than GNOME's, and I have not tested them.
For that matter, there are other GPU drivers out there than AMD's.
Other compositors and other GPU drivers may show different results.Here's the data I captured:Wayland, on average, has roughly 6.5ms more cursor latency than X11 on my system.
I don't have the statistics expertise necessary to properly analyze
whether this difference is statistically significant or not,
but to my untrained eye, it looks like there's a clear and consistent difference.Interestingly, the difference is very close to 1 full screen refresh.
I don't know whether or not that's a coincidence.Here are the numbers in chart form:In my mind, these results are conclusive proof that there  a difference
in input latency between X11 and Wayland, at least with my hardware,
and that the difference is large enough that it's plausible for some people to notice.Further testing on more varied hardware and refresh rates is necessary
to get a clear picture of how wide-spread the problem is and how large it is.
It's likely that the magnitude of the difference varies based on factors
such as which compositor is used and what the refresh rate of the screen is.I probably won't undertake that further testing,
because this is all very time intensive work.
My goal was only to see if I could conclusively measure  difference. I want to add a note here about what this testing does  show.
It does not show that there's higher input latency 
in Wayland compared to X11 in a way which affects, for example, games.
It is possible that this added latency is entirely cursor-specific
and that Wayland and X11 exhibit the exact same input latency
in graphical applications and games.
It is my understanding that the cursor is handled very differently from
normal graphical applications.
Further testing would be necessary to show whether Wayland has more input latency
 than X11.]]></content:encoded></item><item><title>Hard numbers in the Wayland vs X11 input latency discussion</title><link>https://mort.coffee/home/wayland-input-latency/</link><author>/u/mort96</author><category>dev</category><pubDate>Sun, 26 Jan 2025 16:44:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Yesterday, I read this blog post:
Wayland Cursor Lag: Just give me a break already...,
where a Linux user discusses their frustration with input latency under Wayland.
They detailed their own subjective experience of the problem,
but failed to produce hard evidence.Even though I'm a very happy user of wayland overall,
I share the blog post's author's subjective impression that there's more
cursor lag under Wayland than under X11.
In my opinion, their experiment was limited by their phone camera's 90 FPS,
which really doesn't feel like it's enough to get conclusive numbers
when we're talking about differences which are probably on the order of
single screen refresh cycles.So I thought: hey, I have a phone with a 240 FPS camera mode,
I bet that's enough to get some conclusive results!I simply pointed my phone's camera at the screen and desk using my left hand,
made sure to get the mouse cursor, the mouse and my right hand in frame,
and recorded myself repeatedly flicking the mouse with my finger.
I recorded myself flicking it 16 times under Wayland,
logged out of the GNOME Wayland session and into a GNOME X11 session,
then did the same there.I then converted the two resulting video files into a series of JPEGs
using ffmpeg (ffmpeg -i <input file> %04d.jpg),
and counted from the first frame where I could see the mouse move
until the first frame where I could clearly see that the cursor had moved.I elected to include the start and end frame;
so if I saw the mouse move in frame 1045,
then I saw the cursor move in frame 1047,
I would count that as 3 frames.Here's an example which depicts a latency of 3 frames (requires JavaScript):Here's a frame from before the mouse has begun moving.The mouse still hasn't moved.Here, the mouse has just about started moving,
so I consider this "frame 1".
The cursor hasn't moved yet.The mouse moves a bit further, but the cursor still hasn't moved.
This is "frame 2".This is the frame where the cursor starts moving.
This is "frame 3", so I would note down this sequence
as taking 3 frames.Distro: Fedora Workstation 41GPU: AMD Radeon RX 7900XTMonitor: Gigabyte M32U (4k IPS @ 144.99, no DPI scaling)Mouse: Logitech G502 LightspeedCamera: iPhone 15 Pro, slo-mo 240 FPSThe main limitations of this experiment are:240 FPS still isn't  much. With my 144Hz screen, I have less than
two camera frames per screen refresh. This introduces some random variance.Pixels don't switch instantly, so there are ambiguous frames where the cursor
is  starting to become visible in its new location.
I decided to count the cursor as "having moved" when there is a clearly visible
cursor in a new location on the screen, even if the pixels haven't fully lit up.For some reason, the video recording from my phone contains some duplicate frames.
I don't know why this happens. I decided to interpret these duplicate frames
as a representation of a frame's worth of time passing, so I counted them as normal.All these factors introduce some uncertainty in the results.
However, they  affect Wayland and X11 equally, so with enough data,
it should all even out. Another caveat I should clearly point out is that there are
many other Wayland compositors out there than GNOME's, and I have not tested them.
For that matter, there are other GPU drivers out there than AMD's.
Other compositors and other GPU drivers may show different results.Here's the data I captured:Wayland, on average, has roughly 6.5ms more cursor latency than X11 on my system.
I don't have the statistics expertise necessary to properly analyze
whether this difference is statistically significant or not,
but to my untrained eye, it looks like there's a clear and consistent difference.Interestingly, the difference is very close to 1 full screen refresh.
I don't know whether or not that's a coincidence.Here are the numbers in chart form:In my mind, these results are conclusive proof that there  a difference
in input latency between X11 and Wayland, at least with my hardware,
and that the difference is large enough that it's plausible for some people to notice.Further testing on more varied hardware and refresh rates is necessary
to get a clear picture of how wide-spread the problem is and how large it is.
It's likely that the magnitude of the difference varies based on factors
such as which compositor is used and what the refresh rate of the screen is.I probably won't undertake that further testing,
because this is all very time intensive work.
My goal was only to see if I could conclusively measure  difference. I want to add a note here about what this testing does  show.
It does not show that there's higher input latency 
in Wayland compared to X11 in a way which affects, for example, games.
It is possible that this added latency is entirely cursor-specific
and that Wayland and X11 exhibit the exact same input latency
in graphical applications and games.
It is my understanding that the cursor is handled very differently from
normal graphical applications.
Further testing would be necessary to show whether Wayland has more input latency
 than X11.]]></content:encoded></item><item><title>Linux 6.14 Brings Some Systems Faster Suspend and Resume</title><link>https://linux.slashdot.org/story/25/01/25/2343225/linux-614-brings-some-systems-faster-suspend-and-resume?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><pubDate>Sun, 26 Jan 2025 16:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Amid the ongoing Linux 6.14 kernel development cycle, Phoronix spotted a pull request for ACPI updates which "will allow for faster suspend and resume cycles on some systems." 

Wikipedia defines ACPI as "an open standard that operating systems can use to discover and configure computer hardware components" for things like power management and putting unused hardware components to sleep. Phoronix reports:

The ACPI change worth highlighting for Linux 6.14 is switching from msleep() to usleep_range() within the acpi_os_sleep() call in the kernel. This reduces spurious sleep time due to timer inaccuracy. Linux ACPI/PM maintainer Rafael Wysocki of Intel who authored this change noted that it could "spectacularly" reduce the duration of system suspend and resume transitions on some systems... 

Rafael explained in the patch making the sleep change: 

 "The extra delay added by msleep() to the sleep time value passed to it can be significant, roughly between 1.5 ns on systems with HZ = 1000 and as much as 15 ms on systems with HZ = 100, which is hardly acceptable, at least for small sleep time values." 
One 2022 bug report complained a Dell XPS 13 using Thunderbolt took "a full 8 seconds to suspend and a full 8 seconds to resume even though no physical devices are connected." In November an Intel engineer posted on the kernel mailing list that the fix gave a Dell XPS 13 a 42% improvement in kernel resume time (from 1943ms to 1127ms).]]></content:encoded></item><item><title>It&apos;s not a crime if we do it with an app</title><link>https://pluralistic.net/2025/01/25/potatotrac/#carbo-loading</link><author>keepit</author><category>dev</category><pubDate>Sun, 26 Jan 2025 15:24:53 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The core regulatory proposition of the tech industry is "it's not a crime if we do it with an app." It's not an unlicensed taxi if we do it with an app. It's not an illegal hotel room if we do it with an app. It's not an unregistered security if we do it with an app. It's not wage theft if we do it with an app.Inflation is one of the most politically salient factors of this decade, and so much of inflation can be attributed to a crime, done with an app, with impunity for the criminals. The entire food supply has been sewn up by cartels of 2-5 giant companies, and they colluded to raise prices, and bragged about it, and got away with it, because neoclassical economists insist that it's impossible for this kind of price fixing to occur in an "efficient market."Some of these cartels are well-known, like the Coke/Pepsi duopoly. Pepsi's bosses boasted to their shareholders about "Pepsi pricing power," and how they were able to raise prices over the inflationary increases caused by covid and the Russian invasion of Ukraine:You might know that pretty much every packaged good in your grocery store is made by one of two companies, Unilever and Procter and Gamble. Both CEOs boasted to their investors about their above-inflation price increases:But other cartels are harder to spot. It may seem like your grocer's eggs department is filled with many different companies' products. In reality, a single company, Cal-Maine Foods, owns practically every brand of eggs in the case: Farmhouse Eggs, Sunups, Sunny Meadow, Egg-Landâ€™s Best and Land Oâ€™ Lakes. They made record profits after the pandemic and through bird flu, a fact that CFO Max Bowman attributed to "significantly higher selling prices" and "our ability to adapt to inflationary market pressures":But Cal-Maine is comparatively transparent. The other food cartels â€“ especially those that serve the restaurant sector â€“ are harder to spot. In , Katya Schwenk describes how four companies â€“ Lamb Weston, JR Simplot, McCain Foods and Cavendish Farms â€“ have captured the frozen potato market and all that comes with it (fries, tater tots, etc):These companies have been hiking prices for years, but  started to turn the screws during the post-covid inflationary period. One of Schwenk's sources is Josh Saltzman, owner of the DC sports bar Ivy and Coney. Ten years ago, Saltzman charged $3 for fries; now it's $6 â€“ and Saltzman's margins have declined. Saltzman has a limited number of suppliers, and they all get their potatoes from Big Potato, and they bundle those potato orders with their other supplies, making it effectively impossible for Saltzman to buy his potatoes from anyone else.Big Potato controls  of the frozen potato market, and any sector that large and concentrated is going to be pretty cozy. The execs at these companies all meet at industry associations, lobbying bodies, and as they job-hop between companies in the cartel. But they don't have to rely on personal connections to rig the price of potatoes: they do it through a third-party data-broker called Potatotrac. Each cartel member sends all their commercially sensitive data â€“ supply costs, pricing, sales figures â€“ to Potatotrac, and then Potatotrac uses that data to give "advice" to the cartel members about "optimal pricing."This is just price-fixing, with an app. The fact that they don't sit around a table and openly discuss pricing doesn't keep this from being price-fixing. What's more, they . A director at McCain said that "higher ups" forbade anyone in the company from competing on price. A Lamb Weston exec described the arrangement as everyone "behaving themselves," chortling that they'd "never seen margins this high in the history of the potato industry." Lamb Weston's CEO attributed a 111% increase in net income to "pricing actions."Lamb Weston's execs understand that they're driving small restaurants out of business, and that the real beneficiaries are big chains that can pass the price increases onto their customers, like "Chiliâ€™s and the Texas Roadhouses and Cheesecake Factory":This is by no means unique to the potato industry. A data-broker called Agri Stats works with America's largest meat-packers to rig the price of meat â€“ packers send Agri Stats the same kind of data that Big Potato sends to Potatotrac, and Agri Stats sends back the same "recommendations" that allow them to raise meat prices across the board, in lockstep:Lots of food categories are as inbred as meat and potatoes: "four firms controlled nearly 80 percent of the almond milk market, for instance. Three companies controlled 83 percent of the canned tuna market, and four companies controlled more than 86 percent of the microwave popcorn market."The "price fixing is legal if we do it with an app" gambit is not just about food, either. Apps like Realpage let big corporate landlords â€“ who've bought up a sizable fraction of all the available homes in America â€“ collude to raise rents:And private equity companies have rolled up all the  companies, hiking the price of trucks, creating backlogs and bottlenecks for parts and service, and starving the nation's municipalities (including Los Angeles) of fire-fighting equipment:This kind of price-fixing was central to the enforcement actions of the Biden administration's trustbusters at the FTC, and their investigations and actions inspired state AGs and private parties to bring their own antitrust suits. The question is, will Trump's enforcers continue this agenda? And will Trump's judges â€“ steeped in Heritage Foundation economics that insists that monopolies are "efficient" â€“ find in their favor if they do?Inflation has lots of causes, it's true. But when an industry is consolidated enough to take advantage of a data brokerage or just engage in tacit collusion,  source of inflation â€“ war, disease, weather â€“ allows whole sectors to raise prices together, and keep them high, long after the shock has passed.Picks and Shovels: a sequel to "Red Team Blues," about the heroic era of the PC, Tor Books, February 2025Unauthorized Bread: a middle-grades graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2025Enshittification: a nonfiction book about platform decay for Farrar, Straus, Giroux. Status: second pass edit underway (readaloud)A Little Brother short story about DIY insulin PLANNINGPicks and Shovels, a Martin Hench noir thriller about the heroic era of the PC. FORTHCOMING TOR BOOKS FEB 2025This work â€“ excluding any serialized fiction â€“ is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net.Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution.Blog (no ads, tracking, or data-collection):Newsletter (no ads, tracking, or data-collection):Mastodon (no ads, tracking, or data-collection):Medium (no ads, paywalled):Twitter (mass-scale, unrestricted, third-party surveillance and advertising):Tumblr (mass-scale, unrestricted, third-party surveillance and advertising):"When life gives you SARS, you make sarsaparilla" -Joey "Accordion Guy" DeVilla]]></content:encoded></item><item><title>Someone has submitted my project on AUR.</title><link>https://aur.archlinux.org/packages/bunster</link><author>/u/yassinebenaid</author><category>dev</category><pubDate>Sun, 26 Jan 2025 15:23:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SQLC - migrating existing queries to a new schema</title><link>https://www.reddit.com/r/golang/comments/1iaglnn/sqlc_migrating_existing_queries_to_a_new_schema/</link><author>/u/Sliczzz</author><category>dev</category><pubDate>Sun, 26 Jan 2025 14:59:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I've been happily using SQLC with Postgresql as an engine to enjoy type safe SQL in a project I've recently been working on. By now the application is quite mature and I have quite some queries (13 tables, 10-15 query files, 50-100 queries).Now my application is evolving and I want to build another module, which needs its own separate schema. I could go all the way and split the application in two and have two databases, but for now a modular monolith with separate schemas will do just fine.The problem I now have is that those queries/tables assume the schema being used is `public`. Now, I'd like to split my queries into 2 schemas (`game` and `lobby`), but from what I understand the only way to achieve this is to migrate every single query (and corresponding golang imports) to the new schema manually.Is there a way to specify the schema context in which the queries are running and being generated without impacting the existing codebase too much (like, having to change all existing imports because the postgres schema changed)?]]></content:encoded></item><item><title>No one is disrupting banks â€“ at least not the big ones</title><link>https://www.popularfintech.com/p/no-one-is-disrupting-banks</link><author>kazanins</author><category>dev</category><pubDate>Sun, 26 Jan 2025 14:05:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[despite almost 30 years of trying, Fintech companies have not disrupted banksthey hardly challenged the key banking model - taking deposits and issuing loansbanks and Fintech companies will need to learn how to co-existon X/TwitterThe money center banks survived the â€œdeposit flightâ€we expect to see a more visible growth trendwe can already see that trend in consumer checking depositsJeremy Barnum, JPMorgan Q4 2024 earnings calla perfect time for Fintech companies to get depositshigh-yield savings accounts to make people move their money from the incumbentsThese customers want to bank with us nowfor how long will Wells Fargo keep paying 4% to Cash App customers?these are the most valuable deposits in the franchiseAlastair Borthwick, Bank of America Q4 2024 earnings callAt the end of Q3 2024, LendingClub had $9.5 billion in deposits, and SoFi had $24.4 billionFintechs havenâ€™t managed to challenge banks in lending. Affirm quickly got to 1.4 million Affirm Card cardholdersRobinhood claims that 2 million people are on the waitlistRobinhood Gold credit card crossed 2 million on the waitlist and is adding roughly 200,000 waitlist sign-ups per monthVlad Tenev, Robinhood 2024 Investor Dayin 2024, Amex opened 13 million new card accounts, Chase opened 10 millionJPMorgan finished 2024 with $233 billion in credit card loansWe expect healthy card loan growth again this year, but below the 12% pace we saw in 2024Jeremy Barnum, JPMorgan Q4 2024 earnings calllast few years were ideal for building a credit card business as loan balances explodeda Fintech perspective on the consumer side, we really have not seen anything. Not that we don't look at it, not that we're not aware of it.Stephen Squeri, American Express Q4 2024 earnings callBanks might be losing (or have already lost?) payment acceptance business to Fintech companies. have you considered about whether you should get rid of this business and deploy the capital to other areas where you're in a much stronger positionAnalystâ€™s question on U.S. Bank Q4 2024 earnings callbe losing the cross-border payments businessRamp and Brex are certainly becoming a force in commercial cardswe keep our eye on Ramp, Brexthey have good products, and they're making some inroadsStephen Squeri, American Express, Q4 2024 earnings callâ€¦but (and thatâ€™s an important but!) big banks have figured out mobile too. JPMorgan Chase reported 58 million active mobile usersthrew billions on catching up with Fintech companiesstill delivering crazy high profitability. we had probably reached peak modernization spendto focus on features and new product developmentJeremy Barnum, JPMorgan Q4 2024 earnings callFintech companies might be disrupting community banks, but was that the ambition? a dozen or so largest banks (with $250+ in assets) generate 60% of the industryâ€™s profitbanks and Fintech companies will need to learn how to co-existThanks for reading Popular Fintech! This post is public so feel free to share it.Disclaimer: Information contained in this newsletter is intended for educational and informational purposes only and should not be considered financial advice. You should do your own research or seek professional advice before making any investment decisions.]]></content:encoded></item><item><title>Break up with Adobe, switch to Linux</title><link>https://youtu.be/lm51xZHZI6g?si=bl-gjEb2KGa2YKii</link><author>/u/BulkyMix6581</author><category>dev</category><pubDate>Sun, 26 Jan 2025 13:47:22 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>sortedmap â€“ sorted map implementation for Go with a heap and iterators under the hood</title><link>https://www.reddit.com/r/golang/comments/1iaeqla/sortedmap_sorted_map_implementation_for_go_with_a/</link><author>/u/egregors</author><category>dev</category><pubDate>Sun, 26 Jan 2025 13:38:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Check this out: a generic map that maintains order (by keys or values). To be honest, I was surprised not to find something like this in the current state of things. Perhaps it solves too specific problems, idk. But anyway, here is my implementation: sortedmap. I believe it could be useful for someone.The constructor expects a custom  function to define the ordering rule. To maintain order, I use a generic heap of key-value pairs with an  insert time complexity. The map access API is mostly inspired by standard library  and .The only thing I donâ€™t really like is the  method. In the current implementation, it requires a full scan of the heap to find the key-value pair to remove. Iâ€™ll probably find a more efficient way to handle this, but for now, in real-world use cases, it doesnâ€™t seem like a major issue.Why do I need a sorted map?Quite simple: I need a recursive category tree that can be edited at runtime and must always remain ordered.]]></content:encoded></item><item><title>Rustâ€™s worst feature* (spoiler: itâ€™s BorrowedBuf, I hate it with passion)</title><link>https://mina86.com/2025/rusts-worst-feature/</link><author>/u/mina86ng</author><category>dev</category><pubDate>Sun, 26 Jan 2025 12:12:38 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[* available in Rust nightly.There are several aspects of Rust that Iâ€™mÂ not particularly fond of but the one that takes the cake is  which I despise with passion. Itâ€™s aÂ nightly feature which puts in question my extreme emotions about it. On the other hand it means thereâ€™s time to stop it from getting stabilised and figure out something better.In this article Iâ€™ll describe the problem the feature addresses, the issues IÂ have with the solution and describe some alternatives. As it turns out, things arenâ€™t as easy as they seem on the first look.Consider the  routine below which copies data between two I/O streams. On each iteration of the loop, it zero-initialises the buffer which wastes time considering that  will override the data. The compiler doesnâ€™t know that and has no choice but to fill the array with zeros each time. Even an obvious optimisation of moving the buffer declaration outside of the loop isnâ€™t available to the compiler.fn slow_copy(
  mut rd: impl std::io::Read,
  mut wr: impl std::io::Write,
) -> std::io::Result<()> {
  loop {
    let mut buf = [0; 4096];
    let read = rd.read(&mut buf)?;
    if read == 0 {
      break Ok(());
    }
    wr.write_all(&buf[..read])?;
  }
}An attempt at aÂ solution is to use  which makes it possible to declare aÂ region of uninitialised memory. Some explicit pointer casting is necessary, but otherwise code using it is straightforward.use core::mem::MaybeUninit;

pub fn unsound_copy(
    mut rd: impl std::io::Read
    mut wr: impl std::io::Write,
) -> std::io::Result<()> {
  loop {
    let mut buf = [MaybeUninit::<u8>::uninit(); 4096];
    // 
    // For demonstration purposes only.
    let buf = unsafe {
      &mut *(&mut buf as *mut [_] as *mut [u8])
    };
    let read = rd.read(buf)?;
    if read == 0 {
      break Ok(());
    }
    wr.write_all(&buf[..read])?;
  }
}While replacing the array of zeros by an array of uninitialised values may work in specific circumstances, the code is unsound. Change to the compiler, its options, modification of unrelated parts of the code or using the function for aÂ different  trait implementation may break the program in unpredictable ways.The solution in nightly Rust is the  struct. Itâ€™s aÂ bytes slice which remembers how much of it has been initialised. It doesnâ€™t own the memory and operates on aÂ borrowed buffer (hence the name). It can point at an array on the stack or aÂ slice living on the heap (such as â€™s spare capacity). AÂ naÃ¯ve use of the feature is the following:#![feature(core_io_borrowed_buf, read_buf)]

use core::io::BorrowedBuf;
use core::mem::MaybeUninit;

fn almost_good_copy(
    mut rd: impl std::io::Read,
    mut wr: impl std::io::Write,
) -> std::io::Result<()> {
  loop {
    let mut buf = [MaybeUninit::uninit(); 4096];
    let mut buf = BorrowedBuf::from(&mut buf[..]);
    rd.read_buf(buf.unfilled())?;
    if buf.len() == 0 {
      break Ok(());
    }
    wr.write_all(buf.filled())?;
  }
}Issues with the While  appears to work as expected,  isnâ€™t without its share of problems. IÂ describe them below.The  does not seamlessly integrate with existing Rust code. In fact quite the opposite. APIs need to support it explicitly. For example, many third-party  implementations do not provide  method. In its absence, the default version initialises the memory and calls  negating any potential benefits of .Similarly, functions which take output slice as an argumentâ€Šâ€”â€Šsuch as â€™s â€Šâ€”â€Šcould benefit from being able to write to uninitialised memory. However, to offer that benefit, they need to be changed to support . AÂ motivated programmer can try adding necessary support to actively maintained packages, like , but what if one is stuck at an older version of the crate or deals with apparently abandoned crates like  or . To support those cases, forking would be necessary leading the programmer towards deeper circles of dependency hell.Then again, should functions such as  integrate with  in the first place instead of taking an  argument? The issue with the latter is that thereâ€™s no safe way to convert  into . As such, users who so far happily used such functions with regular initialised buffers would need convoluted incantation to make their previously straightforward code to compile. Meanwhile, creating  is somewhat convenient and can be done from initialised and uninitialised buffers alike.In addition to , the  crate offers aÂ  method which fills aÂ generic slice of integers with random data. It could easily work with  except that the struct works on  slices only. As aÂ result, aÂ crate which deals with different types cannot consistently use .IÂ donâ€™t know the reasons why  is not generic. Itâ€™s possible that its design focused on addressing the the  trait use case only. Complications around dealing with  types could have been aÂ contributing factor. However, even then the type could be generic on  types. being optional brings another problem. Without full understanding of the behaviour and interactions of the  type, itâ€™s easy to misuse it such as in . One can be excused from assuming that the function eliminates unnecessary initialisation. It declares an uninitialised region, wraps it in  and reads data into it. Even inspection of the assembly output shows lack of the  call.Alas, while  avoids memory initialisation when reading data from aÂ , it wastes time zeroing the buffer when, for example, decompressing data with help of  crate (which does not offer custom  method) effectively becoming aÂ .Unless the underlying type is known, the programmer must assume that  may resort to filling the memory. The proper use of  is to construct it only once so that it can remember that the memory has been initialised.#![feature(core_io_borrowed_buf, read_buf)]

use core::io::BorrowedBuf;
use core::mem::MaybeUninit;

fn copy(
  mut rd: impl std::io::Read,
  mut wr: impl std::io::Write,
) -> std::io::Result<()> {
  let mut buf = [MaybeUninit::uninit(); 4096];
  let mut buf = BorrowedBuf::from(&mut buf[..]);
  loop {
    buf.clear();
    rd.read_buf(buf.unfilled())?;
    if buf.len() == 0 {
      break Ok(());
    }
    wr.write_all(buf.filled())?;
  }
}With â€™s complexity itâ€™s not hard to imagine why people might use it in inefficient way. The struct is harder to understand than the unsound casting in . This may lead people to use the more straightforward option even if itâ€™s not correct. An analogy to aÂ  with its contents and spare capacity partially helpsâ€Šâ€”â€ŠaÂ  has analogous filled and unfilled partsâ€Šâ€”â€Šbut is an oversimplified view. AÂ  is also split into initialised and uninitialised parts. The documentation visualises it as follows:There are reasons for this madness. Consider loop in the  function above. If  only knew how much of it was filled, each call to  would lose the information about memory being initialised. In the default implementation of  it would need to unnecessarily zero the whole buffer. Separately storing information about how much of the buffer has been filled and initialised, let the type avoid double-initialisation of memory.As an aside, I find modelling  as divided into filled and spare capacity with spare capacity further divided into initialised and uninitialised as more intuitive. Leaning into the analogy of  is in my opinion more natural and it helps by reinforcing terminology used in existing parts of the language rather than introducing new models.Having looked at issues with , letâ€™s consider what people actually want. The easiest mental model is that uninitialised memory stores arbitrary data, unknown unless accessed. To achieve such semantics, the uninitialised memory would need to be . AÂ frozen region becomes safe to read and can be accessed through regular Rust references. With freezing operation available, the buffer definition in the copying routine could be turned into something like:  let mut buf = [MaybeUninit::uninit(); 4096];
  // SAFETY: u8 has no invalid bit patterns.
  let buf = unsafe {
    MaybeUninit::slice_freeze_mut(&mut buf)
  };  let buf = MaybeUninit::frozen();
  // SAFETY: u8 has no invalid bit patterns.
  let mut buf: [u8; 4096] = unsafe { buf.assume_init() };Unsafe blocks are required to account for invalid bit patterns. With aÂ trait like , aÂ safe versions could exist. Either of those alternatives would require no new methods on the  trait and would work without any modifications on methods such as â€™s .Why canâ€™t we have what we want?Reading uninitialised memory is hardly an issue when analysing things on hardware level. So long as aÂ memory address is mapped with proper permissions, accessing data from it will always produce some value. Thereâ€™s no undefined behaviour there. In fact, in typical Linux environment all newly allocated anonymous pages are zero-initialised.tautology:
  cmp  BYTE PTR [rdi], 0
  je   tautology_ok
  cmp  BYTE PTR [rdi], 0
  jne  tautology_ok
  mov  al, 0
  ret
tautology_ok:
  mov  al, 1
  retAn x86 assembly function which checks whether value in memory is zero or non-zero. This seemingly tautological test can fail when operating on aÂ memory page marked with  and the kernel changes the mapping in between the two memory reads.Unfortunately, even when looking from the point of view of machine code, this analysis isnâ€™t completeâ€¦Giving advice about use of memory flag of the  system call allows user space to advise the kernel that (until next write) it no longer cares about contents of specified anonymous pages. This optimisation enables the kernel to discard those pages without swapping them to disk. While the advice is in effect, the user space  access the memory, but has no guarantee whether itâ€™ll read the old values or zeros. Even code written directly in assembly language, like the  function on the right can result in unexpected behaviour.This isnâ€™t aÂ theoretical concern either. jemalloc, aÂ somewhat popular memory allocator, uses  when memory is freed. As aÂ result, new allocations returned from the allocator may point to region of memory where the  advice is in effect. Nicholas Ormrod, in his talk about C++  at Facebook, describes how interaction between jemalloc,  and reading uninitialised memory resulted in outages.To prevent this issue, the proposed  function would need to write into each page of the slice to make sure the kernel notices that the program cares about contents of the page again. This could be a simple loop stepping 4â€ŠKiB at a time and look something like the following:pub unsafe fn slice_freeze_mut<T>(
  slice: &mut [MaybeUninit<T>]
) -> &mut [T] {
  const PAGE_SIZE: usize = 4096;
  let ptr = slice.as_mut_ptr() as *mut _;
  let len = slice.len() * size_of::<T>();
  // SAFETY: Itâ€™s always safe to split MU object into MU bytes.
  let bytes: &mut [MaybeUninit<u8>] = unsafe {
    core::slice::from_raw_parts(ptr, len);
  };
  for el in bytes.iter_mut().step_by(PAGE_SIZE) {
    let p = el.as_mut_ptr();
    // SAFETY: Unsafe without language semantics change
    // since weâ€™re reading uninitialised byte.
    unsafe { p.write_volatile(p.read()) };
  }
  // SAFETY: Caller promises that T has no invalid bit patterns,
  // but this is still unsafe without language semantics change
  // since we havenâ€™t initialised all the bytes.
  unsafe { &mut *(slice as *mut _ as *mut [T]) }
}Unfortunately, this would hardly be the no-operation that people expect from writing into uninitialised memory. It would be an improvement over a full initialisation and would address some issues with  but would do that at the cost of unavoidable page touching.It may seem that the second formâ€Šâ€”â€Šthe MaybeUninit::frozen().assume_init() variantâ€Šâ€”â€Šwhich creates frozen buffer directly on stack could be easier to optimise. The compiler controls the stack and unless it issues , no stack pages will be marked . Unfortunately itâ€™s not clear that always hold true. For example, with async programming the stack lives God-knows-where and there may be other corner cases that would need to be considered.IÂ started this article with aÂ promise of some alternatives to  and yet, as IÂ conclude it, no working alternative is presented. Indeed, this is perhaps what frustrates me the most about the . On the face of it, writing data into uninitialised memory is aÂ feature with an obvious solution, but it doesnâ€™t take long before all the obvious solutions clash with Rustâ€™s safety requirements.So whatâ€™s aÂ lowly programmer to do? Donald Knuth is often quoted as stating that â€˜premature optimisation is the root of all evilâ€™. True to that adage, in most cases itâ€™s safe to pay the price of the memory initialisation. I/O operations usually take orders of magnitude more time so the time saved not initialising the memory is often negligible.But there is more to Knuthâ€™s quote:We  forget about small efficiencies, say about 97% of the time: premature optimisation is the root of all evil.Yet we should not pass up our opportunities in that critical 3%. AÂ good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.For the remaining 3%, the options now are somewhat bleak and depend on the particular code base. They may require switching to nightly compiler, patching third-party crates, going straight to doing unsafe syscalls (e.g. ) or isolating critical code paths and writing them in C.And while we deal with the lack of ideal solution for writing to uninitialised memory, maybe someone will figure out some alternative fast and ergonomic approach.1 The reference conversion itself is safe since all possible values of type  are valid values of type  and both those types have the same layout. However, the latter allows writing arbitrary data into the object which may result in invalid representation of  (see playground demonstration). With interior mutability, even converting shared references may lead to issues.2 IÂ am aware that IÂ presumptuously speak for everyone. However, IÂ do believe that alternatives presented here, if they existed, would be favoured by everyone and that includes contributors to the  struct. As IÂ discuss later, the type is the way it is not because thatâ€™s what anyone finds appealing but due to other constraints.3 Semantics that reading uninitialised memory has arbitrary but consistent value can be useful in practice. Briggs and Torczon describe in An efficient representation for sparse sets an algorithm which is built on such semantics.4 The atypical environment is ÂµClinux which runs on platforms without memory management unit (MMU). It supports  option which skips zeroing of the memory region. However, even with that flag, allocated pages maintain consistent state.5 Donald E. Knuth. 1974. Structured Programming with  Statements. ACM Computing Surveys. Vol. 6, Issue 4 (Dec. 1974), 261â€“301. doi:10.1145/356635.356640.]]></content:encoded></item><item><title>Show HN: Orange intelligence, an open source alternative to Apple Intelligence</title><link>https://github.com/sharingan-no-kakashi/orange-intelligence</link><author>MexicanYoda</author><category>dev</category><pubDate>Sun, 26 Jan 2025 11:02:59 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Hi HN! Iâ€™m excited to share Orange Intelligence, an open-source alternative to Apple Intelligence for macOS.Orange Intelligence allows you to interact with any text on your macOS system in a more powerful and customizable way. It brings a floating text processor that integrates seamlessly with your workflow. Whether youâ€™re a developer, writer, or productivity enthusiast, this tool can boost your efficiency.
Key Features:    Floating Text Processor: Trigger a floating window by double-tapping the Option key to process selected text.
    Run Any Python Function: From basic text manipulations to running large language models (LLM) like OpenAI or local LLaMA, you can execute any Python function on the fly.
    Full Customization: Want to add your own functions or logic? Just write them in Python, and theyâ€™ll appear in the floating window.

How does it work?    Capture: Uses AppleScript to simulate a global Cmd+C and capture selected text from any active macOS app.
    Process: A floating window pops up, letting you choose what to do with the text (run a function, format it, or apply an LLM).
    Replace: After processing, the app returns focus to the original application and pastes the processed text back with a global Cmd+V.

Why open source?I built this to overcome the limitations of Appleâ€™s proprietary tools, and I wanted to make it fully customizable and extendable. Orange Intelligence is built with Python and PyQt6, so itâ€™s easy to adapt, extend, and contribute to.Itâ€™s not just a text processorâ€”itâ€™s a platform for building custom workflows, whether you want to automate simple tasks or integrate with complex AI systems.Iâ€™m looking forward to your thoughts, ideas, and contributions. Thanks!]]></content:encoded></item><item><title>Release of ureq 3.0.0</title><link>https://www.reddit.com/r/rust/comments/1iab9x8/release_of_ureq_300/</link><author>/u/LovelyKarl</author><category>dev</category><pubDate>Sun, 26 Jan 2025 10:33:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[ureq 3.0.0 is released replacing the 2.x branch (which is still maintained).3.x is a ground up rewrite with underpinnings of Sans-IO and retains . The library keeps most of the functionality of 2.x, but pivots to use the common  crate as basis for the API.The goals for ureq remain largely the same: A simple, sync, HTTP/1.1 client with a minimum number of dependencies. and  are now pluggable, meaning it's possible to use alternative socket implementations, TLS or name resolvers separate to the main project.Happy to answer any questions.]]></content:encoded></item><item><title>&apos;First AI software engineer&apos; is bad at its job</title><link>https://www.theregister.com/2025/01/23/ai_developer_devin_poor_reviews/</link><author>/u/Wownever</author><category>dev</category><pubDate>Sun, 26 Jan 2025 10:22:25 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A service described as "the first AI software engineer" appears to be rather bad at its job, based on a recent evaluation.The auto-coder is called â€œDevinâ€ and was introduced in March 2024. The botâ€™s creator, an outfit called Cognition AI, has made claims such as â€œDevin can build and deploy apps end to end," and "can autonomously find and fix bugs in codebases." The tool reached general availability in December 2024, starting at $500 per month."Devin is an autonomous AI software engineer that can write, run and test code, helping software engineers work on personal tasks or their team projects," Cognition's documentation declares. It "can review PRs, support code migrations, respond to on-call issues, build web applications, and even perform personal assistant tasks like ordering your lunch on DoorDash so you can stay locked in on your codebase."The service uses Slack as its main interface for commands, which are sent to its computing environment, a Docker container that hosts a terminal, browser, code editor, and planner. The AI agent supports API integration with external services. This allows it, for example, to send email messages on a user's behalf via SendGrid.Devin is a "compound AI system," meaning it relies on multiple underlying AI models, a set that has included OpenAI's GPT-4o and can be expected to evolve over time.In theory, you should be able to ask it to undertake tasks like migrating code to nbdev, a Jupyter Notebook development platform, and expect it to do so successfully. But that may be asking too much.Early assessments of Devin have found problems. Cognition AI posted a promo video that supposedly showed the AI coder autonomously completing projects on the freelancer-for-hire platform Upwork. Software developer Carl Brown analyzed that vid and debunked it on his Internet of Bugs YouTube channel.Now, three data scientists affiliated with Answer.AI, an AI research and development lab founded by Jeremy Howard and Eric Ries, have tested Devin and found it completed just three out of 20 tasks successfully.In an analysis conducted earlier this month by Hamel Husain, Isaac Flath, and Johno Whitaker, Devin started well, successfully pulling data from a Notion database into Google Sheets. The AI agent also managed to create a planet tracker for checking claims about the historical positions of Jupiter and Saturn.But as the three researchers continued their testing, they encountered problems."Tasks that seemed straightforward often took days rather than hours, with Devin getting stuck in technical dead-ends or producing overly complex, unusable solutions," the researchers explain in their report. "Even more concerning was Devinâ€™s tendency to press forward with tasks that werenâ€™t actually possible."As an example, they cited how Devin, when asked to deploy multiple applications to the infrastructure deployment platform Railway, failed to understand this wasn't supported and spent more than a day trying approaches that didn't work and hallucinating non-existent features.Of 20 tasks presented to Devin, the AI software engineer completed just three of them satisfactorily â€“ the two cited above and a third challenge to research how to build a Discord bot in Python. Three other tasks produced inconclusive results, and 14 projects were outright failures.The researchers said that Devin provided a polished user experience that was impressive when it worked."But thatâ€™s the problem â€“ it rarely worked," they wrote."More concerning was our inability to predict which tasks would succeed. Even tasks similar to our early wins would fail in complex, time-consuming ways. The autonomous nature that seemed promising became a liability â€“ Devin would spend days pursuing impossible solutions rather than recognizing fundamental blockers."Cognition AI did not respond to a request for comment. Â®]]></content:encoded></item><item><title>ðŸ”’ What&apos;s OAuth2, anyway?</title><link>https://www.romaglushko.com/blog/whats-aouth2/</link><author>/u/roma-glushko</author><category>dev</category><pubDate>Sun, 26 Jan 2025 10:17:03 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Have you ever logged into a website using your Google or Facebook account?
Or connected an app to access your GitHub data? If so, youâ€™ve already used OAuth2, whether you knew it or not.OAuth2 is the worldâ€™s most popular, extensible authorization framework.
It allows you to integrate a couple of systems together by delegating access to your data from one service to another.
But here is the thing - most people donâ€™t really understand how OAuth2 really works.Personally, Iâ€™ve implemented several applications that were using OAuth2.
The process was so straightforward that I had no need to stop and think about the protocol itself along the way.
Thatâ€™s by design. OAuth2 is built to be super simple to implement client applications, not to wrestle with complex authentication requirements.But if we pause and dig deeper, thereâ€™s a lot to learn from the software engineering point of view.In this article, we will uncover the â€œwhysâ€ behind the OAuth2 protocol design and
break down the most common authentication grants.Itâ€™s helpful to start with the historical context of the problem that OAuth2 was created to solve
and consider alternatives weâ€™d have without it.Imagine we want to build a user-friendly deployment platform like Fly.io or Vercel.
Right away, we hit the key problem: how can our customers import their code into our platform?These days, almost everyone uses Git.
We could try building a Git hosting functionality directly into the platform,
but thatâ€™s a huge piece of work, while our primary business goal is resource management, autoscaling, load balancing, etc.
On top of that, most of our customers are probably already using one of the existing popular Git hosting services like GitHub, GitLab, or Bitbucket.
Unfortunately, we donâ€™t have any way to convince these platforms to integrate with us.So, whatâ€™s our options? How could we possibly get access to our customersâ€™ Git repositories hosted elsewhere?Our customers log into their Git hosting services using their credentials.
Why canâ€™t they just share their credentials to us?We could store their credentials securely and then, when needed, log in to the Git service on their behalf,
use their session cookies, and fetch the required Git repositories.Plain User Credentials Sharing (sounds great, huh?)At first glance, this sounds like a straightforward idea to let our platform work with customer data, even when they are not around.But then we realize, itâ€™s riddled with problems:. The platform gets full access to everything that our customers can do without a way to limit or control that, even if we only need to access their repositories.. Itâ€™s hard to distinguish between sessions created by the users and those initialized by the platform. If the login process is the same for both, itâ€™s hard to implement more advanced login security measures like MFA.. Once shared, the credentials can be cached and leaked in unexpected ways even if you removed them from the platform UI. The only way to fully revoke access is to change your password.. If you change your password, this would effectively break the platformâ€™s access to your data.. The platform must store the credentials securely, which is a significant responsibility. If the shared credentials are managed in a sloppy way, they may be breached and expose the whole customer account.Itâ€™s a problem that one and the same set of credentials with broad, top level permissions are used for two vastly different purposes.
What could we do about that?Apparently, if we want to do any better, we need to keep the main credentials private.
Instead, we could introduce an alternative type of credentials just for using in integrations.Letâ€™s call these Personal Access Tokens (PATs). Think of them as a static secret string with a relatively long lifespan.
Technically, each PAT could have a custom set of permissions assigned to it, limiting what the platform can do with the associated data.Whenever a customer wants to integrate Git repositories with a new service, like our deployment platform, they would generate a new personal access token with the necessary permissions  and share it with the service.Personal Access Tokens SharingThis approach is a great improvement over sharing the plain credentials, since it addresses its major problems.
However, there are still a couple of things to keep in mind:Keeping track of expiration dates and replacing stale tokens gets tedious very quickly if you need to manage more than a handful of tokens.To minimize the management burden, token lifetime could be extended (we are talking about months or even years). Unfortunately, in that case, a token gets compromised, malicious actors will have plenty of time to exploit it.But do customers really need to manage their tokens for every service and integration they use?
Could we simplify this further, so customers will have to do as little as possible to enable new integrations and the whole token management process is automated by the party that needs it?Thatâ€™s exactly why we need something like the OAuth framework.OAuth2 is a framework that defines how access or permissions are requested or delegated from one
an authoritative entity (like the user) to third-party applications.The core idea behind OAuth2 is to give users the power to decide what applications (beyond those that are natively supported by the resource server)
can access their data. It ensures that the access is controlled and convenient, allowing these applications use your data whenever they need to, even when youâ€™re not around, to extend the base functionality of the resource server.
Letâ€™s break it down a bit.Without mechanisms like OAuth2, the resource server essentially controls which applications can access with your data.
I imagine this happens through partnerships, where two companies collaborate to integrate their services into each otherâ€™s offerings (often in a very custom, non-standardized way).This approach is centralized because:Only allowed partnerâ€™s applications are at playEverything else is effectively blocked.OAuth2 introduces a middle ground, allowing the third-party applications to use the resource server, as long as users are willing to
grant permissions to their data or functionality. In this model, the resource server decides nothing for end users (unless itâ€™s blocking malicious applications to protect users from abuse).This creates a powerful form of decentralization that:Let resource owner extend the resource serverâ€™s functionality in a few clicksHelp to build an ecosystem of tools and applications around the resource serverOAuth2 defines three main roles to organize the delegation process:. This is a service that the client application needs to access, either on user or their own behalf. For example, in our case, itâ€™s the Git hosting provider like GitHub. (the user if itâ€™s a person). The entity that holds permissions to the resource server and can grant access to the client application.. This service issues resource access tokens for the client application in exchange for various forms of authorization or grants. (a.k.a. the client, OAuth application). An application or service that accesses the protected resource server, typically on behalf of the resource owner.OAuth2 Roles and High-Level Interactions Between ThemOAuth2 introduces the Authorization Server, acts as a middleman between
the Resource Owner (who has the authority) and the Client Application (that needs some of that authority).
The Authorization Server is trusted by the target Resource Server (which provides some functionality based on the authority).The Resource Owner reviews the permission request and gives a consent to grant the access to the Client Application via the Authorization Server.Depending on the authorization flow, the Client Application receives an authorization grant in some form and uses it to trade for an access token (or a pair of tokens)
from the Authorization Server.Finally, the Client Application uses the access token to access the Resource Server on behalf of the Resource Owner.The Resource Server knows how to validate the access tokens issued by the Authorization Server, typically through an internal request to the Authorization Server.The journey into the OAuth2 world begins with Client Applications.
There are two types of Client Applications, categorized by their abilities to keep secrets: like in-browser JS applications, desktop or native mobile apps. Any secrets embedded in this type of application can be reverse-engineered and extracted, even if you try to obfuscate their distributions or encrypt them.Private (or confidential) applications, which are typically any web applications with frontend and, most importantly, backend parts. The backend is capable of securely storing secrets and establishing protected communication with the Authorization Server.OAuth2 assumes there are much more Client Applications than Authorization and Resource Servers,
so it aims to simplify the Client Application side as much as possible.
This not only reduces the work to do to implement a Client Application, but also limits opportunity for implementing insecure Clients.To plug our Client Applications into the OAuth2 workflow, they first need to be registered with the Authorization Server.The OAuth2 doesnâ€™t make any assumptions how the registration process should work,
but itâ€™s typically a part of the OAuth2 provider websiteâ€™s settings e.g. functionality to create and manage OAuth apps.The Gitlab OAuth Client RegistrationThe registration form usually includes:Redirect URL(s) - A list of allowed URLs for redirects in interactive authorization flows, such as the authorization code or implicit flows.Scopes - A list of delegated access to the Resource Serverâ€™s functionality e.g. read Git repositories, create issues, etc.Miscellaneous information like application name, icon, privacy and terms of service URLs, etc.There are other, less popular client registration approaches. For example, Iâ€™ve seen:Registration via internal admin requests to the Authorization Server like ORY Hydra.Declarative registration by creating Kubernetes Custom Resources in the cluster using ORY Hydra Maester.At the end of registration, you typically receive the client credentials: (a.k.a App ID) - a public, non-secret identifier of your Client Application. - a secret password that the Client Application keeps privately.The client credentials are used to:authenticate the Client Application requests to the Authorization Serverbind a specific authorization flow to the Client Application that has started it. This ensures itâ€™s not possible to finish that flow with completely different Client ApplicationThe client ID is tied to authorization grants and refresh tokens, so itâ€™s essential to keep it unchanged.
Changing it would invalidate all authorizations (e.g. refresh tokens) that youâ€™ve already obtained.On the other hand, the client secret can, and should be, rotated periodically.
Changing the secret would have an effect of â€œrotationâ€ of all refresh tokens received by the Client Application
even though the tokens would not be affected.
This is because if the client credentials were leaked along with some refresh tokens,
malicious actors would not be able to obtain new access tokens using the old client secret after the client secret rotation.This significantly simplifies the process of secret rotation as you need to rotate only one secret
instead of rotating thousands of refresh tokens for each end user that has ever authorized your Client Application.In practice, you may want to also have a bunch of others that are not defined in OAuth2 directly:Access token introspection endpoint (it has its own RFC) that returns metadata information associated with the given access token. It can be used by resource servers to validate incoming access tokens.Authorization grant revocation endpoint that allows it to revoke the whole authorization grant.Token revocation endpoint that allows to revoke the issued access and refresh tokens.and a bulk of other endpoints that were introduced in the all follow-up RFCs and drafts if you need that.Historically, the Authorization Server OAuth2 endpoints were not fixed nor was there a way to discover them.
The endpoints were extracted by the provider documentation and hardcoded in the OAuth2 libraries or Client Application
(here is an example from the goth library).The Authorization Server is represented as a separate component conceptually, but the protocol has no requirements on how it should be implemented under the hood.
It could be either a separate microservice, or it can be a part of the Resource Server.One important assumption that OAuth2 protocol makes implicitly is that one authorization server can potentially handle authorizations for multiple Resource Servers.
This means that among all OAuth2 components, the Authorization Servers are the rarest to implement.
Thatâ€™s why they are responsible for handling a lot of security nuances around the Authorization Server implementations.
Your OAuth2 is essentially as secure as your Authorization Server.The Authorization Server generates access tokens as a result of the successful authorization flow.The access tokens are a special credential that serves as an alternative method of authentication for the Resource Server.
They can be also seen as an abstraction around the exact authorization flow.
There could be multiple authorization flows supported by the Authorization Server, but they all will result in access tokens that have the same format.
This makes them easier to validate for the Resource Server that doesnâ€™t need to know too much information about how the specific token was obtained.Access Tokens unifies the authorization flowsThe concept of access token is also important because we can generate multiple access tokens with different reduced subset of the originally requested scopes.
If there was no access tokens as a separate credential and we were using the authorization code,
letâ€™s say, for that purpose, it would have all permission scopes requested by the client application at the point of passing authorization flow.OAuth2 doesnâ€™t define how the access tokens should look like.
They are opaque strings to the Client Applications and likely Resource Servers too.Apart from that, when an access token is generated, the Authorization Server indicates what type of token was issued.In the wild, Authorization Servers may issue bearer tokens as:a unique random string. The string should be non-guessable and not possible to generate outside the Authorization Server.or as a self-contained JWT token that includes the signed meta information.Other types of tokens are theoretically possible, but I have never seen them in the wild.OAuth2 requires Authorization Servers to generate access tokens only.
If so, the generated token is considered as a long-lived and thatâ€™s not great for two reasons:The access tokens are linked to the client application, but they are usually passed to the resource server without any additional proof of token possession. Hence, if they are leaked, the malicious actors would have enough time to exploit them.The access token is linked to the original access scopes and there is no way to generate a new access token with a subset of scopes without going through the whole authorization flow again.To address these concerns, itâ€™s the best practice to keep access tokens short-lived.
Along with that, you can generate a separate, long-lived token that generates you fresh access tokens as needed.
This type of token is called refresh token.Authorization scopes are a set of functionalities that the Resource Owner delegates to the Client Application,
allowing the Client to access resources as thought it were the original owner.The scopes are simply a space-separated list of strings, where each string specifies a particular access type.
The scope format is not defined in the OAuth2 protocol, but they are normally structured like this: {resource}_{access level}. may allow the Client to read the current user (e.g. Resource Owner) profile information may allow the Client Application to commit to the repositories to which the Resource Owner has access to.As you can see, the scopes are fairly coarse-grained, they donâ€™t grant access to specific resources,
but rather work on the resource types and access levels (e.g. read/write/admin).Scopes are additive, meaning when multiple scopes are requested, they are combined to broaden the Client Applicationâ€™s or access tokenâ€™s permissions.Authorization flows, also known as authorization grants, are how permissions are delegated to the Client Applications.
Regardless of the flow you use, the end result is a set of access tokens that enable the Client Application to directly access the Resource Server.The main differences between flows are:whenever itâ€™s interactive or nonthe number of participants involved (2- or 3-leg flows)Weâ€™ll start by reviewing the most canonical and secure OAuth2 flow called the authorization code flow.This flow is interactive and works for Client Applications that can keep secrets and perform browser redirects, typical for web services with a backend.The flow consists of two stages:OAuth2 Authorization Code FlowThe whole authorization code flow can be divided into two main parts:The interactions that happen indirectly between the Authorization Server and the Client Application using the browser as a mediator. These actions are performed via the frontend channel and can be potentially intercepted or manipulated along the way (e.g. a malicious browser extension may try to sniff the code parameters).The interactions occur directly between Authorization Server and the client via trusted backend channel.The authorization code flow is designed so that itâ€™s not possible to get access delegation by using only information transmitted via the frontend channel.In order to start the OAuth2 flow, the client application needs to request the authorization with the needed scopes from the Resource Owner.
This happens by redirecting the resource owner to the authorization serverâ€™s  endpoint.The authorization URL usually contains the following URL parameters:The  is what defines what kind of interactive flow we are going to perform. Itâ€™s always  for the authorization code flow (or  for the implicit flow).The  is required as the authorization code is strictly assigned to the client application that has initialized the flow (to prohibit finishing the flow from another Client Application).The  is the URL of the client application callback page where the authorization code will be passed after
the authorization consent. This URL must be specified in the client registration settings.The Resource Owner browser should already have a user session (e.g. session cookie) with the Authorization Server (or login otherwise),
so the redirect can leverage that to seamlessly show the authorization consent screen.The Example of the Authorization Consent ScreenLetâ€™s note that the client application communicates with the Authorization Server indirectly via HTTP redirects
and the Resource Owner browser. This way the Client Application doesnâ€™t have to know about the Resource Owner credentials or session
which is itself the key problem the OAuth2 protocol was born to solve.Because of that, the authorization consent page should not have any client-specific CORS configuration.
This remains true for all OAuth2 flows.Once the Resource Owner approves the delegation of access to the Client Application, the Authorization Server redirects
the Resource Owner back to the Client Application callback URL specified during the authorization request.The client callback redirect looks like this:the  parameter is called the authorization code (it gives this flow its name).the  is returned back if it was specified originally to let the Client Application verify the integrity of the flow.The authorization code is a one-time-use token that represents the specific Resource Ownerâ€™s consent to give to the specific Client Application.
It is tied to the client ID that has obtained it, so itâ€™s not possible to exchange it from another Client Application.
So even if the code was leaked somehow, you would need to have valid client credentials to turn it into access tokens.Finally, to finish the flow, we need to exchange the authorization code for the access tokens.
This is done via the OAuth2 token endpoint:The  defines what kind of flow or grant we want to use to trade for access tokens. Itâ€™s a universal endpoint used in the other flows too, but in the case of this flow, itâ€™s always going to be .The  is mandatory to provide in the authorization code exchange.In response, if everything went fine, you would get response like this:Thatâ€™s all. Now you need to persist the access and refresh tokens and use them to access the Resource Server.The analysis of real-world attacks on the authorization flows has shown that it can be further secured.
Specifically, malicious actors can intercept the authorization code or try to inject it into the callback URL to do token exchange via unauthorized workflows.
These attack vectors are the most probable in public applications like native applications.PKCE is a simple way to prove that the authorization code was obtained via the legitimate authorization request.
The beauty of PKCE is that it just slightly extends the authorization & token requests without major changes to the flow.OAuth2 Authorize Code with PKCEThe Client Applications generate a random string called the  and then hash it with a cryptographically secure algorithm as SHA256. The hashed value is called the .The Client Application keeps the original  privately and shares the  and the hash code (e.g. ) as query params in the authorization request.The Authorization Server remembers the  and the . No other changes are needed to the existing Authorization Server responses.Then, the client sends the  during token exchange. The Authorization Server computes the hash of that value and compares it with the  passed during the authorization request.PKCE supports two hashing methods: - the SHA256 hashing algorithm - the plain text method. Itâ€™s basically just code_challenge = code_verifier. The  method should be avoided as it doesnâ€™t really introduce any challenges. It can only protect you from attacks where nefarious actors can intercept the Authorization Server responses.The PKCE extension allows the public clients to finally leverage the authorization code flow securely.
However, the Authorization Server must be ready to support PKCE for public clients which boils down to not requiring these clients to provide any client secrets.The refresh token is an optional but highly recommended additional token that the OAuth2 token endpoint can return to you.
Unlike the access token, the refresh token is meant to be a long-lived token (either no expiration time or an extended period of time like half a year)
that is sent to the authorization server only.Essentially, the refresh token is an â€œinternalâ€ authorization grant because it implies the authorization that the resource owner has given to the Client Application.The refresh token is important for two reasons:it allows to keep access tokens short-lived, so minimize the attack surface if they are leakedit allows to the generation of access tokens with the reduced access scope that is more limited than the scopes granted to the Client Application during authorization. This enables the clients to implement the least privilege principle on their side.OAuth2 Refresh Token FlowIn order to refresh your access token, you send a request to the OAuth2 token endpoint with the  set to :The refresh token is linked to the specific client credentials, so itâ€™s not possible to leverage it with an unauthorized client.The refresh token request generally returns the same response as we have seen in the authorization code exchange.
It contains the new active access token, its expiration time and the actual access scopes.
In some cases (GitHub and GitLab do this, for instance), the refresh token request may actually also refresh your previous refresh token, so if the token response
contains the  field and itâ€™s different from your current refresh token, it means that this is your new refresh token to persist and use going forward.The refresh token request generally invalidates all previous access tokens (and refresh tokens).We have said that the authorization code flow is designed to make it impossible to
get Resource Owner delegation by using only information passed via the frontend channel (e.g. the authorization code and client ID).
In order to achieve this, that flow requires the Client Application to have a secure backend channel.
But what if the application is public and doesnâ€™t have a place to put a secret, so it remains a secret?The original OAuth2 specification introduced a simplified version of the authorization code flow
that makes a significant security trade-off in order to support public applications, first of all, in-browser JS applications
like browser extensions or single page applications (SPAs) without backends. Itâ€™s called the implicit flow.The implicit flow is also an interactive, redirect-based flow, but there is no explicit code exchange via the backend channel.
Instead, it happens implicitly and the Client Application just receives the access token in the callback URL.The authorization request looks close to what we have seen in the authorization code but
this time we have to specify  as :In JS applications, there are a few ways you can do this request:Do a full-page redirect to the Authorization ServerOpen a separate popup window and do the redirect there and then close it when the callback URL is hit.If you specify the authorization  parameter, the best place to temporarily persist it will be window.sessionStorage.Once the Resource Owner approves the delegation, the Authorization Server redirects them back to the client callback URL which would look like this:The  is returned right away in the callback URL along with other parameters. This is a simple GET request,
so the sensitive access token is a part of the URL and can be potentially intercepted by other browser extensions, malicious scripts injected via XSS attacks, etc.
Additionally, the whole callback URL is cached in the browser history along with the access token.
Thatâ€™s the main reason why the implicit flow is considered insecure.All parameters are returned as URL fragments which means they are intended to be used by browser client applications only (e.g. not shared with any backend servers).Since in-browser applications cannot keep secrets, the returned access token is super short-lived (like 1-2 hours).
For the same reason, OAuth2 requires no refresh tokens in the implicit flow.Finally, the Client Application can use the retrieved access token to access the Resource Server in the same way we have seen in the authorization code flow.
There is one specific though. The Resource Server should be ready to accept these in-browser application requests
by having CORS policies configured.Looking back, there are basically two pieces of information that help to identify validity of the client in the implicit flow:There is no client secret or any other sensitive information to put into the public client application.Letâ€™s continue our what-if thought process. What if there is no resource owner and the Client Application wants to act on its own behalf?This is a common situation when you have a dozen of internal services that have to communicate with each other and you want to secure that communication somehow
to create a zero-trust environment.In this case, there is no Resource Owner involved, so there is no need for the whole frontend channel to be involved.
All we need is to make the Authorization Server accept the client credentials as a valid reason to issue the access tokens.
Therefore, this flow is called the client credentials flow.OAuth2 Client Credentials FlowThe client credentials flow is a non-interactive flow that enables confidential trusted Client Applications
to access the Resource Server (or other internal Client Application).
So the only request we need here is to the token endpoint:The  is set to  to indicate that validity of the client credentials is the reason to give us an access token.The  are optional but recommended to achieve the least privileged access.Thatâ€™s it. The response is the same as in other flows.
There is no big reason to issue refresh tokens here, because the client credentials act as one, so itâ€™s generally omitted.Another thing is the access scope. Since the Client Application acts on its own behalf,
it may not be limited to the resources available to a specific Resource Owner.
There has to be a way for the Resource Server to differentiate this level of access versus regular resource owner delegation.
I have seen two ways of doing this:use a separate set of scopes to mark such an internal, wide accessadd a custom claim to the JWT access token and account for it during access token validationResource Owner CredentialsThe most paradoxical flow out of all OAuth2 standard flows is the resource owner credentials (ROC) flow.
Itâ€™s paradoxical because it was discouraged from use since day one of the OAuth2 protocol, everyone says itâ€™s a very bad idea to use it, yet still it made it into the specification.
Why did that happen?Theoretically, there might be situations where you absolutely have to use your username and password all around to access some resources.
Without the ROC flow, you would be even less secure than if you used it.
This is because the flow limits the credential exposure over the network which reduces the chance of credential leakage. Also, it allows you to limit the access scope (rather than giving the client absolutely all access you have).In this flow, the Resource Owner passes their credentials (e.g. username and password) directly to the Client Application.
Then the application uses the credentials as an authorization grant to issue a pair of access and refresh tokens.
The resource credentials are then discarded and the client uses the tokens solely to access the protected resources going forward.OAuth2 Resource Owner Credentials FlowThis is a backend channel only flow, so the Client Application exclusively communicates with the token endpoint of the
Authorization Server:The  has to be  to indicate the ROC flowThe credentials e.g. username and password are passed as a part of the token requestitâ€™s possible to pass the  param to reduce the delegated access level. Otherwise, it would be the full access that the Resource Owner has (whatever that means for the given Resource Server).In which cases this flow could make some sense?You should have a high degree of trust to share your main credentials with the Client Application. Ideally, it should be something you control (e.g. the first-party client).Your Client Application is highly privileged. For example, it does some actions on behalf of your tenant or organization admins. This is how Microsoft Entra supports it. At the same time, personal accounts could not use this flow (e.g. partially because there are other protections in order to login like MFA).Apparently, the primary target of the original OAuth2 specification was the browser application use case,
but after OAuth2 gained popularity, it has found its way into other contexts. For example, not every environment has an ability to open a browser and do the redirect-based flows
like authorization code. A few examples:When you have got a new TV and you want to watch Netflix on it, you need to authorize that device to access your account and subscription.When you want to analyze your Snowflake data in a cloud-hosted, containerized Jupyter notebook, there might be no easy way to open a browser (itâ€™s a headless linux under the hood).When you try to connect to your game portal from a console that may have a browser, but only limited input capabilities (e.g. no full-fledged keyboard)Thankfully, there is an extension to the original OAuth2 specification that codifies so-called the device authorization flow.The device authorization (or device code) flow is a special kind of interactive flow that doesnâ€™t assume any direct interactions between the Client Application residing on the device
and the Resource Ownerâ€™s browser.Instead, the Client Application instructs how the resource owner can authorize it via browser indirectly by showing the verification URL to visit, QR code to scan or just a call to open the providerâ€™s mobile application.Device Authorization RequestIn order to implement the device authorization flow, they introduced a new endpoint for kicking off the flow called the device authorization endpoint (because it has a completely different semantic than the standard, browser-based  endpoint):The  is required to identify the Client Application.There is no client secret because the device client is close to the public clients in terms of the ability to keep secrets e.g. any built-in secrets can be extracted.The device authorization endpoint returns something like this:The  is where the end user should go to type in the . The URL should be short enough to type in manually. Alternatively, the Authorization Server may give another URL to transform into a QR code, for example. That URL generally contains the user code as a query param.The  is what the device client application keeps secretly in memory and then uses as a grant during polling the token endpoint.The device code serves as a proof of starting the authorization flow. If there was no device code and the device client had only client ID as the client identifier,
attackers may figure out that ID and then try to send the token requests to get the access & refresh tokens before the real device that has requested it.The resource owner has to trigger (or retrigger if the previous request has timed out) the authorization flow, but at the same time,
we pass no information about that user during initializing the authorization request. The authorization server can only match the resource owner with the corresponding client ID after typing in the user code on the verification page.
To be fair, we pass no Resource Owner identifier directly in other interactive flows, too,
but the authorization redirect leverages browser cookies there, so the Authorization Server can identify the end user right off the bat.Then, the user code is shown somehow to the Resource Owner.
Generally, itâ€™s just printed on the device screen, so the user can type it from there.The device authorization is a time-bound process (the lifetime is specified as  field in the response).
The authorization lifetime is typically around 15 minutes.Because there is no way for the Authorization Server to tell the device client when the authorization is granted (that role is played by the callback URL in the other interactive flows)
and itâ€™s a big assumption that the device can accept inbound requests, the protocol only assumes that the device is connected to the internet and can do outbound requests.With these assumptions, the device can poll the token endpoint every so often until the authorization is granted, the authorization request is expired or denied.
The default polling interval is 5 seconds.The polling happens against the token endpoint:The  usually has to indicate what flow we are trying to complete. In this case, the flow code is unusual which means that the flow name is not standard (or custom).The  is also sent to verify the device that is trying to obtain tokens.The specification doesnâ€™t require client authentication when accessing the token endpoint, but itâ€™s possible and some providers
use that (e.g. Googleâ€™s Device Authorization flow implementation).
In that case, itâ€™s still true that you cannot persist the client secret on the end device
and should probably have a backend service somewhere to poll the token endpoint for the device.Itâ€™s very likely that the device client would need to poll the token endpoint a couple of times before the end user actually authorizes it.
In this case, the token endpoint should return a special error indicating that the authorization is not yet granted:If the client polls it too eagerly, another special error is returned that says to expand the polling interval by 5 seconds:When the authorization is finally granted, the token endpoint should return the regular token response we have seen in the authorization code flow.What else can we trade for access & refresh tokens? The OAuth2 specification defines a way to extend the standard grant types
with a custom one. This is called an extension or third-party assertion grant.The assertion grant is a backend channel only flow where the Client Application sends the Authorization Server
a special third-party assertion that proves the clientâ€™s rights to access the protected Resource Server.As with any other backend channel only flow, this one only uses the token endpoint:The grant type in this case is a unique string in a form of URN that includes the organization name and other grant type information. For example:Itâ€™s only important that the target Authorization Server recognizes it and knows how to validate it.The assertion is usually a self-contained secure token that is cryptographically signed by the assertion provider.
Practically, there are two types of assertions you can see in the wild:The client authentication may be optional in this case (if so, the refresh token may not be issued as that grant requires client authentication).After we have reviewed all main OAuth2 flows, which one should you choose for your specific application?I have tried to come up with the following decision tree that asks the main questions to help you.Always try to use the authorization code flow with PKCE if possible, no matter if itâ€™s a public or confidential client application. This may not be possible because your provider may not support it yet.If PKCE is not supported, then the authorization code is only good for private clients unless the dynamic client registration is supported. For public clients, you should go with the implicit flow and dive into the number of recommendations and considerations to implement as securely as possible.If your client application cannot open a browser with the resource owner session or is limited in terms of input capabilities, and your users donâ€™t really trust it, then go with the device code flow.Before falling back to the resource owner credentials flow, try to see if API keys can help you achieve the same goal.Thinking about why OAuth2 protocol has been designed the way it is, turned out to be a great exercise
in threat modeling with immediate, straightforward and practical approaches to mitigate these threats.
They can be reused to solve similar security concerns in other contexts outside of OAuth protocol,
so you can benefit from a deep understanding of the protocol even if you are not a security expert who has to know the ins and outs of OAuth2.Apart from that, OAuth2 is such a vast area that we have been able to only answer the fundamental why questions and
review the most popular delegation grants in this article.A lot of interesting OAuth2 extensions are just briefly referenced, but you would not see them that often in the wild yet,
so that was acceptable to leave them out for now.If you would like to see follow-up articles on OAuth2 protocol and its extensions, please let me know.]]></content:encoded></item><item><title>Kubernetes EKS course</title><link>https://www.reddit.com/r/kubernetes/comments/1iaawqa/kubernetes_eks_course/</link><author>/u/caiolagreca</author><category>dev</category><pubDate>Sun, 26 Jan 2025 10:12:22 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi everyone, Iâ€™m looking to learn Kubernetes and Amazon EKS. I havenâ€™t found many good tutorials on yotube, and the Udemy courses that I had checked have not so good reviews. Could you recommend any good courses based on your experience? Thank you!]]></content:encoded></item><item><title>Prototyping in Rust</title><link>https://corrode.dev/blog/prototyping/</link><author>/u/EightLines_03</author><category>dev</category><pubDate>Sun, 26 Jan 2025 10:05:19 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Programming is an iterative process - as much as we would like to come up with the perfect solution from the start, it rarely works that way.Good programs often start as quick prototypes.
The bad ones stay prototypes, but the best ones evolve into production code.Whether youâ€™re writing games, CLI tools, or designing library APIs, prototyping helps tremendously in finding the best approach before committing to a design.
It helps reveal the patterns behind more idiomatic code.For all its explicitness, Rust is surprisingly ergonomic when iterating on ideas. Contrary to popular belief, it is a joy for building prototypes.You donâ€™t need to be a Rust expert to be productive - in fact, many of the techniques weâ€™ll discuss specifically help you  Rustâ€™s more advanced features.
If you focus on simple patterns and make use of Rustâ€™s excellent tooling, even less experienced Rust developers can quickly bring their ideas to life.How to prototype rapidly in Rust while keeping its safety guaranteesPractical techniques to maintain a quick feedback loopPatterns that help you evolve prototypes into production codeThe common narrative goes like this:When you start writing a program, you donâ€™t know what you want and you change your mind pretty often.
Rust pushes back when you change your mind because the type system is very strict.
On top of that, getting your idea to compile takes longer than in other languages, so the feedback loop is slower.Iâ€™ve found that developers not yet too familiar with Rust often share this preconception.
These developers stumble over the strict type system and the borrow checker while trying to sketch out a solution.
They believe that with Rust youâ€™re either at 0% or 100% done (everything works and has no undefined behavior) and thereâ€™s nothing in between.Here are some typical misbeliefs:â€œMemory safety and prototyping just donâ€™t go together.â€â€œOwnership and borrowing take the fun out of prototyping.â€â€œYou have to get all the details right from the beginning.â€â€œRust always requires you to handle errors.â€It turns out you can avoid all of these pitfalls and still get a lot of value from prototyping in Rust.If youâ€™re happy with a scripting language like Python, why bother with Rust?Thatâ€™s a fair question!
After all, Python is known for its quick feedback loop and dynamic type system, and you can always rewrite the code in Rust later.Yes, Python is a great choice for prototyping.
But Iâ€™ve been a Python developer for long enough to know that Iâ€™ll very quickly grow out of the â€œprototypeâ€ phase
-â€“ which is when the language falls apart for me.One thing I found particularly challenging in Python was hardening my prototype into a robust, production-ready codebase.
Iâ€™ve found that the really hard bugs in Python are often type-related: deep down in your call chain, the program crashes because you just passed the wrong type to a function.
Because of that, I find myself wanting to switch to something more robust as soon as my prototype starts to take shape.The problem is that switching languages is a  undertaking â€“ especially mid-project.
Maybe youâ€™ll have to maintain two codebases simultaneously for a while.
On top of that, Rust follows different idioms than Python, so you might have to rethink the software architecture.
And to add insult to injury, you have to change build systems, testing frameworks, and deployment pipelines as well.Wouldnâ€™t it be nice if you could use a single language for prototyping and production?Using a single language across your entire project lifecycle is great for productivity.
Rust scales from proof-of-concept to production deployment and that eliminates costly context switches and rewrites.
Rustâ€™s strong type system catches design flaws early, but we will see how it also provides pragmatic escape hatches if needed.
This means prototypes can naturally evolve into production code;
even the first version is often production-ready.But donâ€™t take my word for it. Hereâ€™s what Discord had to say about migrating from Go to Rust:Remarkably, we had only put very basic thought into optimization as the Rust version was written. Even with just basic optimization, Rust was able to outperform the hyper hand-tuned Go version. This is a huge testament to how easy it is to write efficient programs with Rust compared to the deep dive we had to do with Go.
â€“ From Why Discord is switching from Go to RustIf you start with Rust, you get a lot of benefits out of the box:
a robust codebase, a strong type system, and built-in linting.All without having to change languages mid-project!
It saves you the context switch between languages once youâ€™re done with the prototype.Python has a few good traits that we can learn from:changing your mind is easyitâ€™s simple to use (if you ignore the edge cases)itâ€™s easy to experiment and refactoryou can do something useful in just a few linesThe goal is to get as close to that experience in Rust as possible while staying true to Rustâ€™s core principles.
Letâ€™s make changes quick and painless and rapidly iterate on our design without painting ourselves into a corner.
(And yes, there will still be a compilation step, but hopefully, a quick one.)Even while prototyping, the type system is not going away.
There are ways to make this a blessing rather than a curse.Use simple types like , ,  in the beginning.
We can always make things more complex later if we have to â€“ the reverse is much harder.Hereâ€™s a quick reference for common prototype-to-production type transitions:When you need to avoid allocations or store string data with a clear lifetimeWhen the owned vector becomes too expensive to clone or you canâ€™t afford the heapWhen  becomes a bottleneck or you donâ€™t want to deal with heap allocationsWhen the reference counting overhead becomes too expensive or you need mutabilityWhen you can guarantee exclusive access and donâ€™t need thread safetyThese owned types sidestep most ownership and lifetime issues, but they do it by allocating memory on the heap - just like Python or JavaScript would.You can always refactor when you actually need the performance or tighter resource usage, but chances are you wonâ€™t.Rust is a statically, strongly typed language.
It would be a deal-breaker to write out all the types all the time if it werenâ€™t for Rustâ€™s type inference.You can often omit (â€œelideâ€) the types and let the compiler figure it out from the context.This is a great way to get started quickly and defer the decision about types to later.
The system scales well with more complex types, so you can use this technique even in larger projects.Hereâ€™s a more complex example which shows just how powerful Rustâ€™s type inference can be:Itâ€™s not easy to visualize the structure of  in your head, but Rust can figure it out.You probably already know about the Rust Playground.
The playground doesnâ€™t support auto-complete, but itâ€™s still great when youâ€™re on the go or youâ€™d like to share your code with others.I find it quite useful for quickly jotting down a bunch of functions or types to test out a design idea.Itâ€™s okay to use  in the early stages of your project.
An explicit  is like a stop sign that tells you â€œhereâ€™s something you need to fix later.â€
You can easily grep for  and replace it with proper error handling later when you polish your code.
This way, you get the best of both worlds: quick iteration cycles and a clear path to robust error handling.
Thereâ€™s also a clippy lint that points out all the s in your code.See all those unwraps?
To more experienced Rustaceans, they stand out like a sore thumb â€“ and thatâ€™s a good thing!Compare that to languages like JavaScript which can throw exceptions your way at any time.
Itâ€™s much harder to ensure that you handle all the edge-cases correctly.
At the very least, it costs time. Time you could spend on more important things.While prototyping with Rust, you can safely ignore error handling and focus on
the happy path without losing track of improvement areas.I like to add  pretty early during the prototyping phase,
to get more fine-grained control over my error handling.
This way, I can use  and  to quickly add more context to my errors without losing momentum.
Later on, I can revisit each error case and see if I can handle it more gracefully.The great thing about  is that itâ€™s a solid choice for error handling in production code as well,
so you donâ€™t have to rewrite your error handling logic later on.There is great IDE support for Rust.IDEs can help you with code completion and refactoring, which keep you in the flow and help you write code faster.
Autocompletion is so much better with Rust than with dynamic languages because the type system gives the IDE a lot more information to work with.As a corollary to the previous section, be sure to use enable inlay hints (or inline type hints) in your editor.
This way, you can quickly see the inferred types right inside your IDE and make sure the types match your expectations.
Thereâ€™s support for this in most Rust IDEs, including RustRover and Visual Studio Code.Rust is not a scripting language; there is a compile step!However, for small projects, the compile times are negligible.
Unfortunately, you have to manually run  every time you make a change
or use rust-analyzer in your editor to get instant feedback.To fill the gap, you can use external tools like  which automatically recompiles and runs your code whenever you make a change.
This way, you can get  the same experience as with a REPL in, say, Python or Ruby.And just like that, you can get some pretty compilation output alongside your code editor.Oh, and in case you were wondering,  was another popular tool for
this purpose, but itâ€™s since been deprecated.Did you know that cargo can also run scripts?For example, put this into a file called :Now you can make the file executable with  and run it with  which it will compile and execute your code!
This allows you to quickly test out ideas without having to create a new project.
There is support for dependencies as well.At the moment,  is a nightly feature, but it will be released soon on stable Rust.
You can read more about it in the RFC.You have to try really really hard to write slow code in Rust.
Use that to your advantage: during the prototype phase, try to keep the code as simple as possible.Especially experienced developers coming from C or C++ are tempted to optimize too early.Rust makes code perform well by default - you get memory safety at virtually zero runtime cost. When developers try to optimize too early, they often run up against the borrow checker by using complex lifetime annotations and intricate reference patterns in pursuit of better performance.
This leads to harder-to-maintain code that may not actually run faster.Resist the urge to optimize too early!
You will thank yourself later. I find that printing values is pretty handy while prototyping.
Itâ€™s one less context switch to make compared to starting a debugger.Most people use  for that, but  has a few advantages:It prints the file name and line number where the macro is called. This helps you quickly find the source of the output.It outputs the expression as well as its value.Itâ€™s less syntax-heavy than ; e.g.  vs. .Where  really shines is in recursive functions or when you want to see the intermediate values during an iteration:The output is nice and tidy:Quite frankly, the type system is one of the main reasons I love Rust.
It feels great to express my ideas in types and see them come to life.
I would encourage you to heavily lean into the type system during the prototyping phase.In the beginning, you wonâ€™t have a good idea of the types in your system.
Thatâ€™s fine!
Start with  and quickly sketch out solutions and gradually add constraints to model the business requirements.
Donâ€™t stop until you find a version that feels just right.
You know youâ€™ve found a good abstraction when your types â€œclickâ€ with the rest of the code. 
Try to build up a vocabulary of concepts and own types which describe your system.Wrestling with Rustâ€™s type system might feel slower at first compared to more dynamic languages, but it often leads to fewer iterations overall.
Think of it this way: in a language like Python, each iteration might be quicker since you can skip type definitions, but youâ€™ll likely need more iterations as you discover edge cases and invariants that werenâ€™t immediately obvious.
In Rust, the type system forces you to think through these relationships up front. Although each iteration takes longer, you typically need fewer of them to arrive at a robust solution.This is exactly what weâ€™ll see in the following example.Say youâ€™re modeling course enrollments in a student system. You might start with something simple:But then requirements come in: some courses are very popular.
More students want to enroll than there are spots available,
so the school decides to add a waitlist.Easy, letâ€™s just add another boolean flag!The problem is that both boolean flags could be set to !
This design allows invalid states where a student could be both enrolled and waitlisted.Think for a second how we could leverage Rustâ€™s type system to make this impossibleâ€¦Now we have a clear distinction between an active enrollment and a waitlisted enrollment.
Whatâ€™s better is that we encapsulate the details of each state in the enum variants.
We can never have someone on the waitlist without a position in said list.Just think about how much more complicated this would be in a dynamic language
or a language that doesnâ€™t support tagged unions like Rust does.In summary, iterating on your data model is the crucial part of any prototyping phase.
The result of this phase is not the code, but a deeper understanding of the problem domain itself.
You can harvest this knowledge to build a more robust and maintainable solution.It turns out you can model a surprisingly large system in just a few lines of code.So, never be afraid to play around with types and refactor your code as you go.One of the cornerstones of prototyping is that you donâ€™t have to have all the answers right away.
In Rust, I find myself reaching for the  macro to
express that idea.I routinely just scaffold out the functions or a module and then fill in the blanks later.We did not do much here, but we have a clear idea of what the program should do.
Now we can go and iterate on the design.
For example, should  take a reference to the data?
Should we create a struct to hold the data and the processing logic?
How about using an iterator instead of a vector?
Should we introduce a trait to support algorithms for processing the data?These are all helpful questions that we can answer without having to worry about the details of the implementation.
And yet our code is typesafe and compiles, and it is ready for refactoring.On a related note, you can use the  macro to mark branches of your code that should never be reached.This is a great way to document your assumptions about the code.
The result is the same as if you had used , but itâ€™s more explicit about the fact that this branch should never be reached:Note that we added a message to the  macro to make it clear what the assumption is.Another way to document your assumptions is to use the  macro.
This is especially useful for invariants that should hold true at runtime.For example, the above code could be rewritten like this:During prototyping, this can be helpful to catch logic bugs early on without having to write a lot of tests
and you can safely carry them over to your production code.Consider using
 for
expensive invariant checks that should only run in test/debug builds.Chances are, you wonâ€™t know which parts of your application should be generic in the beginning.
Therefore itâ€™s better to be conservative and use concrete types instead of generics until necessary.So instead of writing this:If you need the same function for a different type, feel free to just copy and paste the function and change the type.
This way, you avoid the trap of settling on the wrong kind of abstraction too early.
Maybe the two functions only differ by type signature for now, but they might serve a completely different purpose.
If the function is not generic from the start, itâ€™s easier to remove the duplication later.Only introduce generics when you see a clear pattern emerge in multiple places.
I personally avoid generics up until the very last moment. I want to feel the â€œpainâ€ of duplication logic before I abstract it away.
In 50% of the cases, I find that the problem is not missing generics, but that thereâ€™s a better algorithm or data structure that solves the problem more elegantly.Also avoid â€œfancyâ€ generic type signatures:Yes, this allows you to pass in a  or a , but at the cost of readability.Just use an owned type for your first implementation:Chances are, you wonâ€™t need the flexibility after all.In summary, generics are powerful, but they can make the code harder to read and write.
Avoid them until you have a clear idea of what youâ€™re doing.One major blocker for rapid prototyping is Rustâ€™s ownership system.
If the compiler constantly reminds you of borrows and lifetimes it can ruin your flow.
For example, itâ€™s cumbersome to deal with references when youâ€™re just trying to get something to work.This code doesnâ€™t compile because the references are not valid outside of the function.A simple way around that is to avoid lifetimes altogether.
They are not necessary in the beginning.
Use owned types like  and .
Just  wherever you need to pass data around.If you have a type that you need to move between threads (i.e. it needs to be ), you can use an  to get around the borrow checker.
If youâ€™re worried about performance, remember that other languages like Python or Java do this implicitly behind your back.If you feel like you have to use  too often, there might be a design issue.
For example, you might be able to avoid sharing state between threads. is your best friend while prototyping.Stuff your code in there â€“ no need for modules or complex organization yet. This makes it easy to experiment and move things around.First draft: everything in main.rsOnce you have a better feel for your codeâ€™s structure, Rustâ€™s  keyword becomes a handy tool for sketching out potential organization. You can nest modules right in your main file.This inline module structure lets you quickly test different organizational patterns.
You can easily move code between scopes with cut and paste, and experiment with different APIs and naming conventions.
Once a particular structure feels right, you can move modules into their own files.The key is to keep things simple until it calls for more complexity.
Start flat, then add structure incrementally as your understanding of the problem grows.Allow yourself to ignore some of the best practices for production code for a while.Itâ€™s possible, but you need to switch off your inner critic who always wants to write perfect code from the beginning.
Rust enables you to comfortably defer perfection.
You can make the rough edges obvious so that you can sort them out later.
Donâ€™t let perfect be the enemy of good.One of the biggest mistakes I observe is an engineerâ€™s perfectionist instinct to jump on minor details which donâ€™t have a broad enough impact to warrant the effort.
Itâ€™s better to have a working prototype with a few rough edges than a perfect implementation of a small part of the system.Remember: you are exploring!
Use a coarse brush to paint the landscape first.
Try to get into a flow state where you can quickly iterate.
Donâ€™t get distracted by the details too early.
During this phase, itâ€™s also fine to throw away a lot failed attempts.Thereâ€™s some overlap between prototyping and â€œeasy Rust.â€The beauty of prototyping in Rust is that your â€œrough draftsâ€ have the same memory safety and performance as polished code.
Even when I liberally use , stick everything in , and reach for owned types everywhere, the resulting code
is on-par with a Python prototype in reliability, but outperforms it easily.
This makes it perfect for experimenting with real-world workloads, even before investing time in proper error handling.Letâ€™s see how Rust stacks up against Python for prototyping:Initial Development Speedâœ“ Very quick to write initial codeâœ“ No compilation stepâœ“ Dynamic typing speeds up prototypingâœ“ File watchers availableâš ï¸ Slightly slower initial developmentâœ“ Type inference helps provide quick feedbackâœ“ Batteries includedâœ“ Rich ecosystemâŒ Smaller standard libraryâœ“ Growing ecosystem of high-quality cratesâŒ Need extensive testing to catch type errorsâŒ Bad performance might require extra work or rewrite in another languageâœ“ Minimal changes needed beyond error handlingâœ“ Already has good performanceâœ“ Memory safety guaranteedâŒ Type errors surface during runtimeâŒ Refactoring is riskyâœ“ Compiler catches most issuesâœ“ Safe refactoring with type systemâŒ Hard to maintain large codebasesâŒ Type issues compoundâœ“ Compiler guides improvementsâœ“ Types help manage complexityQuite frankly, Rust makes for an excellent prototyping language if you embrace its strengths.
Yes, the type system will make you think harder about your design up front - but thatâ€™s actually a good thing!
Each iteration might take a bit longer than in Python or JavaScript, but youâ€™ll typically need fewer iterations from prototype to production.Iâ€™ve found that my prototypes in other languages often hit a wall where I need to switch to something more robust.
With Rust, I can start simple and gradually turn that proof-of-concept into production code, all while staying in the same language and ecosystem.If you have any more tips or tricks for prototyping in Rust, get in touch and Iâ€™ll add them to the list!]]></content:encoded></item><item><title>Patching 3rd party chart to support secrets - ideas</title><link>https://www.reddit.com/r/kubernetes/comments/1iaal1o/patching_3rd_party_chart_to_support_secrets_ideas/</link><author>/u/0x4ddd</author><category>dev</category><pubDate>Sun, 26 Jan 2025 09:53:16 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I need to install 3rd party Helm chart, unfortunately it expects some of the secret values (like API keys and database credentials) to be provided via plain Helm values. No secret support at all.This doesn't natively align very nicely with storing desired state in Git.What do you typically do in such scenario?utilize helm-secrets (we don't use it at the moment)create some ugly Kustomize patches to make it work with External Secret Operator (we are already using ESO for other charts to sync secrets from cloud KMS)   submitted by    /u/0x4ddd ]]></content:encoded></item><item><title>Show HN: Bagels â€“ TUI expense tracker</title><link>https://github.com/EnhancedJax/Bagels</link><author>EnhancedJax</author><category>dev</category><pubDate>Sun, 26 Jan 2025 08:57:51 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi! I'm Jax and I've been building this cool little terminal app for myself to track my expenses and budgets!Other than challenging myself to learn Python, I built this mainly around the habit of budget tracking at the end of the day. (I tried tracking on-the-go, but the balance was always out of sync.) All data is stored in a single sqlite file, so you can export and process them all you want!The app is built using the textual API for Python! Awesome framework which feels like I'm doing webdev haha.]]></content:encoded></item><item><title>New to Go: Built a Google Chat App with 150k Users â€“ Looking for Feedback on My Code</title><link>https://www.reddit.com/r/golang/comments/1ia9k9g/new_to_go_built_a_google_chat_app_with_150k_users/</link><author>/u/dyaskur</author><category>dev</category><pubDate>Sun, 26 Jan 2025 08:39:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Iâ€™m still relatively new to Go. Previously, I worked with PHP, JavaScript/TypeScript, and Python. I started learning Go by building a Google Chat app. My main focus was to make the app functional rather than strictly following best practices like clean code, as I had very limited time to develop it.The app is now released and has grown to 150k users! Initially, it only used the Google Translate API for translations. At the time, I assumed the free quota would be sufficient. However, the appâ€™s users quickly exceeded my expectations. In the first month alone, my billing increased by a staggering  due to the heavy google translate API usage.To address this, I switched to using  via  (where I have a subscription), as itâ€™s more cost-effective. The app still uses Google Translate API as a fallback when Straico returns invalid or error responses.Now, I want to add some new features to the app. But before that, Iâ€™m focusing on refactoring the existing code to improve its structure and maintainability. Iâ€™ve created a PR for this:https://github.com/dyaskur/google-chat-translator/pull/1 Iâ€™d love to get feedback or a review on my PR from experienced Go developers. Any advice or suggestions would be greatly appreciated!To be honest, Iâ€™m struggling with creating  and  in Go. Iâ€™m still in the process of learning how to do it effectively. If anyone has good references or resources on these topics, Iâ€™d be grateful for the guidance!Thank you all in advance, and Iâ€™m looking forward to your feedback! ðŸ˜Š]]></content:encoded></item><item><title>Oracle and US Investors (Including Microsoft) Discuss Taking Control of TikTok in the US</title><link>https://tech.slashdot.org/story/25/01/26/0037255/oracle-and-us-investors-including-microsoft-discuss-taking-control-of-tiktok-in-the-us?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><pubDate>Sun, 26 Jan 2025 02:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[A plan to keep TikTok available in the U.S. "involves tapping software company Oracle and a group of outside investors," reports NPR, "to effectively take control of the app's global operations, according to two people with direct knowledge of the talks..." 

"[P]otential investors who are engaged in the talks include Microsoft."

Under the deal now being negotiated by the White House, TikTok's China-based owner ByteDance would retain a minority stake in the company, but the app's algorithm, data collection and software updates will be overseen by Oracle, which already provides the foundation of TikTok's web infrastructure... "The goal is for Oracle to effectively monitor and provide oversight with what is going on with TikTok," said the person directly involved in the talks, who was not authorized to speak publicly about the deliberations. "ByteDance wouldn't completely go away, but it would minimize Chinese ownership...." Officials from Oracle and the White House held a meeting on Friday about a potential deal, and another meeting has been scheduled for next week, according to the source involved in the discussions, who said Oracle is interested in a TikTok stake "in the tens of billions," but the rest of the deal is in flux... 


Under a law passed by Congress and upheld by the Supreme Court, TikTok must execute what is known as "qualified divestiture" from ByteDance in order to stay in business in the U.S... A congressional staffer involved in talks about TikTok's future, who was not authorized to speak publicly, said binding legal agreements from the White House ensuring ByteDance cannot covertly manipulate the app will prove critical in winning lawmakers' approval. "A key part is showing there is no operational relationship with ByteDance, that they do not have control," the Congressional staffer said. "There needs to be no backdoors where China can potentially gain access...." 


Chinese regulators, who have for years opposed the selling of TikTok, recently signaled that they would not stand in the way of a TikTok ownership change, saying acquisitions "should be independently decided by the enterprises and based on market principles." The statement, at first, does not seem to say much, but negotiators in the White House believe it indicates that Beijing is not planning to block a deal that gives American investors a majority-stake position in the company.
 

"Meanwhile, Apple and Google still have not returned TikTok to app stores..."]]></content:encoded></item><item><title>Whatâ€™s Going on in the Containerd Neighborhood? - P. Estes, S. Karp, A. Suda, M. Brown, K. Ashok</title><link>https://www.youtube.com/watch?v=kCNhgNXVdxw</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><enclosure url="https://www.youtube.com/v/kCNhgNXVdxw?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 23:44:09 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Whatâ€™s Going on in the Containerd Neighborhood? - Phil Estes, AWS; Samuel Karp, Google; Akihiro Suda, NTT; Michael Brown, IBM; Kirtana Ashok, Microsoft

Our recent maintainer sessions have covered the soon-to-launch containerd v2.0. During this session led by maintainers we will give a brief update on 2.0, but will spend more time looking at the ecosystem around us. Why does containerd exist? What value does it bring to the overall cloud native world? How are other projects using it to build and extend containerd in useful ways? Weâ€™ll spend some time on containerdâ€™s largest subproject, nerdctl, which also has an upcoming 2.0 release, and additionally catch the community up on activity in our Rust subproject ecosystem, the runwasi containerd shim, and lazy loading snapshotters. Since this is KubeCon, weâ€™ll also provide an update on CRI changes and KEP-driven additions around NRI, DRA, and checkpoint/restore. Attendees will leave with a broad view of the larger containerd ecosystem of projects as well as key information on how to get involved if you are interested to help and contribute in any way to the â€œcontainerd neighborhood!â€]]></content:encoded></item><item><title>Welcome &amp; Introduction: A Hitchhiker&apos;s Guide to the CNCF Landscape- Katherine Druckman, Lori Lorusso</title><link>https://www.youtube.com/watch?v=hTCAOa0-VdU</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><enclosure url="https://www.youtube.com/v/hTCAOa0-VdU?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 23:37:56 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Welcome and Introduction: A Hitchhiker's Guide to the CNCF Landscape - Katherine Druckman and Lori Lorusso, CNCF Ambassador

â€œGet your hiking boots ready because we are about to traverse the wild, wonderful world of the CNCF Landscape. Why you ask? We currently have over 190 projects, and finding information about them can be a challenge. â€œJust go to the websiteâ€ isnâ€™t enough, sometimes you need a guide to show you the ropes. In these introductory sessions we will go over some of the diverse set of projects inside the CNCF so that youâ€™re well equipped to find what youâ€™re looking for at KubeCon.]]></content:encoded></item><item><title>Could New Linux Code Cut Data Center Energy Use By 30%?</title><link>https://hardware.slashdot.org/story/25/01/25/2111225/could-new-linux-code-cut-data-center-energy-use-by-30?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><pubDate>Sat, 25 Jan 2025 23:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Two computer scientists at the University of Waterloo in Canada believe changing 30 lines of code in Linux "could cut energy use at some data centers by up to 30 percent," according to the site Data Centre Dynamics. 

It's the code that processes packets of network traffic, and Linux "is the most widely used OS for data center servers," according to the article:

The team tested their solution's effectiveness and submitted it to Linux for consideration, and the code was published this month as part of Linux's newest kernel, release version 6.13. "All these big companies â€” Amazon, Google, Meta â€” use Linux in some capacity, but they're very picky about how they decide to use it," said Martin Karsten [professor of Computer Science in the Waterloo's Math Faculty]. "If they choose to 'switch on' our method in their data centers, it could save gigawatt hours of energy worldwide. Almost every single service request that happens on the Internet could be positively affected by this." 

The University of Waterloo is building a green computer server room as part of its new mathematics building, and Karsten believes sustainability research must be a priority for computer scientists. "We all have a part to play in building a greener future," he said. The Linux Foundation, which oversees the development of the Linux OS, is a founder member of the Green Software Foundation, an organization set up to look at ways of developing "green software" â€” code that reduces energy consumption. 

Karsten "teamed up with Joe Damato, distinguished engineer at Fastly" to develop the 30 lines of code, according to an announcement from the university. "The Linux kernel code addition developed by Karsten and Damato was based on research published in ACM SIGMETRICS Performance Evaluation Review" (by Karsten and grad student Peter Cai). 

Their paper "reviews the performance characteristics of network stack processing for communication-heavy server applications," devising an "indirect methodology" to "identify and quantify the direct and indirect costs of asynchronous hardware interrupt requests (IRQ) as a major source of overhead... 

"Based on these findings, a small modification of a vanilla Linux system is devised that improves the efficiency and performance of traditional kernel-based networking significantly, resulting in up to 45% increased throughput..."]]></content:encoded></item><item><title>Steam Brick: No screen, no controller, just a power button and a USB port</title><link>https://crastinator-pro.github.io/steam-brick/</link><author>sbarre</author><category>dev</category><pubDate>Sat, 25 Jan 2025 22:15:13 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Navigating the Cgroup Transition: Bridging the Gap Between Kubernetes and User Expec... S. Kunkerkar</title><link>https://www.youtube.com/watch?v=JWwwtW8Hbjs</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><enclosure url="https://www.youtube.com/v/JWwwtW8Hbjs?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 20:49:46 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Navigating the Cgroup Transition: Bridging the Gap Between Kubernetes and User Expectations - Sohan Kunkerkar, Red Hat Inc

As Kubernetes and container technologies evolve, shifting from cgroup v1 to cgroup v2 has become a pivotal development. With cgroup v2 available in Kubernetes since v1.25, we're at a crossroads where many users and organizations must decide when and how to transition fully to this new system. Despite the benefits of cgroup v2, including better resource management and enhanced capabilities, users frequently encounter unexpected challenges signaling a gap in readiness and understanding. This talk will address the practical implications of moving to cgroup v2, discuss the coordinated efforts to deprecate cgroup v1, and propose actionable strategies to bridge the gap between the Kubernetes community, system administrators, and developers. By focusing on real-world experiences and providing clear guidance, this session aims to equip you with the knowledge and tools to navigate this significant change confidently.]]></content:encoded></item><item><title>Lightning Talk: Minimizing Data Loss Within the OpenTelemetry (OTel) Collector - Alex Kats</title><link>https://www.youtube.com/watch?v=xxRkfVXdy9E</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><enclosure url="https://www.youtube.com/v/xxRkfVXdy9E?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 20:38:46 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Lightning Talk: Minimizing Data Loss Within the OpenTelemetry (OTel) Collector - Alex Kats, Capital One

The OTel collector is meant to serve as a reliable and highly performant data pipeline. However, as a single component in a wider observability architecture, it is only as reliable as the downstream platforms/services it exports data to. The OTel collector has several built in mechanisms that aim to minimize the impact of unhealthy downstream exporters, including an out of the box sending queue with an additional configuration parameter for persistent queueing. There is a new component in the OTel contrib distribution, the Failover Connector. The Failover Connector allows for dynamic routing or â€œfailoverâ€ of telemetry data based on downstream exporter health. This provides significant improvement to the data resiliency of the collector, as telemetry data can be continuously exported to a set of stable secondary locations, while the issues with the primary are resolved.]]></content:encoded></item><item><title>KubeSlice: Migrate Kubernetes Services With Confidence! | Project Lightning Talk</title><link>https://www.youtube.com/watch?v=Xkl7EnILe-o</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><enclosure url="https://www.youtube.com/v/Xkl7EnILe-o?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 20:21:25 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

KubeSlice: Migrate Kubernetes Services With Confidence! | Project Lightning Talk

SREs have been constantly asked to look for solutions to help them migrate K8S services from one Cloud cluster to another Cloud cluster while continuing to provide secure access to managed Cloud services left behind in the original Cloud.

The K8S services securely access these managed services using private endpoint FQDN. When SREs are asked to move the K8S services to a different Cloud cluster they hit a roadblock - there is no easy solution to provide private endpoint FQDN access to a managed service from a remote Cloud cluster.

CNCF sandbox project KubeSlice solves this use case in an elegant way.

KubeSlice enables SREs to create a Slice across clusters and slice overlay network connects services in the clusters. An external services gateway on the Slice in the original Cloud cluster will provide access to managed services via alias service FQDN import. Services in other clusters can reach the cloud service via the same private endpoint FQDN resolved by the Slice DNS.]]></content:encoded></item><item><title>Now You See Me: Tame MTTR with Real-Time Anomaly Dete... Kruthika Prasanna Simha &amp; Prschita Prschita</title><link>https://www.youtube.com/watch?v=Ipc0SOhB9OM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><enclosure url="https://www.youtube.com/v/Ipc0SOhB9OM?version=3" length="" type=""/><pubDate>Sat, 25 Jan 2025 19:43:00 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Now You See Me: Tame MTTR with Real-Time Anomaly Detection - Kruthika Prasanna Simha & Prschita Prschita, Apple Inc.

Picture this! You are running an application on a Kubernetes cluster & you notice that your nodes have been restarting and your users are noticing that your application is unreachable. As an engineer, you want to identify these failures in real-time & differentiate these from known states, at scale. But we know, static thresholds fail for dynamic metrics! This session explores real-time anomaly detection for cloud-native systems. We'll show you how to reduce MTTR and mean time to analyse by proactively identifying abnormal application behavior using statistical & machine learning algorithms on time series data from Prometheus. Learn to pinpoint issues, identify missing instrumentation, and visualize anomalies using Grafana. This session equips you to achieve faster issue resolution and maintain optimal application health. We'll demo practical techniques for metrics selection, anomaly detection and proactive issue identification to manage your cloud-native applications.]]></content:encoded></item><item><title>Every HTML Element</title><link>https://iamwillwang.com/dollar/every-html-element/</link><author>wxw</author><category>dev</category><pubDate>Sat, 25 Jan 2025 19:02:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
There are over a hundred HTML elements. This page uses all of them.
        You're looking at <p></p> right now.
Or smaller with <h2></h2>
And... nope that's it for headings. These are all in an <hgroup></hgroup> by the way.
      Some text is <pre></pre>-formatted. Just a different look.
    
We can make a list with <ul></ul> 
Or a numbered list with <ol></ol> 
Or even a description list with <dl></dl>.
 and description definitions, <dd></dd>.  <blockquote></blockquote> indents text. How credible!
<q></q> is block's little brother.
Here's a <figure></figure>... It usually contains some other media
        (like an image).

and this <figcaption></figcaption> can then describe said media.

And we can take a break with <hr />. 
We 
get a lot  with text  with elements like
<strong></strong>, <em></em>, and <mark></mark>.
<s></s> is for marking text as accurate... err never mind
 <bdo></bdo> 
Ah, now there's some breathing room, thanks to <br />.
You can <cite></cite> this website as Every HTML Element.And this whole thing is <code></code>,
living on the .
<time></time> too. It's 10:35 PM right now. might all be in your head. Well, actually it's
      in <kbd>.
 is defined. And  is a <var></var>iable.

How bout some hidden gems?
Here's a taste of a <samp></samp>
Here's an opportunity for a break <wbr/>.

The browser will decide when to take it.

All this text... where does it live? Well we're inside the <main></main> <body></body>
of an <html></html> document. There's a <head></head> here with us
      too. The
<head></head> thinks about things like <link>s to other things, <meta>data, our <style></style>, and our <title></title>.

By this point, I think it's clear that HTML elements run the gamut
            from content to container to logic to edge case. Some elements we
            can see, others not so much. Some elements are meant to combine with
            others. Some elements are only meant for your browser to read. Some
            elements had good intentions but never picked up mainstream use.

That concludes this section.

Sometimes it's more fun to listen than read. This is the sound of the
          website being made.
In other words, media! Media everywhere.The root element of the pageWithout this, you're not doing HTML.Now that would've been fun.Disclaimer: No web standards were harmed in the making of this
              table.
You can interact with HTML beyond a form too!

That concludes our journey! I lied a bit at the beginning. There are
        more elements than this: experimental ones, deprecated ones, web
        components, hydrogen, oxygen, and so on. Regardless, I hope this text
        got you a little hyper because, mark my words, you're speaking a new
        language now.
]]></content:encoded></item><item><title>OpenRA â€“ Classic strategy games rebuilt for the modern era</title><link>https://www.openra.net/</link><author>tosh</author><category>dev</category><pubDate>Sat, 25 Jan 2025 18:55:07 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Saboteurs can now cloak while movingadded delete area button to the map editorimproved Battlefield News dropdownimproved formatting of unit descriptions and encyclopediasfixed Dune 2000 not detecting installed music and videosfixed harvesters not always acting correctly with queued ordersfixed a bug where the game window could glitch out on Windows 10and fixed a few other minor bugsStay tuned for more updates and be sure to take part in the playtest. Donâ€™t forget to share your feedback with us on our forum, community Discord server, or GitHub. Good luck on the battlefield, Commanders, and happy holidays!Welcome back, Commanders!After a whole year of development, the OpenRA team proudly presents Playtest 20241116. This latest installment elevates your OpenRA experience with enhanced visuals, new tools and improved performance across the board.Introducing the newly revamped Map Editor, designed with a modern interface and a cohesive design. We took notice of how players use the editor and reimagined it to better align with your creative needs.New Map Editor features include:We added an encyclopedia to the Tiberian Dawn mod! This feature was inspired by the communityâ€™s desire for an in-game resource to consolidate knowledge about units, structures, and technologies. While fan-made wikis have been invaluable, many have become outdated or inconsistent as the game evolves.This latest release has significantly improved support for HD art assets. You should notice faster load times and overall performance improvements for the â€œTiberian Dawn HDâ€ project. It is packaged as a separate release and can be downloaded here. This preview is multiplayer-compatible with the main 20241116 playtest. The C&C Remastered Collection must be installed through Steam or the EA App, and if using macOS or Linux, the project README provides detailed installation instructions.Other notable changes include:game assets can now be installed from the Steam release of The Ultimate Collectionswitched to high-quality Dune 2000 assets and included lots of extra visual polishadded two Red Alert missions and improved the quality of many othersRed Alert and Dune 2000 balance changesskirmish options no longer reset between matchesbehind the scenes we made significant progress towards supporting additional languages in future releases.As always, the full changelog is available on GitHub if you would like to find out more.]]></content:encoded></item><item><title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL</title><link>https://arxiv.org/abs/2501.12948</link><author>gradus_ad</author><category>dev</category><pubDate>Sat, 25 Jan 2025 18:39:49 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CIA now favors lab leak theory to explain Covid&apos;s origins</title><link>https://www.nytimes.com/2025/01/25/us/politics/cia-covid-lab-leak.html</link><author>doctaj</author><category>dev</category><pubDate>Sat, 25 Jan 2025 18:16:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[You have a preview view of this article while we are checking your access. When we have confirmed access, the full article content will load.The C.I.A. has said for years that it did not have enough information to conclude whether the Covid pandemic emerged naturally from a wet market in Wuhan, China, or from an accidental leak at a research lab there.But the agency issued a new assessment this week, with analysts saying they now favor the lab theory.There is no new intelligence behind the agencyâ€™s shift, officials said. Rather it is based on the same evidence it has been chewing over for months.The analysis, however, is based in part on a closer look at the conditions in the high security labs in Wuhan province before the pandemic outbreak, according to people familiar with the agencyâ€™s work.A spokeswoman for the agency said the other theory remains plausible and that the agency will continue to evaluate any available credible new intelligence reporting.Some American officials say the debate matters little: The Chinese government failed to either regulate its markets or oversee its labs. But others argue it is an important intelligence and scientific question.Thank you for your patience while we verify access. If you are in Reader mode please exit andÂ log intoÂ your Times account, orÂ subscribeÂ for all of The Times.Thank you for your patience while we verify access.]]></content:encoded></item><item><title>The impact of competition and DeepSeek on Nvidia</title><link>https://youtubetranscriptoptimizer.com/blog/05_the_short_case_for_nvda</link><author>eigenvalue</author><category>dev</category><pubDate>Sat, 25 Jan 2025 15:30:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[As someone who spent ~10 years working as a generalist investment analyst at various long/short hedge funds (including stints at Millennium and Balyasny), while also being something of a math and computer nerd who has been studying deep learning since 2010 (back when Geoff Hinton was still talking about Restricted Boltzmann Machines and everything was still programmed using MATLAB, and researchers were still trying to show that they could get better results at classifying handwritten digits than by using Support Vector Machines), I'd like to think that I have a fairly unusual perspective on how AI technology is developing and how this relates to equity valuations in the stock market.For the past few years, I have been working more as a developer, and have several popular open-source projects for working with various forms of AI models/services (e.g., see LLM Aided OCR, Swiss Army Llama, Fast Vector Similarity, Source to Prompt, and Pastel Inference Layer for a few recent examples). Basically, I am using these frontier models all day, every day, in about as intense a way as possible. I have 3 Claude accounts so I don't run out of requests, and signed up for ChatGPT Pro within minutes of it being available.I also try to keep on top of the latest research advances, and carefully read all the major technical report papers that come out from the major AI labs. So I think I have a pretty good read on the space and how things are developing. At the same time, I've shorted a ton of stocks in my life and have won the best idea prize on the Value Investors Club twice (for TMS long and PDH short if you're keeping track at home).I say this not to brag, but rather to help establish my bona fides as someone who could opine on the subject without coming across as hopelessly naive to either technologists or professional investors. And while there are surely many people who know the math/science better, and people who are better at long/short investing in the stock market than me, I doubt there are very many who are in the middle of the Venn diagram to the extent I can claim to be.With all that said, whenever I meet with and chat with my friends and ex colleagues from the hedge fund world, the conversation quickly turns to Nvidia. It's not every day that a company goes from relative obscurity to being worth more than the combined stock markets of England, France, or Germany! And naturally, these friends want to know my thoughts on the subject. Because I am such a dyed-in-the-wool believer in the long term transformative impact of this technologyâ€” I truly believe it's going to radically change nearly every aspect of our economy and society in the next 5-10 years, with basically no historical precedentâ€” it has been hard for me to make the argument that Nvidia's momentum is going to slow down or stop anytime soon.But even though I've thought the valuation was just too rich for my blood for the past year or so, a confluence of recent developments has caused me to flip a bit to my usual instinct, which is to be a bit more contrarian in outlook and to question the consensus when it seems to be more than priced in. The saying "what the wise man believes in the beginning, the fool believes in the end" became famous for a good reason.Before we get into the developments that give me pause, let's pause to briefly review the bull case for NVDA shares, which is basically now known by everyone and his brother. Deep learning and AI are the most transformative technologies since the internet, and poised to change basically everything in our society. Nvidia has somehow ended up with something close to a monopoly in terms of the share of aggregate industry capex that is spent on training and inference infrastructure.Some of the largest and most profitable companies in the world, like Microsoft, Apple, Amazon, Meta, Google, Oracle, etc., have all decided that they must do and spend whatever it takes to stay competitive in this space because they simply cannot afford to be left behind. The amount of capex dollars, gigawatts of electricity used, square footage of new-build data centers, and, of course, the number of GPUs, has absolutely exploded and seems to show no sign of slowing down. And Nvidia is able to earn insanely high 90%+ gross margins on the most high-end, datacenter oriented products.We've just scratched the surface here of the bull case. There are many additional aspects to it now, which have made even people who were already very bullish to become incrementally more bullish. Besides things like the rise of humanoid robots, which I suspect is going to take most people by surprise when they are rapidly able to perform a huge number of tasks that currently require an unskilled (or even skilled) human worker (e.g., doing laundry, cleaning, organizing, and cooking; doing construction work like renovating a bathroom or building a house in a team of workers; running a warehouse and driving forklifts, etc.), there are other factors which most people haven't even considered.One major thing that you hear the smart crowd talking about is the rise of "a new scaling law," which has created a new paradigm thinking about how compute needs will increase over time. The original scaling law, which is what has been driving progress in AI since AlexNet appeared in 2012 and the Transformer architecture was invented in 2017, is the pre-training scaling law: that the more billions (and now trillions) worth of tokens we can use as training data, and the larger the parameter count of the models we are training, and the more FLOPS of compute that we expend on training those models on those tokens, the better the performance of the resulting models on a large variety of highly useful downstream tasks.Not only that, but this improvement is somewhat knowable, to the point where the leading AI labs like OpenAI and Anthropic have a pretty good idea of just how good their latest models would be even before they started the actual training runsâ€” in some cases, predicting the benchmarks of the final models to within a couple percentage points. This "original scaling law" has been vitally important, but always caused some doubts in the minds of people projecting the future with it.For one thing, we seem to have already exhausted the world's accumulated set of high quality training data. Of course, that's not literally trueâ€” there are still so many old books and periodicals that haven't yet been properly digitized, and even if they have, are not properly licensed for use as training data. The problem is that, even if you give credit for all that stuffâ€” say the sum total of "professionally" produced English language written content from the year 1500 to, say, the year 2000, it's not such a tremendous amount in percentage terms when you're talking about a training corpus of nearly 15 trillion tokens, which is the scale of current frontier models.For a quick reality check of those numbers: Google Books has digitized around 40mm books so far; if a typical book has 50k to 100k words, or 65k to 130k tokens, then that's between 2.6T and 5.2T tokens just from books, though surely a large chunk of that is already included in the training corpora used by the big labs, whether it's strictly legal or not. And there are lots of academic papers, with the arXiv website alone having over 2mm papers. And the Library of Congress has over 3 billion digitized newspaper pages. Taken together, that could be as much as 7T tokens in total, but since much of this is in fact included in training corpora, the remaining "incremental" training data probably isn't all that significant in the grand scheme of things.Of course, there are other ways to gather more training data. You could automatically transcribe every single YouTube video for example, and use that text. And while that might be helpful on the margin, it's certainly of much lower quality than, say, a highly respected textbook on Organic Chemistry as a source of useful knowledge about the world. So we've always had a looming "data wall" when it comes to the original scaling law; although we know we can keep shoveling more and more capex into GPUs and building more and more data centers, it's a lot harder to mass produce useful new human knowledge which is correct and incremental to what is already out there. Now, one intriguing response to this has been the rise of "synthetic data," which is text that is itself the output of an LLM. And while this seems almost nonsensical that it would work to "get high on your own supply" as a way of improving model quality, it actually seems to work very well in practice, at least in the domain of math, logic, and computer programming.The reason, of course, is that these are areas where we can mechanically check and prove the correctness of things. So we can sample from the vast universe of possible math theorems or possible Python scripts, and then actually check if they are correct, and only include them in our corpus if they are. And in this way, we can very dramatically expand our collection of high quality training data, at least in these kinds of areas.And then there are all the other kinds of data we could be training AI on besides text. For example, what if we take the entire whole genome sequencing (around 200 GB to 300 GB uncompressed for a single human being) for 100 million people? That's a  of data obviously, although the vast majority of it would be nearly identical between any two people. Of course, this could be misleading to compare to textual data from books and the internet for various reasons:Raw genome size isn't directly comparable to token countsThe information content of genomic data is very different from textThe training value of highly redundant data isn't clearThe computational requirements for processing genomic data are differentBut it's still another large source of diverse information that we could train huge models on in the future, which is why I included it.So while there is some hope in terms of being able to capture more and more additional training data, if you look at the rate at which training corpora have grown in recent years, it quickly becomes obvious that we are close to hitting a wall in terms of data availability for "generally useful" knowledge that can get us closer to the ultimate goal of getting artificial super-intelligence which is 10x smarter than John von Neumann and is an absolute world-class expert on every specialty known to man.Besides the limited amount of available data, there have always been a couple other things that have lurked in the back of the mind of proponents of the pre-training scaling law. A big one of these is, after you've finished training the model, what are you supposed to do with all that compute infrastructure? Train the next model? Sure, you can do that, but given the rapid improvement in GPU speed and capacity, and the importance of electricity and other opex in the economic calculations, does it even really make sense to use your 2 year old cluster to train your new model? Surely you'd rather use the brand new data center you just built that costs 10x the old data center and is 20x more powerful because of better technology. The problem is, at some point you do need to amortize the up-front cost of these investments and recoup it with a stream of (hopefully positive) operating profit, right?The market is so excited about AI that it has thankfully ignored this, allowing companies like OpenAI to post breathtaking from-inception, cumulative operating losses while garnering increasingly eye-popping valuations in follow-up investment rounds (although, to their credit, they have also been able to demonstrate very fast growing revenues). But eventually, for this situation to be sustainable over a full market cycle, these data center costs do need to eventually be recouped, hopefully with a profit, which over time is competitive with other investment opportunities on a risk-adjusted basis.OK, so that was the pre-training scaling law. What's this "new" scaling law? Well, that's something that people really just started focusing on in the past year: inference time compute scaling. Before, the vast majority of all the compute you'd expend in the process was the up-front training compute to create the model in the first place. Once you had the trained model, performing inference on that modelâ€” i.e., asking a question or having the LLM perform some kind of task for youâ€” used a certain, limited amount of compute.Critically, the total amount of inference compute (measured in various ways, such as FLOPS, in GPU memory footprint, etc.) was much, much less than what was required for the pre-training phase. Of course, the amount of inference compute does flex up when you increase the context window size of the models and the amount of output that you generate from them in one go (although researchers have made breathtaking algorithmic improvements on this front relative to the initial quadratic scaling people originally expected in scaling this up). But essentially, until recently, inference compute was generally a lot less intensive than training compute, and scaled basically linearly with the number of requests you are handlingâ€” the more demand for text completions from ChatGPT, for instance, the more inference compute you used up.With the advent of the revolutionary Chain-of-Thought ("COT") models introduced in the past year, most noticeably in OpenAI's flagship O1 model (but very recently in DeepSeek's new R1 model, which we will talk about later in much more detail), all that changed. Instead of the amount of inference compute being directly proportional to the length of the output text generated by the model (scaling up for larger context windows, model size, etc.), these new COT models also generate intermediate "logic tokens"; think of this as a sort of scratchpad or "internal monologue" of the model while it's trying to solve your problem or complete its assigned task.This represents a true sea change in how inference compute works: now, the more tokens you use for this internal chain of thought process, the better the quality of the final output you can provide the user. In effect, it's like giving a human worker more time and resources to accomplish a task, so they can double and triple check their work, do the same basic task in multiple different ways and verify that they come out the same way; take the result they came up with and "plug it in" to the formula to check that it actually does solve the equation, etc.It turns out that this approach works almost amazingly well; it is essentially leveraging the long anticipated power of what is called "reinforcement learning" with the power of the Transformer architecture. It directly addresses the single biggest weakness of the otherwise phenomenally successful Transformer model, which is its propensity to "hallucinate".Basically, the way Transformers work in terms of predicting the next token at each step is that, if they start out on a bad "path" in their initial response, they become almost like a prevaricating child who tries to spin a yarn about why they are actually correct, even if they should have realized mid-stream using common sense that what they are saying couldn't possibly be correct.Because the models are always seeking to be internally consistent and to have each successive generated token flow naturally from the preceding tokens and context, it's very hard for them to course-correct and backtrack. By breaking the inference process into what is effectively many intermediate stages, they can try lots of different things and see what's working and keep trying to course-correct and try other approaches until they can reach a fairly high threshold of confidence that they aren't talking nonsense.Perhaps the most extraordinary thing about this approach, beyond the fact that it works at all, is that the more logic/COT tokens you use, the better it works. Suddenly, you now have an additional dial you can turn so that, as you increase the amount of COT reasoning tokens (which uses a lot more inference compute, both in terms of FLOPS and memory), the higher the probability is that you will give a correct responseâ€” code that runs the first time without errors, or a solution to a logic problem without an obviously wrong deductive step.I can tell you from a lot of firsthand experience that, as good as Anthropic's Claude3.5 Sonnet model is at Python programmingâ€” and it is indeed VERY goodâ€” whenever you need to generate anything long and complicated, it invariably ends up making one or more stupid mistakes. Now, these mistakes are usually pretty easy to fix, and in fact you can normally fix them by simply feeding the errors generated by the Python interpreter, without any further explanation, as a follow-up inference prompt (or, more usefully, paste in the complete set of detected "problems" found in the code by your code editor, using what something called a Linter), it was still an annoying additional step. And when the code becomes very long or very complicated, it can sometimes take a lot longer to fix, and might even require some manual debugging by hand.The first time I tried the O1 model from OpenAI was like a revelation: I was amazed how often the code would be perfect the very first time. And that's because the COT process automatically finds and fixes problems before they ever make it to a final response token in the answer the model gives you.In fact, the O1 model used in OpenAI's ChatGPT Plus subscription for $20/month is basically the same model as the one used in the O1-Pro model featured in their new ChatGPT Pro subscription for 10x the price ($200/month, which raised plenty of eyebrows in the developer community); the main difference is that O1-Pro thinks for a lot longer before responding, generating vastly more COT logic tokens, and consuming a far larger amount of inference compute for every response.This is quite striking in that, even a very long and complex prompt for Claude3.5 Sonnet or GPT4o, with ~400kb+ of context given, generally takes less than 10 seconds to begin responding, and often less than 5 seconds. Whereas that same prompt to O1-Pro could easily take 5+ MINUTES before you get a response (although OpenAI does show you some of the "reasoning steps" that are generated during the process while you wait; critically, OpenAI has decided, presumably for trade secret related reasons,to hide from you the exact reasoning tokens it generates, showing you instead a highly abbreviated summary of these).As you can probably imagine, there are tons of contexts where accuracy is paramountâ€” where you'd rather give up and tell the user you can't do it at all rather than give an answer that could be trivially proven wrong or which involves hallucinated facts or otherwise specious reasoning. Anything involving money/transactions, medical stuff, legal stuff, just to name a few.Basically, wherever the cost of inference is trivial relative to the hourly all-in compensation of the human knowledge worker who is interacting with the AI system, that's a case where it become a complete no-brainer to dial up the COT compute (the major drawback is that it increases the latency of responses by a lot, so there are still some contexts where you might prefer to iterate faster by getting lower latency responses that are less accurate or correct).Some of the most exciting news in the AI world came out just a few weeks ago and concerned OpenAI's new unreleased O3 model, which was able to solve a large variety of tasks that were previously deemed to be out of reach of current AI approaches in the near term. And the way it was able to do these hardest problems (which include exceptionally tough "foundational" math problems that would be very hard for even highly skilled professional mathematicians to solve), is that OpenAI threw insane amount of compute resources at the problemsâ€” in some cases, spending $3k+ worth of compute power to solve a single task (compare this to traditional inference costs for a single task, which would be unlikely to exceed a couple dollars using regular Transformer models without chain-of-thought).It doesn't take an AI genius to realize that this development creates a new scaling law that is totally independent of the original pre-training scaling law. Now, you still want to train the best model you can by cleverly leveraging as much compute as you can and as many trillion tokens of high quality training data as possible, but that's just the beginning of the story in this new world; now, you could easily use incredibly huge amounts of compute just to do inference from these models at a very high level of confidence or when trying to solve extremely tough problems that require "genius level" reasoning to avoid all the potential pitfalls that would lead a regular LLM astray.But Why Should Nvidia Get to Capture All The Upside?Even if you believe, as I do, that the future prospects for AI are almost unimaginably bright, the question still remains, "Why should one company extract the majority of the profit pool from this technology?" There are certainly many historical cases where a very important new technology changed the world, but the main winners were not the companies that seemed the most promising during the initial stages of the process. The Wright Brothers' airplane company in all its current incarnations across many different firms today isn't worth more than $10b despite them inventing and perfecting the technology well ahead of everyone else. And while Ford has a respectable market cap of $40b today, it's just 1.1% of Nvidia's current market cap.To understand this, it's important to really understand why Nvidia is currently capturing so much of the pie today. After all, they aren't the only company that even makes GPUs. AMD makes respectable GPUs that, on paper, have comparable numbers of transistors, which are made using similar process nodes, etc. Sure, they aren't as fast or as advanced as Nvidia's GPUs, but it's not like the Nvidia GPUs are 10x faster or anything like that. In fact, in terms of naive/raw dollars per FLOP, AMD GPUs are something like half the price of Nvidia GPUs.Looking at other semiconductor markets such as the DRAM market, despite the fact that it is also very highly consolidated with only 3 meaningful global players (Samsung, Micron, SK-Hynix), gross margins in the DRAM market range from negative at the bottom of the cycle to ~60% at the very top of the cycle, with an average in the 20% range. Compare that to Nvidia's overall gross margin in recent quarters of ~75%, which is dragged down by the lower-margin and more commoditized consumer 3D graphics category.So how is this possible? Well, the main reasons have to do with softwareâ€” better drivers that "just work" on Linux and which are highly battle-tested and reliable (unlike AMD, which is notorious for the low quality and instability of their Linux drivers), and highly optimized open-source code in popular libraries such as PyTorch that has been tuned to work really well on Nvidia GPUs.It goes beyond that thoughâ€” the very programming framework that coders use to write low-level code that is optimized for GPUs, CUDA, is totally proprietary to Nvidia, and it has become a de facto standard. If you want to hire a bunch of extremely talented programmers who know how to make things go really fast on GPUs, and pay them $650k/year or whatever the going rate is for people with that particular expertise, chances are that they are going to "think" and work in CUDA.Besides software superiority, the other major thing that Nvidia has going for it is what is known as interconnectâ€” essentially, the bandwidth that connects together thousands of GPUs together efficiently so they can be jointly harnessed to train today's leading-edge foundational models. In short, the key to efficient training is to keep all the GPUs as fully utilized as possible all the timeâ€” not waiting around idling until they receive the next chunk of data they need to compute the next step of the training process.The bandwidth requirements are extremely highâ€” much, much higher than the typical bandwidth that is needed in traditional data center use cases. You can't really use traditional networking gear or fiber optics for this kind of interconnect, since it would introduce too much latency and wouldn't give you the pure terabytes per second of bandwidth that is needed to keep all the GPUs constantly busy.Nvidia made an incredibly smart decision to purchase the Israeli company Mellanox back in 2019 for a mere $6.9b, and this acquisition is what provided them with their industry leading interconnect technology. Note that interconnect speed is a lot more relevant to the training process, where you have to harness together the output of thousands of GPUs at the same time, than the inference process (including COT inference), which can use just a handful of GPUsâ€” all you need is enough VRAM to store the quantized (compressed) model weights of the already-trained model.So those are arguably the major components of Nvidia's "moat" and how it has been able to maintain such high margins for so long (there is also a "flywheel" aspect to things, where they aggressively invest their super-normal profits into tons of R&D, which in turn helps them improve their tech at a faster rate than the competition, so they are always in the lead in terms of raw performance).But as was pointed out earlier, what customers really tend to care about, all other things being equal, is performance per dollar (both in up-front capex cost of equipment and in energy usage, so performance per watt), and even though Nvidia's GPUs are certainly the fastest, they are not the best price/performance when measured naively in terms of FLOPS.But the thing is, all other things are NOT equal, and the fact that AMD's drivers suck, that popular AI software libraries don't run as well on AMD GPUs, that you can't find really good GPU experts who specialize in AMD GPUs outside of the gaming world (why would they bother when there is more demand in the market for CUDA experts?), that you can't wire thousands of them together as effectively because of lousy interconnect technology for AMDâ€” all this means that AMD is basically not competitive in the high-end data center world, and doesn't seem to have very good prospects for getting there in the near term.Well, that all sounds very bullish for Nvidia, right? Now you can see why the stock is trading at such a huge valuation! But what are the other clouds on the horizon? Well, there are few that I think merit significant attention. Some have been lurking in the background for the last few years, but too small to make a dent considering how quickly the pie has been growing, but where they are getting ready to potentially inflect upwards. Others are very recent developments (as in, the last 2 weeks) that might dramatically change the near-term trajectory of incremental GPU demand.At a very high level, you can think of things like this: Nvidia operated in a pretty niche area for a very long time; they had very limited competition, and the competition wasn't particular profitable or growing fast enough to ever pose a real threat, since they didn't have the capital needed to really apply pressure to a market leader like Nvidia. The gaming market was large and growing, but didn't feature earth shattering margins or particularly fabulous year over year growth rates.A few big tech companies started ramping up hiring and spending on machine learning and AI efforts around 2016-2017, but it was never a truly significant line item for any of them on an aggregate basisâ€” more of a "moonshot" R&D expenditure. But once the big AI race started in earnest with the release of ChatGPT in 2022â€” only a bit over 2 years ago, although it seems like a lifetime ago in terms of developmentsâ€” that situation changed very dramatically.Suddenly, big companies were ready to spend many, many billions of dollars incredibly quickly. The number of researchers showing up at the big research conferences like Neurips and ICML went up very, very dramatically. All the smart students who might have previously studied financial derivatives were instead studying Transformers, and $1mm+ compensation packages for non-executive engineering roles (i.e., for independent contributors not managing a team) became the norm at the leading AI labs.It takes a while to change the direction of a massive cruise ship; and even if you move really quickly and spend billions, it takes a year or more to build greenfield data centers and order all the equipment (with ballooning lead times) and get it all set up and working. It takes a long time to hire and onboard even smart coders before they can really hit their stride and familiarize themselves with the existing codebases and infrastructure.But now, you can imagine that absolutely biblical amounts of capital, brainpower, and effort are being expended in this area. And Nvidia has the biggest target of any player on their back, because they are the ones who are making the lion's share of the profits TODAY, not in some hypothetical future where the AI runs our whole lives.So the very high level takeaway is basically that "markets find a way"; they find alternative, radically innovative new approaches to building hardware that leverage completely new ideas to sidestep barriers that help prop up Nvidia's moat.The Hardware Level ThreatFor example, so-called "wafer scale" AI training chips from Cerebras, which dedicate an entire 300mm silicon wafer to an absolutely gargantuan chip that contains orders of magnitude more transistors and cores on a single chip (see this recent blog post from them explaining how they were able to solve the "yield problem" that had been preventing this approach from being economically practical in the past).To put this into perspective, if you compare Cerebras' newest WSE-3 chip to Nvidia's flagship data-center GPU, the H100, the Cerebras chip has a total die area of 46,225 square millimeters compared to just 814 for the H100 (and the H100 is itself considered an enormous chip by industry standards); that's a multiple of ~57x! And instead of having 132 "streaming multiprocessor" cores enabled on the chip like the H100 has, the Cerebras chip has ~900,000 cores (granted, each of these cores is smaller and does a lot less, but it's still an almost unfathomably large number in comparison). In more concrete apples-to-apples terms, the Cerebras chip can do around ~32x the FLOPS in AI contexts as a single H100 chip. Since an H100 sells for close to $40k a pop, you can imagine that the WSE-3 chip isn't cheap.So why does this all matter? Well, instead of trying to battle Nvidia head-on by using a similar approach and trying to match the Mellanox interconnect technology, Cerebras has used a radically innovative approach to do an end-run around the interconnect problem: inter-processor bandwidth becomes much less of an issue when everything is running on the same super-sized chip. You don't even need to have the same level of interconnect because one mega chip replaces tons of H100s.And the Cerebras chips also work extremely well for AI inference tasks. In fact, you can try it today for free here and use Meta's very respectable Llama-3.3-70B model. It responds basically instantaneously, at ~1,500 tokens per second. To put that into perspective, anything above 30 tokens per second feels relatively snappy to users based on comparisons to ChatGPT and Claude, and even 10 tokens per second is fast enough that you can basically read the response while it's being generated.Cerebras is also not alone; there are other companies, like Groq (not to be confused with the Grok model family trained by Elon Musk's X AI). Groq has taken yet another innovative approach to solving the same fundamental problem. Instead of trying to compete with Nvidia's CUDA software stack directly, they've developed what they call a "tensor processing unit" (TPU) that is specifically designed for the exact mathematical operations that deep learning models need to perform. Their chips are designed around a concept called "deterministic compute," which means that, unlike traditional GPUs where the exact timing of operations can vary, their chips execute operations in a completely predictable way every single time.This might sound like a minor technical detail, but it actually makes a massive difference for both chip design and software development. Because the timing is completely deterministic, Groq can optimize their chips in ways that would be impossible with traditional GPU architectures. As a result, they've been demonstrating for the past 6+ months inference speeds of over 500 tokens per second with the Llama series of models and other open source models, far exceeding what's possible with traditional GPU setups. Like Cerebras, this is available today and you can try it for free here.Using a comparable Llama3 model with "speculative decoding," Groq is able to generate 1,320 tokens per second, on par with Cerebras and far in excess of what is possible using regular GPUs. Now, you might ask what the point is of achieving 1,000+ tokens per second when users seem pretty satisfied with ChatGPT, which is operating at less than 10% of that speed. And the thing is, it does matter. It makes it a lot faster to iterate and not lose focus as a human knowledge worker when you get instant feedback. And if you're using the model programmatically via the API, which is increasingly where much of the demand is coming from, then it can enable whole new classes of applications that require multi-stage inference (where the output of previous stages is used as input in successive stages of prompting/inference) or which require low-latency responses, such as content moderation, fraud detection, dynamic pricing, etc.But even more fundamentally, the faster you can serve requests, the faster you can cycle things, and the busier you can keep the hardware. Although Groq's hardware is extremely expensive, clocking in at $2mm to $3mm for a single server, it ends up costing far less per request fulfilled if you have enough demand to keep the hardware busy all the time.And like Nvidia with CUDA, a huge part of Groq's advantage comes from their own proprietary software stack. They are able to take the same open source models that other companies like Meta, DeepSeek, and Mistral develop and release for free, and decompose them in special ways that allow them to run dramatically faster on their specific hardware.Like Cerebras, they have taken different technical decisions to optimize certain particular aspects of the process, which allows them to do things in a fundamentally different way. In Groq's case, it's because they are entirely focused on inference level compute, not on training: all their special sauce hardware and software only give these huge speed and efficiency advantages when doing inference on an already trained model.But if the next big scaling law that people are excited about is for inference level computeâ€” and if the biggest drawback of COT models is the high latency introduced by having to generate all those intermediate logic tokens before they can respondâ€” then even a company that only does inference compute, but which does it dramatically faster and more efficiently than Nvidia canâ€” can introduce a serious competitive threat in the coming years. At the very least, Cerebras and Groq can chip away at the lofty expectations for Nvidia's revenue growth over the next 2-3 years that are embedded in the current equity valuation.Besides these particularly innovative, if relatively unknown, startup competitors, there is some serious competition coming from some of Nvidia's biggest customers themselves who have been making custom silicon that specifically targets AI training and inference workloads. Perhaps the best known of these is Google, which has been developing its own proprietary TPUs since 2016. Interestingly, although it briefly sold TPUs to external customers, Google has been using all its TPUs internally for the past several years, and it is already on its 6th generation of TPU hardware.Amazon has also been developing its own custom chips called Trainium2 and Inferentia2. And while Amazon is building out data centers featuring billions of dollars of Nvidia GPUs, they are also at the same time investing many billions in other data centers that use these internal chips. They have one cluster that they are bringing online for Anthropic that features over 400k chips.Amazon gets a lot of flak for totally bungling their internal AI model development, squandering massive amounts of internal compute resources on models that ultimately are not competitive, but the custom silicon is another matter. Again, they don't necessarily need their chips to be better and faster than Nvidia's. What they need is for their chips to be good enough, but build them at a breakeven gross margin instead of the ~90%+ gross margin that Nvidia earns on its H100 business.OpenAI has also announced their plans to build custom chips, and they (together with Microsoft) are obviously the single largest user of Nvidia's data center hardware. As if that weren't enough, Microsoft have themselves announced their own custom chips!And Apple, the most valuable technology company in the world, has been blowing away expectations for years now with their highly innovative and disruptive custom silicon operation, which now completely trounces the CPUs from both Intel and AMD in terms of performance per watt, which is the most important factor in mobile (phone/tablet/laptop) applications. And they have been making their own internally designed GPUs and "Neural Processors" for years, even though they have yet to really demonstrate the utility of such chips outside of their own custom applications, like the advanced software based image processing used in the iPhone's camera.While Apple's focus seems somewhat orthogonal to these other players in terms of its mobile-first, consumer oriented, "edge compute" focus, if it ends up spending enough money on its new contract with OpenAI to provide AI services to iPhone users, you have to imagine that they have teams looking into making their own custom silicon for inference/training (although given their secrecy, you might never even know about it directly!).Now, it's no secret that there is a strong power law distribution of Nvidia's hyper-scaler customer base, with the top handful of customers representing the lion's share of high-margin revenue. How should one think about the future of this business when literally every single one of these VIP customers is building their own custom chips specifically for AI training and inference?When thinking about all this, you should keep one incredibly important thing in mind: Nvidia is largely an IP based company. They don't make their own chips. The true special sauce for making these incredible devices arguably comes more from TSMC, the actual fab, and ASML, which makes the special EUV lithography machines used by TSMC to make these leading-edge process node chips. And that's critically important, because TSMC will sell their most advanced chips to anyone who comes to them with enough up-front investment and is willing to guarantee a certain amount of volume. They don't care if it's for Bitcoin mining ASICs, GPUs, TPUs, mobile phone SoCs, etc.As much as senior chip designers at Nvidia earn per year, surely some of the best of them could be lured away by these other tech behemoths for enough cash and stock. And once they have a team and resources, they can design innovative chips (again, perhaps not even 50% as advanced as an H100, but with that Nvidia gross margin, there is plenty of room to work with) in 2 to 3 years, and thanks for TSMC, they can turn those into actual silicon using the exact same process node technology as Nvidia.As if these looming hardware threats weren't bad enough, there are a few developments in the software world in the last couple years that, while they started out slowly, are now picking up real steam and could pose a serious threat to the software dominance of Nvidia's CUDA. The first of these is the horrible Linux drivers for AMD GPUs. Remember we talked about how AMD has inexplicably allowed these drivers to suck for years despite leaving massive amounts of money on the table?Well, amusingly enough, the infamous hacker George Hotz (famous for jailbreaking the original iphone as a teenager, and currently the CEO of self-driving startup Comma.ai and AI computer company Tiny Corp, which also makes the open-source tinygrad AI software framework), recently announced that he was sick and tired of dealing with AMD's bad drivers, and desperately wanted to be able to to leverage the lower cost AMD GPUs in their TinyBox AI computers (which come in multiple flavors, some of which use Nvidia GPUs, and some of which use AMD GPUS).Well, he is making his own custom drivers and software stack for AMD GPUs without any help from AMD themselves; on Jan. 15th of 2025, he tweeted via his company's X account that "We are one piece away from a completely sovereign stack on AMD, the RDNA3 assembler. We have our own driver, runtime, libraries, and emulator. (all in ~12,000 lines!)" Given his track record and skills, it is likely that they will have this all working in the next couple months, and this would allow for a lot of exciting possibilities of using AMD GPUs for all sorts of applications where companies currently feel compelled to pay up for Nvidia GPUs.OK, well that's just a driver for AMD, and it's not even done yet. What else is there? Well, there are a few other areas on the software side that are a lot more impactful. For one, there is now a massive concerted effort across many large tech companies and the open source software community at large to make more generic AI software frameworks that have CUDA as just one of many "compilation targets".That is, you write your software using higher-level abstractions, and the system itself can automatically turn those high-level constructs into super well-tuned low-level code that works extremely well on CUDA. But because it's done at this higher level of abstraction, it can just as easily get compiled into low-level code that works extremely well on lots of other GPUs and TPUs from a variety of providers, such as the massive number of custom chips in the pipeline from every big tech company.The most famous examples of these frameworks are MLX (sponsored primarily by Apple), Triton (sponsored primarily by OpenAI), and JAX (developed by Google). MLX is particularly interesting because it provides a PyTorch-like API that can run efficiently on Apple Silicon, showing how these abstraction layers can enable AI workloads to run on completely different architectures. Triton, meanwhile, has become increasingly popular as it allows developers to write high-performance code that can be compiled to run on various hardware targets without having to understand the low-level details of each platform.These frameworks allow developers to write their code once using high powered abstractions and then target tons of platforms automaticallyâ€” doesn't that sound like a better way to do things, which would give you a lot more flexibility in terms of how you actually run the code?In the 1980s, all the most popular, best selling software was written in hand-tuned assembly language. The PKZIP compression utility for example was hand crafted to maximize speed, to the point where a competently coded version written in the standard C programming language and compiled using the best available optimizing compilers at the time, would run at probably half the speed of the hand-tuned assembly code. The same is true for other popular software packages like WordStar, VisiCalc, and so on.Over time, compilers kept getting better and better, and every time the CPU architectures changed (say, from Intel releasing the 486, then the Pentium, and so on), that hand-rolled assembler would often have to be thrown out and rewritten, something that only the smartest coders were capable of (sort of like how CUDA experts are on a different level in the job market versus a "regular" software developer). Eventually, things converged so that the speed benefits of hand-rolled assembly were outweighed dramatically by the flexibility of being able to write code in a high-level language like C or C++, where you rely on the compiler to make things run really optimally on the given CPU.Nowadays, very little new code is written in assembly. I believe a similar transformation will end up happening for AI training and inference code, for similar reasons: computers are good at optimization, and flexibility and speed of development is increasingly the more important factorâ€” especially if it also allows you to save dramatically on your hardware bill because you don't need to keep paying the "CUDA tax" that gives Nvidia 90%+ margins.Yet another area where you might see things change dramatically is that CUDA might very well end up being more of a high level abstraction itselfâ€” a "specification language" similar to Verilog (used as the industry standard to describe chip layouts) that skilled developers can use to describe high-level algorithms that involve massive parallelism (since they are already familiar with it, it's very well constructed, it's the lingua franca, etc.), but then instead of having that code compiled for use on Nvidia GPUs like you would normally do, it can instead be fed as source code into an LLM which can port it into whatever low-level code is understood by the new Cerebras chip, or the new Amazon Trainium2, or the new Google TPUv6, etc. This isn't as far off as you might think; it's probably already well within reach using OpenAI's latest O3 model, and surely will be possible generally within a year or two.Perhaps the most shocking development which was alluded to earlier happened in the last couple of weeks. And that is the news that has totally rocked the AI world, and which has been dominating the discourse among knowledgeable people on Twitter despite its complete absence from any of the mainstream media outlets: that a small Chinese startup called DeepSeek released two new models that have basically world-competitive performance levels on par with the best models from OpenAI and Anthropic (blowing past the Meta Llama3 models and other smaller open source model players such as Mistral). These models are called DeepSeek-V3 (basically their answer to GPT-4o and Claude3.5 Sonnet) and DeepSeek-R1 (basically their answer to OpenAI's O1 model).Why is this all so shocking? Well, first of all, DeepSeek is a tiny Chinese company that reportedly has under 200 employees. The story goes that they started out as a quant trading hedge fund similar to TwoSigma or RenTec, but after Xi Jinping cracked down on that space, they used their math and engineering chops to pivot into AI research. Who knows if any of that is really true or if they are merely some kind of front for the CCP or the Chinese military. But the fact remains that they have released two incredibly detailed technical reports, for DeepSeek-V3 and DeepSeekR1.These are heavy technical reports, and if you don't know a lot of linear algebra, you probably won't understand much. But what you should really try is to download the free DeepSeek app on the AppStore here and install it using a Google account to log in and give it a try (you can also install it on Android here), or simply try it out on your desktop computer in the browser here. Make sure to select the "DeepThink" option to enable chain-of-thought (the R1 model) and ask it to explain parts of the technical reports in simple terms.This will simultaneously show you a few important things:One, this model is absolutely legit. There is a lot of BS that goes on with AI benchmarks, which are routinely gamed so that models appear to perform great on the benchmarks but then suck in real world tests. Google is certainly the worst offender in this regard, constantly crowing about how amazing their LLMs are, when they are so awful in any real world test that they can't even reliably accomplish the simplest possible tasks, let alone challenging coding tasks. These DeepSeek models are not like thatâ€” the responses are coherent, compelling, and absolutely on the same level as those from OpenAI and Anthropic.Two, that DeepSeek has made profound advancements not just in model quality, but more importantly in model training and inference efficiency. By being extremely close to the hardware and by layering together a handful of distinct, very clever optimizations, DeepSeek was able to train these incredible models using GPUs in a dramatically more efficient way. By some measurements, over ~45x more efficiently than other leading-edge models. DeepSeek claims that the complete cost to train DeepSeek-V3 was just over $5mm. That is absolutely nothing by the standards of OpenAI, Anthropic, etc., which were well into the $100mm+ level for training costs for a single model as early as 2024.How in the world could this be possible? How could this little Chinese company completely upstage all the smartest minds at our leading AI labs, which have 100 times more resources, headcount, payroll, capital, GPUs, etc? Wasn't China supposed to be crippled by Biden's restriction on GPU exports? Well, the details are fairly technical, but we can at least describe them at a high level. It might have just turned out that the relative GPU processing poverty of DeepSeek was the critical ingredient to make them more creative and clever, necessity being the mother of invention and all.A major innovation is their sophisticated mixed-precision training framework that lets them use 8-bit floating point numbers (FP8) throughout the entire training process. Most Western AI labs train using "full precision" 32-bit numbers (this basically specifies the number of gradations possible in describing the output of an artificial neuron; 8 bits in FP8 lets you store a much wider range of numbers than you might expectâ€” it's not just limited to 256 different equal-sized magnitudes like you'd get with regular integers, but instead uses clever math tricks to store both very small and very large numbersâ€” though naturally with less precision than you'd get with 32 bits.) The main tradeoff is that while FP32 can store numbers with incredible precision across an enormous range, FP8 sacrifices some of that precision to save memory and boost performance, while still maintaining enough accuracy for many AI workloads.DeepSeek cracked this problem by developing a clever system that breaks numbers into small tiles for activations and blocks for weights, and strategically uses high-precision calculations at key points in the network. Unlike other labs that train in high precision and then compress later (losing some quality in the process), DeepSeek's native FP8 approach means they get the massive memory savings without compromising performance. When you're training across thousands of GPUs, this dramatic reduction in memory requirements per GPU translates into needing far fewer GPUs overall.Another major breakthrough is their multi-token prediction system. Most Transformer based LLM models do inference by predicting the next tokenâ€” one token at a time. DeepSeek figured out how to predict multiple tokens while maintaining the quality you'd get from single-token prediction. Their approach achieves about 85-90% accuracy on these additional token predictions, which effectively doubles inference speed without sacrificing much quality. The clever part is they maintain the complete causal chain of predictions, so the model isn't just guessingâ€” it's making structured, contextual predictions.One of their most innovative developments is what they call Multi-head Latent Attention (MLA). This is a breakthrough in how they handle what are called the Key-Value indices, which are basically how individual tokens are represented in the attention mechanism within the Transformer architecture. Although this is getting a bit too advanced in technical terms, suffice it to say that these KV indices are some of the major uses of VRAM during the training and inference process, and part of the reason why you need to use thousands of GPUs at the same time to train these modelsâ€” each GPU has a maximum of 96 gb of VRAM, and these indices eat that memory up for breakfast.Their MLA system finds a way to store a compressed version of these indices that captures the essential information while using far less memory. The brilliant part is this compression is built directly into how the model learnsâ€” it's not some separate step they need to do, it's built directly into the end-to-end training pipeline. This means that the entire mechanism is "differentiable" and able to be trained directly using the standard optimizers. All this stuff works because these models are ultimately finding much lower-dimensional representations of the underlying data than the so-called "ambient dimensions". So it's wasteful to store the full KV indices, even though that is basically what everyone else does.Not only do you end up wasting tons of space by storing way more numbers than you need, which gives a massive boost to the training memory footprint and efficiency (again, slashing the number of GPUs you need to train a world class model), but it can actually end up improving model quality because it can act like a "regularizer," forcing the model to pay attention to the truly important stuff instead of using the wasted capacity to fit to noise in the training data. So not only do you save a ton of memory, but the model might even perform better. At the very least, you don't get a massive hit to performance in exchange for the huge memory savings, which is generally the kind of tradeoff you are faced with in AI training.They also made major advances in GPU communication efficiency through their DualPipe algorithm and custom communication kernels. This system intelligently overlaps computation and communication, carefully balancing GPU resources between these tasks. They only need about 20 of their GPUs' streaming multiprocessors (SMs) for communication, leaving the rest free for computation. The result is much higher GPU utilization than typical training setups achieve.Another very smart thing they did is to use what is known as a Mixture-of-Experts (MOE) Transformer architecture, but with key innovations around load balancing. As you might know, the size or capacity of an AI model is often measured in terms of the number of parameters the model contains. A parameter is just a number that stores some attribute of the model; either the "weight" or importance a particular artificial neuron has relative to another one, or the importance of a particular token depending on its context (in the "attention mechanism"), etc.Meta's latest Llama3 models come in a few sizes, for example: a 1 billion parameter version (the smallest), a 70B parameter model (the most commonly deployed one), and even a massive 405B parameter model. This largest model is of limited utility for most users because you would need to have tens of thousands of dollars worth of GPUs in your computer just to run at tolerable speeds for inference, at least if you deployed it in the naive full-precision version. Therefore most of the real-world usage and excitement surrounding these open source models is at the 8B parameter or highly quantized 70B parameter level, since that's what can fit in a consumer-grade Nvidia 4090 GPU, which you can buy now for under $1,000.So why does any of this matter? Well, in a sense, the parameter count and precision tells you something about how much raw information or data the model has stored internally. Note that I'm not talking about reasoning ability, or the model's "IQ" if you will: it turns out that models with even surprisingly modest parameter counts can show remarkable cognitive performance when it comes to solving complex logic problems, proving theorems in plane geometry, SAT math problems, etc.But those small models aren't going to be able to necessarily tell you every aspect of every plot twist in every single novel by Stendhal, whereas the really big models can potentially do that. The "cost" of that extreme level of knowledge is that the models become very unwieldy both to train and to do inference on, because you always need to store every single one of those 405B parameters (or whatever the parameter count is) in the GPU's VRAM at the same time in order to do any inference with the model.The beauty of the MOE model approach is that you can decompose the big model into a collection of smaller models that each know different, non-overlapping (at least fully) pieces of knowledge. DeepSeek's innovation here was developing what they call an "auxiliary-loss-free" load balancing strategy that maintains efficient expert utilization without the usual performance degradation that comes from load balancing. Then, depending on the nature of the inference request, you can intelligently route the inference to the "expert" models within that collection of smaller models that are most able to answer that question or solve that task.You can loosely think of it as being a committee of experts who have their own specialized knowledge domains: one might be a legal expert, the other a computer science expert, the other a business strategy expert. So if a question comes in about linear algebra, you don't give it to the legal expert. This is of course a very loose analogy and it doesn't actually work like this in practice.The real advantage of this approach is that it allows the model to contain a huge amount of knowledge without being very unwieldy, because even though the aggregate number of parameters is high across all the experts, only a small subset of these parameters is "active" at any given time, which means that you only need to store this small subset of weights in VRAM in order to do inference. In the case of DeepSeek-V3, they have an absolutely massive MOE model with 671B parameters, so it's much bigger than even the largest Llama3 model, but only 37B of these parameters are active at any given timeâ€” enough to fit in the VRAM of two consumer-grade Nvidia 4090 GPUs (under $2,000 total cost), rather than requiring one or more H100 GPUs which cost something like $40k each.It's rumored that both ChatGPT and Claude use an MoE architecture, with some leaks suggesting that GPT-4 had a total of 1.8 trillion parameters split across 8 models containing 220 billion parameters each. Despite that being a lot more doable than trying to fit all 1.8 trillion parameters in VRAM, it still requires multiple H100-grade GPUs just to run the model because of the massive amount of memory used.Beyond what has already been described, the technical papers mention several other key optimizations. These include their extremely memory-efficient training framework that avoids tensor parallelism, recomputes certain operations during backpropagation instead of storing them, and shares parameters between the main model and auxiliary prediction modules. The sum total of all these innovations, when layered together, has led to the ~45x efficiency improvement numbers that have been tossed around online, and I am perfectly willing to believe these are in the right ballpark.One very strong indicator that it's true is the cost of DeepSeek's API: despite this nearly best-in-class model performance, DeepSeek charges something like 95% less money for inference requests via its API than comparable models from OpenAI and Anthropic. In a sense, it's sort of like comparing Nvidia's GPUs to the new custom chips from competitors: even if they aren't quite as good, the value for money is so much better that it can still be a no-brainer depending on the application, as long as you can qualify the performance level and prove that it's good enough for your requirements and the API availability and latency is good enough (thus far, people have been amazed at how well DeepSeek's infrastructure has held up despite the truly incredible surge of demand owing to the performance of these new models).But unlike the case of Nvidia, where the cost differential is the result of them earning monopoly gross margins of 90%+ on their data-center products, the cost differential of the DeepSeek API relative to the OpenAI and Anthropic API could be simply that they are nearly 50x more compute efficient (it might even be significantly more than that on the inference sideâ€” the ~45x efficiency was on the training side). Indeed, it's not even clear that OpenAI and Anthropic are making great margins on their API servicesâ€” they might be more interested in revenue growth and gathering more data from analyzing all the API requests they receive.Before moving on, I'd be remiss if I didn't mention that many people are speculating that DeepSeek is simply lying about the number of GPUs and GPU hours spent training these models because they actually possess far more H100s than they are supposed to have given the export restrictions on these cards, and they don't want to cause trouble for themselves or hurt their chances of acquiring more of these cards. While it's certainly possible, I think it's more likely that they are telling the truth, and that they have simply been able to achieve these incredible results by being extremely clever and creative in their approach to training and inference. They explain how they are doing things, and I suspect that it's only a matter of time before their results are widely replicated and confirmed by other researchers at various other labs.A Model That Can Really ThinkThe newer R1 model and technical report might even be even more mind blowing, since they were able to beat Anthropic to Chain-of-thought and now are basically the only ones besides OpenAI who have made this technology work at scale. But note that the O1 preview model was only released by OpenAI in mid-September of 2024. That's only ~4 months ago! Something you absolutely must keep in mind is that, unlike OpenAI, which is incredibly secretive about how these models really work at a low level, and won't release the actual model weights to anyone besides partners like Microsoft and other who sign heavy-duty NDAs, these DeepSeek models are both completely open-source and permissively licensed. They have released extremely detailed technical reports explaining how they work, as well as the code that anyone can look at and try to copy.With R1, DeepSeek essentially cracked one of the holy grails of AI: getting models to reason step-by-step without relying on massive supervised datasets. Their DeepSeek-R1-Zero experiment showed something remarkable: using pure reinforcement learning with carefully crafted reward functions, they managed to get models to develop sophisticated reasoning capabilities completely autonomously. This wasn't just about solving problemsâ€” the model organically learned to generate long chains of thought, self-verify its work, and allocate more computation time to harder problems.The technical breakthrough here was their novel approach to reward modeling. Rather than using complex neural reward models that can lead to "reward hacking" (where the model finds bogus ways to boost their rewards that don't actually lead to better real-world model performance), they developed a clever rule-based system that combines accuracy rewards (verifying final answers) with format rewards (encouraging structured thinking). This simpler approach turned out to be more robust and scalable than the process-based reward models that others have tried.What's particularly fascinating is that during training, they observed what they called an "aha moment," a phase where the model spontaneously learned to revise its thinking process mid-stream when encountering uncertainty. This emergent behavior wasn't explicitly programmed; it arose naturally from the interaction between the model and the reinforcement learning environment. The model would literally stop itself, flag potential issues in its reasoning, and restart with a different approach, all without being explicitly trained to do this.The full R1 model built on these insights by introducing what they call "cold-start" dataâ€” a small set of high-quality examplesâ€” before applying their RL techniques. They also solved one of the major challenges in reasoning models: language consistency. Previous attempts at chain-of-thought reasoning often resulted in models mixing languages or producing incoherent outputs. DeepSeek solved this through a clever language consistency reward during RL training, trading off a small performance hit for much more readable and consistent outputs.The results are mind-boggling: on AIME 2024, one of the most challenging high school math competitions, R1 achieved 79.8% accuracy, matching OpenAI's O1 model. On MATH-500, it hit 97.3%, and it achieved the 96.3 percentile on Codeforces programming competitions. But perhaps most impressively, they managed to distill these capabilities down to much smaller models: their 14B parameter version outperforms many models several times its size, suggesting that reasoning ability isn't just about raw parameter count but about how you train the model to process information.The recent scuttlebutt on Twitter and Blind (a corporate rumor website) is that these models caught Meta completely off guard and that they perform better than the new Llama4 models which are still being trained. Apparently, the Llama project within Meta has attracted a lot of attention internally from high-ranking technical executives, and as a result they have something like 13 individuals working on the Llama stuff who each individually earn more per year in total compensation than the combined training cost for the DeepSeek-V3 models which outperform it. How do you explain that to Zuck with a straight face? How does Zuck keep smiling while shoveling multiple billions of dollars to Nvidia to buy 100k H100s when a better model was trained using just 2k H100s for a bit over $5mm?But you better believe that Meta and every other big AI lab is taking these DeepSeek models apart, studying every word in those technical reports and every line of the open source code they released, trying desperately to integrate these same tricks and optimizations into their own training and inference pipelines. So what's the impact of all that? Well, naively it sort of seems like the aggregate demand for training and inference compute should be divided by some big number. Maybe not by 45, but maybe by 25 or even 30? Because whatever you thought you needed before these model releases, it's now a lot less.Now, an optimist might say "You are talking about a mere constant of proportionality, a single multiple. When you're dealing with an exponential growth curve, that stuff gets washed out so quickly that it doesn't end up matter all that much." And there is some truth to that: if AI really is as transformational as I expect, if the real-world utility of this tech is measured in the trillions, if inference-time compute is the new scaling law of the land, if we are going to have armies of humanoid robots running around doing massive amounts of inference constantly, then maybe the growth curve is still so steep and extreme, and Nvidia has a big enough lead, that it will still work out.But Nvidia is pricing in a LOT of good news in the coming years for that valuation to make sense, and when you start layering all these things together into a total mosaic, it starts to make me at least feel extremely uneasy about spending ~20x the 2025 estimated sales for their shares. What happens if you even see a slight moderation in sales growth? What if it turns out to be 85% instead of over 100%? What if gross margins come in a bit from 75% to 70%â€” still ridiculously high for a semiconductor company?At a high level, NVIDIA faces an unprecedented convergence of competitive threats that make its premium valuation increasingly difficult to justify at 20x forward sales and 75% gross margins. The company's supposed moats in hardware, software, and efficiency are all showing concerning cracks. The whole worldâ€” thousands of the smartest people on the planet, backed by untold billions of dollars of capital resourcesâ€” are trying to assail them from every angle.On the hardware front, innovative architectures from Cerebras and Groq demonstrate that NVIDIA's interconnect advantageâ€” a cornerstone of its data center dominanceâ€” can be circumvented through radical redesigns. Cerebras' wafer-scale chips and Groq's deterministic compute approach deliver compelling performance without needing NVIDIA's complex interconnect solutions. More traditionally, every major NVIDIA customer (Google, Amazon, Microsoft, Meta, Apple) is developing custom silicon that could chip away at high-margin data center revenue. These aren't experimental projects anymoreâ€” Amazon alone is building out massive infrastructure with over 400,000 custom chips for Anthropic.The software moat appears equally vulnerable. New high-level frameworks like MLX, Triton, and JAX are abstracting away CUDA's importance, while efforts to improve AMD drivers could unlock much cheaper hardware alternatives. The trend toward higher-level abstractions mirrors how assembly language gave way to C/C++, suggesting CUDA's dominance may be more temporary than assumed. Most importantly, we're seeing the emergence of LLM-powered code translation that could automatically port CUDA code to run on any hardware target, potentially eliminating one of NVIDIA's strongest lock-in effects.Perhaps most devastating is DeepSeek's recent efficiency breakthrough, achieving comparable model performance at approximately 1/45th the compute cost. This suggests the entire industry has been massively over-provisioning compute resources. Combined with the emergence of more efficient inference architectures through chain-of-thought models, the aggregate demand for compute could be significantly lower than current projections assume. The economics here are compelling: when DeepSeek can match GPT-4 level performance while charging 95% less for API calls, it suggests either NVIDIA's customers are burning cash unnecessarily or margins must come down dramatically.The fact that TSMC will manufacture competitive chips for any well-funded customer puts a natural ceiling on NVIDIA's architectural advantages. But more fundamentally, history shows that markets eventually find a way around artificial bottlenecks that generate super-normal profits. When layered together, these threats suggest NVIDIA faces a much rockier path to maintaining its current growth trajectory and margins than its valuation implies. With five distinct vectors of attackâ€” architectural innovation, customer vertical integration, software abstraction, efficiency breakthroughs, and manufacturing democratizationâ€” the probability that at least one succeeds in meaningfully impacting NVIDIA's margins or growth rate seems high. At current valuations, the market isn't pricing in any of these risks.I hope you enjoyed reading this article. If you work at a hedge fund and are interested in consulting with me on NVDA or other AI-related stocks or investing themes, I'm already signed up as an expert on GLG and Coleman Research.]]></content:encoded></item><item><title>Show HN: I built a DIY plane spotting system at home</title><link>https://pilane.obviy.us/</link><author>obviyus</author><category>dev</category><pubDate>Sat, 25 Jan 2025 13:14:37 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>