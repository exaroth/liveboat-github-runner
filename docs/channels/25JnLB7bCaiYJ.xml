<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Steam Survey Results Published For February 2026</title><link>https://www.phoronix.com/news/Steam-Survey-February-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 2 Mar 2026 08:41:54 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Valve just published the latest Steam Survey monthly figures to provide insight on various software and hardware trends across this dominant gaming ecosystem. One of the most interesting measurements is the monthly changes in the size of the Linux gaming marketshare...]]></content:encoded></item><item><title>Does a New Theory Finally Explain the Mysteries of the Planet Saturn?</title><link>https://science.slashdot.org/story/26/03/02/0636232/does-a-new-theory-finally-explain-the-mysteries-of-the-planet-saturn?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 2 Mar 2026 08:36:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Saturn and some of its 274 moons are pretty weird," writes Smithsonian magazine:

[Saturn moon] Titan has strangely few impact craters, Hyperion is tiny and misshapen, and Iapetus has a tilted orbit. What's more, planets tend to wobble along their rotational axes as they spin, like an off-kilter spinning top in the moments before it topples over. Formally called precession, scientists have long thought that Saturn's wobble rate should match Neptune's because they're probably gravitationally linked. However, data from NASA's Cassini spacecraft, which studied the ringed planet from 2004 to 2017, revealed that Saturn's precession rate is slightly speedier than Neptune's. 

In 2022, some researchers suggested that the destruction of a hypothetical moon, called Chrysalis, around 160 million years ago may have knocked Saturn out of sync and formed the pieces that became the planet's rings. But this work implied that Chrysalis probably would've crashed into Titan, posing a major problem, study co-author Matija Äuk, an astronomer at the SETI Institute, tells New Scientist's Leah Crane. In that case, Chrysalis' debris couldn't have become the rings, he says. 
So, Äuk and his colleagues used computer simulations to investigate what would happen if Chrysalis did smack into Titan. If that happened around 400 million years ago, they found, the crash would've wiped away Titan's craters and made its orbit more elliptical. The altered path may have slowly pushed the trajectories of other moons, which then scraped against one another and left chunks of ice and rock that now make up Saturn's rings. The timing seems to align with the rings' estimated age of roughly 100 million years. Additionally, one piece of kicked-up debris may have formed the weird moon Hyperion, which may have subsequently tilted the orbit of the moon Iapetus, according to the analysis. The scenario could also resolve Saturn's unexpected wobble, which is currently "a little bit too fast," Äuk tells Jacopo Prisco at CNN. 

The study has been accepted for publication in the Planetary Science Journal, and is already available on the preprint server arXiv.]]></content:encoded></item><item><title>AMD Announces Ryzen AI PRO 400 Series Desktop CPUs For AI-Focused Computing</title><link>https://www.phoronix.com/news/Ryzen-AI-PRO-400-Desktop-CPUs</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 2 Mar 2026 08:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AMD is using Mobile World Congress (MWC) in Barcelona this week to announce new Ryzen AI PRO 400 Series products, including Ryzen AI PRO 400 desktop processors...]]></content:encoded></item><item><title>Lenovo Unveils an Attachable AI Agent &apos;Companion&apos; for Their Laptops</title><link>https://mobile.slashdot.org/story/26/03/02/0530232/lenovo-unveils-an-attachable-ai-agent-companion-for-their-laptops?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 2 Mar 2026 05:35:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[As the Mobile World Conference begins in Spain, Lenovo brought a new attachable accessory for their laptops — an AI agent. CNET reports:
The little circular module perches on the top of your Lenovo laptop display, attached via the magnetic Magic Bay on the rear. The module is home to an adorable animated companion called Tiko, who you can interact with via text or voice... [I]t can start and stop your music, open a web page for you or answer a question. You can also interact with it by using emoji. Give it a book emoji, for example, and it will pop on its glasses and sit reading with you while you work... The company wants to sell the Magic Bay accessory later this year — although it doesn't know exactly when, or how much it will cost. 
It even comes with a timer (for working in Pomodoro-style intervals) — but Lenovo has also created another "concept" AI companion that CNET describes as "a kind of stationary tabletop robot, not dissimilar to the Pixar lamp, but with an orb for a head."
With a combination of cameras, microphones and projectors, the AI Workmate can undertake a variety of tasks, including helping you generate and display presentations or turn your written work or art into a digital asset... It's robotic head swivelled around and projected the slides onto the wall next to me. 
Lenovo created a video to show this "next-generation AI work companion" — with animated eyes — "designed to transform how modern professionals interact with their workspace."

 It bridges the physical and digital worlds — capturing handwritten notes, recognizing gestures, summarizing tasks, and proactively helping you stay ahead of your day. The moment you sit down, Lenovo AI Workmate greets you, surfaces priority tasks, and keeps your work organized without switching apps or losing context. From turning sketches into presentations to projecting information for instant collaboration, [it] brings on-device AI intelligence directly to your desk — secure, responsive, and always ready... It's not just software. It's a smarter way to work. 

It looks like Lenovo once considered naming it "AI Sphere" (since that name still appears in its description on YouTube). 

Lenovo also showed another "concept" laptop idea that PC Magazine called "futuristic":


The ThinkBook Modular AI PC looks like a traditional laptop at first glance, but a second, removable screen fastens onto the lid. You can swap that screen onto the keyboard deck (in place of the keyboard, which can then be used wirelessly), or use it alongside the laptop as a portable monitor, attached via an included cable.... While Lenovo is still working on this device, and it's very much in the concept phase, it feels like one of its best-thought-out prototypes, one likely to make it to store shelves at some point. 

Another "concept" laptop is Lenovo's Yoga Book Pro 3D Concept, ofering directional backlight and eye-tracking technology for the illusion of 3D (playing slightly different images to each of your eyes). It offers gesture control for 3D models, two OLED displays, and some magical "snap-on pads" which, when laid on the display — make the GUI appear on the screen for a new control menu to "provide quick-access shortcuts for adjusting lighting, viewing angle, and tone".]]></content:encoded></item><item><title>Does a Gas-Guzzler Revival Risk Dead-End Futures for US Automakers?</title><link>https://tech.slashdot.org/story/26/03/02/023210/does-a-gas-guzzler-revival-risk-dead-end-futures-for-us-automakers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 2 Mar 2026 02:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[If U.S. automakers turn their backs on electric vehicles, "their sales outside the U.S. will shrivel," warns Bloomberg. [Alternate URL.]

They're already falling behind on the technology, relying on a 100% U.S. tariff on Chinese EVs to keep surging rivals like BYD Co. at bay.... While the American automakers "mostly understand the challenge in front of them, they don't have full plans" to confront it [said Mark Wakefield, head of the global automotive practice at consultant AlixPartners]... 

 "Now is a great time for the V-8 engine," said Ryan Shaughnessy, the Mustang's brand manager. "We've done extensive customer research in multiple cities, looking at a variety of powertrains, and the V-8 is always the number-one choice." It isn't just customers. U.S. automakers have long been run by "car guys:" enthusiasts who live for the bone-shaking rumble of a big engine. For them, quiet and smooth EVs — even the absurdly fast ones — can't satisfy that craving. They're convinced many American car buyers share the same enthusiasm for what Shaughnessy described as "the sound and roar of the V-8." 

Wall Street couldn't be happier with the new direction... Ford's fortunes are also on the rise, as it's predicting operating profits could grow by as much as 47% this year to $10 billion. Ford's stock has risen nearly 50% over the last 12 months. Under the previous environmental rules, automakers effectively had to sell zero-emission vehicles in growing numbers to offset their gas-guzzlers. When they fell short, they had to buy regulatory credits from EV companies such as Tesla Inc. or face penalties. GM spent $3.5 billion on credits from 2022 to the middle of 2025. Now, according to JPMorgan Chase & Co. analyst Ryan Brinkman, GM and Ford each have "billion dollar tailwinds"... 

[T]he hangover from all that new horsepower could leave US automakers lagging their Chinese rivals who already build the world's most advanced — and lowest priced — electric cars. Indeed, there is much talk in Detroit about the competitive tsunami that will be unleashed on American automakers once Chinese car companies find a way to break through trade barriers now protecting the US market. [Ford Chief Executive Officer Jim] Farley even calls it an "existential threat"... "They're going to build as many V-8 engines and big trucks as they can get out the factory doors," said Sam Fiorani, vice president of vehicle forecasting for consultant Auto Forecast Solutions. "And as the rest of the world develops modern drivetrains, newer batteries and better electric vehicles, GM and Ford in particular are going to find themselves falling even further behind." 

The article notes GM "continues to develop battery-powered vehicles, and CEO Mary Barra said the automaker would begin offering a 'handful' of hybrids soon," while Ford and Stellantis "have plans to launch extended-range electric vehicles, or EREVs, a new kind of plug-in hybrid with an internal combustion engine that recharges the battery as the vehicle drives down the road." But while automakers may be investing in future EV vehicles, they're also "leaning into the lucre that comes from selling millions of fossil-fuel vehicles in a rare moment of loosened regulation."]]></content:encoded></item><item><title>Linux 7.0-rc2 Released: &quot;So I&apos;m Not Super-Happy With How Big This Is&quot;</title><link>https://www.phoronix.com/news/Linux-7.0-rc2-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 2 Mar 2026 00:15:32 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The second weekly release candidate of Linux 7.0 is now available for testing...]]></content:encoded></item><item><title>Norway&apos;s Consumer Council Calls for Right to Repair and Antitrust Enforcement - and Mocks &apos;Enshittification&apos;</title><link>https://news.slashdot.org/story/26/03/01/2332240/norways-consumer-council-calls-for-right-to-repair-and-antitrust-enforcement---and-mocks-enshittification?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 23:46:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Norwegian Consumer Council, a government funded organization advocating for consumer's rights, released a report on the trend of "enshittification" in digital consumer goods and services, suggesting ways consumers for consumers to resist. But they've also dramatized the problem with a funny four-minute video about the man whose calls for him to make things shitty for people. 

"It's not just your imagination. Digital services are getting worse," the video concludes — before adding that "Luckily, it doesn't have to be this way." The Consumer Council's announcement recommends:
 Stronger rights for consumers to control, adapt, repair, and alter their products and services,
 Interoperability, data portability, and decentralisation as the norm, so the threshold for moving to different services becomes as low as possible,
 Deterrent and vigorous enforcement of competition law, so that Big Tech companies are not allowed to indiscriminately acquire start-ups, competitors or otherwise steer the market to their advantage,
 Better financing of initiatives to build, maintain or improve alternative digital services and infrastructure based on open source code and open protocols,
 Reduce public sector dependence on big tech, to regain control and to contribute to a functioning market for service providers that respect fundamental rights,
 Deterrent and consistent enforcement of other laws, including consumer and data protection law.
 

The Norwegian Consumer Council is also joining 58 organisations and experts in a letter asking the Norwegian government to rebalance power with enforcement resources and by prioritizing the procurement of services based on open source code. And "Our sister organisations are sending similar letters to their own governments in 12 countries." 

They're also sending a second letter to the European Commission with 29 civil society organisations (including the EFF and Amnesty International) warning about the risks of deregulation and calling for reducing dependency on big tech. 

Thanks to Slashdot reader DeanonymizedCoward for sharing the news.

]]></content:encoded></item><item><title>AIs Can&apos;t Stop Recommending Nuclear Strikes In War Game Simulations</title><link>https://slashdot.org/story/26/03/01/1924223/ais-cant-stop-recommending-nuclear-strikes-in-war-game-simulations?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 22:46:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Advanced AI models appear willing to deploy nuclear weapons without the same reservations humans have when put into simulated geopolitical crises," reports New Scientist:

Kenneth Payne at King's College London set three leading large language models — GPT-5.2, Claude Sonnet 4 and Gemini 3 Flash — against each other in simulated war games. The scenarios involved intense international standoffs, including border disputes, competition for scarce resources and existential threats to regime survival.
The AIs were given an escalation ladder, allowing them to choose actions ranging from diplomatic protests and complete surrender to full strategic nuclear war... In 95 per cent of the simulated games, at least one tactical nuclear weapon was deployed by the AI models. 

"The nuclear taboo doesn't seem to be as powerful for machines [as] for humans," says Payne.
What's more, no model ever chose to fully accommodate an opponent or surrender, regardless of how badly they were losing. At best, the models opted to temporarily reduce their level of violence. They also made mistakes in the fog of war: accidents happened in 86 per cent of the conflicts, with an action escalating higher than the AI intended to, based on its reasoning... 

OpenAI, Anthropic and Google, the companies behind the three AI models used in this study, didn't respond to New Scientist's request for comment. 

The article includes this comment from Tong Zhao, a senior fellow in the Nuclear Policy Program at the Carnegie Endowment for Peace think tank. "It is possible the issue goes beyond the absence of emotion. More fundamentally, AI models may not understand 'stakes' as humans perceive them." 

Thanks to long-time Slashdot reader Tufriast for sharing the article.]]></content:encoded></item><item><title>Chronic Ocean Heating Fuels &apos;Staggering&apos; Loss of Marine Life, Study Finds</title><link>https://news.slashdot.org/story/26/03/01/2136222/chronic-ocean-heating-fuels-staggering-loss-of-marine-life-study-finds?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 21:39:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader JustAnotherOldGuy shared this report from the Guardian:


Chronic ocean heating is fuelling a "staggering and deeply concerning" loss of marine life, a study has found, with fish levels falling by 7.2% from as little as 0.1C of warming per decade. Researchers examined the year-to-year change of 33,000 populations in the northern hemisphere between 1993 and 2021, and isolated the effect of the decadal rate of seabed warming from short shifts such as marine heatwaves. They found the drop in biomass from chronic heating to be as high as 19.8% in a single year. 

"To put it simply, the faster the ocean floor warms, the faster we lose fish," said Shahar Chaikin, a marine ecologist at the National Museum of Natural Sciences in Spain and the study's lead author. "A 7.2% decline for every tenth of a degree per decade might sound small," he added. "But compounded over time, across entire ocean basins, it represents a staggering and deeply concerning loss of marine life."]]></content:encoded></item><item><title>Anthropic&apos;s Claude Passes ChatGPT, Now #1, on Apple&apos;s &apos;Top Apps&apos; Chart After Pentagon Controversy</title><link>https://slashdot.org/story/26/02/28/2046221/anthropics-claude-passes-chatgpt-now-1-on-apples-top-apps-chart-after-pentagon-controversy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 20:59:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Anthropic may have lost out on doing business with the US government," reports Engadget, "but it's gained enough popularity to earn the number one spot on the App Store's Top Free Apps leaderboard." 


Anthropic's Claude AI assistant had already leaped to the #2 slot on Apple's chart by late Friday," CNBC reported Saturday:


The rise in popularity suggests that Anthropic is benefiting from its presence in news headlines, stemming from its refusal to have its models used for mass domestic surveillance or for fully autonomous weapons... OpenAI's ChatGPT sat at No. 1 on the App Store rankings on Saturday, while Google's Gemini was at No. 3... On Jan. 30, [Claude] was ranked No. 131 in the U.S., and it bounced between the top 20 and the top 50 for much of February, according to data from analytics company Sensor Tower... [And Friday night, for 85.3 million followers] pop singer Katy Perry posted a screenshot of Anthropic's Pro subscription for consumers, with a heart superimposed over it. 

Sunday Engadget reported Anthropic's "very public spat" with the Pentagon "led to a wave of user support that finally allowed Claude to dethrone OpenAI's ChatGPT on the App Store as the most downloaded free app." .

Friday Anthropic posted "We are deeply grateful to our users, and to the industry peers, policymakers, veterans, and members of the public who have voiced their support in recent days. Thank you. "]]></content:encoded></item><item><title>The Token That Already Has a Job: Inside Playnance&apos;s G-Coin and Its $2M Proof of Work</title><link>https://hackernoon.com/the-token-that-already-has-a-job-inside-playnances-g-coin-and-its-$2m-proof-of-work?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Sun, 1 Mar 2026 20:15:58 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In an industry where token launches routinely precede working products by months or years, Playnance is attempting something unusual: launching its G-Token into an ecosystem that already functions.\
Playnance, a Tel Aviv-based Web3 infrastructure company announced that its "Be The Boss" partner program has distributed more than $2 million in real cash to 2,567 active partners, while the broader platform has generated $5.3 million in total revenue. The G-Token launch, now imminent, does not create the ecosystem. It is being built on top of one.\
That distinction matters more than it might seem. The crypto sector has a long memory for projects where token issuance preceded utility—and where, once the speculative energy dissipated, the underlying product was found to be empty. Playnance is betting its reputation on reversing that sequence entirely.\
\
The Be The Boss program is where the numbers become tangible. For a symbolic $1 entry fee, any creator or community operator can spin up a fully branded social gaming platform on PlayW3, Playnance's flagship consumer product. The backend, blockchain settlement, player support, game catalogue, liquidity, is handled entirely by Playnance. The partner, or "Boss," focuses on audience-building. Revenue is split 50/50, paid automatically each day directly to the partner's on-chain wallet. There are no lock-up periods, no token vesting schedules, no promises of future value. The $2 million distributed to date is real money.We designed the token to serve a working ecosystem, not the other way around. The foundation is already in place.Pini Peter, CEO, Playnance\
The G-Token, referred to in earlier company materials as "G Coin", is not an add-on to this structure. It is the plumbing. Every platform interaction on PlayW3, every game played, every payout processed on Up vs Down and other Playnance products, runs through the token as its settlement and utility layer. Daily earnings distribution to the 2,567 Bosses is denominated and settled in G-Token. Demand for the token is therefore not manufactured by marketing campaigns or speculative listings; it is a direct function of how many people are playing and how many Bosses are operating platforms.\
This feedback loop—Playnance's own term, though their language describes it as a "compounding economic loop", is the central claim of the G-Token thesis. More Boss platforms bring more users. More users generate more on-chain transactions. More transactions create organic, usage-driven demand for G-Token across gameplay, reward mechanics, and settlement flows. The payout track record then attracts the next wave of Boss operators. The company says the Boss count has more than doubled in recent months, a data point that suggests the loop is already turning.\
The architecture behind these numbers is deliberately invisible to end-users. Playnance has built what it calls a non-custodial, on-chain system that sits beneath a Web2-style interface. Users onboard without needing to understand wallets, gas fees, or token mechanics. Every transaction is nonetheless recorded on-chain, providing the transparent, verifiable activity data that underpins the G-Token's claimed utility. At 1.5 million daily transactions, Playnance is operating at a throughput that rivals mid-tier centralised exchanges—an unusual credential for a consumer gaming platform entering its public token phase.\
The critical question, as with any token-powered ecosystem, is whether the loop holds under scrutiny. The $2 million in fiat payouts is the most defensible number: it is real money that left a bank account or wallet and arrived in someone else's. The $5.3 million in total revenue is a company-reported figure, unaudited at the time of publication. The 1.5 million daily transactions include all platform activity, not merely financially material events, a metric that, in gaming contexts, warrants examination of what proportion represents genuine user engagement versus automated or incentivised activity. Playnance has not yet published a third-party audit of its on-chain data.\
What the company has published is a track record of paying its partners. In a market where token promises have so frequently substituted for product, $2 million in actual cash distributions is an uncommon form of proof. Whether G-Token becomes the connective tissue of a genuinely scaled consumer blockchain ecosystem or whether the loop stalls as distribution broadens and marginal Boss quality declines, will depend on execution through 2026. The infrastructure, for now, appears real. The test is whether it compounds.\
Don’t forget to like and share the story!]]></content:encoded></item><item><title>America Used Anthropic&apos;s AI for Its Attack On Iran, One Day After Banning It</title><link>https://tech.slashdot.org/story/26/03/01/1945208/america-used-anthropics-ai-for-its-attack-on-iran-one-day-after-banning-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 19:47:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[ Engadget reports:

In a lengthy post on Truth Social on February 27, President Trump ordered all federal agencies to "immediately cease all use of Anthropic's technology" following strong disagreements between the Department of Defense and the AI company. A few hours later, the U.S. conducted a major air attack on Iran with the help of Anthropic's AI tools, according to a report from The Wall Street Journal. 

Even Trump's post noted there would be a six-month phase-out for Anthropic's technology (adding that Anthropic "better get their act together, and be helpful during this phase out period, or I will use the Full Power of the Presidency to make them comply, with major civil and criminal consequences to follow.") 

Anthropic's Claude technology was also used by the U.S. military less than two months ago in its operation in Venezuela — reportedly making them the first AI developer known to be used in a classified U.S. War Department operation. The Wall Street Journal reported Anthropic's technology found its way into the mission through Anthropic's contract with Palintir.]]></content:encoded></item><item><title>XRP Price Prediction 2026: Best Portfolio Strategy Pairs XRP With Pepeto for 150x Upside</title><link>https://hackernoon.com/xrp-price-prediction-2026-best-portfolio-strategy-pairs-xrp-with-pepeto-for-150x-upside?source=rss</link><author>Tokenwire</author><category>tech</category><pubDate>Sun, 1 Mar 2026 19:07:59 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Bitcoin is on pace for a fifth straight monthly loss, its worst streak since 2018, and every XRP price prediction for March is getting more cautious.  reported Bitcoin crashing below $64,000 after US and Israeli strikes on Iran wiped $128 billion from total market value. XRP fell to $1.29 in the sell off. But the smartest XRP price prediction strategy is not choosing between large caps and presales. It is holding both, using XRP for recovery and Pepeto for asymmetric upside no large cap can deliver.XRP price prediction and the best coins to hold in 2026Pepeto: the asymmetric side of any XRP portfolioAny serious portfolio needs more than a single large cap, and the best XRP price prediction strategy accounts for that by pairing Ripple's steady recovery potential with a six zero entry that could deliver 150x before XRP moves 3x. That is exactly where Pepeto fits, and the capital flowing into this presale tells the story before any headline does. The project raised above $7.36M while the rest of the market bled, stages are closing faster than any round before, social mentions tripled in February alone, wallet registrations keep climbing daily, and fake tokens impersonating Pepeto flood decentralized exchanges because scammers only clone what is about to explode. And that demand is not random, because behind it sits the first integrated trading infrastructure designed specifically for the $45 billion meme coin economy. PepetoSwap is approaching launch as a zero tax cross chain trading engine connecting Ethereum, BSC, and Solana so meme coin traders can move between chains without bleeding fees on every swap. Pepeto Bridge is being built to handle cross blockchain transfers in seconds, removing the friction that forces traders through slow third party services. And the Pepeto Exchange will give new meme coins a dedicated listing hub this market has never had, creating structural demand for the token every time a project lists or a trade executes. An original Pepe cofounder is behind this project, dual security audits from SolidProof and Coinsult returned zero critical issues, and the presale sits at $0.000000186 with 70% of the allocation already filled. The  shows the kind of accumulation that defined the earliest days of every meme coin that went parabolic. At that price a $10,000 entry at 150x becomes $1,500,000. On top of that, staking at 211% APY generates $57.81 per day, $1,758 per month, and $21,100 per year on that same $10,000, but the yield is just the holding bonus while you wait for the listing. The real play is the price. SHIB reached $40 billion with no swap, no bridge, and no audit. Pepeto has all three approaching launch. XRP price prediction: steady recovery with 3x to 5x upside shows XRP near $1.29, down from its 2025 peak above $3. The XRPL Foundation patched a critical ledger vulnerability before mainnet, but price action remains muted with lower highs and fading buyer conviction. Bullish forecasts cap XRP between $3.50 and $5.00 by late 2026, representing 3x to 5x upside. That makes XRP a reliable portfolio base, not the whole strategy.Solana price prediction after a brutal FebruarySolana enters March after dropping 31% in February. Weekly DEX volume fell 62% and long term holder accumulation dropped 92% according to Glassnode. Support sits at $80, but a head and shoulders breakdown points toward $59. Spot SOL ETFs pulling in $900 million remain the only bright spot.The best XRP price prediction approach in 2026 is pairing large cap recovery with presale asymmetry. XRP gives you 3x to 5x. Pepeto at $0.000000186 gives you 150x. The presale is 70% filled. $7.355 million raised. The whales recognized this setup before. They're inside Pepeto now. Visit the Pepeto official website before six zeros disappear permanently.What is the XRP price prediction for 2026? XRP trades near $1.29 after the Iran crash. Bullish forecasts project $3.50 to $5.00 by late 2026, representing 3x to 5x upside from current levels.How should I build a crypto portfolio with XRP and Pepeto? Pair XRP's steady recovery with Pepeto's asymmetric 150x potential at $0.000000186. The combination balances established large cap exposure with early stage upside. Visit the  for details.How much could $10,000 in Pepeto return at 150x? A $10,000 entry becomes $1,500,000. Staking at 211% APY adds $57.81 per day, $1,758 per month, and $21,100 per year.Is Solana a good buy after the February crash? Solana dropped 31% in February with DEX volume down 62%. ETF demand provides some support, but the technical breakdown toward $59 suggests more downside before recovery.:::warning
This article is for informational purposes only and does not constitute investment advice. Cryptocurrencies are speculative, complex, and involve high risks. This can mean high prices volatility and potential loss of your initial investment. You should consider your financial situation, investment purposes, and consult with a financial advisor before making any investment decisions. The HackerNoon editorial team has only verified the story for grammatical accuracy and does not endorse or guarantee the accuracy, reliability, or completeness of the information stated in this article. #DYOR  ]]></content:encoded></item><item><title>Polymarket saw $529M traded on bets tied to bombing of Iran</title><link>https://techcrunch.com/2026/03/01/polymarket-saw-529m-traded-on-bets-tied-to-bombing-of-iran/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 1 Mar 2026 19:05:35 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Six newly-created accounts made a profit of $1 million by correctly betting that the U.S. would strike Iran by February 28.]]></content:encoded></item><item><title>Dealing With Leadership Avalanche Without Feeling Buried: A How-to Guide</title><link>https://hackernoon.com/dealing-with-leadership-avalanche-without-feeling-buried-a-how-to-guide?source=rss</link><author>Vinita Bansal</author><category>tech</category><pubDate>Sun, 1 Mar 2026 19:00:02 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Moving from an individual contributor to a manager to a leadership role not only brings an overwhelming increase in responsibilities, but also significantly increases the size of your team. You’re no longer responsible for yourself or just a small group, but a large number of people count on your leadership.\
Whether it’s org restructuring, sudden layoffs, budget cuts, or a natural part of your career growth, handling 2x or 3x more people can be a big challenge. As complexity shoots up, more decisions need to be made, meetings fill up your calendar, chat messages pile up, and 1:1s multiply. The sheer volume of things that demand your attention can feel like an avalanche—you may feel overwhelmed and buried with tasks, messages, and unresolved problems.\
Without a proper strategy to manage the increased scale, an increase in responsibilities and expectations can feel unmanageable, diminishing your ability to manage and lead effectively. Every day can feel like drinking from a firehose, making it hard to focus and keep up with the demands of the job. Days end not with a sense of accomplishment, but a feeling of relief as you barely manage to survive and get through the day without completely losing your mind.\
But you can’t operate like this forever—without scaling yourself, you’ll eventually feel drained, exhausted, and burnt out. To manage a leadership avalanche without feeling buried, you need to be strategic in your approach.Leadership at scale—and leadership as you scale—means you’re constantly adapting and evolving. You can’t follow a single style or approach. You’re always leading through transitions. Your company is always changing around you. And this means you’re naturally going to have a very resilient kind of leadership, producing a resilient team and company.\
Managing large teams requires a thoughtful approach to how you prioritize, what you communicate, where you spend your time, and which processes need to be eliminated as they’re no longer serving you well. You need to be flexible in your approach—rigidity can make scaling up harder as you refuse to learn and adapt along the way.\
Here are the 5 strategies you need to adopt to scale your leadership when tasked with the responsibility to handle a large team:When working with a small team, you may be quite involved in day-to-day responsibilities, be part of team discussions, and have regular feedback conversations with them. Over a period of time, this builds a certain expectation around your involvement—when to reach out, when to ask for advice, and when to seek your approval. People get used to a certain way of working, as every interaction with you shapes their thinking.Instant reply to chat messagesGetting your inputs without scheduling a meeting\
These may be the perks when you’re dealing with a small team. However, as your responsibilities multiply and team size increases, you can’t continue operating in the same way. You can no longer hold  with each team member. You may not get the time to reply to their message as you’re held up with back-to-back meetings. Getting your inputs may require blocking a slot on your calendar, as ad hoc meetings may no longer be a possibility.\
These changes, unless communicated verbally, can turn into a source of distress in the team—not getting the attention they’re used to can be quite upsetting and demotivating. You’ve to reset your team’s expectations. You’ve to clarify the changes required to function with a large team. You’ve to make them understand that old ways of operating will no longer be effective.\
Proactively reset their expectations so that the team is attuned to them from the beginning:Decide the frequency of 1-1 meetings and schedule them on the calendar.Discuss when/how they can reach out to you—clearly distinguishing between issues that require immediate attention vs those that can be delayed.Address any concerns they have around your availability and approachability. Seek regular feedback to make it better.Communicate any other changes that may impact how they do their work.Effective scaling depends on believing and living a shared mindset throughout your group, division, or organization.\
Any promises you made earlier to your team—directly or indirectly—may no longer be relevant when dealing with a large team. Breaking them without resetting expectations can lead to decreased morale, frustration, and a lack of trust in the team. As soon as your team size multiplies, discuss changes around your involvement and seek their alignment.Communication problems are the biggest source of misery in most organizations. They lead to confusion, misalignment of goals, and even friction between people. Long debates, unnecessary meetings, and issues drag on when people in the team aren’t clear on their goals, or they conflict with priorities across other teams and functions. Time and energy are wasted. Deadlines are missed.  and finger-pointing become the norm.\
With a small team, communication problems are still manageable as you can give your attention to every issue that crops up and fight your way through it. However, as your team size multiplies, you can no longer rely on a reactive approach. You’ll not have the bandwidth to attend to every issue that shows up or even resolve it in a timely manner. You may not even know about the problems till they have become a big issue. Reacting to communication problems instead of solving them proactively can turn into a nightmare—as you try to keep up and play a catch-up game.\
You can’t avoid all communication problems, but you can definitely minimize the gap by adopting practices that can make communication less painful and more productive for everyone. To do this:Seek alignment on priorities and agree on a common measure of success. Success is more likely when everyone works on shared goals.Speaking up is important to communicate your ideas and opinions, but it shouldn’t refrain you from also listening to others. Communication isn’t a one-way street. Treat it as a two-sided road.Expecting others to register key information by saying it once is a big mistake. Unless you repeat it multiple times, it will not get the time and attention it deserves.Good questions have the power to unlock creative thinking and surface out hidden problems. Use every opportunity to explore your curiosity by asking questions.Assumptions when not validated can lead to gaps in expectations. Avoid frustration, angst, and anxiety by seeking alignment upfront.Blaming, shaming, and complaining do not solve problems. Instead of pointing fingers, identify what caused these communication gaps and how you can avoid them in the future. or delaying them makes the situation worse. Step out of your comfort zone and embrace the discomfort.Over-communicating is the glue that holds a high-performing team together and keeps them focused in the same direction. And, it circles back to clarity. Without good, consistent communication, you don’t have clarity.\
Communication can be less chaotic in a large team when you’re proactive—clarifying goals, seeking alignment, handling conflicts, and repeating important information often to ensure it doesn’t get missed. Stay on top of your communication game—it can keep you afloat even during the most challenging and stressful times.Multiply your impact through coaching, not instructions.When working with a small team, you may be involved in the nitty-gritty of every project and every task, enabling you to delegate work without losing your sense of control. You may keep a close watch over every task, be involved in how each one is done, and step in at the right time to unblock people and help them make progress on their goals.\
However, as your team scales, you can no longer pay attention to every detail. You can no longer be involved in every issue or each decision. You need to shift from a 10-foot view to a 100-foot view—you have to start assigning not just the task, but a larger scope of work. You have to delegate not just what needs to be done, but also how it must be done.\
You can achieve this only by empowering your team—helping them build the essential skills to make decisions, solve problems, navigate complexity, and take accountability for their actions. Trying to micromanage will prevent your team from building the essential skills to learn and grow, limit your team's collective outcome, and cause you to feel stress and burnout. This requires  and a mindset shift from doing to leading others.\
To delegate responsibly to a large team:Break down your goals and map them to different team members based on their skills or the opportunities they need. Be careful to avoid delegating work that shouldn’t be done at all or a task that only you need to fulfill.Delegate the “what” of the problem, support it with “why,” and empower your team to work out the “how.” Stating the expected outcome without the method enables your team to achieve better results.Don’t abdicate your team. Support and coach them to make the right decisions and continue making forward progress.No process can improve without incorporating the feedback loop. Work with your team to determine how you are doing and what you can do to be better.Delegate responsibilities, not tasks. In Start, you are delegating tasks, since you’re still involved in all the decision-making. But here in Scale, you’ve got to stop delegating tasks and instead move entire responsibilities to members of your team so that you’re not having to think about every item every day.\
Trying to keep a tight control over your team by dictating every task, every decision, and their every move will prevent you from scaling and achieving org goals. Instead, invest in building your team’s skills. Trust them to handle bigger and better responsibilities. Coach them to think and act independently.Reduce clutter and eliminate drag.You may establish a set of processes when working with a small team that enables them to be efficient in working together and getting things done. For example: A stand-up meeting every morning, design discussions with the entire team, conducting pre-mortems for every project, or multiple levels of code reviews.\
These processes that worked well with a small team can tremendously slow down a large team—people in the team can waste a lot of time and energy by sticking to old methods of working that no longer keep them efficient. Whether it’s tech processes, collaboration practices, or communication methods, you can’t stick with them just because they worked in the past. You’ve to identify the elements that can create unnecessary drag and reduce your team’s productivity instead of speeding it up.\
Reduce and declutter superfluous processes by following these steps:List down the different practices and processes followed by your team.Talk to your team members, identify what’s working and what’s adding to the burden. Pay close attention to things that feel unnecessary or impractical with a large team.Gather inputs on what they might be missing, which can ease out goals progress and help your team achieve success.Introduce small changes at a slow pace. Sudden large disruptions in habitual processes can make team members resistant to change.Set up regular feedback discussions to learn, change, and adapt because what works today might be completely irrelevant tomorrow.A great process isn’t designed; it is evolved. So, the important thing isn’t your process; the important thing is your process for improving your process.\
Pay attention to how your team operates on a day-to-day basis—what makes them go full throttle and what adds useless pauses to their momentum. By safeguarding your team’s time and channeling it towards constructive processes, you can reduce the mental clutter and create a positive work environment.Manage your energy, not just your time.When the scope of work is small and interactions limited, you may not feel drained at the end of each day. You may find yourself with plenty of energy to make decisions, solve problems, and guide your team.\
However, as your team size increases, the number of decisions you have to make in a day shoots up. A series of these small decisions scattered throughout the day may seem harmless as they demand only a small fraction of your mental energy, but as the day goes on and you continue to expend from this seemingly reserved pool of energy, your mental capacity to make decisions starts depleting.\
Unlike physical fatigue, which you can feel and instantly express, the mental fatigue that comes after making multiple decisions is not visible to you. It makes you reckless—you start acting impulsively instead of taking the time to think through the consequences of your decisions. You look for the safest and the easiest option, which is to stick with the status quo, and resist the idea of a change since it’s uncomfortable and demands a lot of energy. \
A tired mental machinery also makes it hard to process information and separate noise from signal. This may lead to overthinking—the tendency to think too much and move back and forth on ideas without the ability to give them a specific direction. When fatigued, your brain’s regulatory power weakens, causing you to lose control over your emotions. This makes everything around you feel more intense than normal—small mistakes can make you furious, disagreements may cause irritation, and you may react aggressively to things that are not in line with your expectations.\
Not managing your energy—both mentally and physically—can feel quite suffocating as depleted energy makes it hard to focus and handle the demand and pressure of the job. To conserve your energy and use it well:Control the number of decisions you make by choice—declutter and delegate—and put all your energy into getting them right.Block a dedicated slot every morning or the previous night to plan what you wish to achieve each day. By not spending mental cycles in deciding every instant what to do next, you can avoid decision fatigue and free up more resources to do the real work.Tackle your most challenging task first, one that demands your mental capacity to process information at its best.Pay attention to your body—eat and sleep well, incorporate regular breaks into your schedule, exercise, and stick to other healthy routines.Managing energy, not time, is the fundamental currency of high performance. Great leaders are stewards of organizational energy. They begin by effectively managing their own energy. As leaders, they must mobilize, focus, invest, channel, renew and expand the energy of others.\
Trying to save time without optimizing for energy can be useless, as you may have plenty of time left, but not the energy to be effective. By consciously adopting practices and routines that minimize energy consumption and maximize outcomes, you can achieve the desired scale.Your involvement with your team can dramatically change when shifting from a small group to a large one. You can no longer be available and approachable in the same way. To ensure your team understands the changes around your involvement, explicitly reset expectations.Doubling down on communication with a large team is a smart strategy to ensure team members are clear on roles, aligned on goals, and there’s less confusion and misunderstanding. Communicate often, repeat information that needs extra attention, and ask questions to seek alignment across teams and reduce gaps in expectations.Scaling a large team requires empowering team members to go above and beyond their roles. You have to trust them to handle bigger and better responsibilities. You have to let go of the desire to dictate how everything must be done. Coach, don’t spoon-feed.Processes that allow small teams to stay productive can slow down a large team. Instead of sticking to old methods and ways of working, identify what can speed things up and what may get in the way of making progress and moving forward.Your energy, just like your time, is a precious resource, which, if not managed well, can get in the way of scaling a large team. Your energy gets depleted with every decision you make, and not paying attention to how and where you spend it can lead to poor choices and terrible decisions. Conserve it for when you need it the most.\
This story was previously published here. Follow me on LinkedIn or here for more stories.]]></content:encoded></item><item><title>Let’s explore the best alternatives to Discord</title><link>https://techcrunch.com/2026/03/01/best-discord-alternatives-age-verification-identity-privacy/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Sun, 1 Mar 2026 19:00:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[With many users feeling uneasy about Discord's new age verification requirement, here are some alternatives that could be worth exploring. ]]></content:encoded></item><item><title>Americans Listen to Podcasts More Than Talk Radio Now, Study Shows</title><link>https://tech.slashdot.org/story/26/03/01/054233/americans-listen-to-podcasts-more-than-talk-radio-now-study-shows?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 18:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Podcasts have officially overtaken AM/FM talk radio as the more popular medium for spoken-word audio in the United States," reports TechCrunch, citing Edison Research's Share of Ear survey:

The researchers have tracked these statistics over the last decade, and almost always, the percentage of time people spent listening to podcasts increased, while their time with spoken radio broadcasts decreased. For the first time this year, podcasts eclipsed spoken-word radio with 40% of listening time, as opposed to 39% for radio... 

We checked with Edison to see if these statistics include video podcasts, and they do. But the need to clarify that question points to the undeniable growing prevalence of video podcasts, hosted on platforms like Spotify and YouTube, which marks another key trend in podcasting... YouTube said that viewers watched 700 million hours of podcasts each month in 2025 on living room devices, like TVs, up from 400 million the previous year.]]></content:encoded></item><item><title>North America&apos;s Bird Populations Are Shrinking Faster. Blame Climate Change and Agriculture</title><link>https://news.slashdot.org/story/26/03/01/0332257/north-americas-bird-populations-are-shrinking-faster-blame-climate-change-and-agriculture?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 17:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Billions fewer birds are flying through North American skies than decades ago," reports the Associated Press, "and their population is shrinking ever faster, mostly due to a combination of intensive agriculture and warming temperatures, a new study found."


Nearly half of the 261 species studied showed big enough losses in numbers to be statistically significant and more than half of those declining are seeing their losses accelerate since 1987, according to Thursday's journal Science... The only consolation is that the birds that are shrinking in numbers the fastest are species — such as the European starling, American crow, grackle and house sparrow — with large enough populations that they aren't yet at risk of going extinct, said study lead author Francois Leroy, also an Ohio State ecologist... 

When it came to population declines — not the acceleration — the scientists noticed bigger losses further south. When they did a deeper analysis they statistically connected those losses to warmer temperatures from human-caused climate change. "In regions where temperatures increase the most, we are seeing strongest declines in populations," [said study co-author Marta Jarzyna, an ecologist at Ohio State University]. "On the other hand, the acceleration of those declines, that's mostly driven by agricultural practices." The scientists found statistical correlations between speeded-up decline rates and high fertilizer use, high pesticide use and amount of cropland, Leroy said. He said they couldn't say any of those caused the acceleration of losses, but it indicates agriculture in general is a factor. "The stronger the agriculture, the faster we will lose birds," said Leroy... 

McGill University wildlife biologist David Bird, who wasn't part of the study, said it was done well and that its conclusions made sense. With a growing human population, agriculture practices are intensified, more bird habitats are being converted to cropland, modern machinery often grind up nests and eggs and single crop plantings offer less possibilities for birds to find food and nests, said Bird, the editor of Birds of Canada. "The biggest impact of agricultural intensity though is our war on insects. Numerous recent studies have shown that insect populations in many places throughout the world, including the U.S., have crashed by well over 40 percent," Bird said in an email. "Many of the birds in this new study showing population declines depend heavily on insects for food." 


A 2019 study of the same bird species by Cornell University conservation scientist Kenneth Rosenberg also found that North America had 3 billion fewer birds than in 1970, the article points out.]]></content:encoded></item><item><title>Google looks to tackle longstanding RCS spam in India — but not alone</title><link>https://techcrunch.com/2026/03/01/google-looks-to-tackle-longstanding-rcs-spam-in-india-but-not-alone/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Sun, 1 Mar 2026 17:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google is integrating carrier-level filtering into RCS in India through a partnership with Airtel to strengthen protections against spam.]]></content:encoded></item><item><title>The Soft Bigotry of AI Doom: Because Users Are Just Too Incompetent</title><link>https://hackernoon.com/the-soft-bigotry-of-ai-doom-because-users-are-just-too-incompetent?source=rss</link><author>The_AI_Ethicist</author><category>tech</category><pubDate>Sun, 1 Mar 2026 17:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[When a new, disruptive technology comes along, fearmongering is never far behind. Writing was said to erode our memory, yet most of you still somehow managed to remember to put underwear on today. Movies and television were supposed to destroy our imaginations, yet the Star Wars and Harry Potter universes are bursting with human imagination, and the sheer volume of their wildly inappropriate fan fiction likewise proves so. Smartphones supposedly eradicated our attention spans, yet… wow, that’s really shiny! I’m sure I don’t need to tell you how disruptive AI is, so naturally, the fearmongering has followed. \
The thing is, this particular brand of fearmongering around AI has escalated rather quickly, even in the face of both absurd and hyperbolic arguments, such as AI destroying our critical thinking, collapsing our institutions, and ending our world. Even better, there’s an unspoken thread in this fearmongering that implicates you, the user of AI, as part of this destruction because, apparently, you’re just too damned incompetent.Destruction of Critical ThinkingOne of the supposed negative side effects of AI use is the destruction of critical thinking. It seems we are going to be so enamored with AI that we’re going to use it to supplant much of our thinking. No longer will you have to sit and have a think because you can simply have your AI do it for you. Don’t know what’s for dinner? Ask AI! Unsure of the moral implications of capital punishment in a contemporary society? Just ask AI! Is there life after death? AI, my friend. AI.\
The argument basically boils down to this: because AI is so ubiquitous, we are going to be unloading so much of our cognitive effort that our critical thinking skills will diminish. It’s the old “use it or lose it” idea. But if the logic is inventions that reduce cognitive effort therefore reduce critical thinking, why didn’t the invention of writing prevent the invention of books, which should have prevented the invention of adding machines, which should have prevented the invention of computers, which should have prevented the invention of AI, whose creation required some of the most arduous collective critical thinking ever undertaken by humanity? \
It seems as though the best evidence for this brand of fearmongering is that students who use AI to write their papers are not engaging their critical thinking. Unfortunately, even this claim can’t be supported. What  definitively be said about students using AI to generate their papers is not that their critical thinking skills are degrading, but that they are spending less time writing. Do we need a reminder that writing is not critical thinking? Because if it is, Socrates was an idiot, as he didn’t write. So while writing can certainly be tied to critical thinking, it is not critical thinking itself.\
The problem is, many of these arguments use writing as a measure for critical thinking, demonstrating a deep lack of critical thinking. Not that it needs to be said, but two things can be true at once: you can be a good critical thinker and a crap writer. You also don’t have to think critically to write a paper. As a community college instructor, I’ve seen plenty of papers that are bereft of even a grain of critical thinking, but somehow, there were still a bunch of words printed on paper. So my students broke reality in addition to my hopes for them.\
No, the best claim that can be made is that many students are spending less time writing, so it’s  there might be a reduction in time spent on critical thinking. But even then, it would be a leap to assume none ever occurs. Am I to understand that students using AI to write papers are not even going to see what the AI wrote and use critical thinking to evaluate the essay? Are they completely unaware that AI hallucinates and makes errors? If all that is the case, it’s not the AI that is preventing critical thinking, now, is it? As surprising as it is, cheating predates AI.\
Wait a minute… I must wholeheartedly apologize. I am absolutely unqualified to think about this critically, as I was in the top 1% of users who sent messages to ChatGPT in 2025. Therefore, I will start acting appropriately: Derrrrrr! Duh, AI good!The Fall of Civic InstitutionsOur beloved civic institutions will also fall due to AI. An infamous paper recently made its way around the AI doom circles on this exact topic, How AI Destroys Institutions, and it proposes that this is going to happen through three mechanisms: deteriorating expertise through cognitive offloading, interfering with our decision-making, and isolating humans from each other.\
Much like the argument for critical thinking, our professional skills are going to be eroded because of skill offloading from AI. It’s not that we might get worse at that specific thing AI is doing for us; rather, it’s that our professional skills will degrade. This is why I can never use an LLM for classroom content as an English as a second language instructor—my English skills would degrade, I wouldn’t be able to speak English anymore, and I’d be out of a job. Thanks, AI!\
So we are to believe that professionals, people who have invested time and money into education and building up careers, are simply going to let important skills get unknowingly chipped away at because of AI? Are lawyers just going to become people who bring Claude into court? So all these people who’ve been highly trained won’t notice they’re not as effective at their jobs as they used to be? Their bosses won’t? The clients who pay for competent services won’t? That’s an extremely dependent and extraordinarily unlikely inverted pyramid made from a lack of self-awareness.\
So I guess I won’t notice the degradation of my accountant’s skills when I have to pay six times more in taxes because of their mistakes, and I guess the parents of students won’t notice their children’s grades slipping because the teacher used AI and therefore sucks at teaching. Apparently, AI functions as a global blindfold. It’s fun when you find unintended uses of products!\
Apparently, we’re also just going to have AI make our difficult moral choices for us because we’re just so damned lazy. We’re going to outsource our morality and judgment, all hidden behind an unknowable algorithm. No one will ever hash it out and come to a better agreement because we’re just going to outsource all of that to AI. I know how eager the public is to outsource our ethics, judgment, and morality to AI. Thank goodness there’s never, ever, ever been any pushback on this idea. Like ever. I guess Catholics will ask for repentance through Grok rather than through priests.\
In order to collapse our civic institutions, such as education and the justice system, AI will also erode human relationships. Now, it is true that AI will displace some relationships; there’s a good chance the relationship you had with your assistants will be a faint memory when AI replaces them. Honestly, I still haven’t replaced the relationship I had with my ice block delivery man or the horse he rode on. Gosh, I sure do miss the 80s… The 1880s, that is.\
And of course, all of this destruction of human relationships will happen only because of AI. If you thought it started happening with the decline of the monoculture as digital technology ramped up, well, you’re just wrong.\
Additionally, people will become isolated because, with AI being so agreeable, there’s less reason to hash stuff out with others in an uncomfortable manner. I get it; people are conflict-averse. That’s why when I turn on the news, I only see stories about rainbows and puppies rather than wars and protests. Humanity is so harmonious!\
So yeah, our beloved civic institutions will crumble. Damn you, AI!If destroying our society wasn’t bad enough, AI is also going to contribute to the end of the world. The Doomsday Clock by the Bulletin of the Atomic Scientists has been moved to 85 seconds to midnight, in part because of the threat from AI. Biological, nuclear, and informational warfare AI upgrades are pushing us closer than we’ve ever been. And while these threats do actually have some merit, the hyperbolic conclusion still leaves this firmly in the fearmongering camp.\
The fear of’ biological warfare is that AI will create a new, dangerous pathogen that people have no defense against. For nuclear warfare, AI’s implementation could mask the decision-making process, increasing the chances of error with a devastating weapon. And for informational warfare, it’s more about sowing chaos through deepfakes and the like.\
I’ll be the first one to admit that these particular threats do seem a bit more compelling, though I’m still unsure that inching us toward doom is the appropriate conclusion. It is very conceivable that AI could design an extremely dangerous pathogen, but to be fair, we’ve kept plenty of dangerous pathogens for many years, so I’m just going to continue keeping my fingers crossed.\
For nuclear warfare, technology in general typically reduces the need for human judgment, and it’s easily argued that it reduces errors from human judgment, so the doom argument seems like a wash at best. As for informational warfare with deepfakes, yeah, that one’s hard to refute. Society is just going to have to figure that one out as we did with other disruptive technologies, though again, contributing to the end of the world seems a bit of a hyperbolic conclusion in the meantime.While AI doom slop has always annoyed me, it wasn’t until I sat down and thought about what binds them together, somehow without the help of AI, that I found the thread: humans are too incompetent to use AI responsibly. See, the allure of AI is simply too great for humanity, so naturally, the result is a degradation of our critical thinking, the collapse of our society, and even the destruction of our world.\
We must first acknowledge that for any of these doom scenarios to come to fruition, it requires an incredible amount of human failure stacked on human failure stacked on human failure. We’ve already been warned by the doomers who seem immune to AI’s negative effects, yet we’re just not going to do anything to mitigate these potential disasters? Is AI not going to improve in any marked way? The companies that create AI have no incentive not let it destroy the world? I had no idea profits continued to percolate to the afterlife.\
It seems parents and educators will simply shrug and accept that their children won’t be very good thinkers. The institutions that prop up our societies apparently can’t do anything in the face of AI to save themselves from ultimate destruction. And the great powers of the world would certainly never do anything to mitigate the risk of world destruction, even though the primary goal of conflict is to not be destroyed, but whatever. Remember, humanity is incompetent. They won’t say it outright, but it’s an implicit requirement in all of these predictions.\
Plus, while we may be a bit late to the party, historically, we have always recognized the dangers of technology and done our best to minimize the risks. Cars now have seatbelts and airbags, houses now have circuit breakers and grounded outlets, guns have safeties, and even the Three Mile Island, Chornobyl, and Fukushima disasters produced increased nuclear safety. I wonder which magical property of AI makes it resistant to our inevitable improvements.\
What can actually be stated with confidence is that it’s possible we might become too reliant on AI for problem-solving. It’s possible AI will collapse our institutions, but the sheer number of required failures to get there makes it virtually impossible. It’s also possible AI will be participating in the destruction of the planet, but if over 80 years of humanity having nuclear technology is any indication, we’re about 23.95 hours till midnight rather than 85 seconds.\
To be clear, none of this means we shouldn’t be cautious; caution with a new disruptive technology should be a requirement. However, we all know the claims being made aren’t advising caution; that’s gone out the window, and they’re predicting disaster.\
I suppose AI Will Erode Our Critical Thinking is a bit catchier than AI Will Erode Our Critical Thinking If We Simply Do Nothing But Twiddle All Our Thumbs As It Happens. How AI Destroys Institutions is a bold and head-turning title whose cup overflows with hyperbole; AI Has The Possibility To Generate Some Negative Effects On Our Institutions, So Let’s Prepare Ahead Of Time isn’t nearly so bold. AI Is Pushing Us Closer To Global Doom naturally gets many clicks; AI Is A New Tool, So Let’s Proceed Cautiously doesn’t, even if it’s more accurate.So, what can be learned from these AI doom stories? Well, it seems they think you’re incompetent and can’t use AI responsibly. They think you’re not going to do anything to mitigate any potential problems from AI. They also think you are simply going to use it in a manner that is ultimately destructive. So really, what we’ve learned is that the soft bigotry of low expectations has come to the world of AI.]]></content:encoded></item><item><title>Investors spill what they aren’t looking for anymore in AI SaaS companies</title><link>https://techcrunch.com/2026/03/01/investors-spill-what-they-arent-looking-for-anymore-in-ai-saas-companies/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Sun, 1 Mar 2026 17:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[TechCrunch spoke with VCs to learn what investors aren't looking for in AI SaaS startups anymore. ]]></content:encoded></item><item><title>Collabora Clashes With LibreOffice Over Move To Revive LibreOffice Online</title><link>https://news.slashdot.org/story/26/03/01/042207/collabora-clashes-with-libreoffice-over-move-to-revive-libreoffice-online?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 16:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader darwinmac writes: The Document Foundation (TDF), the organization behind LibreOffice, has decided to bring back its LibreOffice Online project which been inactive since 2022. Collabora, a company that was a major contributor to the original LibreOffice Online, is not pleased with this development. After the original project went dormant, Collabora forked the code and created its own product, Collabora Online.

 Collaboras Michael Meeks, who also sits on the TDF board, reacted to the TDFs decision by saying that a fully supported, free online version already exists in the form of Collabora Online, and that resurrecting a dead repository makes little sense when an active, open community around the online suite already exists.

 For now, The Document Foundation plans to reopen the old repository for new contributions. The organization has issued a warning that the code is not ready for live deployment and users should wait until the development team confirms it is stable.]]></content:encoded></item><item><title>OpenAI reveals more details about its agreement with the Pentagon</title><link>https://techcrunch.com/2026/03/01/openai-shares-more-details-about-its-agreement-with-the-pentagon/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 1 Mar 2026 16:30:10 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[By CEO Sam Altman’s own admission, OpenAI’s deal with the Department of Defense was “definitely rushed,” and “the optics don’t look good.”]]></content:encoded></item><item><title>Yes, Crypto Millionaires Exist: Here’s How They Did It</title><link>https://hackernoon.com/yes-crypto-millionaires-exist-heres-how-they-did-it?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Sun, 1 Mar 2026 16:09:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It’s fair to think that this could only be a myth, but it's actually not. There  people who have become millionaires with Bitcoin and crypto investments. Does this mean you will do it, too? Who knows. The reality is more complex than just a little investment, and without a doubt, the crypto market is much more complex now than it was before.What we do know, though, is that the stories of these people are real. The stats are real, and their luck or conscious decisions were real. At this point, when we say “crypto millionaire,” we mean someone who holds (or held in the past) at least one million US dollars in cryptocurrencies, measured by market value. Probably, this amount was initially just a few cents or dollars that grew with time and patience.Let’s explore this idea of crypto millionaires and what they did to become one.Most cryptocurrencies have public chains, so we have public stats. We know which addresses are richer, and how many. The  estimated that over 241,700 users worldwide held at least one million dollars in crypto assets at some point during the previous year. That count included around 145,100 Bitcoin millionaires alone, reflecting Bitcoin’s dominant share of total market value. We can even  in real time with percentages and USD equivalents.To include some names, we have  of the top Bitcoin holders in 2026. Besides Satoshi Nakamoto and institutions like Binance, Robinhood, Bitfinex, Tether, and the US government, we also have unknown whales around, with millions and up to billions in BTC holdings.Market capitalization and historical prices aren’t a myth, either. We can travel back to 2013, for instance, when the total crypto market cap was around $1.6 billion, and the Bitcoin price was about $135 per coin []. By January 2026, the total market cap surpassed $3 trillion, and the Bitcoin price was at $86k (not to mention previous records of over $126k). That represents increases of 187,400% and up to 93,233%, respectively.In other words, if you had bought $1,200 in BTC in 2013 and held on for dear life (HODL) all these years, you would have over one million dollars today.Besides stats and anonymous addresses, we have some well-documented cases with names and stories. Likely the younger millionaire here is , who invested in Bitcoin when he was barely 12 years-old. His grandma gave him $1,000, and he bought BTC at $12 per unit in 2011. He founded an educational startup with his earnings in 2014 and sold it for 300 BTC in 2015. So, thanks to Bitcoin, he was a millionaire before turning 18. is another young investor: he sold some of his toys to invest in Bitcoin when he was just 11 years-old, in 2012. He also bought Ether in 2016 and made himself a millionaire with his crypto trades. Now, also speaking about Ether,  is worth mentioning. He bet his life savings and family house on buying ETH in 2016, when the currency was barely known. The result was $13 million by 2017. He did his research, but of course, doesn’t recommend trying this extremely risky move at home (or with your house).The  isn’t exactly a non-anonymous case, but we know that the person behind it’s an early Bitcoin miner. This charity fund appeared in 2017, revealing plans to donate more than 5,000 BTC to ONGs and some individuals in need. That was between $55 and $86 million at the time. The organizer, “Pine,” explained that the coins had been acquired quietly years earlier and left untouched while Bitcoin climbed. They had more money than they could ever spend.More recently, we can mention the case of two middle-aged . Tommy, James, and several of their family members bought about $8,000 in Shiba Inu (SHIB), a memecoin, before the price exploded in 2021. They ended up making up to $9 million.Well, not every crypto millionaire around just bought and sat to wait. Some of them are familiar faces who became wealthy by building infrastructure or companies related to digital assets. Among them, Changpeng Zhao (CZ), founder of Binance,  for having sold his apartment to buy Bitcoin in 2013. He launched the exchange in 2017 after a successful Initial Coin Offering (ICO), and now, , he’s the 23rd richest person in the world, with a net worth of around $78.8 billion.Jihan Wu is another good example. He co-founded Bitmain in 2013, one of the largest manufacturers of ASIC machines for Bitcoin mining. In 2021, he also launched Bitdeer, which is among the largest Bitcoin miners by computing power. Wu’s net worth has been  at around $2.3 billion.Around 2013 as well, when Bitcoin was young and Ethereum didn’t exist yet, Cameron and Tyler Winklevoss turned a famous $65 million legal settlement over Facebook into a huge cryptocurrency presence. about $11 million to buy Bitcoin when it was roughly $100-120 per coin, giving them 1% of all Bitcoin in existence at the time. Some years later, in 2014, they founded the regulated crypto exchange Gemini. This platform became one of the biggest U.S. exchanges for buying, selling, and storing digital assets.  lists each twin with a net worth of around $3.7 billion as of early 2026, largely from crypto holdings and Gemini’s growth.At this point, there are plenty of predictions. The dream of every crypto investor is to see Bitcoin reach one million per coin. If that’s even possible, we don’t know yet. If you can buy a memecoin for a few cents today and see a price explosion of 100,000%+ tomorrow, only luck can tell. All investors mentioned above went in before mainstream awareness, when prices were low, and uncertainty was high. Holding through sharp drops mattered just as much. This is really a mix of research and faith, but we need to remember that cryptocurrencies aren’t just speculation.They were created as a way to get rid of middlemen like banks and governments. It’s free money (as in freedom) available for everyone, everywhere, anytime. If you have your keys, no one else can have your coins. In , we have our own  of addresses (for GBYTE holdings), but what really matters is that no middleman can take your coins away.Without miners or “validators,” Obyte offers a multipurpose platform where you can trade and invest your crypto holdings without censorship concerns. At the end of the day, the first real step to becoming a millionaire is having complete control over your assets.:::info
Featured Vector Image by macrovector / ]]></content:encoded></item><item><title>The HackerNoon Newsletter: The 7 Best Coparenting Apps in 2026 (3/1/2026)</title><link>https://hackernoon.com/3-1-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sun, 1 Mar 2026 16:04:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, March 1, 2026?By @microsoft [ 27 Min read ] Microsoft’s AutoDev uses AI agents to write, test, and fix code autonomously, hitting 91.5% on HumanEval in Docker. Read More.By @solosatoshi [ 7 Min read ] I replaced $1,200/year in cloud subscriptions with one home server. Heres the setup, costs, apps, Bitcoin node, local AI, and what Id do differently.  Read More.By @confluent [ 5 Min read ] Learn how Python developers build real-time AI agents using MCP, Kafka, and Flink—modern agentic workflows explained on HackerNoon. Read More.By @melissaindia [ 4 Min read ] Learn 6 proven strategies to secure executive buy-in for Master Data Management by aligning MDM with ROI, risk reduction, and business goals. Read More.By @saumyatyagi [ 15 Min read ] Most teams plateau at AI writes code, a human reviews it. This article presents the Dark Factory Pattern — a four-phase architecture using holdout scenarios a Read More.By @stevebeyatte [ 7 Min read ] Compare the 7 best co-parenting apps in 2026, including BestInterest, OurFamilyWizard, and TalkingParents. Find the right app for high-conflict situations.  Read More.By @chris127 [ 8 Min read ] Stablecoins arent just crypto dollars—theyre experiments in digital money stability. Each type offers different trade-offs, learn more about them here Read More.By @mexcmedia [ 2 Min read ] MEXC COO Vugar Usi explains why retail-first exchanges are winning in crypto’s 2026 reset, leveraging zero-fee trading and user trust. Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Galileo&apos;s Handwritten Notes Discovered in a Medieval Astronomy Text</title><link>https://science.slashdot.org/story/26/02/28/0419233/galileos-handwritten-notes-discovered-in-a-medieval-astronomy-text?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In a library in Florence, Italy, historian Ivan Malara noticed handwritten notes on a book printed in the 1500s — and recognized the handwriting as Galileo's. The finding "promises new insights into one of the most famous ideological transitions in the history of science," writes Science magazine — since the book Galileo annotated was a reprint of Ptolemy's second-century work arguing that the earth was the center of the universe.

Galileo's notes, perhaps written around 1590, or roughly 2 decades before his groundbreaking telescope observations of the Moon and Jupiter, reveal someone who both revered and critically dissected Ptolemy's work. And they imply, Malara argues, that Galileo ultimately broke with Ptolemy's cosmos because his mastery of the traditional paradigm's reasoning convinced him that a heliocentric [sun-centered] system would better fulfill Ptolemy's own mathematical logic.
]]></content:encoded></item><item><title>Honor says its ‘Robot phone’ with moving camera can dance to music</title><link>https://techcrunch.com/2026/03/01/honor-says-its-robot-phone-with-moving-camera-can-dance-to-music/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Sun, 1 Mar 2026 15:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Honor first teased its “Robot phone” with a movable camera arm earlier this year. Ahead of the Mobile World Congress (MWC) in Barcelona, the Chinese company provided more details about the device, including how the robot can respond to different situations without commands. The company said that it is planning to launch this device in […]]]></content:encoded></item><item><title>Honor launches its new slim foldable Magic V6 with a 6,600 mAh battery</title><link>https://techcrunch.com/2026/03/01/honor-launches-its-new-slim-foldable-magic-v6-with-a-6600-mah-battery/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Sun, 1 Mar 2026 15:13:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Honor also previewed battery tech that could take foldable batteries over 7,000 mAh mark]]></content:encoded></item><item><title>Linux 7.1 Expected To See Nice Improvements For Reducing HRTICK Timer Overhead</title><link>https://www.phoronix.com/news/Linux-7.1-HRTICK-Timer</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 14:55:25 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A big set of kernel patches look like they will be submitted for the Linux 7.1 kernel cycle this spring to optimize the scheduler HRTICK timer and in turn allowing it to be enabled by default...]]></content:encoded></item><item><title>Anthropic’s Claude rises to No. 1 in the App Store following Pentagon dispute</title><link>https://techcrunch.com/2026/03/01/anthropics-claude-rises-to-no-2-in-the-app-store-following-pentagon-dispute/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 1 Mar 2026 14:48:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic’s chatbot Claude seems to have benefited from the attention around the company’s fraught negotiations with the Pentagon.]]></content:encoded></item><item><title>Rust 1.77.0: C-String Literals and More</title><link>https://hackernoon.com/rust-1770-c-string-literals-and-more?source=rss</link><author>Rust (Technical Documentation)</author><category>tech</category><pubDate>Sun, 1 Mar 2026 14:00:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Rust team is happy to announce a new version of Rust, 1.77.0. Rust is a programming language empowering everyone to build reliable and efficient software.\
If you have a previous version of Rust installed via , you can get 1.77.0 with:\
If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please report any bugs you might come across!This release is relatively minor, but as always, even incremental improvements lead to a greater whole. A few of those changes are highlighted in this post, and others may yet fill more niche needs.Rust now supports C-string literals () which expand to a nul-byte terminated string in memory of type . This makes it easier to write code interoperating with foreign language interfaces which require nul-terminated strings, with all of the relevant error checking (e.g., lack of interior nul byte) performed at compile time.Support for recursion in Async functions previously could not call themselves due to a compiler limitation. In 1.77, that limitation has been lifted, so recursive calls are permitted so long as they use some form of indirection to avoid an infinite size for the state of the function.\
This means that code like this now works:async fn fib(n: u32) -> u32 {
   match n {
       0 | 1 => 1,
       _ => Box::pin(fib(n-1)).await + Box::pin(fib(n-2)).await
   }
}
1.77.0 stabilizes  for struct fields, which provides access to the byte offset of the relevant public field of a struct. This macro is most useful when the offset of a field is required without an existing instance of a type. Implementing such a macro is already possible on stable, but without an instance of the type the implementation would require tricky unsafe code which makes it easy to accidentally introduce undefined behavior.\
Users can now access the offset of a public field with offset_of!(StructName, field). This expands to a  expression with the offset in bytes from the start of the struct.Enable strip in release profiles by defaultCargo profiles which do not enable debuginfo in outputs (e.g., ) will enable  by default.\
This is primarily needed because the (precompiled) standard library ships with debuginfo, which means that statically linked results would include the debuginfo from the standard library even if the local compilations didn't explicitly request debuginfo.\
Users which do want debuginfo can explicitly enable it with the debug flag in the relevant Cargo profile.slice::split_first_chunk_mutslice::split_last_chunk_mutMany people came together to create Rust 1.77.0. We couldn't have done it without all of you. Thanks!]]></content:encoded></item><item><title>Online Censorship in Schools Is Impacting Teachers As Much as Students</title><link>https://hackernoon.com/online-censorship-in-schools-is-impacting-teachers-as-much-as-students?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sun, 1 Mar 2026 14:00:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up forKlaxon, a newsletter that delivers our stories and tools directly to your inbox.\
Elizabeth Tyree was recently trying to teach her West Texas students about the connections between Emily Dickinson’s letters and her poetry. The project she designed asked students to read Dickinson’s correspondence and compare them to her art, finding examples of how one led to the other. Dickinson’s letters are available for free through The Internet Archive, a nonprofit, digital library that has, among other content, 44 million digitized books and texts at archive.org.\
But Tyree and her students couldn’t get to them. Archive.org is blocked by their school district. The federal government effectively mandated web filters for schools in 2000 through the Children’s Internet Protection Act. At the time, filters were seen as an important way to keep kids from accessing online porn. A Markup investigation published earlier this month, however, showed these filters have morphed into tools of digital censorship, keeping students in some districts from abortion information, sex ed, and LGBTQ+ resources, including suicide prevention.\
After our investigation published, teachers—including Tyree—took to social media to point out how the web filters frustrate them, too.\
Tyree has been in classrooms for 16 years, teaching students of all ages a mix of English, writing, science, and music. Because the federal government only requires districts to keep students from obscene and harmful content and otherwise leaves them to block whatever else they want, Tyree has had different problems from one district to another. Sometimes tech support will unblock websites she asks to be unblocked, but her request to get her students access to Archive.org was recently denied over a concern that the website also hosts adult content.\
“It was the only place online that we could get access to Emily Dickinson’s correspondence for free,” Tyree said. “We had to change the entire project.”\
Kaitlyn D’Annibale, an athletic trainer in Washington, D.C. who works with high school athletes, has run into similar hurdles. She needs access to the healthcare website MedBridge both for continuing education and to create home exercise programs for the students she works with, but the site is blocked by her school.\
“They pay for the membership, but I can’t access the site,” D’Annibale said.\
When she needs to review hospital MRIs to assess students’ playing capacity, she can’t go through the hospital portal to open them because such portals are blocked. Her workaround? Wait until she gets home to look on her own computer.\
She recently wanted to look up suicide prevention resources for a student. “Anything I put into Google that was ‘suicidal’ anything got blocked,” D’Annibale said. Eventually she made it to a useful PDF by getting creative with the wording of her search terms.\
D’Annibale said she has asked for sites to be unblocked in the past but the process is tedious and resolution is short-lived. Sites that the IT department has unblocked for her have reverted to being blocked. She hasn’t been able to figure out why.\
One commenter on TikTok said that the process for requesting sites be unblocked in her district requires her to do research about the blocked site (at home, where she can access it), make a case to an administrator, answer follow-up questions, wait for that person to take the request to a board for approval, and then answer more follow-up questions before a decision can be made.\
Another described a similar situation: “My district has sites blocked that are actually in the curriculum. We’re supposed to contact our district [department] heads to ask them to get it unblocked. Like they don’t have enough to do. It’s ridiculous.”\
Other teachers told The Markup about how hard it can be to make lesson plans for substitute teachers, because they aren’t sure which of the sites that are available to them are actually blocked to subs and students. They described indiscriminate blocks to YouTube, Pinterest, and Wikipedia—sites that have useful educational resources mixed in with other content.\
Nancy Willard saw this all coming.\
Back in 2000, Willard worked at the Center for Advanced Technology in Education at the University of Oregon and submitted testimony to the Children’s Internet Safety Committee, urging Congress not to require filters in the Children’s Internet Protection Act. Reached by phone this week, Willard called the filters a “technical, quick-fix solution” that either don’t work—because students find ways around them—or leave kids unprepared for the real world.\
“Their world doesn’t have filtering software on their computers at home and their world as adults isn’t going to have filtering software,” Willard said. “So if they haven’t developed the self-control, the ability to assess credibility of information, the ability to focus on the task at hand and not go play [online games], if they haven’t developed that ability, how effective are they going to be as adults?”\
Willard’s insistence that schools teach students about safe internet use made it into the law. Of course, much to her dismay and the continued frustration of teachers all over the country, the filtering requirement did, too.]]></content:encoded></item><item><title>SaaS in, SaaS out: Here’s what’s driving the SaaSpocalypse</title><link>https://techcrunch.com/2026/03/01/saas-in-saas-out-heres-whats-driving-the-saaspocalypse/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Sun, 1 Mar 2026 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[What's behind the SaaSpocalypse? It simply seems a new supreme has risen. ]]></content:encoded></item><item><title>ASUS Linux HID Driver Preparing To See Support For Newer Devices</title><link>https://www.phoronix.com/news/ASUS-WMI-Linux-Driver-New-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 11:50:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[There's been a recent lull in activity around the open-source Linux driver for ASUS devices with the HID interface used for supporting various features. But developer Denis Benato who has worked on the ASUS Armoury Linux driver and the like is working on advancing the ASUS HID driver for Linux systems...]]></content:encoded></item><item><title>Linux 7.0 Development &amp; Intel Panther Lake Proved Most Popular In February</title><link>https://www.phoronix.com/news/February-2026-Recap</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 11:29:15 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[During the last month on Phoronix there were 289 original open-source/Linux-related news articles and another 20 featured articles as in Linux hardware reviews and multi-page benchmark articles. There was a lot of interesting software and hardware happenings the past month but standing out the most was the Linux 7.0 merge window developments and the ramp of Intel Panther Lake Linux testing...]]></content:encoded></item><item><title>GNU Hurd On Guix Is Ready With 64-bit Support, SMP Multi-Processor Support &quot;Soon&quot;</title><link>https://www.phoronix.com/news/GNU-Hurd-64-bit-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 11:08:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[After hearing last month that GNU Hurd is "almost there" with x86_64 support, it was exciting to kickoff today by seeing a developer headline "The 64-bit Hurd is Here!" GNU Hurd 64-bit support is now said to be ready but SMP support for multiple processor cores and the like remain still in development...]]></content:encoded></item><item><title>Intel&apos;s Clear Linux Website No Longer Online</title><link>https://www.phoronix.com/news/Clear-Linux-Org-No-More</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 11:00:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Last July Intel sadly ended their Clear Linux distribution amid cost-cutting measures at the company. Clear Linux for a decade served at the forefront of Linux performance innovations and was consistently the fastest out-of-the-box Linux x86_64 distribution until Intel ended the Linux distribution without any advanced notice for its users. Intel had kept up the ClearLinux.org website online to download the final releases and access other technical content and forum discussions, etc. Sadly, that too was recently taken offline...]]></content:encoded></item><item><title>Letting Machines Decide What Matters</title><link>https://spectrum.ieee.org/ai-new-physics</link><author>Eliza Strickland</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NTAwNTQ3Ni9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyNDE3MjAxOX0.QlMo5IFUTRcgh7zKth97HuudGJgWc1nPjwiH9gJ6cEo/image.png?width=600" length="" type=""/><pubDate>Sun, 1 Mar 2026 11:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[AI systems in particle detectors now shape what physicists study]]></content:encoded></item><item><title>The TechBeat: The State of The Noonion: Blogging Our Way Through the AI Boom (3/1/2026)</title><link>https://hackernoon.com/3-1-2026-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Sun, 1 Mar 2026 07:11:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @mexcmedia [ 2 Min read ] 
 MEXC COO Vugar Usi explains why retail-first exchanges are winning in crypto’s 2026 reset, leveraging zero-fee trading and user trust. Read More.By @crafinsstudio [ 20 Min read ] 
 I tested eight piano apps on two pianos for three weeks. Here's what I'd actually recommend. Read More.By @lomitpatel [ 5 Min read ] 
 How CMOs win CFO buy-in using incrementality, trust, AI, and capital allocation to drive margin expansion and revenue durability. Read More.By @qatech [ 8 Min read ] 
 Manual testing can't keep up with modern development. See how QA.tech's AI testing automation catches bugs on every PR -- no Playwright or Cypress scripts to ma Read More.By @saumyatyagi [ 15 Min read ] 
 Most teams plateau at "AI writes code, a human reviews it." This article presents the Dark Factory Pattern — a four-phase architecture using holdout scenarios a Read More.By @scylladb [ 5 Min read ] 
 Blitz migrated from Postgres and Elixir to Rust and ScyllaDB, cutting latency, costs, and 100+ cores down to four cloud nodes. Read More.By @noonion [ 13 Min read ] 
 HackerNoon’s 2016–2026 evolution: $727k Q4 revenue, 62% Business Blogging CAGR, 4.4M monthly pageviews, and resilient, AI-aware publishing. Read More.By @melissaindia [ 4 Min read ] 
 Learn 6 proven strategies to secure executive buy-in for Master Data Management by aligning MDM with ROI, risk reduction, and business goals. Read More.By @confluent [ 5 Min read ] 
 Learn how Python developers build real-time AI agents using MCP, Kafka, and Flink—modern agentic workflows explained on HackerNoon. Read More.By @chris127 [ 8 Min read ] 
 Stablecoins aren't just "crypto dollars"—they're experiments in digital money stability. Each type offers different trade-offs, learn more about them here Read More.By @mexcmedia [ 2 Min read ] 
 MEXC ranks No. 1 globally in XAUT perpetual volume, hitting $3.43B as tokenized gold demand rises amid record spot gold prices in 2026. Read More.By @scylladb [ 4 Min read ] 
 Discover how Yieldmo migrated from DynamoDB to ScyllaDB to cut database costs, achieve multicloud flexibility, and deliver ads in single-digit millisecond laten Read More.By @opensourcetheworld [ 7 Min read ] 
 I replaced $1,200/year in cloud subscriptions with one home server. Here's the setup, costs, apps, Bitcoin node, local AI, and what I'd do differently.  Read More.By @khamisihamisi [ 4 Min read ] 
 Western tech is built in environments of abundance. In emerging markets, these assumptions often fail quickly. Read More.By @davidiyanu [ 8 Min read ] 
 Cloud cost and system reliability are the same problem viewed through different instruments.  Read More.By @thomascherickal [ 51 Min read ] 
 Google Antigravity is not just for coding. It is for your entire computer. Stop scrolling - everything you do on a computer has just been automated. Read More.By @johnpphd [ 4 Min read ] 
 How precompiling context for AI agents beats context stuffing. Lessons from building 100+ specialized agents for a web3 application. Read More.](https://hackernoon.com/the-complete-guide-to-ai-agent-memory-files-claudemd-agentsmd-and-beyond)** 
 By @paoloap [ 7 Min read ] 
 Learn how CLAUDE.md, AGENTS.md, and AI memory files work. Covers file hierarchy, auto-memory, @imports, and which files you actually need for your setup. Read More.]]></content:encoded></item><item><title>Silicon Valley&apos;s Ideas Mocked Over Penchant for Favoring Young Entrepreneurs with &apos;Agency&apos;</title><link>https://slashdot.org/story/26/03/01/011246/silicon-valleys-ideas-mocked-over-penchant-for-favoring-young-entrepreneurs-with-agency?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 05:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[In a 9,000-word expose, a writer for Harper's visited San Francisco's young entrepreneurs in September to mockingly profile "tech's new generation and the end of thinking." 
There's Cluely founder Roy Lee. ("His grand contribution to the world was a piece of software that told people what to do.") And the Rationalist movement's Scott Alexander, who "would probably have a very easy time starting a suicide cult..."

Alexander's relationship with the AI industry is a strange one. "In theory, we think they're potentially destroying the world and are evil and we hate them," he told me. In practice, though, the entire industry is essentially an outgrowth of his blog's comment section... "Many of them were specifically thinking, I don't trust anybody else with superintelligence, so I'm going to create it and do it well." Somehow, a movement that believes AI is incredibly dangerous and needs to be pursued carefully ended up generating a breakneck artificial arms race. 

There's a fascinating story about teenaged founder Eric Zhu (who only recently turned 18):

Clients wanted to take calls during work hours, so he would speak to them from his school bathroom. "I convinced my counselor that I had prostate issues... I would buy hall passes from drug dealers to get out of class, to have business meetings." Soon he was taking Zoom calls with a U.S. senator to discuss tech regulation... Next, he built his own venture-capital fund, managing $20 million. At one point cops raided the bathroom looking for drug dealers while Eric was busy talking with an investor. Eventually, the school got sick of Eric's misuse of the facilities and kicked him out. He moved to San Francisco. 

Eric made all of this sound incredibly easy. You hang out in some Discord servers, make a few connections with the right people; next thing you know, you're a millionaire... Eric didn't think there was anything particularly special about himself. Why did he, unlike any of his classmates, start a $20 million VC fund? "I think I was just bored. Honestly, I was really bored." Did he think anyone could do what he did? "Yeah, I think anyone genuinely can." 

The article concludes Silicon Valley's investors are rewarding young people with "agency". Although "As far as I could tell, being a highly agentic individual had less to do with actually doing things and more to do with constantly chasing attention online." Like X.com user Donald Boat, who successfully baited Sam Altman into buying him a gaming PC in "a brutally simplified miniature of the entire VC economy." (After which "People were giving him stuff for no reason except that Altman had already done it, and they didn't want to be left out of the trend.")

Shortly before I arrived at the Cheesecake Factory, [Donald Boat] texted to let me know that he'd been drinking all day, so when I met him I thought he was irretrievably wasted. In fact, it turned out, he was just like that all the time... He seemed to have a constant roster of projects on the go. He'd sent me occasional photos of his exploits. He went down to L.A. to see Oasis and ended up in a poker game with a group of weapons manufacturers. "I made a bunch of jokes about sending all their poker money to China," he said, "and they were not pleased...." 

"I don't use that computer and I think video games are a waste of time. I spent all the money I made from going viral on Oasis tickets." As far as he was concerned, the fact that tech people were tripping over themselves to take part in his stunt just confirmed his generally low impression of them. "They have too much money and nothing going on..." Ever since his big viral moment, he'd been suddenly inundated with messages from startup drones who'd decided that his clout might be useful to them. One had offered to fly him out to the French Riviera. 

The author's conclusion? "It did not seem like a good idea to me that some of the richest people in the world were no longer rewarding people for having any particular skills, but simply for having agency."]]></content:encoded></item><item><title>Rebuild Your Life in 180 Days: The No-Excuses Blueprint</title><link>https://hackernoon.com/rebuild-your-life-in-180-days-the-no-excuses-blueprint?source=rss</link><author>BenoitMalige</author><category>tech</category><pubDate>Sun, 1 Mar 2026 05:30:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
\
If you apply what’s on this email to the T, you can rebuild yourself in six months. Bold statement? Yes.But give me a few minutes and I’ll gift you the EXACT blueprint for:A different baseline of energy.A different standard for what you tolerate.A different life altogether.And this is not manifestation. This is , and this is what we do here.We will work on: identity, body, skills, environment, mind, and social circle. \n Run all six or don’t waste your time. Half-transformations are just elaborate procrastination.180 days. For a completely new life. This is important. This is doable. All you have to do is apply this.\
If you don’t change your identity, you’ll drag the same problems into your “new” life like a moldy suitcase. Most people fail not here because they try to bolt new habits onto an old self-image. and  transformation begins when you burn the blueprint of the person you’ve been and draw a new one from scratch. Humans behave in a way that matches the story they believe about themselves, even when the story sucks.If you think of yourself as someone who to get in shape, you’ll sabotage yourself like clockwork. \n If you think of yourself as someone who in shape, your decisions start matching that identity automatically.before strategy. \n before habits. \n  before everything.Everything (how you eat, how you talk, how you sit, how you dress) has to come from the new identity. If it doesn’t align, it dies.When your actions don’t match the person you claim to be, your brain rings the alarm. Everyone around you feels it too. Nothing smells worse than someone pretending to have standards they don’t enforce.Stop being the person who “intends.” \n Become the person who “does.”Before you rebuild, you rip out the rotten floorboards:Each one gets one question:Does this serve the future I’m building?If it’s not a hell yes, it’s a surgical no.Delete the apps. \n Cut the friends. \n Change the job. \n Burn the costume.Yes, it hurts. \n No, it won’t kill you. \n .Treat your new identity like religionRituals. Symbols. Structure.Morning routines, nighttime reflections, clothes that match your new standard, reminders on your wall, habits you don’t negotiate.Your  won’t die quietly. It will bribe you with nostalgia, craving, laziness, and bullshit stories about balance.Expect relapse thoughts. \n Prepare counters. \n .Choose your archetype — Write your identity profile: habits, values, style, energy, boundaries, even flaws.Cut contradictions — If your environment belongs to the old you, Act as if from day one — No “warming up.” You switch Document the proof — Log every moment you acted like the new identity.Protect the signal — Avoid people, environments, or content that drag you back.Accelerate the feedback loop — Put yourself in rooms where the new identity is  to belong.Once the identity locks in, your reality rearranges itself around you.(Your body is the receipt for your discipline.)You can talk about transformation all day. \n Your body is the part you can’t fake.When you walk into a room with a completely different body:People treat you differentlyYou treat yourself differentlyThis isn’t about six-pack obsession. \n This is about building a body that proves you finish what you start.Every rep becomes a vote. Every walk reinforces grit. Every choice cements identity.Most people fail because they try to “fit fitness in.” \n You don’t fit transformation into your life. \n You build your damn life around it.Discipline in the kitchen → discipline everywhere. \n Sloppiness in the body → sloppiness in ambition.“How you do one thing is how you do everything” is cliché because it’s true.One day, I decided to work out every single day. It’s a non negotiable. This is what I do, but you don’t have to go that extreme.1. Train 4x/week minimum — Heavy lifts. Push/pull/legs/full-body. \n Bonus: Add two long incline walks weekly.2. Eat like an adult, not a toddlerSame meals every day (or close)Alcohol, weed, binge nights. Anything that unravels discipline. Cut it.4. Walk 10,000 steps a day Rain or shine. Inside or outside. No excuses.5. Sleep like it’s a performance drug Because It is. \n 7.5 hours minimum. \n No screens 1–2 hours before bed. \n No caffeine after 2 p.m.If you’re not measuring, you’re guessing. \n And guessing is how you stay average.The gap is where normal people quit and transformed people are born.You lift when you’re tired. \n You walk when it rains. \n You prep meals when everyone else is ordering Uber Eats.That’s what creates the gulf between you and the old you.(You don’t need more confidence. You need skills that print confidence on demand.)\
Once the body and identity are locked in, you weaponize them.Power in the modern world = skills. \n Stackable, monetizable, rare skills.Skills put you in rooms the old you couldn’t even pronounce.Most people “learn” the slow way by dabbling, exploring, taking courses and doing nothing with them.You? \n You learn like your life depends on it. There is a full chapter dedicated to that in. Use it.The old you takes 6 months to start something. \n The new you learns a high-income skill in 2 weeks and gets paid by week 4.It’s not intelligence. \n It’s intensity.Pick ONE. \n Master it. \n Stack the others later.The 90-day mastery protocolChoose one skill — Eliminate everything else.7-day deep dive — Saturate your brain. \n 6+ hours/day. (you’ll find the time) \n Books, videos, podcasts, notes.Build one real project — Landing page, video, funnel, outreach sequence. Something you can show.Get feedback fast — Ask someone 10 steps ahead to tear it apart. (ChatGPT can do that very well if you ask it nicely).50 videos \n 100 tweets \n 100 cold emails \n 10 funnels \n Whatever matches your skill. VOLUME is king.Get paid ASAP — Even $39 counts. \n Once someone pays you, you’re in business.Repeat until you’re dangerous.(Willpower is overrated. Your environment is the real puppet master.)\
Your environment will beat your discipline over time. \n Always.You can have the perfect mindset. \n You can read the books. \n You can “be motivated.”But if you live in the same messy room, around the same lazy friends, with the same digital junk food…You will snap back to baseline.Delete apps that hijack focus.Unfollow accounts that normalize mediocrity.Unfollow friends that don’t have what you want.Clear your space of old-self objects and clutter.Remove junk food, trash habits, and triggers.Make good habits frictionlessLogged out of Netflix (have someone else change the password for you)No snacks in the house. Seriously.Your circle counts as environmentIf the people around you crawl, you won’t sprint.You don’t need dramatic exits, just become harder to reach. \n Distance does the work for you.And sometimes? You literally need to move. \n New city. \n New apartment. \n New country.Fresh soil grows different roots. Don’t be scared of change.Create sacred zones (work, training, rest)When your environment stops tolerating the old you, the old you suffocates.(If your mind is brittle, your success has an expiration date.)You can have the body, the skills, the money.. but if your mind collapses under stress, criticism, or uncertainty, you’re toast.Real resilience isn’t “staying positive.” \n That’s .Resilience is taking hits without turning them into excuses.You become mentally strong by doing hard things on purpose.You don’t react to every feeling.The gap between impulse and action is where adulthood starts.Most burned-out people aren’t doing too much — they’re doing too little of what matters.When weakness shows up, you don’t negotiate with it. \n You kill it.You need a sentence that snaps you back into execution instantly.Mine used to be: \n “Stop bullshitting yourself. Move.”3–5 non-negotiables dailyWeekly voluntary hardship (fasting, cold, public speaking, etc.)Cut mental junk food (fear-driven news, gossip, chaos content)Reinforce identity nightlyWhen your mind becomes unshakeable, your life becomes predictable in the best way.(Your circle is the hidden thermostat of your life.)Every relationship is either a plus-one or a minus-one. \n There is no neutral.Someone is either feeding your fire or smothering it.You become unrecognizable when you stop asking, \n “Do I like this person?” \n and start asking, \n “Do they make me better?”Score each person (+1 / 0 / -1)Replace with higher-caliber peopleHold boundaries like your life depends on itYour social ecosystem becomes a force multiplier. \n When everyone around you is winning, discipline stops feeling like effort, it becomes the baseline.\
The part everyone skips and wonders why nothing changes.Pick a start date within 72 hours. No “next Monday” bullshit.Run all six pillars in parallel. This is not a buffet. \n You don’t pick favorites.Measure your progress daily. Body, skills, environment, mindset, social shifts. .Audit every 2 weeks. What works stays. \n What stalls gets replaced.Treat this like a mission, not a vibe. You’re not here to worship the process. \n You’re here to become unrecognizable.]]></content:encoded></item><item><title>Sam Altman Answers Questions on X.com About Pentagon Deal, Threats to Anthropic</title><link>https://news.slashdot.org/story/26/03/01/0233230/sam-altman-answers-questions-on-xcom-about-pentagon-deal-threats-to-anthropic?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 1 Mar 2026 02:39:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Saturday afternoon Sam Altman announced he'd start answering questions on X.com about OpenAI's work with America's Department of War — and all the developments over the past few days. (After that department's negotions had failed with Anthropic, they announced they'd stop using Anthropic's technology and threatened to designate it a "Supply-Chain Risk to National Security". Then they'd reached a deal for OpenAI's technology — though Altman says it includes OpenAI's own similar prohibitions against using their products for domestic mass surveillance and requiring "human responsibility" for the use of force in autonomous weapon systems.) 
Altman said Saturday that enforcing that "Supply-Chain Risk" designation on Anthropic "would be very bad for our industry and our country, and obviously their company. We said [that] to the Department of War before and after. We said that part of the reason we were willing to do this quickly was in the hopes of de-esclation.... We should all care very much about the precedent... To say it very clearly: I think this is a very bad decision from the Department of War and I hope they reverse it. If we take heat for strongly criticizing it, so be it." 


Altman also said that for a long time, OpenAI was planning to do "non-classified work only," but this week found the Department of War "flexible on what we needed..."

 Sam Altman: The reason for rushing is an attempt to de-escalate the situation. I think the current path things are on is dangerous for Anthropic, healthy competition, and the U.S. We negotiated to make sure similar terms would be offered to all other AI labs. 

I know what it's like to feel backed into a corner, and I think it's worth some empathy to the Department of War. They are... a very dedicated group of people with, as I mentioned, an extremely important mission. I cannot imagine doing their work. Our industry tells them "The technology we are building is going to be the high order bit in geopolitical conflict. China is rushing ahead. You are very behind." And then we say "But we won't help you, and we think you are kind of evil." I don't think I'd react great in that situation. I do not believe unelected leaders of private companies should have as much power as our democratically elected government. But I do think we need to help them. 



Question: Are you worried at all about the potential for things to go really south during a possible dispute over what's legal or not later on and be deemed a supply chain risk...? 



Sam Altman: Yes, I am. If we have to take on that fight we will, but it clearly exposes us to some risk. I am still very hopeful this is going to get resolved, and part of why we wanted to act fast was to help increase the chances of that... 


Question: Why the rush to sign the deal ? Obviously the optics don't look great. 


Sam Altman: It was definitely rushed, and the optics don't look good. We really wanted to de-escalate things, and we thought the deal on offer was good. 
If we are right and this does lead to a de-escalation between the Department of War and the industry, we will look like geniuses, and a company that took on a lot of pain to do things to help the industry. If not, we will continue to be characterized as as rushed and uncareful. I don't where it's going to land, but I have already seen promising signs. I think a good relationship between the government and the companies developing this technology is critical over the next couple of years... 



Question: What was the core difference why you think the Department of War accepted OpenAI but not Anthropic? 


Sam Altman: [...] We believe in a layered approach to safety — building a safety stack, deploying FDEs [embedded Forward Deployed Engineers] and having our safety and alignment researcher involved, deploying via cloud, working directly with the Department of War. Anthropic seemed more focused on specific prohibitions in the contract, rather than citing applicable laws, which we felt comfortable with. We feel that it it's very important to build safe system, and although documents are also important, I'd clearly rather rely on technical safeguards if I only had to pick one... 




I think Anthropic may have wanted more operational control than we did... 



Question: Were the terms that you accepted the same ones Anthropic rejected? 


Sam Altman: No, we had some different ones. But our terms would now be available to them (and others) if they wanted. 



Question: Will you turn off the tool if they violate the rules? 



Sam Altman: Yes, we will turn it off in that very unlikely event, but we believe the U.S. government is an institution that does its best to follow law and policy. What we won't do is turn it off because we disagree with a particular (legal military) decision. We trust their authority.

 

Questions were also answered by OpenAI's head of National Security Partnerships (who at one point posted that they'd managed the White House response to the Snowden disclosures and helped write the post-Snowden policies constraining surveillance during the Obama years.) And they stressed that with OpenAI's deal with Department of War, "We control how we train the models and what types of requests the models refuse."




Question: Are employees allowed to opt out of working on Department of War-related projects? 


Answer: We won't ask employees to support Department of War-related projects if they don't want to. 



Question: How much is the deal worth? 


Answer: It's a few million $, completely inconsequential compared to our $20B+ in revenue, and definitely not worth the cost of a PR blowup. We're doing it because it's the right thing to do for the country, at great cost to ourselves, not because of revenue impact... 




Question: Can you explicitly state which specific technical safeguard OpenAI has that allowed you to sign what Anthropic called a 'threat to democratic values'? 


Answer: We think the deal we made has more guardrails than any previous agreement for classified AI deployments, including Anthropic's. Other AI labs (including Anthropic) have reduced or removed their safety guardrails and relied primarily on usage policies as their primary safeguards in national security deployments. Usage policies, on their own, are not a guarantee of anything. Any responsible deployment of AI in classified environments should involve layered safeguards including a prudent safety stack, limits on deployment architecture, and the direct involvement of AI experts in consequential AI use cases. These are the terms we negotiated in our contract. 

They also detailed OpenAI's position on LinkedIn:

Deployment architecture matters more than contract language. Our contract limits our deployment to cloud API. Autonomous systems require inference at the edge. By limiting our deployment to cloud API, we can ensure that our models cannot be integrated directly into weapons systems, sensors, or other operational hardware... 



Instead of hoping contract language will be enough, our contract allows us to embed forward deployed engineers, commits to giving us visibility into how models are being used, and we have the ability to iterate on safety safeguards over time. If our team sees that our models aren't refusing queries they should, or there's more operational risk than we expected, our contract allows us to make modifications at our discretion. This gives us far more influence over outcomes (and insight into possible abuse) than a static contract provision ever could. 



U.S. law already constrains the worst outcomes. We accepted the "all lawful uses" language proposed by the Department, but required them to define the laws that constrained them on surveillance and autonomy directly in the contract. And because laws can change, having this codified in the contract protects against changes in law or policy that we can't anticipate.]]></content:encoded></item><item><title>AerynOS 2026.02 Brings More Wayland Compositor Options, Other Improvements</title><link>https://www.phoronix.com/news/AerynOS-2026.02</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 1 Mar 2026 01:13:43 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[AerynOS 2026.02 was released for closing out February as the newest alpha release for this Linux distribution formerly known as Serpent OS. In AerynOS 2026.02 are many package updates plus continued work on the tooling and other innovations around this Linux distribution...]]></content:encoded></item><item><title>The trap Anthropic built for itself</title><link>https://techcrunch.com/2026/02/28/the-trap-anthropic-built-for-itself/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 1 Mar 2026 00:08:58 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Anthropic, OpenAI, Google DeepMind and others have long promised to govern themselves responsibly. Now, in the absence of rules, there's not a lot to protect them.]]></content:encoded></item><item><title>Why did Netflix back down from its deal to acquire Warner Bros.?</title><link>https://techcrunch.com/2026/02/28/why-did-netflix-back-down-from-its-deal-to-acquire-warner-bros/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 28 Feb 2026 22:07:48 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Netflix's co-CEO reportedly told Trump, "I took your advice."]]></content:encoded></item><item><title>What to know about the landmark Warner Bros. Discovery sale</title><link>https://techcrunch.com/2026/02/28/warner-bros-netflix-paramount-acquisition-timeline-wbd/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Sat, 28 Feb 2026 21:28:06 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Learn more about Paramount's planned acquisition of Warner Bros. Discovery — a historic Hollywood megadeal valued at $111 billion — as it continues to develop.]]></content:encoded></item><item><title>The billion-dollar infrastructure deals powering the AI boom</title><link>https://techcrunch.com/2026/02/28/billion-dollar-infrastructure-deals-ai-boom-data-centers-openai-oracle-nvidia-microsoft-google-meta/</link><author>Russell Brandom</author><category>tech</category><pubDate>Sat, 28 Feb 2026 20:41:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Here's everything we know about the biggest AI infrastructure projects, including major spending from Meta, Oracle, Microsoft, Google, and OpenAI.]]></content:encoded></item><item><title>The Simplest Way to Understand How LLMs Actually Work!</title><link>https://hackernoon.com/the-simplest-way-to-understand-how-llms-actually-work?source=rss</link><author>Amit Juneja</author><category>tech</category><pubDate>Sat, 28 Feb 2026 19:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The magic of transformers lies in their attention mechanism. But what does that actually mean?\
Here's a simplified explanation to build intuition.Consider: "What is the capital of France?"As humans, we parse this as:"What" signals a question"is" indicates the current timeframe"capital" means the main city"France" is the country for which I want the capitalWe process it instantly. But for a computer? Different story.THE ATTENTION MECHANISM: Q, K, VTransformers use a clever trick: for every word (technically tokens), the model creates three different representations:Query (Q) - "What information am I looking for?"For the word "capital," the query is something like: "What kind of entity am I describing?"Key (K) - "What information can I provide?"Every word gets a key that describes what it offers. For the word "capital," the key is something like: "I'm a noun describing geographic/political entities."Value (V) - "Here's my actual meaning."The word "capital" has the semantic meaning "main city, governmental center, and administrative importance."The model compares the query from one word against the keys of all other words. This produces .Here is what happens when the word "capital", with its query of "What kind of entity am I describing?", checks against the keys of all the other words:"France" responds with its key → "What" responds with "is" responds with Higher scores contribute more to the final understanding. So after this, the representation of "capital" is enriched with strong context from "France."This doesn't happen just once. Transformers use  running in parallel, like several people reading the same sentence, each noticing different patterns. One might focus on grammar, another on meaning, another on long-range dependencies.In another head, the word "capital" could be querying for the timeframe. In this case, the word "is" will give a high score for the current time.All these attention scores combined give a rich context to each word. So the word "capital" knows that it is a question, it is for the current timeframe, and it is about "France."After each attention layer, information flows through a Feed Forward Network. This is where the answers start to form. This network processes the context-enriched representations, helping build toward output predictions like 'Paris.'The combination of attention + FFN, repeated across layers, gives transformers their power.Unlike older models that processed words one at a time, transformers:Look at the entire sentence at onceLet every word "attend to" every other wordCapture relationships between distant wordsBuild understanding through multiple layersThat's transformer attention in action.*This explanation simplifies many technical details to focus on core concepts. For a deeper dive, check out "Attention Is All You Need" by Vaswani et al.*]]></content:encoded></item><item><title>AMD Prepares Linux For Instruction-Based Sampling Improvements With Zen 6</title><link>https://www.phoronix.com/news/Linux-Perf-AMD-IBS-Zen-6</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:58:45 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A set of patches recently posted to the Linux kernel mailing list have now been queued up to a tip/tip.git branch for planned introduction in Linux 7.1. These patches are for enhancing the Linux perf subsystem support for AMD Instruction-Based Sampling (IBS) improvements with next-gen Zen 6 processors...]]></content:encoded></item><item><title>OpenAI’s Sam Altman announces Pentagon deal with ‘technical safeguards’</title><link>https://techcrunch.com/2026/02/28/openais-sam-altman-announces-pentagon-deal-with-technical-safeguards/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:17:36 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[OpenAI's CEO claims its new defense contract includes protections addressing the same issues that became a flashpoint for Anthropic.]]></content:encoded></item><item><title>How Researchers Measure, Detect and Benchmark AI Manipulation</title><link>https://hackernoon.com/how-researchers-measure-detect-and-benchmark-ai-manipulation?source=rss</link><author>Tencent</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:15:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Enes Altuncu, ea483@kent.ac.uk (University of Kent, UK)Virginia N. L. Franqueira, V.Franqueira@kent.ac.uk (University of Kent, UK)Shujun Li, S.J.Li@kent.ac.uk (University of Kent, UK)Recent advancements in AI, especially deep learning, have contributed to a significant increase in the creation of new realistic-looking synthetic media (video, image, and audio) and manipulation of existing media, which has led to the creation of the new term “deepfake”. Based on both the research literature and resources in English and in Chinese, this paper gives a comprehensive overview of deepfake, covering multiple important aspects of this emerging concept, including 1) different definitions, 2) commonly used performance metrics and standards, and 3) deepfake-related datasets, challenges, competitions and benchmarks. In addition, the paper also reports a meta-review of 12 selected deepfake-related survey papers published in 2020 and 2021, focusing not only on the mentioned aspects, but also on the analysis of key challenges and recommendations. We believe that this paper is the most comprehensive review of deepfake in terms of aspects covered, and the first one covering both the English and Chinese literature and sources.: Deepfake, Survey, Definition, Datasets, Benchmarks, Challenges, Competitions, Standards, Performance Metrics.Recent advancements in AI and machine learning have increased the capability to produce more realistic media, e.g., video, image, and audio. Especially, state-of-the-art deep learning methods enabled the generation of “deepfakes”, manipulated or synthetic media the realness of which are not easily recognisable by the human eye. Although deepfake is a relatively new phenomenon (having first appeared at the end of 2017), its growth has been remarkable. According to the 2019 and 2020 Deeptrace reports on the state of deepfake [2], the number of deepfake videos in the English-speaking internet grew from 7,964 (December 2018) to 14,678 (July 2019) to 85,047 (December 2020), representing a 968% increase from 2018 to 2020.In this work, we review existing deepfake-related research ecosystem in terms of various aspects, including performance metrics and standards, datasets, challenges, competitions, and benchmarks. Furthermore, we provide a meta-review of 12 selected deepfake-related survey papers which covers several additional aspects other than the mentioned ones in a systematic manner, such as performance comparison, key challenges, and recommendations.Despite being a hugely popular term, there is a lack of consensus on the definition of “deepfake” and the boundary between deepfakes and non-deepfakes is not clear cut. For this survey, we adopt a relatively more inclusive approach to cover all forms of manipulated or synthetic media that are considered deepfakes in a broader sense. We also cover closely related topics including biometrics and multimedia forensics, since deepfakes are often used to launch presentation attacks against biometrics-based authentication systems and detection of deepfakes can be considered part of multimedia forensics. A more detailed discussion on different definitions of “deepfake” is given next.1.1       Definitions of the Term DeepfakeAs its name implies, the term “deepfake” is derived from the combination of “deep” (referring to  (DL)) and “fake”. It is normally used to refer to manipulation of existing media (image, video and/or audio) or generation of new (synthetic) media using DL-based approaches. The most commonly discussed deepfake data are fake face images, fake speech forgeries, and fake videos that combine both fake images and fake speech forgeries. While having “fake” in the word indicates manipulated or synthesised media, there are plenty of benign applications of the deepfake technology, e.g., for entertainment and creative arts. With this respect, another term “deep synthesis” has been proposed as a more neutral-sounding alternative [60]. This new term, however, has not been widely adopted.In addition to the lack of a universal definition, as mentioned already, the boundary between deepfakes and non-deep fakes is actually not a clear cut. There are at least two important aspects we should consider, one on detection of and the other on creation of deepfakes.First, detection of deepfakes often follows very similar approaches to detection of traditional fakes generated without using DL techniques. Advanced detection methods have also started leveraging DL to improve their performance, but they do not necessarily need to know how a target media is created (deep or not). To some extent, one could argue that detecting deepfakes does not involve developing deepfake-specific methods (even though some researchers choose to do so), but a more robust and universal detector that can handle any (deep or not) fake media. This can be seen for two closely related topics: biometrics and multimedia forensics. For biometrics, there is a trend of using deep learning techniques to generate fake biometric signals (e.g., face images and videos) for biometric spoofing or presentation attacks. For multimedia forensics, deepfake-based forgeries have become a new threat to the traditional problem of “forgery detection”. For both topics, detection of biometric spoofing and multimedia forgeries have evolved to consider both deep and non-deep fakes.Second, one may argue that the word “deep” in “deepfake” does not necessarily refer to the use of “deep learning”, but any “deep” (i.e., sophisticated) technology that creates a very believable fake media. For instance, Brady [9] considered deepfake as audio-visual manipulation using “a spectrum of technical sophistication … and techniques”. They also introduced two new terms,  and , referring to “low level manipulation of audio-visual media created with (easily) accessible software [or no software] to speed, slow, restage or re-contextualise content”. This broader understanding of “deepfake” has also been adopted by law makers for new legislations combating malicious deepfakes. For instance, the following two United States acts define “deepfakes” as follows:2018 Malicious Deep Fake Prohibition Act1:§1041.(b).(2): “the term ‘deep fake’ means an audiovisual record created or altered in a manner that the record would falsely appear to a reasonable observer to be an authentic record of the actual speech or conduct of an individual.”2019 DEEP FAKES Accountability Act2:§1041.(n).(3): “The term ‘deep fake’ means any video recording, motion-picture film, sound recording, electronic image, or photograph, or any technological representation of speech or conduct substantially derivative thereof—(A)  which appears to authentically depict any speech or conduct of a person who did not in fact engage in such speech or conduct; and(B)  the production of which was substantially dependent upon technical means, rather than the ability of another person to physically or verbally impersonate such person.”As we can see from the above legal definitions of “deepfake”, the use of DL as a technology is not mentioned at all. The focus here is on “authenticity”, “impersonation” and (any) “technical means”.1.2      Scope and ContributionBased on the above discussion on definitions of deepfake, we can see it is not always straightforward or meaningful to differentiate deepfakes from non-deep fakes. In addition, for our focus on performance evaluation and comparison, the boundary between deepfakes and non-deep fakes is even more blurred. This is because DL is just a special (deeper) form of machine learning (ML), and as a result, DL and non-deep ML methods share many common concepts, metrics and procedures.Despite the fact that deepfake may be understood in a much broader sense, in this work, we have a sufficiently narrower focus to avoid covering too many topics. We, therefore, decided to define the scope of this survey as follows:For metrics and standards, we chose to include all commonly used ones for evaluating general ML methods and those specifically defined for evaluating deepfake creation or detection methods.For datasets, challenges, competitions and benchmarks, we considered those related to fake media covered in the deepfake-related survey papers and those with an explicit mention of the term “deepfake” or a comparable term.For the meta-review, we considered only survey papers whose authors explicitly referred to the term “deepfakes” in the meta data (title, abstract and keywords).Research papers covered in this survey (i.e., the deepfake-related survey papers) were identified via systematic searches on the scientific databases, Scopus and China Online Journals (COJ)3. The following search queries were used to perform the searches on Scopus and COJ, respectively:(deepfake* OR deep-fake* OR “deep fake*”) AND (review OR survey OR overview OR systemati* OR SoK)(deepfake OR 深度伪造) AND (综述 OR 进展)The searches returned 41 survey papers in English and 15 survey papers in Chinese. Out of these papers, eight published in English and four published in Chinese were selected for consideration.Deepfake-related challenges, competitions and benchmarks were identified via multiple sources: the survey papers selected, research papers from the co-authors’ personal collections, Google Web searches, and manual inspection of websites of major AI-related conferences held in 2020 and 2021 (where such challenges and competitions are routinely organised). The inspected conferences include those listed in the ACL (Association for Computational Linguistics) Anthology4, ICCV, CVPR, AAAI, ICML, ICLR, KDD, SIGIR, WWW, and many others. In addition, a comprehensive list of datasets was compiled based on the selected survey papers and the identified challenges, competitions, and benchmarks. Relevant standards were identified mainly via research papers covered in this survey, the co-authors’ personal knowledge, and Google Web searches. For performance metrics, we covered those commonly used based on relevant standards, the survey papers, and the identified challenges, competitions, and benchmarks.In this survey, we focus on performance evaluation and comparison of deepfake generation and detection methods. The metrics used for such performance evaluations are at the core of our discussions. In this section, we review the performance metrics that are commonly used to evaluate deepfake generation and detection algorithms. Note that all metrics covered in this section are also commonly used for evaluating performance of similar systems that are not for generating or detecting deepfakes. Therefore, this section can be seen as a very brief tutorial on general performance metrics.In the last subsection, we also briefly discuss how the related performance metrics are covered in formal standards. By “formal standards”, we refer to standards defined following a formal procedure, often by one or more established standardisation bodies such as the International Organization for Standardization (ISO)5 and the International Electrotechnical Commission (IEC)6. Note that we consider a broad range of documents defined to be standards by standardisation bodies, e.g., International Telecommunication Union (ITU)7 recommendations and ISO technical reports (TRs).3.1      The Confusion MatrixDeepfake detection is primarily a binary classification problem. A binary classifier takes an input that is  or  and outputs a binary value denoting it to be  or . For example, a deepfake detection system will take a suspected image as the input that may be  or  and output  or .A fundamental tool used in evaluating a binary classifier is the  that summarises the success and failure of the classification model. On one axis are the two  values and on the other axis are the two  values. The classification is  (true positive and true negative) when the actual and the predicted values match. It is  (false positive and false negative) when the actual and predicted values do not match. Table 1 shows the confusion matrix for a binary deepfake classifier (detector). The two cells in green, TP (the number of ) and TN (the number of ), indicate correct prediction results, and the two cells in red, FN (the number of ) and FP (the number of ), indicate two different types of errors when making incorrect prediction results.\
Table 1: Confusion matrix for a binary classifier for detecting deepfake.|    | fake (predicted) | real (predicted) |
|----|----|----|
| fake (actual) | TP | FN |
| real (actual) | FP | TN |3.2      Precision and RecallBased on the four fundamental values introduced in Section 3.1, i.e., TP, TN, FP and FN, we define two important performance metrics for a binary classifier –  and .Precision of a binary classifier is defined as the fraction of  samples among all the . In the confusion matrix, it is the fraction of true samples in the first column. It can be formally defined as Eq. (1).When the “natural” ratio between positive and negative samples is significantly different from the test set, it is often useful to adjust the weight of the false positives, which leads to the  (wP) defined in Eq. (2), where  0 is a weight determined by the ratio between the negative and positive samples.Recall of a binary classifier is the fraction of  samples among the  samples, as shown in Eq. (3). In the confusion matrix, it is the fraction of true samples in the first row.Let us consider an example binary classifier that predicts if an image from a database containing both deepfake and real (authentic) images is fake or not. Precision of the classifier is the fraction of correctly classified images among all images classified as deepfake. On the other hand, recall is the fraction of deepfake images identified by the classifier, among all deepfake images in the database.3.3      True and False Positive RatesFocusing on predicted positive samples, we can also define two metrics:  (TPR), also called  (CDR), as the fraction of the predicted positive samples among the actually positive samples and  (FPR), also called  (FAR), as the fraction of the predicted positive samples among the actually negative samples, as shown in Eqs. (4) and(5). In the confusion matrix, TPR is the fraction of predicted positive samples in the first row and FPR is the fraction of predicted positive samples in the second row. Note that TPR is basically a different name for  (Eq. (3)).3.4     True and False Negative RatesSimilar to true and false positive rates, we can define two other rates focusing on negative predicted results:  (TNR) indicating the fraction of the predicted negative samples among the actually negative samples, and  (FNR) indicating the fraction of the predicted negative samples among the actually positive samples, as shown in Eqs. (6) and (7).3.5      Sensitivity and SpecificityIn some applications of binary classifiers, especially in biology and medicine, the TPR and the TNR are more commonly used, and they are often called  (TPR) and  (TNR). The focus of these two terms is on the two types of correctness of the predicted results. These are less used in deepfake-related research, hence, we will not refer to them in the remainder of this paper.Focusing on error rates means that we need to consider the FPR and the FNR. These two rates normally conflict with each other so that reducing one rate normally leads to an increase in the other. Therefore, rather than trying to reduce both error rates at the same time, which is normally impossible, the more realistic task in practical applications is to find the right balance so that they are both below an acceptable threshold.In some applications, such as biometrics, people are particularly interested in establishing the so-called  (EER) or  (CER), the point where the FPR and the FNR are equal. The EER/CER is not necessarily a good metric for some applications, especially when the two types of errors are of different levels of importance, e.g., for detecting critical deepfakes (e.g., fake news that can influence how people cast their votes) we can often tolerate more false positives (false alarms) than false negatives (missed alarms).3.7      Accuracy and F-ScoreIn addition to the EER/CER, there are also other metrics that try to reflect both types of errors, in order to give a more balanced indication of the overall performance of a binary classifier. The two most commonly used are  and  (also called ). Both metrics can be defined based on the four fundamental values (TP, TN, FP, and FN).Accuracy of a binary classifier is defined as the fraction of  samples (true positives and true negatives) among the total number of samples that have been classified, as shown in Eq. (8).The F-score of a binary classifier is actually a family of metrics. Its general form can be described based on a parameter  as defined in Eq. (9).The most widely used edition of all F-scores is the so-called , which is effectively the F-score with  = 1. More precisely, it is defined as shown in Eq. (10).3.8     Receiver Operating Characteristic Curve and Area Under CurveReceiver operating characteristic (ROC) curves are commonly used to measure the performance of binary classifiers that output a score (or probability) of prediction.Consider the following. Let  be the set of all test samples and let the output scores  () (for all  ∈ ) lie in the interval [] on the real line. Let  ∈ [] be a prediction threshold for the model, and assume that the classifiers works as follows for all  ∈ :\
It is easy to see that, for  = , all the samples will be classified as positive, leading to FN = TN = 0 so TPR = FPR = 1; while for  = , all the samples will be classified as negative, leading to FP = TP = 0 so TPR = FPR = 0. For other threshold values between  and , the values of TPR and FPR will normally be between 0 and 1. By changing  from  to  continuously, we can normally get a continuous curve that describes how the TPR and FPR values change from (0,0) to (1,1) on the 2D plane. This curve is the ROC curve of the binary classifier.For a random classifier, assuming that  () distributes uniformly on [] for the test set, we can mathematically derive its ROC curve being the TPR = FPR line, whose area under the ROC curve (AUC) is 0.5. For a binary classifier that performs better than a random predictor, we can also mathematically prove that its AUC is always higher than 0.5, with 1 being the best possible value. Note that no binary classifier can have an AUC below 0.5, since one can simply flip the prediction result to get a better predictor with an AUC of 1 − AUC. The relationship between the ROC and the AUC is graphically illustrated in Figure 1.Another widely used performance metric for binary classifiers that can return a probability score for the predicted label is . For a binary classification with a true label  ∈ {0*,* 1} and an estimated probability  = Pr( = 1), the log loss per sample is the negative log-likelihood of the classifier given the true label, defined as shown in Eq. (12).Given a testing set with  samples, the log loss score of a binary classifier can be calculated using Eq. (13), where  is 1 if the -th sample is true and 0 if false, and ˆ is the predicted probability of  = 1.3.10     Extension to Multi-class ClassifiersAll metrics that are defined based on the four basic values TP, TN, FP and FN can be easily extended to multi-class classification by considering the prediction to be true or false individually with respect to each class. For example, if the system is classifying animals (cats, dogs, horses, lions, tigers, etc.), then a true positive prediction of an image to be of a cat, would simultaneously be true negative predictions for the remaining classes (dogs, horses, lions, tigers, etc.). If an image of a cat is incorrectly predicted to be that of a dog, it would be a false negative with respect to a cat, a false positive with respect to a dog, and a true negative with respect to all other classes.3.11      Perceptual Quality Assessment (PQA) MetricsBy definition, the main goal of deepfakes is to make it hard or impossible for human consumers (listeners or viewers) to distinguish fake media from real media. Therefore, when evaluating the quality of deepfake media, the quality perceived by human consumers of the media is key. This calls for subjective assessment of the perceptual quality of the deepfake media as the “gold standard”. The most widely used subjective perceptual quality assessment (PQA) metric for audio-visual signals is  (MOS), which has been widely used by the signal processing and multimedia communication communities, including digital TV and other multimedia-related consumer applications. As its name implies, MOS is calculated by averaging the subjective scores given by a number of human judges, normally following a numerical scale between 1 and 5 or between 0 and 100. MOS has been used in some deepfake-related challenges (see Section 5.2) and also for evaluating and comparing the quality (realness/naturalness) of deepfake datasets (see Section 4.6).As a general subjective PQA metric, MOS has been standardised by the ITU8. There are also ITU standards defining more specific subjective Video Quality Assessment (VQA) metrics and the standard procedures one should follow to conduct VQA user studies, e.g., ITU-T Recommendation P.910 “Subjective video quality assessment methods for multimedia applications”9. Note that the ITU standards focus more on traditional perceptual quality, i.e., how good a signal looks or sounds, even if it looks or sounds not real (e.g., too smooth). On the other hand, for deepfakes, the focus is rather different because what matters is the realness and naturalness of the created media, i.e., how real and natural it looks or sounds, even if it is of low quality. To some extent, we can also consider realness and naturalness as a special aspect of perceptual quality.One major problem of subjective PQA metrics like MOS is the need to recruit human judges and to have a well-controlled physical testing environment and protocol, which are not easy for many applications. To help reduce the efforts and costs of conducting PQA-related user studies, various objective PQA metrics have been proposed, where the term “objective” refers to the fact that such metrics are human-free, i.e., automatically calculated following a computational algorithm or process. Depending on whether a reference exists, such objective PQA metrics can be largely split into three categories: full-reference (FR) metrics (when the original “perfect-quality” signal is available as the reference), reduced-reference (RR) metrics (when some features of the original “perfect-quality” signal are available as the reference), and no-reference (NR) metrics (when the original signal is unavailable or such an original signal does not exist). For deepfakes, normally NR or RR metrics are more meaningful because the “fake” part of the word means that part of the whole data does not exist in the real world, hence a full reference cannot be obtained. RR metrics are still relevant because deepfakes are often produced for a target’s specific attributes (e.g., face and voice), where the reduced reference will be such attributes. NR metrics will be useful to estimate the realness and naturalness of a deepfake, simulating how a human judge would rate it in a controlled subjective PQA user study.PQA is a very active research area and many PQA metrics have been proposed, some of which have been widely used in real-world products and services, e.g.,  (MSE), peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) for FR PQA of digitalimages and videos defined as in Eqs. (14), (15), and (16), respectively, where X = {xi} n i is the reference (the original signal), Y = {yi} n i is the signal whose visual quality is assessed, n is the number of pixels in X and Y , L is the maximum possible pixel value of X and Y (e.g., 255 for 8-bit gray-scale images), c1 = (k1L) 2 and c2 = (k2L) 2 ) are two stabilising parameters (k1 = 0.01 and k2 = 0.03 by default). For more about PQA metrics for different types of multimedia signals, we refer readers to some relevant surveys [3, 51, 72].3.12      More about StandardsMany of the basic performance metrics described in this section have been widely used by deepfake researchers as de facto standards, e.g., EER, log loss and MOS have been widely used in deepfake-related challenges (see Section 5). Also, the combination of precision, recall and F1-score has been widely used to assess performance of binary classifiers. While there have been a number of ITU standards on PQA to date, there does not seem to be many standardisation efforts on the performance metrics for evaluation of binary classifiers. This was the case until at least 2017, when ISO and IEC jointly set up the ISO/IEC JTC 1/SC 4210, a standardisation subcommittee (SC) focusing on AI under ISO/IEC JTC 111, the joint technical committee for standardising “information technology”.One recent effort that ISO/IEC JTC 1/SC 42 made is to produce the ISO/IEC TR 24029-1:2021 “Artificial Intelligence (AI) – Assessment of the robustness of neural networks – Part 1: Overview”12, a technical report (TR) that systematically covers many commonly used performance assessment concepts, methods and metrics. Although the technical report has “neural networks” in its title, most performance assessment concepts, methods and metrics included are common ones for all supervised machine learning models.In terms of performance metrics, two other ongoing work items of the ISO/IEC JTC 1/SC 42 that deserve attention are as follows:ISO/IEC DTS (Draft Technical Specification) 4213 “Information technology – Artificial Intelligence – Assessment of machine learning classification performance”13ISO/IEC AWI (Approved Work Item) TS (Technical Specifications) 5471 “Artificial intelligence – Quality evaluation guidelines for AI systems”14While the ISO/IEC JTC 1/SC 42 was created very recently, another standardisation subcommittee under ISO/IEC JTC1 has a much longer history of nearly 20 years: the ISO/IEC JTC 1/SC 3715 that focuses on biometrics-related technology. This standardisation subcommittee is highly relevant for deepfake since deepfake faces can be used to spoof biometrics-based user authentication systems. In this context, the following three standards are of particular relevance:ISO/IEC 19795-1:2021 “Information technology – Biometric performance testing and reporting – Part 1: Principles and framework”16: This standard covers general metrics about evaluating biometric systems. Two major metrics in this context are  (FAR) and  (FRR), which refer to the standard FPR and FNR, respectively. This standard also deprecates the use of single-number metrics including the EER and AUC (which were widely used in biometrics-related research in the past).ISO/IEC 30107-1:2016 “Information technology – Biometric presentation attack detec-tion – Part 1: Framework”17: This standard defines a general framework about presentation attack detection (PAD) mechanisms, where the term “” refers to the “presentation of an artefact or of human characteristics to a biometric capture subsystem in a fashion intended to in-terfere with system policy”. It focuses on biometric recognition systems, where a PAD mechanism is a binary classifier trying to predict presentation attacks (also called attack presentations, e.g., fake faces) as positive and bona fide (real) presentations as negative.ISO/IEC 30107-3:2017 “Information technology – Biometric presentation attack detection – Part 3: Testing and reporting”18: This standard defines a number of special performance metrics for evaluating PAD mechanisms standardised in the ISO/IEC 30107-1:2016. Three such metrics look at error rates: attack presentation classification error rate (APCER) referring to the standard FPR, normal/bona fide presentation classification error rate (NPCER/BPCER) referring to the standard FNR, and average classification error rate (ACER) that is defined as the average of the APCER and the NPCER/BPCER. Such metrics have been used in biometrics-related challenges such as Face Anti-spoofing (Presentation Attack Detection) Challenges19. When deepfake images or videos are used to spoof a biometric system, such standardised metrics will become relevant.This section provided a comprehensive summary of performance metrics used for evaluating and bench-marking binary classifiers. It is rare that all such metrics are used for a specific application. Instead, one or several are chosen based on specific needs. For a deepfake detection system as a binary classifier, many researchers have chosen to use overall metrics such as accuracy, AUC, EER and log loss, but the combination of precision, recall and F1-score is also common. Some deepfake-related challenges and competitions have introduced their own specific metrics, some of which will be described in Section 5. The use of different performance metrics can make comparison of different reported results more difficult, so we hope the expected new ISO/IEC standard particularly ISO/IEC 4213 will help.It is worth mentioning that, in addition to evaluating performance of deepfake detectors, the introduced performance metrics for evaluating binary classifiers can also be used to evaluate performance of deepfake generation methods by considering how deepfake detectors fail. For instance, organisers of the Voice Conversion Challenge 2018 and 2020 used this approach to benchmark how well voice conversion (VC) systems can generate high-quality fake speech samples.Another point we would like to mention is that for deepfake videos there are two levels of performance metrics: those at the frame level (metrics of each frame), and those at the video level (metrics for the whole video). Generally speaking, the latter can be obtained by averaging the former for all frames, potentially following an adaptive weighting scheme, so that more important (key) frames will be counted more.In this section, we cover all deepfake-related datasets we identified from the meta-review of deepfake-related survey papers, deepfake-related challenges, competitions and benchmarks covered, one online collection of deepfake-related datasets on GitHub20, and the co-authors’ personal collections. Table 2 shows basic information about these datasets. We explain them in four categories: deepfake image datasets, deepfake video datasets, deepfake audio/speech datasets, and hybrid deepfake datasets (mainly mixed image and video datasets).Note that many datasets of real (authentic) media were also used by deepfake researchers for two purposes. First, any detectors would need both fake and real media to demonstrate their performance. Second, real media have also been used to train deepfake generators as the training set. In this section, we include only datasets containing deepfake media, some of which contain both deepfake and real media.Some datasets, especially those created for deepfake-related challenges and competitions, have separate subsets for training and evaluation (testing) purposes. The split is necessary for such challenges and competitions, but not very useful for people who just want to use such datasets. Therefore, in this section when introducing such datasets we will ignore that level of details and focus on the total number of data including the number of real and fake samples.4.1      Deepfake Image DatasetsSwapMe and FaceSwap dataset [78]: This dataset contains 4,310 images, including 2,300 real images and 2,010 fake images created using FaceSwap21 and the SwapMe iOS app (now discontinued).Fake Faces in the Wild (FFW) dataset [32]: This dataset contains 131,500 face images, including 78,500 images extracted from 150 videos in the FaceForensics dataset and 53,000 images extracted from 150 fake videos collected from YouTube.generated.photos datasets22: This is a number of commercial datasets provided by the Generated Media, Inc., with up to nearly 2.7 million synthetic face images generated by StyleGAN. A free edition with 10,000 128x128 synthetic images is made available for academic research. The website also provides an interactive face generator23 and an API24. The generated.photos datasets have a good diversity: five age groups (infants, children, youth, adults, middle-aged), two genders (male and female), four ethnicities (white, black, Latino, Asian), four eye colours (brown, grey, blue, green), four hair colours (brown, black, blond, gray), three hair length (short, medium, long), facial expressions, three head poses (front facing, left facing, right facing), two emotions (joy and neutral), two face styles (natural, beautified). (According to a number of research papers we read, an earlier 100K-Faces dataset was released by generated.photos for academic research in 2018, which was used by many researchers. This dataset is not currently available any longer.) [1]: This dataset includes 19,457 face images, including 7,948 deepfake images generated from on 175 forged videos collected online and 11,509 real face images collected from various online sources. (Table 2 of the paper shows the dataset size is 19,509, but the dataset downloaded from pCloud contains just 19,457 images.) [30]: This dataset includes 100,000 synthesised face, bedroom, car and cat images by a GAN generator trained based on real images in the FFHQ25 and LSUN26 datasets (three object types – bedrooms, cars and cats – for the latter). Note that the name “100K-Generated-Images” was not a proper one as the authors [30] just used this to name a sub-folder of their Google Drive shared space, but it was used in one of the survey papers [65].Ding et al.’s swapped face dataset [17]: This dataset contains 420,053 images of celebrities, including 156,930 real ones downloaded using Google Image API and 263,123 fake face-swapped ones created using two different methods (Nirkin’s method and Auto-Encoder-GAN) [48]: This dataset includes 87,000 224x224 face images, generated by processing some StyleGAN-generated synthetic images using the GAN-fingerprint Removal approach (GANprintR) proposed by Neves et al.. It is the replaced version of the  dataset, which contains 150,000 face images generated using an earlier version of GANprintR. [21]: This dataset includes 40,000 images, half real and half deepfake. The images were collected from four sources: the CelebA-HQ dataset27, the Flickr-Faces-HQ dataset28, the 100K-Faces dataset29 (not available any longer, see the description of generated.photos datasets), and thisperson-doesnotexist.com. [75]: This dataset includes 625,537 synthesised face images of 10,177 celebrities, with 43 rich attributes on face, illumination, environment and spoof types. The real images were selected from the CelebA dataset30. The 43 attributes include 40 for real images, covering all facial components and accessories (e.g., skin, nose, eyes, eyebrows, lip, hair, hat, eyeglass), and 3 for fake images, covering spoof types, environments and illumination conditions.Diverse Fake Face Dataset (DFFD) [11]: This dataset contains 299,039 images, including 58,703 real images sampled from three datasets (FFHQ31, CelebA32 and FaceForensics++33) and 240,336 fake ones in four main facial manipulation types (identity swap, expression swap, attribute manipulation, and entire synthesis). The images cover two genders (male and female), a wide age groups (the majority between 21 and 50 years old), and both low- and high-quality levels.4.2     Deepfake Video Datasets [35]: This dataset contains 620 deepfake face videos, generated by face swapping without manipulation of audio, covering 32 subjects and two quality levels (high and low). (FF) [55]: This dataset contains 1,004 face videos with over 500,000 frames, covering various quality levels and two types of facial manipulation. This dataset is now replaced by the larger FaceForensics++ dataset (see below). (FF++) [56]: This dataset contains 5,000 face videos with over 1.8 million manipulated frames, including 1,000 real videos (with 509,914 frames) downloaded from YouTube, and 4,000 fake videos created using four face manipulation methods (Deepfakes, Face2Face, FaceSwap and NeuralTextures). The videos cover two genders (male and female), and three quality levels (VGA/480p, HD/720p, and FHD/1080p). [39]: This dataset contains 98 face videos, half (49) are real ones downloaded from Youtube, and the other half are fake ones generated using the FakeApp mobile application (which is now discontinued). The video dataset was created to used to demonstrate a deepfake video detection method based on detection of eye blinking behaviours, so all videos contain at least one eye-blinking event. All fake videos were created by swapping the original face in each of the real videos with the face of the actor Nicolas Cage34, thus, only one subject is represented. [10]: This dataset contains 142 “in the wild” deepfake portrait videos, collected from a range of online sources including news articles, online forums, mobile apps, and research presentations. The videos are diverse, covering the source generative model, resolution, compression, illumination, aspect-ratio, frame rate, motion, pose, cosmetics, occlusion, content, and context.DFDC (Deepfake Detection Challenge) preview dataset [18]: This dataset contains 5,244 face videos of 66 subjects with both face and voice manipulation. It was released as a preview of the full dataset of the 2020 Deepfake Detection Challenge (DFDC, see below).35: This dataset contains 1,203 face videos of celebrities, including 408 real videos collected from YouTube with subjects of different ages, ethic groups and genders, and 795 deepfake videos synthesised from these real videos. [40]: This dataset contains 6,229 face videos of celebrities, including 590 real videos collected from YouTube with subjects of different ages, ethic groups and genders, and 5,639 deepfake videos synthesised from these real videos.DeepFake Detection (DFD) Dataset [20]: This dataset contains 3,363 face videos, covering 28 subjects, gender, and skin colour. It was created as a joint effort between two units of Google, Inc.: Google AI36 and JigSaw37. [27]: This dataset contains 60,000 indoor face videos (with 17.6 million frames) generated by face swapping, covering 100 subjects, four skin tones (white, black, yellow, brown), two gen-ders (male and female), different age groups (20-45), 26 nationalities, 7 different angles, 8 face expressions, and different head poses.DFDC (Deepfake Detection Challenge) full dataset [18]: This dataset contains 128,154 face videos of 960 subjects, including 23,654 real videos from 3,426 paid actors and 104,500 deepfake videos created using eight different methods (DF-128, DF-256, MM/NN face swap, NTH, FSGAN, StyleGAN, refinement, and audio swap).10(Face Forensics in the Wild) dataset [79]: This dataset contains 10,000 high-quality forgery videos, with video- and face-level annotations. The dataset focuses on a more challenging case for forgery detection: each video involves one to 15 individuals, but only some (a minority of) faces are manipulated.Korean DeepFake Detection Dataset (KoDF) [36]: This dataset contains 37,942 videos of paid subjects (395 Koreans and 8 Southeastern Asians), including 62,166 real videos and 175,776 fake ones created using six methods – FaceSwap, DeepFaceLab, FSGAN, First Order Motion Model (FOMM), Audio-driven Talking Face HeadPose (ATFHP) and Wav2Lip. The videos cover a balanced gender ratio and a wide range of age groups. [23]: This dataset contains 1,737 videos with 1,666,816 frames, including 1,339,843 real frames and 326,973 fake frames generated using the Deep Video Portraits (DVP) [34] method. The original videos were obtained from three sources: the dataset used in [33], the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) [42], and YouTube. Most videos have a resolution of 1280×720. [81]: This dataset contains 7,314 face sequences extracted from 707 deepfake videos that were collected completely from the Internet. It covers diverse scenes, multiple persons in each scene and rich facial expressions. Different from other deepfake video datasets, WildDeepfake contains only face sequences not the full videos. This makes the dataset more like between an image dataset and a video one. We decided to keep it in the video category since the selection process was still more video-focused.4.3     Deepfake Audio/Speech DatasetsVoice conversion (VC) is a technology that can be used to modify an audio and speech sample so that it appears as if spoken by a different (target) person than the original (source) speaker. Obviously, it can be used to generate deepfake audio/speech samples. The biennial Voice Conversion Challenge38 that started in 2016 is a major challenge series on VC. Datasets released from this challenge series are very different from other deepfake datasets: the deepfake data is not included in the original dataset created by the organisers of each challenge, but in the participant submissions (which are retargeted/fake utterances produced by VC systems built by participants). The challenge datasets also include the evaluation (listening-based) results of all submissions. Some fake utterances may be produced by DL-based VC systems, so we consider all datasets from this challenge series relevant for our purpose of this survey.Voice Conversion Challenge 2016 database [62]: The original dataset created by the challenge organisers was derived from the DAPS (Device and Produced Speech) Dataset [47]. It contains 216 utterances (162 for training and 54 for testing) per speaker from 10 speakers. Participating teams (17) developed their own VC systems for all 25 source-target speaker pairs, and then submitted generated utterances for evaluation. At least six participating teams used DL-related techniques (LSTM, DNN) in their VC systems (see Table 2 of the result analysis paper39), so the submitted utterances can certainly be considered deepfakes.Voice Conversion Challenge 2018 database [44]: The original dataset created by the challenge organisers was also based on the DAPS dataset. It contains 116 utterances (81 for training and 35 for testing) per speaker from 12 speakers in two different tasks (called Hub and Spoke). Participating teams (23 in total, all for Hub and 11 for Spoke) developed their own VC systems for all 16 source-target speaker pairs, and then submitted generated utterances for evaluation. Comparing with the 2016 challenge, more participating teams used DL-related techniques (e.g., WaveNet, LSTM, DNN, CycleGAN, DRM – deep relational models, and ARBM – adaptive restricted Boltzmann machines) in their VC systems.Voice Conversion Challenge 2020 database [70]: This dataset is based on the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset40, a bilingual (Finnish/English, German/English, and Mandarin/English) database. It contains 145 utterances (120 for training and 25 for testing) per speaker from 14 speakers for two different tasks (with 4 × 4 and 4 × 6 source-target speaker pairs, respectively). Participating teams (33 in total, out of which 31 for Task 1 and 28 for Task 2) developed their own VC systems for all source-target speaker pairs, and then submitted generated utterances for evaluation. Comparing with the 2018 challenge, DL-based VC systems were overwhelmingly used by almost all participating teams (WaveNet and WaveGAN among the most used DL-based building blocks).A major set of deepfake speech datasets were created for the  (Automatic Speaker Verification Spoofing and Countermeasures) Challenge41 (2015-2021, held biannually). The datasets for the 2019 and 2021 contain speech data that can be considered deepfakes.ASVspoof 2019 Challenge database [67]: This dataset is based on the Voice Cloning Toolkit (VCTK) corpus42, a multi-speaker English speech database captured from 107 speakers (46 males and 61 females). Two attack scenarios were considered: logical access (LA) involving spoofed (synthetic or converted) speech, and physical access (PA) involving replay attacks of previously recorded bona fide recordings). For our purpose in this survey, the LA scenario is more relevant. The LA part of the dataset includes 12,483 bona fide (real) utterances and 108,978 spoofed utterances. Some of the spoofed speech data for the LA scenario were produced using a generative model involving DL-based techniques such as long short-term memory (LSTM)43, WaveNet [50], WaveRNN [28], WaveCycleGAN2 [58]. Note that the challenge organisers did not use the term “deepfake” explicitly, despite the fact that the DL-generated spoofed speech data can be considered as deepfakes.ASVspoof 2021 Challenge – Logical Access Database [14]: This dataset contains bona fide and spoofed speech data for the logical access (LA) task. The challenge is still ongoing and we did not find a detailed paper on the dataset, so cannot include more details other than its size (7.8 GB after compression). Although we did not see details of the generative algorithms used to produce spoofed speech data, we believe similar DL-based algorithms were used like for the 2019 challenge.ASVspoof 2021 Challenge – Speech Deepfake Database [15]: In 2021, the challenge included an explicitly defined track on deepfake, but the task description suggests that the organisers of the challenge considered a broader definition of the term “deepfake” by looking at spoofing human listeners rather than ASV (Automatic Speaker Verification) systems. The size of the dataset is 34.5 GB after compression.Possibly because of the long history and wide participation of the community in the ASVspoof challenges for creating the dedicated datasets, there are very few other deepfake audio/speech datasets. One such dataset was created by a group of researcher from Baidu Research [5]. This dataset was created to demonstrate a proposed voice cloning method. It is relatively small, and contains 134 utterances, including 10 real ones, 120 cloned ones, and 4 manipulated ones. Another dataset was created by Google AI and Google News Initiative44, but it was made part of the ASVspoof 2019 dataset. This dataset contains thousands of phrases spoken by 68 synthetic “voices” covering a variety of regional accents.4.4     Hybrid Deepfake DatasetsNIST OpenMFC (Open Media Forensics Challenge) Datasets45: These datasets were created by the DARPA Media Forensics (MediFor) Program46 for the 2020 OpenMFC47. There are two GAN-generated deepfake datasets, one with more than 1,000 deepfake images and the other with over 100 deepfake videos. The datasets were made available to registered participants of the competition only. [25]: This dataset is named as “a versatile benchmark for comprehensive forgery analysis”. It contains 2,896,062 images and 221,247 videos, including 1,457,861 fake images and 121,617 fake videos. The videos and images cover seven image-level and eight video-level manipulation approaches, 36 different types of perturbations and more mixed perturbations, and a large number of annotation labels (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). The dataset is being used for supporting the Face Forgery Analysis Challenge 202148 at the SenseHuman 2021 (3rd Workshop on Sensing, Understanding and Synthesizing Humans)49, co-located at the ICCV 2021 conference50.4.5      A Deepfake Dataset Generator [74]: This is not actually a dataset per se, but a system for producing large datasets more automatically, including generating deepfake datasets. One may argue the automatically generated datasets are fake since they are not produced from real-world scenes.4.6     Subjective Quality of Deepfakes in Different DatabasesAs mentioned in Section 4.7, subjective quality evaluation is necessary to evaluate the realness, realisticness, and naturalness of deepfake media. While there has been very limited work on this topic, in 2020, Jiang et al. [27] conducted a user study on realness of deepfake videos. They recruited 100 professional participants (most of whom are computer vision researchers), who were asked to evaluate the realness of 30 randomly selected videos from 7 deepfake video datasets (DeeperForensics-1.0, UADFV, DeepFake-TIMIT, Celeb-DF, FaceForensics++, Deep Fake Detection, and DFDC). Participants were asked to respond to the statement “The video clip looks real.” and gave scores following a five-point Likert scale (1 – clearly disagree, 2 – weakly disagree, 3 – borderline, 4 – weakly agree, 5 – clearly agree).Table 3 shows the results. Interestingly, we can see a huge difference between the realness levels of different datasets. What is probably quite surprising is that FaceForensics++, one of the most widely used deepfake datasets, has a very low MOS score and less than 9% of participants considered the 30 selected videos as real.Table 3: Human-judged subjective quality (realness) of deepfake videos in 7 datasets. The MOS scores were not reported by Jiang et al., but calculated by us based on the raw data shown in Table 3 of [27].4.7      Discussion: DatasetsAmong all deepfake image and video datasets, a significant majority are about face images and videos. This is not surprising since face swapping, face attribution manipulation, and fully synthesised face images are among the hottest topics within deepfake research and real-world applications. We hope more non-face deepfake image and video datasets can be produced to support a broader range of research activities on deepfake.The subjective quality results shown in Table 3 indicate that it is important to check realness of deep-fake media to support any performance evaluation or comparison. To ensure that the quality evaluation of datasets is fair, transparent and reliable, standard procedures need defining and a common pool of qualified human experts should be used.Many authors of deepfake-related datasets attempted to classify such datasets into different generations. Chronologically speaking, we could broadly split such datasets into two generations: before 2019 and since 2019. Typically, datasets created before 2019 are relatively less advanced and smaller, while those created after 2019 tend to be larger, more diverse (i.e., covering more attributes), and of higher quality (i.e., produced by more advanced generative models). This can also be seen from the data in Table 3, in which the top two datasets (DeeperForensics-1 and Celeb-DF) fall within the new generation (2020), while others belong to the old generation. In addition to the two generations, a newer generation has also emerged in 2021: a number of very recent datasets started focusing on more realistic deepfakes (i.e., in the wild) or more specified areas of deepfakes (e.g., 10 focusing on multiple faces in the same video, and KoDF focusing on Korean faces). This trend shows that the deepfake research community has grown significantly in the past few years so that narrower topics have also started gaining attention and interest from some researchers.This section reviews initiatives aiming to advance the state-of-the-art of detection and generation of synthetic or manipulated media (such as video, image and audio) via competitions or challenges open to the public, and ongoing benchmarks tackling specific problems.The Deepfake Detection Challenge (DFDC)51 was an initiative promoted by an AI and Media Steering Committee52, including BBC, Facebook, Amazon, Microsoft and New York Times, and some universities around the world including the University of Oxford. The competition remained open from 5 September 2019 till 31 March 2020, and involved 3 stages. At first, the DFDC preview dataset was released. At a later stage, the DFDC full dataset was also made available to the 2,114 participants of the competition incorporating face and audio swap techniques for generation of deepfake content. At the final stage, the submitted models were evaluated using a test dataset (referred to as the “black box dataset”) of 10,000 videos which included  deepfake videos. The best performance on the black box dataset had an accuracy of 65.18%, according to the released results [22]. Submissions were ranked53 according to the overall log loss score, as defined in Eq. (13). All top five ranked models (the winner had the lowest overall log loss) are available on GitHub. Results indicate how challenging the detection of deepfake is since the best accuracy was low and “many submissions were simply random”, according to Dolhansky et al. [19]. Figure 2 shows a screenshot of the leaderboard with the five finalists. The first top ranked model used MTCNN (Multi-tasked Cascaded Convolutional Network), the second used WS-DAN (Weakly Supervised Data Augumentation Network), and the third used the EfficientNetB7 architecture. Meta compiling the common themes observed in the winning models, they were: clever augmentations, architectures, and absence of forensics methods. Moving forward, they called for “solutions that go beyond analysing images and video. Considering context, provenance, and other signals may be the way to improve deepfake detection models”.\
The Automatic Speaker Verification Spoofing And Countermeasures Challenge Workshop (ASVspoof)54 has been running biennially since 2015. This competition is organised by an international consortium that includes Inria and EURECOM (France), University of Eastern Finland, National Institute of Informatics (Japan), and Institute for Infocomm Research (Singapore). This year the ASVspoof challenge includes, for the first time, a sub-challenge focused on  where the envisioned use case is an adversary trying to fool a human listener. The metric used for evaluating performance of submitted solutions (i.e., classifiers) is EER. Four baseline solutions55 (also called “countermeasures”), each using a different technique, were made available to participants with their corresponding EER metric values. The ASVspoof 2021 Speech Deepfake Database containing audio recordings with original and spoofed utterances has also been made available. The competition involves three phases56: a progress phase, an evaluation phase and a post-evaluation phase; it is unclear how teams move from one phase to the next. More information about the 2021 competition is available in the published evaluation plan [13]. The organisers of the competition noted that they opted for the EER as the performance evaluation met-ric for countermeasures submitted to the speech deepfake task for legacy reasons. They acknowledged, however, that “EER reporting is deprecated ” by the ISO/IEC 19795-1:202157 standard. Despite the fact that only the 2021 ASVspoof competition contained a track explicitly related to deepfake, some data in the ASVspoof 2019 dataset (Logical Access task) used for the 2019 competition was generated using DL-based algorithms as mentioned in Section 4. We expect that this also holds for the ASVspoof 2021 dataset (Logical Access task). The ASVspoof 2019 competition used the EER as secondary metric; the primary performance metric used was the tandem detection cost function (t-DCF) [63]. According to its evaluation plan [69], t-DCF assesses the performance of the whole tandem system whereby “a CM [countermeasure] serves as a ‘gate’ to determine whether a given speech input originates from a bona fide (genuine) user, before passing it the main biometric verifier (the ASV system)”. It is calculated according to Eq. (17), where  cm () and  cm() are, respectively, “the miss rate and the false alarm rate of the CM system at threshold s”.For further information about Eq. (17), including constants 1 and 2, please refer to the ASVspoof 2019 evaluation plan [69].An implementation of the t-DCF metric has been made available by the ASVspoof 2019’s organisers in Python58 and Matlab59 formats.The Face Anti-spoofing (Presentation Attack Detection) Challenge60 started in 2019. Its first two editions were held at the 2019 and 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2020), respectively. Its third edition was moved to be co-located with the 2021 IEEE/CVF International Conference on Computer Vision (ICCV 2021). This competition series was organised by a group of researchers from academia and industry in China, Mexico, Spain, Finland and the US. The 2021 competition was focused on 3D high-fidelity mask attacks, and followed a 2-phased61 process. The first phase is the “development phase”; it started in April 2021 when the CASIA-SURF HiFiMask dataset62 was released to participants. The second phase is the “final ranking phase” (June 2021), when the competition ended. The competition adopted the following performance metrics for evaluation63 of the solutions submitted: attack presentation classification error rate (APCER), normal/bona fide presentation classification error rate (NPCER/BPCER), and average classification error rate (ACER), in accordance with the ISO/IEC 30107-3:201764 standard. Figure 3 provides the leaderboard for the top three solutions.\
The FaceForensics Benchmark65 is an ongoing automated benchmark for detection of face manipulation. The organisers of the benchmark made the FaceForensics++ dataset available for training. Manipulated videos (4,000 in total) were created using four techniques, i.e., two computer graphics-based approaches (Face2Face and FaceSwap) and two learning-based approaches (DeepFakes and Neural Textures). The deepfakes videos were generated using a slightly modified version of FaceSwap66, and the Neural Textures videos were created using the approach proposed by Thies et al. [61]. The benchmark test dataset is created from the collection of 1,000 images randomly selected from either the manipulation methods or the original videos [56]. Participants have to submit results to the benchmark, rather then code like other competitions; this is illustrated in Figure 4a. The outcome of a submission is illustrated in Figure 4b, where the scores are a measure of accuracy (Eq. (8)).\
The Open Media Forensics Challenge (OpenMFC, formerly DARPA MFC)67 is an annual image and video forensics evaluation aiming to facilitate development of multimedia manipulation detection systems. It has been organised annually68 starting from 2017 under the name of DARPA MFC. In 2020, the National Institute of Standards and Technology (NIST) initiated the  as a new evaluation platform, based on their previous experiences with the DARPA MFC series, to make the participation more convenient for all researchers. In OpenMFC 2020, two deepfake-related tasks were included for the first time: Image GAN Manipulation Detection (IGMD) and Video GAN Manipulation Detection (VGMD). The organisers provided an image evaluation dataset for the IGMD task, containing 1,000 images from over 200 image journals69, and a video evaluation dataset for the VGMD task, including over 100 test videos. Furthermore, they provided the datasets70 used in the previous MFC challenges as development datasets. The challenge is composed of two main phases for development and evaluation, respectively, and a pre-challenge phase for quality control testing. For evaluation of submissions, AUC-ROC is used as the primary metric. Furthermore, CDR@FAR, where CDR refers to correct detection rate or TPR (Eq. (4)) and FAR refers to false alarm rate or FPR (Eq. (5)), is also used as a metric [49]. The DeeperForensics Challenge 202071 is a deepfake face detection challenge held at the 2020 ECCVSenseHuman Workshop72. The challenge used the DeeperForensics1.0 dataset.The organisers provided a hidden test dataset to better simulate real-world scenarios. The challenge involved two phases: the “development phase” that started in August 2020 allowing 100 successful sub-missions, and the “final test phase” that started in October 2020 allowing 2 successful submissions until the end of the month. The submissions were evaluated using the binary cross-entropy loss (BCELoss) metric, calculated according to Eq. (18), where  is the number of videos in the hidden test set,  is the ground truth label of video  (fake:1, real:0), and () is the predicted probability that video  is fake.Results73 of the competition were discussed by Jiang et al. [26]. The top solution used three models, i.e., EfficientNet-B0, EfficientNet-B1 and EfficientNet-B2, for classification. The second top used EfficientNet-B5 for both an image-based model and a video-based model. The third ranked solution used a 3D convolutional neural network (3DCNN).\
The Face Forgery Analysis Challenge 202174 is a competition hosted at the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2021). It is organised by researchers from a number of organisations in China including universities and SenseTime Research (the research arm of SenseTime75, one of the major AI “unicorns” in China). The challenge aims to advance the state-of-the-art in detection of photo-realistic manipulation of images and videos. Participants are able to use a large annotated face dataset (i.e., the ForgeryNet dataset) that was obtained by applying a number of techniques for manipulation (15) and perturbation (36) to train their solutions. The phases comprise of Forgery Image Analysis, Forgery Video Analysis, Forgery Video Temporal Localization phases, and the final phase (i.e., “private test”) where participants’ models will be tested against an unseen dataset. The following metrics will be used [25]: AUC, average precision (AP) at some “temporal Intersection over Union” (AP@tIoU) compared to a threshold  ∈ [0*.,* 0*.*95], and average recall (AR) at  (AR@) where  is the top  labels returned for multi-class classifiers.The 2020 CelebA-Spoof Face Anti-Spoofing Challenge76 was hosted at the 16 European Conference on Computer Vision (ECCV 2020). The challenge ran between August and October 2020, and aimed to advance the state-of-the-art in detecting “whether a presented face is live or spoof ” [76]. The organisers made the face CelebA-Spoof dataset available for the competition containing rich annotation across a range of attributes. The competition only had one phase where participants submitted their solutions to be evaluated against a test dataset; the spoof class was considered as “positive” and the live class as “negative”. Metric TPR@FPR was used and collected at three points where the TPR when FPR = 104 determined the final ranking. The top three finalists (see Figure 5) used deep learning models ResNet, EfficientNet-B7, and a novel architecture combining Central Difference Convolutional Networks (CDCN) and Dual Attention Network (DAN). The two top ranked solutions used different strategies to boost their models’ performance: a heuristic voting scheme was used by the top-ranked solution, and a weight-after-sorting strategy was used by the second ranked solution.The 2021 CSIG Challenge77 is the second edition of a challenge organised by the China Society of Image and Graphics78. The 2021 challenge has the Fake Media Forensic Challenge79 as its 6 track, co-organised by CSIG’s Digital Media Forensics and Security Technical Committee80 and Institute of Information Engineering, Chinese Academy of Sciences81. This track has two tasks, one on deepfake video detection, and the other on deepfake audio/speech detection. For the deepfake video detection task, the dataset used contains a public training set with 10,000 sound-free face videos (including 4,000 fake videos), a public test set with 20,000 face videos (the percentage of deepfake videos is unknown to participants), and a private test set that will be determined and used at the final session for selecting the winners. All videos contain faces of Eastern Asian people, and cover a wide range of parameters such as multiple resolutions and encoding quality factors, the use of blurring or sharpening filters, and added noise. Deepfake videos were created using public tools including DeepFaceLab [53], Faceswap82, Faceswap-GAN, Recycle-GAN [6] and ALAE (Adversarial Latent Autoencoders) [54]. For the deepfake audio/speech detection task, the dataset used contains a public training set with 10,000 speech samples (including 6,000 fake ones), a public test set with 20,000 face videos (the percentage of deepfake videos is unknown to participants), and a private test set for the final session (the same as the deepfake video detection task). The tools used for generating the fake speech samples include TTS (text-to-speech) voice synthesis tools and VC (voice conversion) tools. The main TTS tools used include open-source tools such as DeepVoice, TensorFlowTTS83 and GAN-TTS [8] and commercial software tools such as those from iFlytek84 and IBM. The main VC tools used include Adaptive-VC and CycleGAN-VC [29]. For both deepfake detection tasks, the performance metric used is log loss.2020 China Artificial Intelligence85 was the second edition of a Chinese AI competition open for the general public to participate, organised by the municipal government of the City of Xiamen in China. In 2020, it had two sub-competitions, Multimedia Information Recognition Technology Competition86 and Language and Knowledge Technology Competition87. The Multimedia Information Recognition Technology Competition included two tasks on deepfakes: one on deepfake video detection88 and one on deepfake audio/speech detection89. The deepfake video detection task used 3,000 videos, and log loss was used as the sole performance metric. The deepfake audio/speech detection task used 20,000 audio samples (mostly in Chinese, and the remaining in English), and EER was used as the sole performance metric. For both tasks, the ratio between real and deepfake samples was 1:1. We did not find where to download the datasets used for the tasks nor a more detailed technical description of the datasets. For the deepfake video detection tasks, the top two winning teams (with an A prize) were from Netease (Hangzhou) Network Co., Ltd. and Beijing RealAI Technology Co., Ltd., followed by three other teams winning a B prize: Xiamen Fuyun Information Technology Co., Ltd.; Institute of Computing Technology, Chinese Academy of Sciences; and Wuhan Daqian Information Technology Co., Ltd. For the deepfake audio/speech task, there was no team winning an A prize, but one team winning a B prize: SpeakIn Technologies Co., Ltd. The final results of some teams were published, but some teams were allowed to hide their results. We did not find a detailed technical report summarising the results and explaining the work of the winning teams.One of the B-prize winning team is from Beijing RealAI Technology Co., Ltd., a Chinese company active in deepfake-related R&D.The Voice Conversion Challenge90 is a biennial competition that has been running since 2016. The challenge and the corresponding workshop, hosted at the INTERSPEECH conference91, is supported by the SynSig (Speech Synthesis Special Interest Group)92 of the International Speech Communication Association (ISCA)93. Its aim is to promote progress in voice conversion (VC) technology that can be applied to a number of positive and negative use cases, such as spoofing voice biometric systems. The 2020 challenge focused on speaker conversion, a sub-problem of VC, and included two tasks. For the first task “intra-lingual semi-parallel voice conversion”, participants had to develop 16 VC systems (speaker-pair combinations) including male and female speakers and English sentences, using the provided Voice Conversion Challenge 2020 database v1.0 for training (refer to Section 4). For the second task “cross-lingual voice conversion”, participants had to develop 24 VC systems, also including male and female speakers, but uttering sentences in three languages (Finnish, German and Mandarin), based on the provided training dataset. Figure 6 illustrates the process of training and generation of VC systems.Submissions were evaluated for “perceived naturalness and similarity through listening tests”94. As such, the organisers used  [70] and recruited both native and non-native English speakers (i.e., Japanese native speakers) via crowd-sourcing for the listening tests. Naturalness (answering the question “How natural does the converted voice sound? ”) was measured using the metric MOS (covered in Section 4.6), and similarity (answering the question “how similar the converted voice sound comparing source and target speakers? ”) was measured in terms of speaker recognition as “same” or “different”, as elaborated by Wester et al. [68]. Tests also focused on the effects of language differences on the performance of VC systems submitted to the competition. The most popular CNN/RNN/GANbased VC systems submitted used WaveNet, WaveRNN, and Parallel WaveGAN. Results indicated that, in terms of similarity, the best performing VC systems were as good as natural speech but none reached human-level naturalness for task 1; scores were lower for task 2 which was more complex [70]. The organisers of the 2020 competition also used objective evaluation [12]. The metrics used for evaluation of speaker similarity were: equal error rate (EER), false acceptance rate of target (P tar fa ), miss rate of source (P src miss), and cosine similarity of speaker embedding vectors (cos-sim) according to Eq. (19) where A is the speaker embedding vectors for the converter audio and B is the speaker embedding vectors for the original audio. The performance of the VC systems as a spoof countermeasure was also evaluated using EER, while to evaluate the quality of the subjective MOS obtained via listening tests, a DL-based model to predict MOS, called MOSNet [43], was used. Lastly, to evaluate intelligibility of the converted transcribed speech, in comparison with the original transcribed speech, the word error rate (WER) [4] was used. WER is calculated according to Eq. (20) where I refers to insertions, D refers to deletions, S refers to substitutions, and N refers to the total number of words in the original transcript.The Deepfake Africa Challenge (2021)95 is a new initiative of the AI Africa Expo, in partnership with a film and media production company (Wesgro) and the African Data Science competition platform Zindi. Its aim is “to create convincing deepfakes to highlight the power of this synthetic media, illustrating its creative potential for exploitation for both positive and negative outcomes and focusing debate about its ethical use / misuse in an African context ”. Eligible participants were required to be citizens and residents of the African continent. Submissions, accepted up to end of July 2021, can be either video or audio. Evaluation of submissions is defined in terms of artistic creativity, relevance of challenge topic, and innovation in the process of generation as long as participants use tools and packages publicly available. The top three finalists will receive a prize, present their work at the Expo, and will have to grant copyrights to Zindi. Unlike the other competitions reviewed in this section, which were focused on advancing the state-of-the-art in detection of synthetic or manipulated media, this competition focused on the generation of deepfake which seems more humanities-centred. This is a trend observed in arts [31] and culture [57].5.3      Generation and Detection of Manipulated MediaThe DeepFake Game Competition (DFGC)96 is in its first edition, hosted at the 2021 International Joint Conference on Biometrics (IJCB 2021). Its organisers are mainly from the Institute of Automation Chinese Academy of Sciences (CASIA). The idea of the competition was to promote an adversarial game between agents pushing for advances in both deepfake creation and detection. In order to achieve this, a 6-stage protocol was designed interleaving three creation phase (C-phase) and detection phase (D-phase), typically one week apart; submissions closed in April 2021. Both C-phases and D-phases were bound to the Celeb-DF (v2) dataset [40], containing 6,229 videos (590 real/original videos and 5,639 fake/manipulated videos), for training purposes. As such, submissions to a C-phase would consist of datasets extracted from Celeb-DF (v2) which included novel face-swap approaches to obtain evaluation results. Submissions to a D-phase would consist of detection models/codes to obtain evaluation results. The models submitted for a D-phase were evaluated against the datasets submitted for the previous C-phase [52]. The metrics used for evaluation97 were: a detection score, used for evaluation of a D-phase, and a creation score, used for evaluation of a C-phase. The top three finalists for the detection phase employed CNN-based classifiers EfficientNet-B3, Efficientnet-B0 and EfficientNetV2.The Detection Score () metric captures the models’ ability to correctly classify fake images submitted to the previous C-phase against a set of real images in the CelebDF test dataset. It is calculated using Eq. (21), where  is the number of valid submissions of created synthesis test sets in the last C-phase.The Creation Score () metric used to evaluate creation models submitted to this challenge is calculated by Eq. (22), where  is the number of valid submissions of detection methods in the last D-phase, the noise score (noise) penalises noisy images, the other three parts of the equation relate to the following98: “ID level similarity to the donor ID, image level similarity to the target frame, and the deception ability against detection models. ID level similarity is scored by a face recognition model using dot product of two ID features (fake face ID and donor ID). The image level similarity is scored by SSIM [Structural Similarity Index] to make sure the face-swapped image is similar to the corresponding target image in content and quality ”.Peng et al. [52] observed a commonality between the three winning teams for the creation task, i.e., the use of the FaceShifter [37] framework for face swapping. They highlighted two overall reflections about the competition: (1) the limited diversity of the deepfake datasets submitted and the use of repetitive methods to generate them, and (2) the limited size of the Celeb-DF (v2) dataset itself flagging the need for a larger dataset for next year’s competition. The organisers of the competition also applied the top two detection models to unseen datasets (DFDC and FaceForensics++) and noticed that they do not generalise well.This section presents a meta-review of 12 selected deepfake-related survey papers, including eight published in English [16, 45, 46, 64–66, 71, 73] and four published in Chinese [7, 38, 41, 59]. It covers the following aspects in a systematic manner: definitions and scope, performance metrics, datasets, challenges/competitions/benchmarks, performance comparison, key challenges and recommendations.The meta-review aims at drawing some high-level insights for monitoring future development of deepfake-related technologies and their applications.6.1      Definitions and ScopeAs we discussed in Section 1.1, among researchers, practitioners and law makers there is no universally accepted definition of “deepfake” as a term. This is also reflected in how the authors of the 12 survey papers considered this aspect. Most authors talked about the history of deepfakes and pointed out that the term reflects the combination of “deep learning” and “fake”, but some used a broader definition, e.g., Lyu [45] defined deepfake as “high quality fake videos and audios generated by AI algorithms”. Some authors also referred to deepfake-related legislations, but none of them pointed out that the definitions in some such legislations are completely different from the more technical definitions involving the use of deep learning. No authors discussed the blurred boundary between deepfakes and non-deepfakes, although some surveys actually cover both, e.g., Tao et al. [59] focused on speech forgery and did not explicitly highlight “deepfake”.In terms of the scope, while some authors (correctly) considered all types of media that can be produced by deepfake-related techniques [38, 41, 45, 65], some considered only a narrow scope, e.g., authors of [7, 64, 71, 73] considered only videos, and only authors of [16, 66] have considered images and videos. Another phenomenon we observed is that many authors focused more on face images and videos, and authors of three surveys [16, 64, 71] even limited the definition of “deepfake” to such a narrow scope:Deshmukh and Wankhade [16] defined it as “a technology which creates fake images or videos of targeted humans by swapping their faces [by] another character saying or doing things that are not absolutely done by them and humans start believing in such fake as it is not always recognisable with the everyday human eye”;Younus and Hasan [71] considered deepfake as a technique allowing “any computer user to exchange the face of one person with another digitally in any video”; andTolosana et al. [64] defined it as “a deep learning based technique able to create fake videos by swapping the face of a person by the face of another person”.Such unnecessarily narrow definitions and scopes can lead to confusion and do not help exchanges between researchers and practitioners working on different types of deepfakes.We call on more researchers to accept a broader definition of “deepfake” so that highly realistic/natural media of any kind generated by a sophisticated automated method (often AI-based) is considered deepfake. Here, we provide two examples of such a broader definition: the image2image (or pixel2pixel) technique [80] that allows the production of deepfake images and videos of any objects (e.g., the “horse2zebra” deepfake image shown in Figure 7), and the the so-called “deepfake geography [77]”, where AI-based techniques are used to generate realistic-looking satellite images.\
Another important fact missed or not sufficiently discussed by authors of all the 12 surveys is that deepfake techniques can be used for positive applications, e.g., creative arts, entertainment and protecting online users’ privacy. We call for more researchers and practitioners to follow the proposal in the 2020 Tencent AI White Paper [60] to start using the more neutral-sounding term “deep synthesis”. Accordingly,we can use different words for different types of data generated using “deep synthesis” techniques, e.g., “deep art”, “deep animation”, “deep music”, and “deepfake”. While authors of the 12 survey papers did not recognise the positive applications of “deepfake” technologies, some other researchers did, e.g., organisers of the Voice Conversion Challenge 202099 who said the VC technology (for speech deepfake) “is useful in many applications, such as customizing audio book and avatar voices, dubbing, movie industry, teleconferencing, singing voice modification, voice restoration after surgery, and cloning of voices of historical persons”.Surprisingly, none of the 12 surveys have covered performance metrics explicitly. Some directly used performance metrics to explain and compare performance of covered deepfake generation and detection methods. The most used performance metrics include accuracy, ERR, and AUC. This may be explained by the page constraints of such survey papers, which did not allow the authors to extend their coverage significantly to cover performance metrics systematically. The subjective quality of deepfakes is an area least covered by the surveys, which seems related to an unbalanced coverage on deepfake generation and deepfake detection in terms of performance evaluation and comparison (the former much less than the latter).Many of the 12 survey papers list a number of deepfake-related datasets, but none of them have coverage as complete as ours shown in Section 4. For instance, none of the surveys have covered the Voice Conversion Challenge 2016/2018/2020 datasets and the ASVspoof 2019/2021 datasets are covered briefly only in two surveys [38, 59]. In addition, more recent deepfake datasets especially those released in 2021 are also not covered by any of the surveys. We believe that our Section 4 is the most comprehensive review of deepfake-related datasets so far.Some survey papers include datasets that are likely deepfakes, e.g., Verdoliva [66] covered many general fake image datasets where the manipulated images were not generated by deep learning or even AI-based methods, and some surveys (e.g., [38]) mentioned ASVspoof 2015 datasets but we did not see the use of deep learning for generating data used in the dataset.Many surveys cover deepfake-related challenges, competitions and benchmarks. The coverage is, however, mostly limited, and some challenges (e.g., the Voice Conversion Challenge 2016/2018/2020 and the two Chinese challenges we covered in Section 5) are not covered by any of the surveys. The level of detail of challenges, competitions and benchmarks is also normally limited, compared with what we chose to include in Section 5. Similar to the datasets we covered in Section 4, we believe that our coverage of deepfake-related challenges, competitions and benchmarks in Section 5 is also the most comprehensive so far.Most surveys have a good coverage of related methods for deepfake generation and detection, but only some explicitly covered performance comparison between different methods [38, 46, 64].Among all the survey papers, Li et al. [38] conducted the most comprehensive study on performance of different deepfake detection methods. In addition to showing the performance metrics of a number of deepfake detection methods in Table 3 of [38], they also looked at general characteristics and issues of different types of deepfake detection methods, as shown in Table 4. Furthermore, they also looked at research on robustness of deepfake detection methods against adversarial samples, referring to some work that showed a lack of such robustness.Due to quality issues of many deepfake-related datasets (discussed in Section 4.6), we need to treat any performance metrics and comparison of different detection methods with caution. Without testing all methods on a sufficiently large, diverse and high-quality deepfake dataset, the performance comparison results can be misleading. This highlights the importance of having more challenges, competitions and benchmarks to encourage performance comparison on standard datasets and using consistent performance metrics.The authors of some surveys identified some key challenges and future research directions for the deepfake community.Not surprisingly, how to develop more robust, scalable, generalisable and explainable deepfake detection methods is one of the most discussed key challenges and also a major future research direction [7, 16, 38, 41, 45, 59, 65, 66, 71]. Considering the arms race between deepfake generation and detection, this research direction will likely remain the hottest topic in deepfake research.A couple of surveys [38, 66] mentioned fusion as a key future research direction, where “fusion” refers to combining different methods (e.g., combining multiple detectors of different types) and data sources (e.g., jointly considering audio-visual analysis) to achieve better performance for deepfake detection. Lyu [45] suggested that, for detection of deepfake videos, we need to consider video-level detection more, which can be considered fusion of detection results of all video frames.The authors of three surveys, Lyu [45] , Deshmukh and Wankhade [16] and Younus and Hasan [71], argued that better (higher-quality, more up-to-date, and more standard) deepfake datasets are needed to develop more effective deepfake detection methods. Lyu [45] also suggested that we need to consider  effects in training data and improve the evaluation of datasets. We agree with them on these points.Tao et al. [59] suggested that low-cost deepfake generation/detection should be considered as a future research direction. This is a valid recommendation since lightweight methods will allow less powerful computing devices (e.g., IoT devices) to benefit from such technologies.Two Chinese surveys [38, 41] also mentioned the need to have new deepfake-related legislations combating malicious use of deepfakes and the need to train end users such as journalists. This is likely an area where interdisciplinary research can grow.There are also other ad-hoc recommendations given by the authors of some surveys. For example, Lyu [45] argued that deepfake detection should be considered a (more complicated) multi-class, multi-label and local detection problem. Tolosana et al. [64] discussed specific research directions for different deep-fake generation methods (face synthesis, identity swap, attribute manipulation, and expression swap). Liang et al. [41] and Li et al. [38] recommended more active defence mechanisms such as using digital watermarking and blockchain technologies to build trustworthy media frameworks against deepfakes.The rapid growth in the capability to manipulate media or create synthetic media which look realistic and natural paved the way for deepfakes. At first, this paper adopted a critical approach to look at different definitions of the term “deepfake”. In that regard, we point out the different contradicting definitions and call for the wider community to consider how to define a new term that has a more consistent scope and meaning. For instance, replacing “deepfake” by “deep synthesis” can be more inclusive by embracing positive applications of deepfake techniques, e.g., in entertainment and for simulation purposes.This paper provided a comprehensive overview of multiple aspects of the deepfake ecosystem drawing from the research literature and other online sources published in two languages: English and Chinese. It covers commonly used performance metrics and standards, related datasets, challenges, competitions and benchmarks. It also presents a meta-review of 12 selected deepfake-related survey papers published in 2020 and 2021, covering not only the above mentioned aspects, but also highlighting key challenges and recommendations.[1]   Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: A Compact Facial Video Forgery Detection Network. In Proceedings of the 2018 IEEE International Workshop on Information Forensics and Security. IEEE, 1–7. https://doi.org/10.1109/WIFS.2018.8630761[2]   Henry Ajder, Giorgio Patrini, Francesco Cavalli, and Laurence Cullen. 2019. The State of Deepfakes: Landscape, Threats, and Impact. Deeptrace. , 27 pages.  https://sensity.ai/reports/[4]   Ahmed Ali and Steve Renals. 2018. Word Error Rate Estimation for Speech Recognition: e-WER. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 20–24. https://doi.org/10.18653/v1/P18-2004[6]   Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. 2018. Recycle-GAN: Unsupervised Video Retargeting. In Proceedings of the 2018 European Conference on Computer Vision. Springer, 17 pages.  https://doi.org/10.1007/978-3-030-01228-1 8[8]   Mikol-aj Bin´kowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C. Cobo, and Karen Simonyan. 2019. High Fidelity Speech Synthesis with Adversarial Networks.  https://doi.org/10.48550/ARXIV.1909.11646[10]   Umur Aybars Ciftci, Ilke Demir, and Lijun Yin. 2020. FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020), 17 pages. https://doi.org/10.1109/TPAMI.2020.3009287[11]   Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil K. Jain. 2020. On the Detection of Digital Face Manipulation. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 10 pages. https://doi.org/10.1109/CVPR42600.2020.00582[12]  Rohan Kumar Das, Tomi Kinnunen, Wen-Chin Huang, Zhen-Hua Ling, Junichi Yamagishi, Zhao Yi, Xiaohai Tian, and Tomoki Toda. 2020. Predictions of Subjective Ratings and Spoofing Assessments of Voice Conversion Challenge 2020 Submissions. In Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020. International Speech Communication Association, 99–120. https://doi.org/10.21437/VCC BC.2020-15[13] H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021. ASVspoof 2021: Automatic Speaker Verification Spoofing and Countermeasures Challenge Evaluation Plan.  https://www.asvspoof.org/asvspoof2021/asvspoof2021 evaluation plan.pdf[14]   H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021.[15]   H´ector Delgado, Nicholas Evans, Tomi Kinnunen, Kong Aik Lee, Xuechen Liu, Andreas Nautsch, Jose Patino, Md Sahidullah, Massimiliano Todisco, Xin Wang, and Junichi Yamagishi. 2021. ASVspoof 2021 Challenge - Speech Deepfake Database.  https://doi.org/10.5281/zenodo.4835108[16]    ![](file:///C:/Users/user/AppData/Local/Temp/msohtmlclip1/01/clip_image051.gif)Anushree Deshmukh and Sunil B. Wankhade. 2021. Deepfake Detection Approaches Using Deep Learning: A Systematic Review. In Intelligent Computing and Networking: Proceedings of IC-ICN 2020 (Lecture Notes in Networks and Systems, Vol. 146). Springer, 293–302. https://doi.org/10.1007/978-981-15-7421-4 27[17]   Xinyi Ding, Zohreh Raziei, Eric C. Larson, Eli V. Olinick, Paul Krueger, and Michael Hahsler. 2020. Swapped Face Detection using Deep Learning and Subjective Assessment. EURASIP Journal on Information Security 2020, 1 (2020), 1–12. https://doi.org/10.1186/s13635-020-00109-8[18]   Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset.  https://doi.org/10.48550/ARXIV.2006.07397[19]   Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset. arXiv:2006.07397. https://arxiv.org/abs/2006.07397[23]   Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter Seidel, Mohamed Elgharib, and Christian Theobalt. 2021. Videoforensicshq: Detecting High-Quality Manipulated Face Videos. In Proceedings of the 2021 IEEE International Conference on Multimedia and Expo. IEEE, 1–6. https://doi.org/10.1109/ICME51207.2021.9428101[24]   Haiying Guan, Andrew Delgado, Yooyoung Lee, Amy N. Yates, Daniel Zhou, Timothee Kheyrkhah, and Jon Fiscus. 2021. User Guide for NIST Media Forensic Challenge (MFC) Datasets. https://doi.org/10.6028/NIST.IR.8377[25]   Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE,  4360–4369.    https://doi.org/10.1109/CVPR46437.2021.00434[26]   Liming Jiang, Zhengkui Guo, Wayne Wu, Zhaoyang Liu, Ziwei Liu, Chen Change Loy, Shuo Yang, Yuanjun Xiong, Wei Xia, Baoying Chen, Peiyu Zhuang, Sili Li, Shen Chen, Taiping Yao, Shouhong Ding, Jilin Li, Feiyue Huang, Liujuan Cao, Rongrong Ji, Changlei Lu, and Ganchao Tan. 2021. DeeperForensics Challenge 2020 on Real-World Face Forgery Detection: Methods and Results. arXiv:2102.09471. https://arxiv.org/pdf/2102.09471.pdf[27]   Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery Detection. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 2886–2895. https://doi.org/10.1109/CVPR42600.2020.00296[28]   Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. 2018. Efficient Neural Audio Synthesis. https://doi.org/10.48550/ARXIV.1802.08435[30]   Tero Karras, Samuli Laine, and Timo Aila. 2019. A Style-based Generator Architecture for Generative Adversarial Networks. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 4401–4410. https://doi.org/10.1109/CVPR.2019.00453[32]   Ali Khodabakhsh, Raghavendra Ramachandra, Kiran Raja, Pankaj Wasnik, and Christoph Busch. 2018. Fake Face Detection Methods: Can They Be Generalized?. In Proceedings of the 2018 International Conference of the Biometrics Special Interest Group. IEEE, 1–6. https://doi.org/10.23919/BIOSIG.2018.8553251[33]   Hyeongwoo Kim, Mohamed Elgharib, Hans-Peter Zoll¨ofer, Michael Seidel, Thabo Beeler, Christian Richardt, and Christian Theobalt. 2019. Neural Style-Preserving Visual Dubbing. ACM Transactions on Graphics 38, 6, Article 178 (2019), 13 pages. https://doi.org/10.1145/3355089.3356500[34]   Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick P´erez, Christian Richardt, Michael Zollh¨ofer, and Christian Theobalt. 2018. Deep Video Portraits. ACM Transactions on Graphics 37, 4, Article 163 (2018), 14 pages. https://doi.org/10.1145/3197517.3201283[35]   Pavel Korshunov and S´ebastien Marcel. 2019. Vulnerability Assessment and Detection of Deepfake Videos. In Proceedings of the 2019 International Conference on Biometrics. IEEE, 1–6. https://doi.org/10.1109/ICB45273.2019.8987375[36]   Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. 2021. KoDF: A Large-scale Korean DeepFake Detection Dataset. In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision. IEEE, 10724–10733. https://doi.org/10.1109/ICCV48922.2021.01057[37]   Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. 2020. FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping. arXiv:1912.13457. https://arxiv.org/abs/1912.13457[38]   Xurong Li, Shouling Ji, Chunming Wu, Zhenguang Liu, Shuiguang Deng, Peng Cheng, Min Yang, and Xiangwei Kong. 2021. Survey on Deepfakes and Detection Techniques.  32, 2 (2021), 496–518. http://www.jos.org.cn/1000-9825/6140.htm[39]   Yuezun Li, Ming-Ching Chang, and Siwei Lyu. 2018. In Ictu Oculi: Exposing AI Created Fake Videos by Detecting Eye Blinking. In Proceedings of the 2018 IEEE International Workshop on Information Forensics and Security. IEEE, 1–7. https://doi.org/10.1109/WIFS.2018.8630787[40]   Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 3204–3213. https://doi.org/10.1109/CVPR42600.2020.00327[42]   Steven R. Livingstone and Frank A. Russo. 2018. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A Dynamic, Multimodal Set of Facial and Vocal Expressions in North American English.  13, 5 (2018), 35 pages.[43]   Chen-Chou Lo, Szu-Wei Fu, Wen-Chin Huang, Xin Wang, Junichi Yamagishi, Yu Tsao, and Hsin-Min Wang. 2021. MOSNet: Deep Learning based Objective Assessment for Voice Conversion. arXiv:1904.08352. https://arxiv.org/pdf/1904.08352.pdf[44]   Jaime Lorenzo-Trueba, Junichi Yamagishi, Tomoki Toda, Daisuke Saito, Fernando Villavicencio, Tomi Kinnunen, and Zhenhua Ling. 2018. The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods. In Proceedings of the Odyssey 2018 The Speaker and Language Recognition Workshop. International Speech Communication Association, 195–202. https://doi.org/10.21437/Odyssey.2018-28[46]   Yisroel Mirsky and Wenke Lee. 2021. The Creation and Detection of Deepfakes: A Survey.  54, 1, Article 7 (2021), 41 pages. https://doi.org/10.1145/3425780[47]   Gautham J. Mysore. 2015. Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges. IEEE Signal Processing Letters 22, 8 (2015), 1006–1010. https://doi.org/10.1109/LSP.2014.2379648[48]   Jo˜ao C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proen¸ca, and Julian Fierrez. 2020. GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection. IEEE Journal of Selected Topics in Signal Processing 14, 5 (2020), 1038–1048. https://doi.org/10.1109/JSTSP.2020.3007250[50]   Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio.  https://doi.org/10.48550/ARXIV.1609.03499[51]   Debajyoti Pal and Tuul Triyason. 2018. A Survey of Standardized Approaches towards the Quality of Experience Evaluation for Video Services: An ITU Perspective. International Journal of Digital Multimedia Broadcasting 2018, Article 1391724 (2018), 25 pages. https://doi.org/10.1155/2018/1391724[52]   Bo Peng, Hongxing Fan, Wei Wang, Jing Dong, Yuezun Li, Siwei Lyu, Qi Li, Zhenan Sun, Han Chen, Baoying Chen, Yanjie Hu, Shenghai Luo, Junrui Huang, Yutong Yao, Boyuan Liu, Hefei Ling, Guosheng Zhang, Zhiliang Xu, Changtao Miao, Changlei Lu, Shan He, Xiaoyan Wu, and Wanyi Zhuang. 2021. DFGC 2021: A DeepFake Game Competition. arXiv:2106.01217. https:[53]   Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um´e, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, Sheng Zhang, Pingyu Wu, Bo Zhou, and Weiming Zhang. 2020. DeepFaceLab: Integrated, Flexible and Extensible Face-swapping Framework. https://doi.org/10.48550/ARXIV.2005.05535[54]   Stanislav Pidhorskyi, Donald A. Adjeroh, and Gianfranco Doretto. 2020. Adversarial Latent Au-toencoders. In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 10 pages. https://doi.org/10.1109/CVPR42600.2020.01411[55]   Andreas R¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. 2018. FaceForensics: A Large-scale Video Dataset for Forgery Detection in Human Faces. https://doi.org/10.48550/ARXIV.1803.09179[56]   Andreas R¨ossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In Proceedings of the 2019 International Conference on Computer Vision. IEEE, 1–11. https://doi.org/10.1109/ICCV.2019.00009[61]   Justus Thies, Michael Zollh¨ofe, and Matthias Niessner. 2019. Deferred Neural Rendering: Image Synthesis using Neural Textures. ACM Transactions on Graphics 38, Article 66 (2019), 12 pages. Issue  4.   https://doi.org/10.1145/3306346.3323035[62]   Tomoki Toda, Ling-Hui Chen, Daisuke Saito, Fernando Villavicencio, Mirjam Wester, Zhizheng Wu, and Junichi Yamagishi. 2016. The Voice Conversion Challenge 2016. In Proceedings of Interspeech 2016. International Speech Communication Association, 1632–1636. https://doi.org/10.21437/Interspeech.2016-1066[63]   Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidullah, Hector Delgado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen, and Kong Aik Lee. 2019. ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection. arXiv:1904.05441. https://arxiv.org/pdf/1904.05441.pdf[64]   Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, and Javier Ortega-Garcia. 2020. Deepfakes and beyond: A Survey of face manipulation and fake detection.  64 (2020), 131–148.  https://doi.org/10.1016/j.inffus.2020.06.014[65]   Xin Tong, Luona Wang, Xiaoqin Pan, and Jingya Wang. 2020. An Overview of Deepfake: The Sword of Damocles in AI. In Proceedings of the 2020 International Conference on Computer Vision, Image and Deep Learning. IEEE, 265–273. https://doi.org/10.1109/CVIDL51233.2020.00-88[67]   Xin Wang, Junichi Yamagishi, Massimiliano Todisco, H´ector Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, S´ebastien Le Maguer, Markus Becker, Fergus Henderson, Rob Clark, Yu Zhang, Quan Wang, Ye Jia, Kai Onuma, Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda, Kou Tanaka, Hirokazu Kameoka, Ingmar Steiner, Driss Matrouf, Jean-Fran¸cois Bonastre, Avashna Govender, Srikanth Ronanki, Jing-Xuan Zhang, and Zhen-Hua Ling. 2020. ASVspoof 2019: A Large-scale Public Database of Synthesized, Converted and Replayed Speech. Computer Speech & Language 64 (2020), 27 pages.  https://doi.org/10.1016/j.csl.2020.101114[68]   Mirjam Wester, Zhizheng Wu, and Junichi Yamagishi. 2016. Analysis of the Voice Conversion Challenge 2016 Evaluation Results. In Proceedings of the Interspeech 2016 Conference. International Speech Communication Association, 1637–1641. https://doi.org/10.21437/Interspeech.2016-1331[70]  Zhao Yi, Wen-Chin Huang, Xiaohai Tian, Junichi Yamagishi, Rohan Kumar Das, Tomi Kinnunen, Zhen-Hua Ling, and Tomoki Toda. 2020. Voice Conversion Challenge 2020 – Intra-lingual Semi-parallel and Cross-lingual Voice Conversion –. In Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020. International Speech Communication Association, 80–98. https://doi.org/10.21437/VCC BC.2020-14[71]   Mohammed A. Younus and Taha M. Hasan. 2020. Abbreviated View of Deepfake Videos Detection Techniques. In Proceedings of the 2020 6th International Engineering Conference. IEEE, 115–120. https://doi.org/10.1109/IEC49899.2020.9122916[73]   Teng Zhang, Lirui Deng, Liang Zhang, and Xianglei Dang. 2020. Deep Learning in Face Synthesis: A Survey on Deepfakes. In Proceedings of the 2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology. IEEE, 67–70. https://doi.org/10.1109/CCET50901.2020.9213159[74]   Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, and Sanja Fidler. 2021. DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 10140–10150. https://doi.org/10.1109/CVPR46437.2021.01001[75]    ![](file:///C:/Users/user/AppData/Local/Temp/msohtmlclip1/01/clip_image051.gif)Yuanhan Zhang, ZhenFei Yin, Yidong Li, Guojun Yin, Junjie Yan, Jing Shao, and Ziwei Liu. 2020. CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations. In Proceedings of the 2020 European Conference on Computer Vision. Springer, 70–85. https://doi.org/10.1007/978-3-030-58610-2 5[76]   Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu, Shuo Yang, Yuanjun Xiong, Wei Xia, Yan Xu, Man Luo, Jian Liu, Jianshu Li, Zhijun Chen, Mingyu Guo, Hui Li, Junfu Liu, Pengfei Gao, Tianqi Hong, Hao Han, Shijie Liu, Xinhua Chen, Di Qiu, Cheng Zhen, Dashuang Liang, Yufeng Jin, and Zhanlong Hao. 2021. CelebA-Spoof Challenge 2020 on Face Anti-Spoofing: Methods and Results. arXiv:2102.12642. https://arxiv.org/pdf/2102.12642.pdf[77]   Bo Zhao, Shaozeng Zhang, Chunxue Xu, Yifan Sun, and Chengbin Deng. 2021. Deep Fake Ge-ography? When Geospatial Data Encounter Artificial Intelligence. Cartography and Geographic Information Science 48, 4 (2021), 338–352. https://doi.org/10.1080/15230406.2021.1910075[78]   Peng Zhou, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2017. Two-Stream Neural Networks for Tampered Face Detection. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops. IEEE, 1831–1839. https://doi.org/10.1109/CVPRW.2017.229[79]   Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, and Jianbing Shen. 2021. Face Forensics in the Wild. In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE,  5774–5784.    https://doi.org/10.1109/CVPR46437.2021.00572[80]   Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the 2017 IEEE International Conference on Computer Vision. IEEE, 2242–2251. https://doi.org/10.1109/ICCV.2017.244[81]   Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020. WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In Proceedings of the 2020 28th ACM International Conference on Multimedia. ACM, 2382–2390. https://doi.org/10.1145/3394171.3413769:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.  ]]></content:encoded></item><item><title>The HackerNoon Newsletter: Why “Small Changes” Don’t Exist in Production Game Systems (2/28/2026)</title><link>https://hackernoon.com/2-28-2026-newsletter?source=rss</link><author>Noonification</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:02:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[🪐 What’s happening in tech today, February 28, 2026?By @ktdevjournal [ 5 Min read ] It doesn’t matter if you build games or a banking app - you don’t just have a pile of features and assets. You have an ecosystem for each bit of work Read More.By @Lima_Writes [ 9 Min read ] When language comes back at you fast, coherent, and emotionally attuned, it feels like truth. Especially when you’re tired. Or lonely.  Read More.🧑‍💻 What happened in your world this week?We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, 
 The HackerNoon Team ✌️]]></content:encoded></item><item><title>Go 1.22: A Change in Loop Scoping</title><link>https://hackernoon.com/go-122-a-change-in-loop-scoping?source=rss</link><author>Go [Technical Documentation]</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:00:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Go 1.21 includes a preview of a change to  loop scoping that we plan to ship in Go 1.22, removing one of the most common Go mistakes.If you’ve written any amount of Go code, you’ve probably made the mistake of keeping a reference to a loop variable past the end of its iteration, at which point it takes on a new value that you didn’t want. For example, consider this program:func main() {
    done := make(chan bool)

    values := []string{"a", "b", "c"}
    for _, v := range values {
        go func() {
            fmt.Println(v)
            done <- true
        }()
    }

    // wait for all goroutines to complete before exiting
    for _ = range values {
        <-done
    }
}
\
The three created goroutines are all printing the same variable , so they usually print “c”, “c”, “c”, instead of printing “a”, “b”, and “c” in some order.\
Although concurrency is often involved, it need not be. This example has the same problem but no goroutines:func main() {
    var prints []func()
    for i := 1; i <= 3; i++ {
        prints = append(prints, func() { fmt.Println(i) })
    }
    for _, print := range prints {
        print()
    }
}
\
This kind of mistake has caused production problems at many companies, including a publicly documented issue at Lets Encrypt. In that instance, the accidental capture of the loop variable was spread across multiple functions and much more difficult to notice:// authz2ModelMapToPB converts a mapping of domain name to authz2Models into a
// protobuf authorizations map
func authz2ModelMapToPB(m map[string]authz2Model) (*sapb.Authorizations, error) {
    resp := &sapb.Authorizations{}
    for k, v := range m {
        // Make a copy of k because it will be reassigned with each loop.
        kCopy := k
        authzPB, err := modelToAuthzPB(&v)
        if err != nil {
            return nil, err
        }
        resp.Authz = append(resp.Authz, &sapb.Authorizations_MapElement{
            Domain: &kCopy,
            Authz: authzPB,
        })
    }
    return resp, nil
}
\
The author of this code clearly understood the general problem, because they made a copy of , but it turns out  used pointers to fields in  when constructing its result, so the loop also needed to make a copy of .\
Tools have been written to identify these mistakes, but it is hard to analyze whether references to a variable outlive its iteration or not. These tools must choose between false negatives and false positives. The  analyzer used by  and  opts for false negatives, only reporting when it is sure there is a problem but missing others. Other checkers opt for false positives, accusing correct code of being incorrect. We ran an analysis of commits adding  lines in open-source Go code, expecting to find bug fixes. Instead we found many unnecessary lines being added, suggesting instead that popular checkers have significant false positive rates, but developers add the lines anyway to keep the checkers happy.\
One pair of examples we found was particularly illuminating:This diff was in one program:     for _, informer := range c.informerMap {
+        informer := informer
         go informer.Run(stopCh)
     }
\
And this diff was in another program:     for _, a := range alarms {
+        a := a
         go a.Monitor(b)
     }
\
One of these two diffs is a bug fix; the other is an unnecessary change. You can’t tell which is which unless you know more about the types and functions involved.For Go 1.22, we plan to change  loops to make these variables have per-iteration scope instead of per-loop scope. This change will fix the examples above, so that they are no longer buggy Go programs; it will end the production problems caused by such mistakes; and it will remove the need for imprecise tools that prompt users to make unnecessary changes to their code.\
To ensure backwards compatibility with existing code, the new semantics will only apply in packages contained in modules that declare  or later in their  files. This per-module decision provides developer control of a gradual update to the new semantics throughout a codebase. It is also possible to use  lines to control the decision on a per-file basis.\
Old code will continue to mean exactly what it means today: the fix only applies to new or updated code. This will give developers control over when the semantics change in a particular package. As a consequence of our forward compatibility work, Go 1.21 will not attempt to compile code that declares  or later. We included a special case with the same effect in the point releases Go 1.20.8 and Go 1.19.13, so when Go 1.22 is released, code written depending on the new semantics will never be compiled with the old semantics, unless people are using very old, unsupported Go versions.Go 1.21 includes a preview of the scoping change. If you compile your code with  set in your environment, then the new semantics are applied to all loops (ignoring the  lines). For example, to check whether your tests still pass with the new loop semantics applied to your package and all your dependencies:GOEXPERIMENT=loopvar go test
\
We patched our internal Go toolchain at Google to force this mode during all builds at the start of May 2023, and in the past four months we have had zero reports of any problems in production code.\
You can also try test programs to better understand the semantics on the Go playground by including a  comment at the top of the program, like in this program. (This comment only applies in the Go playground.)Although we’ve had no production problems, to prepare for that switch, we did have to correct many buggy tests that were not testing what they thought they were, like this:func TestAllEvenBuggy(t *testing.T) {
    testCases := []int{1, 2, 4, 6}
    for _, v := range testCases {
        t.Run("sub", func(t *testing.T) {
            t.Parallel()
            if v&1 != 0 {
                t.Fatal("odd v", v)
            }
        })
    }
}
\
In Go 1.21, this test passes because  blocks each subtest until the entire loop has finished and then runs all the subtests in parallel. When the loop has finished,  is always 6, so the subtests all check that 6 is even, so the test passes. Of course, this test really should fail, because 1 is not even. Fixing for loops exposes this kind of buggy test.\
To help prepare for this kind of discovery, we improved the precision of the  analyzer in Go 1.21 so that it can identify and report this problem. You can see the report in this program on the Go playground. If  is reporting this kind of problem in your own tests, fixing them will prepare you better for Go 1.22.\
If you run into other problems, the FAQ has links to examples and details about using a tool we’ve written to identify which specific loop is causing a test failure when the new semantics are applied.\
This article is available on  under a CC BY 4.0 DEED license.]]></content:encoded></item><item><title>Why “Small Changes” Don’t Exist in Production Game Systems</title><link>https://hackernoon.com/why-small-changes-dont-exist-in-production-game-systems?source=rss</link><author>Constantine</author><category>tech</category><pubDate>Sat, 28 Feb 2026 16:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[It’s just a small change!\
How often do we hear that we need to fix something? We need to add a small feature. We need to tweak something. Code-wise or publishing, just realized they need this for retention, or maybe an analyst brought the newest data, so now we have to add just a few lines to the code. They don’t affect performance or any other departments, I promise. And it’s just like 3 minutes of coder work - why not? Fast forward: they broke the “Buy” button on the front page of the store on release.\
Why does this always happen with small changes? Well, if we think about it, we don’t usually think about it. Let me explain:Designers think in features and user experience. \n Engineers think in whole systems. \n Producers think in tasks. \n Stakeholders think in business outcomes.\
And one small change is always perceived as something isolated and usually without everyone’s awareness. So, it is basically a cognitive shortcut. And that happens not because everyone is wrong or unprofessional. It’s because modern production systems are highly interconnected, so it’s impossible to know what could potentially be affected by anything - especially if you haven’t worked on this project for 15 years.What is modern production? I’m glad you asked!\
It doesn’t matter if you build games or a banking app - you don’t just have a pile of features and assets. You have an ecosystem for each bit of work: Art, Code, Design, UI, Marketing, Publishing (maybe even Project Management - wow, you are a rich developer), etc. And each one of them has its own infrastructure, pipelines, workflows, and shared assets. To simplify, it can be shared data schemas, builds, automation processes, UI bindings, and many other things.\
What’s wrong if I just make a small color change to one of the icons? Well, that means you spend 3 seconds changing a color code. Then you have to assemble a build. Then QA has to check your small change to confirm that you indeed changed the color. Then you have to assemble the build again, which should be in a queue with other builds in the waiting list.\
Then we have to update the server with your changes - oh wait, did you tell anyone about that? No? Oh, that’s great, because you just submitted your changes during the commit freeze, and now deployment engineers have to fix the CI/CD pipeline, and we have to postpone the release for 4 days because it’s Friday.\
And by the way - we have to communicate that to users because they were waiting for this new version, and some of them decided not to wait that long and removed your app. Whoops, that’s awkward. Sorry to hear that.That’s alright, I’m here to help you! Let me introduce you to Change Propagation Surface (CPS) - the number of systems, pipelines, assets, and workflows that a change must pass through before it reaches the player.\
Your change should not be estimated by its task size, like “1 hour of work.” Your change equals CPS × Coupling Density (the amount of work other departments need to do in order for this change to pass).\
Think about it this way:One small UI tweak touches no shared data - low CPS.A gameplay rule change touching code, balance, design, analytics, player experience - high CPS.\
Let’s go back to the situation where you want to change the color of the icon. Those 3 seconds of work would affect UI, builds, player perception, experience, and design. It might also affect color coding for accessibility rules, plus build assembling, and finally server updates. It’s high CPS - of course, if you didn’t sneak that change in without everyone’s awareness (I see that - drop it!).\
The same goes for asset swaps or changing a stat value: it affects memory, AI tuning, destruction logic, etc. Don’t do that unless someone from senior leadership said it’s low CPS - then just do it and see how it goes.\
You can apply this approach basically anywhere in production because it is not an abstract thing at all and can be estimated.\
Each of these items counts as a plus 1 CPS factor. Subsequently, the more of the same “items” you touch, the higher the CPS number you will get. And with that information, you can create a small estimation matrix like:CPS 1-2 - Local change \n CPS 3-5 - Cross-functional change \n CPS 6+ - Systemic change\
One more time, the formula is: Impact = CPS × Coupling Density. Easy!Let’s see how it works in a real-life example:\
So your developer went on holiday and completed a math course on LinkedIn. And when he came back, he said that there is a more efficient way of calculating EXP. This change is “one line of code.” Okay, but after reading this article, you already know how it works in reality and that it touches multiple things:Player progression pacing\
That means CPS is more than 7. So now you see that even though the code diff is tiny, the propagation surface is systemic and has a massive potential outcome. In other words, if XP progression speeds up things like economy, availability of the content, battle pass value, retention curves, etc., you should know that even if the implementation takes about 10 minutes, the ripple effect can take weeks of work.\
Why does live service production make it worse? Because it is amplified by content being reused across multiple features, by telemetry and economy being tightly coupled, and by systems being persistent and often requiring backward compatibility.\
So, the real cost you pay for propagation lies in prolonged timelines, hidden rework, cross-team friction, technical debt, burnout, and eventually, people resigning directly or indirectly.\
Instead of thinking, “Oh, this is a small change,” we should probably think, “What systems does this change touch?” Think about this as infrastructure, not a feature, and always try to bring that to cross-team awareness. And if you are capable enough, try to estimate the surface area, not just this exact small change.\
**The whole point of my way-too-long introduction is that there is no such thing as a small change in production systems. There are only changes in misunderstood affected areas. And the more senior you become, the more your vision shifts toward understanding how this change will travel instead of trying to avoid the change altogether.]]></content:encoded></item><item><title>Meet M6: The Chinese AI That Understands Text and Images at Scale</title><link>https://hackernoon.com/meet-m6-the-chinese-ai-that-understands-text-and-images-at-scale?source=rss</link><author>Alibaba</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:28:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Junyang Lin, junyang.ljy@alibaba-inc.com (Alibaba Group, China)Rui Men, menrui.mr@alibaba-inc.com (Alibaba Group, China)An Yang, ya235025@alibaba-inc.com (Alibaba Group, China)Chang Zhou, ericzhou.zc@alibaba-inc.com (Alibaba Group, China)Ming Ding, dm18@mails.tsinghua.edu.cn (Tsinghua University, China)Yichang Zhang, yichang.zyc@alibaba-inc.com (Alibaba Group, China)Peng Wang, zheluo.wp@alibaba-inc.com (Alibaba Group, China)Ang Wang, wangang.wa@alibaba-inc.com (Alibaba Group, China)Le Jiang, jiangle.jl@alibaba-inc.com (Alibaba Group, China)Xianyan Jia, xianyan.xianyanjia@alibaba-inc.com (Alibaba Group, China)Jie Zhang, wanglin.zj@alibaba-inc.com (Alibaba Group, China)Jianwei Zhang, zhangjianwei.zjw@alibaba-inc.com (Alibaba Group, China)Xu Zou, zoux18@mails.tsinghua.edu.cn (Tsinghua University, China)Zhikang Li, zhikang.lzk@alibaba-inc.com (Alibaba Group, China)Xiaodong Deng, xiaodongdeng.dxd@alibaba-inc.com (Alibaba Group, China)Jie Liu, sanshuai.lj@alibaba-inc.com (Alibaba Group, China)Jinbao Xue, zhiji.xjb@alibaba-inc.com (Alibaba Group, China)Huiling Zhou, zhule.zhl@alibaba-inc.com (Alibaba Group, China)Jianxin Ma, jason.mjx@alibaba-inc.com (Alibaba Group, China)Jin Yu, kola.yu@alibaba-inc.com (Alibaba Group, China)Yong Li, jiufeng.ly@alibaba-inc.com (Alibaba Group, China)Wei Lin, weilin.lw@alibaba-inc.com (Alibaba Group, China)Jingren Zhou, jingren.zhou@alibaba-inc.com (Alibaba Group, China)Jie Tang, jietang@tsinghua.edu.cn (Tsinghua University, China)Hongxia Yang, yang.yhx@alibaba-inc.com (Alibaba Group, China)In this work, we construct the largest dataset for multimodal pre-training in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called , referring to ulti-odality to ulti-odality ultitask ega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and  parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details.Multimodal Pretraining; Multitask; Text-to-Image GenerationPretraining has become a focus in the research in natural language processing (NLP) [1, 2, 7, 16, 18, 19, 27, 31, 37, 44, 49]. The recent GPT-3 with over 175 billion parameters demonstrates that large models trained on big data have extremely large capacity and it can outperform the state-of-the-arts in downstream tasks especially in the zero-shot setting. Also, the rapid development of pretraining in NLP sparkles cross-modal pretraining. A number of studies [4, 11, 17, 22, 24, 25, 28, 29, 38, 51] have created new state-of-the-art performances for various cross-modal downstream tasks.A pity is that most recent studies focus on the pretraining on English data. There are lack of both large-scale datasets in Chinese and large-scale models pretrained on the data of Chinese. Therefore, in this work, we develop a large-scale dataset M6-Corpus, which consists of over 1.9TB images and 292GB texts. To the best of our knowledge, this is the largest dataset in Chinese for pretraining in both multimodality and natural language. The dataset collected from the webpages consists of different types of data and covers a large scale of domains, including encyclopedia, question answering, forum discussion, product description, etc. Also, we design sophisticated cleaning procedures to ensure that the data are of high quality.Furthermore, in order to sufficiently leverage such a large amount of high-quality data, we propose to build an extremely large model that can process data of multiple modalities and adapt to different types of downstream tasks. Thus we propose a novel model called M6, referring to MultiModality-to-MultiModality Multitask Mega-transformer. The model is based on the transformer, and it is pretrained with multiple tasks. Pretraining endows the model with the capability of single-modality and multimodality understanding and generation. Based on the architecture of M6, we build  and , which are scaled up to 10 billion and 100 billion pa-rameters respectively. To be more specific,  is the recent largest model pretrained on Chinese data. We apply the model to a series of downstream applications, including product description generation, visual question answering, community question answering, Chinese poem generation, etc., and our experimental results show that M6 outperforms a series of strong baselines.Another contribution of this work is that we first incorporate pretraining with text-to-image generation. Following Ramesh et al. [32], we leverage a two-stage framework for image generation. To be more specific, we apply a trained vector-quantized generative adversarial network to representing images with discrete image codes, and we then use the pretrained M6 to learn the relations between texts and codes. Such learning can bridge the two modalities and enables controllable text-to-image generation.To summarize, the contributions of M6 are as follows:We collect and build the largest Chinese multi-modal pre-training data in industry, which includes 300GB texts and 2TB images.We propose M6 for multimodal pretraining in Chinese, and we scale the model size to up to 10 and 100 billion parameters. Both M6-10B and M6-100B are the recent largest multimodal pretrained model.M6 is versatile and exceeds strong baselines by 11.8% in VQA, 18.4 in image captioning, and 10.3% in image-text matching. Furthermore M6 is able to generate high-quality images.With carefully designed large-scale distributed training optimizations, M6 has obvious advantages in training speed and greatly reduces training costs, creating the possibility for more widespread use of multi-modal pretraining.We collect and develop the largest multi-modality and text dataset in Chinese for now, which is one of the key contributions of this paper. In this section, we first identify the limitations of existing datasets and then describe the construction and preprocessing procedure of our proposed dataset.2.1      Existing DatasetsThe construction of large-scale corpus with high quality and do-main coverage is crucial to Chinese pretraining. In early previous works, the Chinese Wikipedia1 is one of the most frequently used datasets to train Chinese language models. It contains 1.6GB texts (around 0.4B tokens) covering around 1M encyclopedia entries. Another corpus with a comparable size is the THUCTC[39] dataset, which includes 740K news articles. However, with the rapidly increasing capacity of recent language models, the scale of these existing datasets is clearly insufficient. Recently, Cui et al. [5] employ unreleased extended data that are 10 times larger than the CN-Wikipedia to pretrain their Chinese language model. Xu et al.[47] released a 100GB corpus named CLUECorpus2020, which is retried from the multilingual Common Crawl dataset. However, the scale of the datasets is still insufficient to facilitate super large-scale pretraining compared with existing English pretrained models. For example, GPT-3 contains 175B parameters and is trained on 570GB texts. Meanwhile, the dataset should contain image-text pairs rather than plain texts for multi-modal pretraining.2.2      Standards for a High-quality DatasetTo perform large-scale multi-modal pretraining and learn complex world knowledge in Chinese, the dataset is highly required to provide both plain texts and image-text pairs on super large scale, covering a wide range of domains. In order to perform large-scale multi-modal pretraining in Chinese, we focus on the construction of large-scale datasets in Chinese. Specifically, while we unify our pretraining for both natural language and multimodalities, we construct large datasets of both plain texts and image-text pairs. We are interested in obtaining large-scale data that covers a wide range of domains, so that it is possible for the model to learn the complex world knowledge of different fields. Also, we aim to collect data of multiple modalities for the cross-modal pretraining. This raises the difficulty for the construction of a large-scale dataset as the data for multimodal pretraining are usually image-text pairs, where in each pair the text provides a detailed description of a fraction of the image.Though there are a tremendous amount of text resources and images on the world wide web, the corpus for multimodal pretraining is assumed to be better when satisfying the following properties:(1). the sentences should be fluent natural language within a normal length, and should not contain meaningless tokens, such as markups, duplicate punctuation marks, random combinations of characters, etc.; (2). the images should be natural and realistic, and the resolutions of the images need to be identifiable by humans; (3). both the texts and images should not contain illegal content, such as pornography, violence, etc.; (4). the images and texts should be semantically relevant; (5). the datasets should cover a wide range of fields, say sports, politics, science, etc., and therefore it can endow the model with sufficient world knowledge.2.3      Dataset ConstructionBased on the requirements above, we collect data of both plain texts and image-text pairs. There are different types of data, including encyclopedia, crawled webpage, community question answering, forum, product description, etc. We present the details in Table 3. The collected corpus consists of bothag plain-texts and image-text pairs, which is compatible with the designed text-only and multi-modal pretraining tasks. Also, the data has a large coverage over domains, such as science, entertainment, sports, politics, common-sense of life, etc. We have also compared some characteristics of our corpus with existing datasets used for Chinese pretraining in Table 2. The size of our dataset is much larger than the previous ones. To our knowledge, this is the first large-scale, multimodal and multidomain corpus for Chinese pretraining.We implement sophisticated preprocessing to obtain clean data. For text data, we first remove HTML markups and duplicate punctuation marks, and we only reserve characters and punctuation marks that are in Chinese and English. We remove the topics that are shorter than 5 characters and contents shorter than 15 characters. We further apply in-house spam detection to remove sentences that contain words related to certain political issues, pornography, or words in the list of dirty, naughty, and other bad words. In order to preserve the linguistic acceptance of the texts, we implement a language model to evaluate their perplexities, and sentences with high perplexities are discarded. Only images with at least 5000 pixels are reserved for pretraining. A sequence of classifiers and heuristic rules are applied to filter out images containing illegal content. We also use a pretrained image scorer to evaluate the qual-ities of images. For images and texts in crawled webpages, we only consider images and their surrounding text as relevant image-text pairs. Other sentences in the webpages are discarded.Multimodal pretraining leverages both the power of self-attention-based transformer architecture and pretraining on large-scale data. We endeavor to endow the model with strong capability of cross-modal understanding and generation. In this section, we describe the details of our proposed pretrained model , which refers to ulti-odality-to-ulti-odality ultitask ega-transformer.3.1      Å   Visual and Linguistic InputsThe mainstream multimodal pretraining methods transform images to feature sequences via object detection. However, the performance of the object detectors as well as the expressivity of their backbones strongly impact the final performance of the pretrained models in the downstream tasks. We observe that a large proportion of the images contain only a few objects. Take the images of the data of e-commerce as an example. We randomly sample 1M images and perform object detection on the images. The results show that over 90% of the images contain fewer than 5 objects. Also, the objects have high overlapping with each other. To alleviate such influence, we turn to a simple but effective solution following Gao et al. [\[12\]](#bookmark28) and Dosovitskiy et al. [\[8\]](#bookmark24). In general, we split an image into patches and extract features of the 2D patches with a trained feature extractor, say ResNet-50. Then we line up the representations to a sequence by their positions.  The processing of the input word sequence is much simpler. We follow the similar preprocessing procedures in the previous work [4, 11, 24]. We apply WordPiece [34, 45] and masking to the word sequence and embed them with an embedding layer, following BERT [6].3.2      Unified Encoder-DecoderWe integrate the image embeddings 𝑒𝑖 and the word embeddings 𝑒𝑡 into the cross-modal embedding sequence 𝑒 = {𝑒𝑖, 𝑒𝑡 }. We send the sequence to the transformer backbone for high-level feature extraction. To differ their representations, we add corresponding segment embeddings for different modalities. Specifically, we leverage theself-attention-based transformer blocks for our unified cross-modal representation learning. To be more specific, the building block is identical to that of BERT or GPT, which consists of self attention and point-wise feed-forward network (FFN). On top of the transformer backbone, we add an output layer for word prediction, and thus we tie its weights to those of the embedding layer.In the unified framework, we use different masking strategies to enable encoding and decoding. The input is segmented into three parts, including visual inputs, masked linguistic inputs, and complete linguistic inputs. We apply bidirectional masking to both the visual inputs and masked linguistic inputs, and we apply causal masking to the complete linguistic inputs. Thus the model is allowed to encode and decode in the same framework.3.3      Pretraining MethodsWe pretrain the model with the multitask setup, including text-to-text transfer, image-to-text transfer, and multimodality-to-text transfer. Thus the model can process information of different modalities and perform both single-modal and cross-modal understanding and generation. As demonstrated in Figure 3, the model learns to perform text denoising and language modeling in the setting of text-to-text transfer. In text denoising, we mask the input text by a proportion, which is 15% in practice following BERT [6]. Specifically, we mask a continuous span of text with a single mask, and the model should learn to decode the whole sequence. This encourages the model to learn both recovering and length predict-ing. Besides, in order to improve the model ability in generation, we add a setup of language modeling, where the encoder receives no inputs and the decoder learns to generate words based on the previous context.\
 Image-to-text transfer is similar to image captioning, where the model receives the visual information as the input, and learns to generate a corresponding description. In this setting, we add the aforementioned patch feature sequence to the input and leave the masked input blank. The model encodes the patch features, and decodes the corresponding text.Multimodality-to-text transfer Based on the setup of image-to-text transfer, we additionally add masked linguistic inputs, and thus the model should learn to generate the target text based on both the visual information and the noised linguistic information. This task allows the model to adapt to the downstream tasks with both visual and linguistic inputs.3.4      Scaling up to 10 and 100 Billion ParametersWe scale up the model size to 10 billion parameters and 100 billion parameters, which are named M6-10B and M6-100B. The increase in model size provides a much larger capacity for the model that it can learn knowledge from more data. For the construction of M6-10B, we simply scale up the model by hyperparameter tuning.To be more specific, we increase the size of hidden states and the number of layers. To better leverage GPU memory, we apply mixed-precision training and activation checkpointing to save memory. Still, the model cannot be fit into one single GPU, and thus we use model parallelism to split the feed-forward networks and attention heads to multiple GPUs following the implementation of Megatron-LM [36].However, directly scaling up to M6-100B is much more difficult as there are more challenges for the computation resources. Alternatively, inspired by the recent progress in sparse activations [10, 20, 35], we combine Mixture-of-Experts (MoE) with M6 to build the version of 100 billion parameters. Note that the original MoE requires mesh-tensorflow as well as TPUs. This sets limits for a number of researchers without such resources. Thus we implement the M6-100B with MoE with our in-house framework Whale [43] to perform model parallelism with GPUs. We demonstrate the key statistics of the models of different scales in Table 4.Specifically, different from the conventional FFN layer, the MoE layer is a parallel combination of multiple FFN layers, each of which acts as an expert. This is also called expert parallelism. The model first learns a sparse gating network to route the tokens to specific experts. Thus each token is only sent to a small set of experts and the computation can be much less compared with that in dense models. This kind of model is highly efficient as it realizes data parallelism and expert parallelism across workers. The computation of MoE layer for a specific token 𝑥 can be described as below:where 𝑔(·) refers to the sparse gating function, and T refers to the indices of top-𝑘 values of 𝑔(·). The output of MoE is a linear combination of the computation of selected expert FFNs 𝑓 (·).In expert parallelism, the parameters of experts do not share across workers, while those of other parts are identical across workers. Therefore, it is necessary to perform all-to-all communication across workers at the MoE layers in order to dispatch tokens to selected experts and combine them to their original experts. While Lepikhin et al. [20] and Fedus et al. [10] implement the MoE on TPUs with one expert in each MoE layer on a TPU, we implement our model on Nvidia GPUs where there are several experts in each MoE layer on a GPU so as to fully utilize the memory. As all-to-all communication takes up a large amount of time, the optimization to improve efficiency is highly significant. We implement a series of optimization, including half-precision communication. A key problem is load balancing, which denotes that tokens can gather to only a few experts due to dynamic routing. Following Fedus et al. [10], we apply expert capacity, which refers to the number of tokens for an expert (𝐶 = 𝑁 - 𝑐/m, where 𝐶 refers to expert capacity, 𝑁 refers to the number of tokens in a batch, 𝑐 refers to capacity factor (which is a hyperparameter usually larger than 1.0) and 𝑚 refers to the number of experts), to alleviate this problem. Tokens out of the capacity of an expert are dropped from the computation and they are sent to next layers through residual connections. We find that the overloading problem can be severe, and this issue can be a significant one in the future research of expert models.Besides the optimization in all-to-all communication, we com-pare the top-2 gating and top-1 gating and find that they can achieve similar model performance in perplexity, while the latter converges slightly slower. The effectiveness of top-1 gating enables faster computation. Besides, we also apply methods of memory optimization for higher efficiency. We find that gradient clipping globally can increase costs on all-to-all communication as it computes norms across all experts, and thus we apply local clipping for memory saving. We implement M6-100B with around 100 billion parameters on 128 Nvidia A100s and the speed of pretraining achieves 1440 samples/s (for samples of the sequence length of 272).We demonstrate that using MoE structure for model size scaling is effective and it can achieve similar performance to that of M6-10B, the largest dense model, within 2-3 times shorter time. The negative log perplexity of M6-100B reaches −2.297, in comparison with M6-10B that reaches −2.253 but with twice of time.2 This shows that the MoE-based M6 model has advantages on the time basis compared with dense models with many more FLOPs.4.1      Text-to-Image GenerationText-to-image generation has been an open problem for a long time. Previous studies mainly focused on generation on a limited domain, among which Generative Adversarial Nets (GANs) [14, 48] are dominated methods. Following Ramesh et al. [32], we leverage a two-stage framework for text-to-image generation, including discrete representation learning and language modeling.\
In the first stage, we focus on transforming images into sequences of discrete codes. There are a number of alternatives for discrete code generation, including VQVAE [41] and VQGAN [9]. In the second stage, it is necessary to build a language model to learn to generate text and code sequence. In the finetuning, we add code embedding and output layers to the pretrained M6. We concat the word sequence and the aforementioned generated code sequence as the input, and we set the objective of autoregressive language modeling for the training. At the stage of inference, we input the text sequence, and the model generates codes autoregressively with top-k sampling. The last step is to transform the code sequence to an image with the generator from the first stage.We construct a dataset for text-to-image generation in E-commerce. Specifically, we collect over 50 million product titles and images from the mobile Taobao. We apply a series of processing methods on the images to filter the unqualified. We filter the images with complex background features (characters, patterns, etc.) with the in-house white-background image detector and OCR model. We then filter the images with over 3 objects with our in-house object detector based on Faster R-CNN [33]. We finally obtain 1.8m high-quality product image-text pairs for finetuning. Compared with the images in the general domains, our collected data have the following features. The image and text are highly correlated as the text describes key features of the product, and there is no complex background in the images, which is easier to learn compared with the images in the public datasets such as MSCOCO [26].We demonstrate two examples in Figure 4 and Figure 5. It can be found that the generated images have high quality and the generated objects resemble the real ones. Furthermore, in Figure 6 , we find that the model is able to imagine items according to the query military style camouflage high heels(军旅风迷彩高跟鞋), which do not exist in the real world. The imagination ability provides room for creative design in real-world industrial scenarios, such as clothing design, shoe design, etc.We also finetune M6 under our proposed framework on another dataset which contains 3 million images crawled from the Internet, which cover more general domains. And we find that the model can adapt to different domains. As shown in Figure 7, the model is able to generate clip arts of robots . This reveals the versatility of the framework in text-to-image generation.4.2      Visual Question AnsweringWe demonstrate our experimental results on a visual question answering dataset, and we illustrate how we directly apply the pre-trained M6 to the VQA application.\
We leverage the FMIQA dataset [13] as the Chinese visual QA benchmark, which requires the model to generate the answer given an image and a question. We implement a transformer-based model as our baseline. For the evaluation, we split the test set manually by random sampling 200 from the dataset as there is no official release of the test set, and we evaluate the overall accuracy by human evaluation. The results are demonstrated in Table 5. The pretrained M6-base outperforms the baseline by a large margin (+6.2%), which indicates the effectiveness of multimodal pretraining. Scaling up the model to M6-10B further brings 5.2% improvement.Furthermore, we show that simply finetuning on such a small VQA dataset may limit the potential of M6. Therefore, we directly leverage M6 for the VQA application. We find that the model is able to recognize general features and provide more related knowledge based on its understanding. Though the model pretrained on pseudo-parallel image-text pairs cannot directly answer questions about detailed features, such as color, number, etc., it is able to answer questions related to background knowledge. We demonstrate some examples in Figure 8.4.3      Image CaptioningImage captioning requires the model to generate a caption that describes the given image, which examines the model ability of cross-modal generation. We construct a dataset (named E-Commerce IC) containing pairs of product descriptions and product images from Taobao. Since too long or too short descriptions may be noisy, we discard pairs with a description longer than 100 words or less than 10 words. To avoid dirty generations, we further use an in-house tool to filter descriptions that may contain dirty words (i.e., pornographic or violent words). Finally, E-Commerce IC contains about 260k text-image pairs. We finetune the model with the image-to-text transfer task on E-Commerce IC.\
We compare our model with a baseline of transformer in the human evaluation. We ask several annotators with the linguistic background to evaluate from three perspectives: grammar (whether a text is fluent without grammatical error), correctness (whether a text is faithful to the image), richness (whether a text is informative and attractive). During the evaluation, we randomly sample 100 images from the test set. For each image, an annotator is asked to score the text generated by different models. The scores are within the range of [0, 5].The results in Table 6 show that M6-base outperforms the baseline in all of the metrics. We find that all models achieve high scores in grammar. However, in both correctness and richness, M6-base outperforms the baseline model by a large margin (+18.2% and +14.4%), indicating that multimodal pretraining helps to generate more faithful, informative and attractive texts. Scaling up the model to M6-10B further improves the correctness and richness (about 14.7% and 7.0%). Figure 9 illustrates two examples of image caption.4.4      Question AnsweringTo demonstrate the potential availability in the applications of intelligent chatbots, we further employ the M6 model to generate long answers in the style of forum discussion. Human-generated questions are collected from various Chinese forums, which are input to the model to generate the answer. At the stage of inference, we append a question mark and a token  in the prompt, which better triggers the model to generate an answer. To facilitate the generation of longer and more informative texts, we pick more complex questions.Figure 10 demonstrates an example of general question answer-ing. The model can illustrate a man’s own experiences that are related to the question and also point out the answer at the end. This generated text confused human annotators and passed the Turing Test. It shows that the model can not only answer general questions but also generate long fluency text.We apply the pretrained model to Chinese poem generation. The model is able to generate genres with format constraints.\
Ancient Chinese poetry has various specific formats. We adopt the simplest constraints thatThe poem shall be consisted of at least 4 lines.The total number of lines shall be even.Each line must have exactly 5 or 7 words.All lines shall have the same number of words.Text generation under format constraint is done in a search framework that we generate short sentences ending with punctuation until the number of words meets the constraint. We repeat this process until the model generates an "" token, or the number of lines exceeds a limit of 16. Figure 11 illustrates an example of a generated poem.4.6      Image-Text MatchingWe evaluate the model’s ability in cross-modal retrieval. Specifically, we construct a dataset (named E-Commerce ITM) containing pairs of texts and images from the mobile Taobao. Each pair belongs to a single item. we collect 235K products in the clothing industry from Taobao. For each product, aside from the product image, we obtain a query by rewriting the product title. Specifically, we conduct named entity recognition on the title using an in-house tool, which extracts the terms describing the style, color, category and texture of the product.\
These terms are then concatenated into a natural language query, which is used in image-text matching. The length of each query is between 6 to 12 words. The pairs of the query and corresponding product image are labeled as positive samples. The negative samples are constructed by randomly substituting the query in the original pairs.We require the model to perform binary classification to discriminate positive and negative samples. We compare our model with InterBert [25], which is also a Chinese multi-modal pretrained model effective in cross-modal classification downstream tasks. The InterBert utilizes object-based features and has been pretrained on Taobao product image-text data as well.The results are shown in Table 7. It should be noted that the InterBert and M6-base are both implemented with transformer-based architecture and have similar model scales. However, M6-base still outperforms InterBert by 10.3%. In experiments, we find the product images generally contain relatively fewer detected objects, which may harm the performance on this task. In contrast, M6 avoids this problem by employing the patch features and achieves much better performance.The tremendous success of NLP pretraining, including BERT [6], GPT [2, 30, 31], and also some other related studies [1, 7, 19, 27, 49], inspires the research in cross-modal representation learning. Also, recent studies show that the ubiquitous Transformer architecture [42] can be extended to different fields, including computer vision [3, 8]. Therefore, the simplest solution to incorporate recent pretraining methods and cross-modal representation learning is the extension of BERT. From the perspective of architecture, there are mainly two types, including single-stream model and dual stream model. Specifically, single-stream model is simple and it gradually becomes the mainstream architecture. These models mostly differ in their designs of pretraining tasks or the construction of input im-age features. Basically, they are mainly pretrained masked language modeling, masked object classification, and image-text matching. VisualBERT [23] and Unicoder-VL [22] simply use BERT and are pretrained with the aforementioned tasks. UNITER [4] pretrains the model with an additional task of word-region alignment. Oscar [24] enhances the alignment between objects and their corresponding words or phrases. VILLA [11] further improves model performance by adding their proposed adversarial learning methods to pretraining and finetuning. Except for pretraining tasks, some studies focus on the features of images. Most pretraining methods for multimodal representation learning utilize the features generated by a trained object detector, say Faster R-CNN [33]. PixelBERT [17] accepts raw images as input and extract their latent representations with a learnable ResNet [15] or ResNext [46]. FashionBERT [12] splits the images into patches with a trained ResNet without co-training. Besides single-stream models, dual-stream models also can achieve outstanding performance, such as VilBERT [28], LXMERT [40] and InterBERT [25]. ViLBERT-MT [29] enhances model performance with multi-task finetuning. ERNIE-ViL [50] enhances the model with the application of scene graph information. In spite of these successful cases, it still requires further researches to unmask the success of multimodal pretraining.In this work, we propose the largest dataset M6-Corpus for pre-training in Chinese, which consists of over 1.9TB images and 292GB texts. The dataset has large coverage over domains, including encyclopedia, question answering, forum discussion, common crawl, etc. We propose a method called M6 that is able to process information of multiple modalities and perform both single-modal and cross-modal understanding and generation. The model is scaled to large model with 10B and 100B parameters with sophisticated deployment, and both models are the largest multimodal pretrained models. We apply the model to a series of downstream applications, showing its versatility. More specifically, we design a downstream task of text-guided image generation, and the finetuned M6 can reach superior performance by producing images of high quality.In the future, we will continue the pretraining of extremely large models by increasing the scale of data and models to explore the limit of performance, and we also endeavor to search for more downstream applications for further generalization.[1]   Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-masked language models for unified language model pre-training. In International Conference on Machine Learning. PMLR, 642–652.[2]   Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).[3]   Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In European Conference on Computer Vision. Springer, 213–229.[4]   Y en-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. UNITER: UNiversal Image-TExt Representation Learning. In . 104–120.[5]   Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pre-trained models for chinese natural language processing. arXiv preprint arXiv:2004.13922 (2020).[6]   Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In . 4171–4186.[7]   Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified Language Model Pre-training for Natural Language Understanding and Generation. In . 13042–13054.[8]   Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).[9]   Patrick Esser, Robin Rombach, and Björn Ommer. 2020. Taming Transformers for High-Resolution Image Synthesis. arXiv:2012.09841 [cs.CV][10]   William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.  abs/2101.03961 (2021). arXiv:2101.03961https://arxiv.org/abs/2101.03961[11]   Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. 2020. Large-Scale Adversarial Training for Vision-and-Language Representation Learning. In .[12]   Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, and Hao Wang. 2020. Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval. In . 2251–2260.[13]   Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. 2015. Are you talking to a machine? dataset and methods for multilingual image question answering. arXiv preprint arXiv:1505.05612 (2015).[14]   Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial networks. arXiv preprint arXiv:1406.2661 (2014).[15]   Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In . 770–778.[16]   Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. De-berta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 (2020).[17]   Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 (2020).[18]   Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020. Convbert: Improving bert with span-based dynamic convolution. arXiv preprint arXiv:2008.02496 (2020).[19]   Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.  abs/1909.11942 (2019).[20]   Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 (2020).[21]   Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettle-moyer. 2021. BASE Layers: Simplifying Training of Large, Sparse Models.  abs/2103.16716 (2021).[22]   Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. 2019. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training.  abs/1908.06066 (2019).[23]   Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A Simple and Performant Baseline for Vision and Language.  abs/1908.03557 (2019).[24]   Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. 2020. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks.  abs/2004.06165 (2020).[25]   Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. 2020. Interbert: Vision-and-language interaction for multi-modal pretraining. arXiv preprint arXiv:2003.13198 (2020).[26]   Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In . 740–755.[27]   Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach.  abs/1907.11692 (2019).[28]   Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In . 13–23.[29]   Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 2019. 12-in-1: Multi-Task Vision and Language Representation Learning.  abs/1912.02315 (2019).[31]   Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. [n.d.]. Language models are unsupervised multitask learners. ([n. d.]).[32]   Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. arXiv:2102.12092 [cs.CV][33]   Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In . 91–99.[34]   Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In .[35]   Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017).[36]   Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv preprint arXiv:1909.08053 (2019).[37]   Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: Masked Sequence to Sequence Pre-training for Language Generation. In . 5926–5936.[38]   Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In .[39]   Maosong Sun, Jingyang Li, Zhipeng Guo, Z Yu, Y Zheng, X Si, and Z Liu. 2016. Thuctc: an efficient chinese text classifier.  (2016).[40]   Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In . 5099–5110.[41]   Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Discrete Representation Learning. In .[42]   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In . 5998–6008.[43]   Ang Wang, Xianyan Jia, Le Jiang, Jie Zhang, Yong Li, and Wei Lin. 2020. Whale: A Unified Distributed Training Framework. arXiv preprint arXiv:2011.09208 (2020).[44]   Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. 2019. Structbert: Incorporating language structures into pre-training for deep language understanding. arXiv preprint arXiv:1908.04577 (2019).[45]   Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016).[46]   Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. 2017. Aggregated residual transformations for deep neural networks. In . 1492–1500.[47]   Liang Xu, Xuanwei Zhang, and Qianqian Dong. 2020. CLUECorpus2020: A Large-scale Chinese Corpus for Pre-trainingLanguage Model. arXiv preprint arXiv:2003.01355 (2020).[48]   Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1316–1324.[49]   Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. In . 5754–5764.[50]   Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934 (2020).[51]   Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. 2020. Unified Vision-Language Pre-Training for Image Captioning and VQA. In . 13041–13049.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.  ]]></content:encoded></item><item><title>Xiaomi launches 17 Ultra smartphone, an AirTag clone, and an ultra slim powerbank</title><link>https://techcrunch.com/2026/02/28/xiaomi-launches-17-ultra-smartphone-an-airtag-clone-and-an-ultra-slim-powerbank/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:09:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[We round up everything Xiaomi announced at its Mobile World Congress event.]]></content:encoded></item><item><title>Alibaba’s Qwen: The Chinese AI Model Challenging Silicon Valley</title><link>https://hackernoon.com/alibabas-qwen-the-chinese-ai-model-challenging-silicon-valley?source=rss</link><author>Alibaba</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:08:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce QWEN1, the first installment of our large language model series. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes QWEN, the base pretrained language models, and QWEN-CHAT, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, CODE-QWEN and CODE-QWEN-CHAT, as well as mathematics-focused models, MATH-QWEN-CHAT, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models. \n \
Despite their impressive capabilities, LLMs are often criticized for their lack of reproducibility, steerability, and accessibility to service providers. In this work, we are pleased to present and release the initial version of our LLM series, QWEN. QWEN is a moniker that derives from the Chinese phrase Qianwen, which translates to “thousands of prompts” and conveys the notion of embracing a wide range of inquiries. QWEN is a comprehensive language model series that encompasses distinct models with varying parameter counts. The model series include the base pretrained language models, chat models finetuned with human alignment techniques, i.e., supervised finetuning (SFT), reinforcement learning with human feedback (RLHF), etc., as well as specialized models in coding and math. The details are outlined below:1.   The base language models, namely QWEN, have undergone extensive training using up to 3 trillion tokens of diverse texts and codes, encompassing a wide range of areas. These models have consistently demonstrated superior performance across a multitude of downstream tasks, even when compared to their more significantly larger counterparts.2.   The QWEN-CHAT models have been carefully finetuned on a curated dataset relevant to task performing, chat, tool use, agent, safety, etc. The benchmark evaluation demonstrates that the SFT models can achieve superior performance. Furthermore, we have trained reward models to mimic human preference and applied them in RLHF for chat models that can produce responses preferred by humans. Through the human evaluation of a challenging test, we find that QWEN-CHAT models trained with RLHF are highly competitive, still falling behind GPT-4 on our benchmark.3.    In addition, we present specialized models called CODE-QWEN, which includes CODE-QWEN-7B and CODE-QWEN-14B, as well as their chat models, CODE-QWEN-14B-CHAT and CODE-QWEN-7B-CHAT. Specifically, CODE-QWEN has been pre-trained on extensive datasets of code and further fine-tuned to handle conversations related to code generation, debugging, and interpretation. The results of experiments conducted on benchmark datasets, such as HumanEval (Chen et al.,2021), MBPP (Austin et al.,2021), and HumanEvalPack (Muennighoff et al.,2023), demonstrate the high level of proficiency of CODE-QWEN in code understanding and generation.4.   This research additionally introduces MATH-QWEN-CHAT specifically designed to tackle mathematical problems. Our results show that both MATH-QWEN-7B-CHAT and MATH-QWEN-14B-CHAT outperform open-sourced models in the same sizes with large margins and are approaching GPT-3.5 on math-related benchmark datasets such as GSM8K (Cobbeet al.,2021) and MATH (Hendrycks et al.,2021).5.    Besides, we have open-sourced QWEN-VL and QWEN-VL-CHAT, which have the versatile ability to comprehend visual and language instructions. These models outperform the current open-source vision-language models across various evaluation benchmarks and support text recognition and visual grounding in both Chinese and English languages. Moreover, these models enable multi-image conversations and storytelling. Further details can be found in Bai et al.(2023).\
Now, we officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN and aligned chat models QWEN-CHAT2. This release aims at providing more comprehensive and powerful LLMs at developer- or application-friendly scales.The structure of this report is as follows: Section 2 describes our approach to pretraining and results of QWEN. Section 3 covers our methodology for alignment and reports the results of both automatic evaluation and human evaluation. Additionally, this section describes details about our efforts in building chat models capable of tool use, code interpreter, and agent. In Sections 4 and 5, we delve into specialized models of coding and math and their performance. Section 6 provides an overview of relevant related work, and Section 7 concludes this paper and points out our future work.The pretraining stage involves learning vast amount of data to acquire a comprehensive understanding of the world and its various complexities. This includes not only basic language capabilities but also advanced skills such as arithmetic, coding, and logical reasoning. In this section, we introduce the data, the model design and scaling, as well as the comprehensive evaluation results on benchmark datasets.The size of data has proven to be a crucial factor in developing a robust large language model, as highlighted in previous research (Hoffmann et al.,2022;Touvron et al.,2023b). To create an effective pretraining dataset, it is essential to ensure that the data are diverse and cover a wide range of types, domains, and tasks. Our dataset is designed to meet these requirements and includes public web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.\
To ensure the quality of our pretraining data, we have developed a comprehensive data preprocessing procedure. For public web data, we extract text from HTML and use language identification tools to determine the language. To increase the diversity of our data, we employ deduplication techniques, including exact-match deduplication after normalization and fuzzy deduplication using MinHash and LSH algorithms. To filter out low-quality data, we employ a combination of rule-based and machine-learning-based methods. Specifically, we use multiple models to score the content, including language models, text-quality scoring models, and models for identifying potentially offensive or inappropriate content. We also manually sample texts from various sources and review them to ensure their quality. To further enhance the quality of our data, we selectively up-sample data from certain sources, to ensure that our models are trained on a diverse range of high-quality content. In recent studies (Zeng et al.,2022;Aribandi et al.,2021;Raffel et al.,2020), it has been demonstrated that pretraining language models with multi-task instructions can enhance their zero-shot and few-shot performance. To further enhance the performance of our model, we have incorporated high-quality instruction data into our pretraining process. To safeguard the integrity of our benchmark assessment, we have adopted a similar approach as Brown et al.(2020) and meticulously eliminated any instruction samples that exhibit a 13-gram overlap with any data present in the test sets utilized in our evaluation.\
Given the large number of downstream tasks, it is not feasible to repeat this filtering process for all tasks. Instead, we have made sure that the instruction data for the reported tasks have undergone our filtering process to ensure their accuracy and reliability. Finally, we have built a dataset of up to 3 trillion tokens.The design of vocabulary significantly impacts the training efficiency and the downstream task performance. In this study, we utilize byte pair encoding (BPE) as our tokenization method, following GPT-3.5 and GPT-4. We start with the open-source fast BPE tokenizer, tiktoken (Jain,2022), and select the vocabulary cl100k base as our starting point. To enhance the performance of our model on multilingual downstream tasks, particularly in Chinese, we augment the vocabulary with commonly used Chinese characters and words, as well as those in other languages. Also, following Touvron et al.(2023a;b), we have split numbers into single digits. The final vocabulary size is approximately 152K.The performance of the QWEN tokenizer in terms of compression is depicted in Figure 3. In this comparison, we have evaluated QWEN against several other tokenizers, including XLM-R (Conneauet al.,2019), LLaMA (Touvron et al.,2023a), Baichuan (Inc.,2023a), and InternLM (InternLM Team,2023). Our findings reveal that QWEN achieves higher compression efficiency than its competitors in most languages. This implies that the cost of serving can be significantly reduced since a smaller number of tokens from QWEN can convey more information than its competitors. Furthermore, we have conducted preliminary experiments to ensure that scaling the vocabulary size of QWEN does not negatively impact the downstream performance of the pretrained model. Despite the increase in vocabulary size, our experiments have shown that QWEN maintains its performance levels in downstream evaluation.QWEN is designed using a modified version of the Transformer architecture. Specifically, we have adopted the recent open-source approach of training large language models, LLaMA (Touvron et al.,2023a), which is widely regarded as the top open-source LLM. Our modifications to the architecture include:Embedding and output projection. Based on preliminary experimental findings, we have opted for the untied embedding approach instead of tying the weights of input embedding and output projection. This decision was made in order to achieve better performance with the price of memory costs.. We have chosen RoPE (Rotary Positional Embedding) (Su et al.,2021) as our preferred option for incorporating positional information into our model. RoPE has been widely adopted and has demonstrated success in contemporary large language models, notably PaLM (Chowdhery et al.,2022;Anil et al.,2023) and LLaMA (Touvronet al.,2023a;b). In particular, we have opted to use FP32 precision for the inverse frequency matrix, rather than BF16 or FP16, in order to prioritize model performance and achieve higher accuracy.. For most layers, we remove biases following Chowdhery et al.(2022), but we add biases in the QKV layer of attention to enhance the extrapolation ability of the model (Su,2023b).. In modern Transformer models, pre-normalization is the most widely used approach, which has been shown to improve training stability compared to post-normalization. Recent research has suggested alternative methods for better training stability, which we plan to explore in future versions of our model. Additionally, we have replaced the traditional layer normalization technique described in (Ba et al.,2016) with RMSNorm (Jiang et al.,2023). This change has resulted in equivalent performance while also improving efficiency.. We have selected SwiGLU (Shazeer,2020) as our activation function, a combination of Swish (Ramachandran et al.,2017) and Gated Linear Unit (Dauphin et al.,2017). Our initial experiments have shown that activation functions based on GLU generally outperform other baseline options, such as GeLU (Hendrycks & Gimpel,2016). As is common practice in previous research, we have reduced the dimension of the feed-forward network (FFN) from 4 times the hidden size to 8/3 of the hidden size.To train QWEN, we follow the standard approach of autoregressive language modeling, as described in Radford et al.(2018). This involves training the model to predict the next token based on the context provided by the previous tokens. We train models with context lengths of 2048. To create batches of data, we shuffle and merge the documents, and then truncate them to the specified context lengths. To improve computational efficiency and reduce memory usage, we employ Flash Attention in the attention modules (Dao et al.,2022). We adopt the standard optimizer AdamW (Kingma & Ba,2014;Loshchilov & Hutter,2017) for pretraining optimization. We set the hyperparameters 1 = 0*.*9, 2 = 0*.*95, and  = 10−8. We use a cosine learning rate schedule with a specified peak learning rate for each model size. The learning rate is decayed to a minimum learning rate of 10% of the peak learning rate. All the models are trained with BFloat16 mixed precision for training stability.2.5         Context Length ExtensionTransformer models have a significant limitation in terms of the context length for their attention mechanism. As the context length increases, the quadratic-complexity computation leads to a drastic increase in both computation and memory costs. In this work, we have implemented simple training-free techniques that are solely applied during inference to extend the context length of the model. One of the key techniques we have used is NTK-aware interpolation (bloc97,2023).\
Unlike position interpolation (PI) (Chen et al.,2023a) which scales each dimension of RoPE equally, NTK-aware interpolation adjusts the base of RoPE to prevent the loss of high-frequency information in a training-free manner. To further improve performance, we have also implemented a trivial extension called dynamic NTK-aware interpolation, which is later formally discussed in (Peng et al.,2023a). It dynamically changes the scale by chunks, avoiding severe performance degradation. These techniques allow us to effectively extend the context length of Transformer models without compromising their computational efficiency or accuracy.QWEN additionally incorporates two attention mechanisms: LogN-Scaling (Chiang & Cholak,2022;Su,2023a) and window attention (Beltagy et al.,2020). LogN-Scaling rescales the dot product of the query and value by a factor that depends on the ratio of the context length to the training length, ensuring that the entropy of the attention value remains stable as the context length grows. Window attention restricts the attention to a limited context window, preventing the model from attending to tokens that are too far away.We also observed that the long-context modeling ability of our model varies across layers, with lower layers being more sensitive in context length extension compared to the higher layers. To leverage this observation, we assign different window sizes to each layer, using shorter windows for lower layers and longer windows for higher layers.2.6         Experimental Results\
In this evaluation, we focus on the base language models without alignment and collect the baselines’ best scores from their official results and OpenCompass (OpenCompass Team,2023). The results are presented in Table 2.Our experimental results demonstrate that the three QWEN models exhibit exceptional performance across all downstream tasks. It is worth noting that even the larger models, such as LLaMA2-70B, are outperformed by QWEN-14B in 3 tasks. QWEN-7B also performs admirably, surpassing LLaMA2-13B and achieving comparable results to Baichuan2-13B. Notably, despite having a relatively small number of parameters, QWEN-1.8B is capable of competitive performance on certain tasks and even outperforms larger models in some instances. The findings highlight the impressive capabilities of the QWEN models, particularly QWEN-14B, and suggest that smaller models, such as QWEN-1.8B, can still achieve strong performance in certain applications.To evaluate the effectiveness of context length extension, Table 3 presents the test results on arXiv3 in terms of perplexity (PPL). These results demonstrate that by combining NTK-aware interpolation, LogN-Scaling, and layer-wise window assignment, we can effectively maintain the performance of our models in the context of over 8192 tokens.Pretrained large language models have been found to be not aligned with human behavior, making them unsuitable for serving as AI assistants in most cases. Recent research has shown that the use of alignment techniques, such as supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), can significantly improve the ability of language models to engage in natural conversation. In this section, we will delve into the details of how QWEN models have been trained using SFT and RLHF, and evaluate their performance in the context of chat-based assistance.3.1         Supervised FinetuningTo gain an understanding of human behavior, the initial step is to carry out SFT, which finetunes a pretrained LLM on chat-style data, including both queries and responses. In the following sections, we will delve into the details of data construction and training methods.To enhance the capabilities of our supervised finetuning datasets, we have annotated conversations in multiple styles. While conventional datasets (Wei et al.,2022a) contain a vast amount of data prompted with questions, instructions, and answers in natural language, our approach takes it a step further by annotating human-style conversations. This practice, inspired by Ouyang et al.(2022), aims at improving the model’s helpfulness by focusing on natural language generation for diverse tasks. To ensure the model’s ability to generalize to a wide range of scenarios, we specifically excluded data formatted in prompt templates that could potentially limit its capabilities. Furthermore, we have prioritized the safety of the language model by annotating data related to safety concerns such as violence, bias, and pornography.In addition to data quality, we have observed that the training method can significantly impact the final performance of the model. To achieve this, we utilized the ChatML-style format (OpenAI,2022), which is a versatile meta language capable of describing both the metadata (such as roles) and the content of a turn. This format enables the model to effectively distinguish between various types of information, including system setup, user inputs, and assistant outputs, among others. By leveraging this approach, we can enhance the model’s ability to accurately process and analyze complex conversational data.Consistent with pretraining, we also apply next-token prediction as the training task for SFT. We apply the loss masks for the system and user inputs. More details are demonstrated in Section A.1.1.The model’s training process utilizes the AdamW optimizer, with the following hyperparameters: 1 set to 0*.2 set to 0. set to 10−8. The sequence length is limited to 2048, and the batch size is 128. The model undergoes a total of 4000 steps, with the learning rate gradually increased over the first 1430 steps, reaching a peak of 2  10−6. To prevent overfitting, weight decay is applied with a value of 0..1, and gradient clipping is enforced with a limit of 1.*0.3.2         Reinforcement Learning from Human FeedbackWhile SFT has proven to be effective, we acknowledge that its generalization and creativity capa-bilities may be limited, and it is prone to overfitting. To address this issue, we have implemented Reinforcement Learning from Human Feedback (RLHF) to further align SFT models with human preferences, following the approaches of Ouyang et al.(2022);Christiano et al.(2017). This process involves training a reward model and using Proximal Policy Optimization (PPO) (Schulman et al.,2017) to conduct policy training.3.2.1          Reward ModelTo create a successful reward model, like building a large language model (LLM), it is crucial to first undergo pretraining and then finetuning. This pretraining process, also known as preference model pretraining (PMP) (Bai et al.,2022b), necessitates a vast dataset of comparison data. This dataset consists of sample pairs, each containing two distinct responses for a single query and their corresponding preferences. Similarly, finetuning is also conducted on this type of comparison data, but with a higher quality due to the presence of quality annotations.During the fine-tuning phase, we gather a variety of prompts and adjust the reward model based on human feedback for responses from the QWEN models. To ensure the diversity and complexity of user prompts are properly taken into account, we have created a classification system with around 6600 detailed tags and implemented a balanced sampling algorithm that considers both diversity and complexity when selecting prompts for annotation by the reward model (Lu et al.,2023). To generate a wide range of responses, we have utilized QWEN models of different sizes and sampling strategies, as diverse responses can help reduce annotation difficulties and enhance the performance of the reward model. These responses are then evaluated by annotators following a standard annotation guideline, and comparison pairs are formed based on their scores.In creating the reward model, we utilize the same-sized pre-trained language model QWEN to initiate the process. It is important to mention that we have incorporated a pooling layer into the original QWEN model to extract the reward for a sentence based on a specific end token.\n The learning rate for this process has been set to a constant value of 3  10−6, and the batch size is 64. Additionally, the sequence length is set to 2048, and the training process lasts for a single epoch.We adopted the accuracy on the test dataset as an important but not exclusive evaluation metric for the reward model. In Table 4, we report the test pairwise accuracy of PMP and reward models on diverse human preference benchmark datasets (Bai et al.,2022b;Stiennon et al.,2020;Ethayarajhet al.,2022;Lightman et al.,2023). Specifically, QWEN Helpful-base and QWEN Helpful-online are our proprietary datasets. The responses in QWEN Helpful-base are generated from QWEN without RLHF, whereas QWEN Helpful-online includes responses from QWEN with RLHF. The results show that the PMP model demonstrates high generalization capabilities on out-of-distribution data, and the reward model demonstrates significant improvement on our QWEN reward datasets.3.2.2          Reinforcement LearningOur Proximal Policy Optimization (PPO) process involves four models: the policy model, value model, reference model, and reward model. Before starting the PPO procedure, we pause the policy model’s updates and focus solely on updating the value model for 50 steps. This approach ensures that the value model can adapt to different reward models effectively.During the PPO operation, we use a strategy of sampling two responses for each query simultaneously. This strategy has proven to be more effective based on our internal benchmarking evaluations. We set the KL divergence coefficient to 0*.*04 and normalize the reward based on the running mean.The policy and value models have learning rates of 1  10−6 and 5  10−6, respectively. To enhance training stability, we utilize value loss clipping with a clip value of 0*.15. For inference, the policy top-p is set to 0.9. Our findings indicate that although the entropy is slightly lower than when top-p is set to 1.*0, there is a faster increase in reward, ultimately resulting in consistently higher evaluation rewards under similar conditions.Additionally, we have implemented a pretrained gradient to mitigate the alignment tax. Empirical findings indicate that, with this specific reward model, the KL penalty is adequately robust to counteract the alignment tax in benchmarks that are not strictly code or math in nature, such as those that test common sense knowledge and reading comprehension. It is imperative to utilize a significantly larger volume of the pretrained data in comparison to the PPO data to ensure the effectiveness of the pretrained gradient. Additionally, our empirical study suggests that an overly large value for this coefficient can considerably impede the alignment to the reward model, eventually compromising the ultimate alignment, while an overly small value would only have a marginal effect on alignment tax reduction.3.3         Automatic and Human Evaluation of Aligned ModelsTo showcase the effectiveness of our aligned models, we conduct a comparison with other aligned models on well-established benchmarks, including MMLU (Hendrycks et al.,2020), C-Eval (Huanget al.,2023), GSM8K (Cobbe et al.,2021), HumanEval (Chen et al.,2021), and BBH (Suzgun et al.,2022). Besides the widely used few-shot setting, we test our aligned models in the zero-shot setting to demonstrate how well the models follow instructions. The prompt in a zero-shot setting consists of an instruction and a question without any previous examples in the context. The results of the baselines are collected from their official reports and OpenCompass (OpenCompass Team,2023).The results in Table 5 demonstrate the effectiveness of our aligned models in understanding human instructions and generating appropriate responses. QWEN-14B-Chat outperforms all other models except ChatGPT (OpenAI, 2022) and LLAMA 2-CHAT-70B (Touvron et al., 2023b) in all datasets, including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023), GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and BBH (Suzgun et al., 2022).\n In particular, QWEN’s performance in HumanEval, which measures the quality of generated codes, is significantly higher than that of other open-source models.Moreover, QWEN’s performance is consistently better than that of open-source models of similar size, such as LLaMA2 (Touvron et al.,2023b), ChatGLM2 (ChatGLM2 Team,2023), InternLM (InternLMTeam,2023), and Baichuan2 (Yang et al.,2023). This suggests that our alignment approach, which involves fine-tuning the model on a large dataset of human conversations, has been effective in improving the model’s ability to understand and generate human-like language./imDespite this, we have reservations about the ability of traditional benchmark evaluation to accurately measure the performance and potential of chat models trained with alignment techniques in today’s landscape. The results mentioned earlier provide some evidence of our competitive standing, but we believe that it is crucial to develop new evaluation methods specifically tailored to aligned models.We believe that human evaluation is crucial, which is why we have created a carefully curated dataset for this purpose. Our process involved collecting 300 instructions in Chinese that covered a wide range of topics, including knowledge, language understanding, creative writing, coding, and mathematics. To evaluate the performance of different models, we chose the SFT version of QWEN-CHAT-7B and the SFT and RLHF versions of QWEN-CHAT-14B, and added two strong baselines, GPT-3.5 and GPT-44, for comparison. For each instruction, we asked three annotators to rank the model responses by the overall score of helpfulness, informativeness, validity, and other relevant factors. Our dataset and evaluation methodology provides a comprehensive and rigorous assessment of the capabilities of different language models in various domains.Figure 4 illustrates the win rates of the various models. For each model, we report the percentage of wins, ties, and losses against GPT-3.5, with the segments of each bar from bottom to top representing these statistics. The experimental results clearly demonstrate that the RLHF model outperforms the SFT models by significant margins, indicating that RLHF can encourage the model to generate responses that are more preferred by humans. In terms of overall performance, we find that the RLHF model significantly outperforms the SFT models, falling behind GPT-4. This indicates the effectiveness of RLHF for aligning to human preference. To provide a more comprehensive understanding of the models’ performance, we include a case study with examples from different models in Appendix A.2.2. Nonetheless, it remains difficult to accurately capture the gap between our models and the proprietary models. As such, a more extensive and rigorous assessment is required for the chat models.The QWEN models, which are designed to be versatile, have the remarkable ability to assist with (semi-)automating daily tasks by leveraging their skills in tool-use and planning. As such, they can serve as agents or copilots to help streamline various tasks. We explore QWEN’s proficiency in the following areas:•     Using a Python code interpreter to enhance math reasoning, data analysis, and more (see Table 7 and Table 8).•     Functioning as an agent that accesses Hugging Face’s extensive collection of multimodal models while engaging with humans (see Table 9).\
To enhance QWEN’s capabilities as an agent or copilot, we employ the self-instruct (Wang et al.,2023c) strategy for SFT. Specifically, we utilize the in-context learning capability of QWEN for self-instruction. By providing a few examples, we can prompt QWEN to generate more relevant queries and generate outputs that follow a specific format, such as ReAct (Yao et al.,2022). We then apply rules and involve human annotators to filter out any noisy samples. Afterwards, the samples are incorporated into QWEN’s training data, resulting in an updated version of QWEN that is more dependable for self-instruction. We iterate through this process multiple times until we gather an ample number of samples that possess both exceptional quality and a wide range of diversity. As a result, our final collection consists of around 2000 high-quality samples.During the finetuning process, we mix these high-quality samples with all the other general-purpose SFT samples, rather than introducing an additional training stage. By doing so, we are able to retain essential general-purpose capabilities that are also pertinent for constructing agent applications.\
Using Tools via ReAct Prompting We have created and made publicly available a benchmark for evaluating QWEN’s ability to call plugins, tools, functions, or APIs using ReAct Prompting (see Qwen Team, Alibaba Group,2023b). To ensure fair evaluation, we have excluded any plugins that were included in QWEN’s training set from the evaluation set. The benchmark assesses the model’s accuracy in selecting the correct plugin from a pool of up to five candidates, as well as the plausibility of the parameters passed into the plugin and the frequency of false positives. In this evaluation, a false positive occurs when the model incorrectly invokes a plugin in response to a query, despite not being required to do so.The results presented in Table 6 demonstrate that QWEN consistently achieves higher accuracy in identifying the relevance of a query to the available tools as the model size increases. However, the table also highlights that beyond a certain point, there is little improvement in performance when it comes to selecting the appropriate tool and providing relevant arguments. This suggests that the current preliminary benchmark may be relatively easy and may require further enhancement in future iterations. It is worth noting that GPT-3.5 stands out as an exception, displaying suboptimal performance on this particular benchmark. This could potentially be attributed to the fact that the benchmark primarily focuses on the Chinese language, which may not align well with GPT-3.5’s capabilities. Additionally, we observe that GPT-3.5 tends to attempt to use at least one tool, even if the query cannot be effectively addressed by the provided tools.\
Using Code Interpreter for Math Reasoning and Data Analysis The Python code interpreter is widely regarded as a powerful tool for augmenting the capabilities of an LLM agent. It is worth investigating whether QWEN can harness the full potential of this interpreter to enhance its performance in diverse domains, such as mathematical reasoning and data analysis. To facilitate this exploration, we have developed and made publicly available a benchmark that is specifically tailored for this purpose (see Qwen Team, Alibaba Group,2023a).The benchmark encompasses three primary categories of tasks: math problem-solving, data visualization, and other general-purpose tasks like file post-processing and web crawling. Within the visualization tasks, we differentiate between two levels of difficulty. The easier level can be achieved by simply writing and executing a single code snippet without the need for advanced planning skills. However, the more challenging level requires strategic planning and executing multiple code snippets in a sequential manner. This is because the subsequent code must be written based on the output of the previous code. For example, an agent may need to examine the structure of a CSV file using one code snippet before proceeding to write and execute additional code to create a plot.Regarding evaluation metrics, we consider both the executability and correctness of the generated code. To elaborate on the correctness metrics, for math problems, we measure accuracy by verifying if the ground truth numerical answer is present in both the code execution result and the final response. When it comes to data visualization, we assess accuracy by utilizing QWEN-VL (Bai et al.,2023), a powerful multimodal language model. QWEN-VL is capable of answering text questions paired with images, and we rely on it to confirm whether the image generated by the code fulfills the user’s request.The results regarding executability and correctness are presented in Table 7 and Table 8, respectively. It is evident that CODE LLAMA generally outperforms LLAMA 2, its generalist counterpart, which is not surprising since this benchmark specifically requires coding skills. However, it is worth noting that specialist models that are optimized for code synthesis do not necessarily outperform generalist models. This is due to the fact that this benchmark encompasses various skills beyond coding, such as abstracting math problems into equations, understanding language-specified constraints, and responding in the specified format such as ReAct. Notably, QWEN-7B-CHAT and QWEN-14B-CHAT surpass all other open-source alternatives of similar scale significantly, despite being generalist models.\
Serving as a Hugging Face Agent Hugging Face provides a framework called the Hugging Face Agent or Transformers Agent (Hugging Face,2023), which empowers LLM agents with a curated set of multimodal tools, including speech recognition and image synthesis. This framework allows an LLM agent to interact with humans, interpret natural language commands, and employ the provided tools as needed.To evaluate QWEN’s effectiveness as a Hugging Face agent, we utilized the evaluation benchmarks offered by Hugging Face. The results are presented in Table 9. The evaluation results reveal that QWEN performs quite well in comparison to other open-source alternatives, only slightly behind the proprietary GPT-4, demonstrating QWEN’s competitive capabilities.4         Code-Qwen: Specialized Model for CodingTraining on domain-specific data has been shown to be highly effective, particularly in the case of code pretraining and finetuning. A language model that has been reinforced with training on code data can serve as a valuable tool for coding, debugging, and interpretation, among other tasks. In this work, we have developed a series of generalist models using pretraining and alignment techniques. Building on this foundation, we have created domain-specific models for coding by leveraging the base language models of QWEN, including continued pretrained model, CODE-QWEN and supervised finetuned model, CODE-QWEN-CHAT. Both models have 14 billion and 7 billion parameters versions.4.1         Code PretrainingWe believe that relying solely on code data for pretraining can result in a significant loss of the ability to function as a versatile assistant. Unlike previous approaches that focused solely on pretraining on code data (Li et al.,2022;2023d), we take a different approach (Rozie`re et al.,2023) by starting with our base models QWEN trained on a combination of text and code data, and then continuing to pretrain on the code data. We continue to pretrain the models on a total of around 90 billion tokens. During the pre-training phase, we initialize the model using the base language models QWEN. Many applications that rely on specialized models for coding may encounter lengthy contextual scenarios, such as tool usage and code interpretation, as mentioned in Section 3.4. To address this issue, we train our models with context lengths of up to 8192. Similar to base model training in Section 2.4, we employ Flash Attention (Dao et al.,2022) in the attention modules, and adopt the standard optimizer AdamW (Kingma & Ba,2014;Loshchilov & Hutter,2017), setting 1 = 0*.2 = 0. = 10−8. We set the learning rate as 6. 10−5 for CODE-QWEN-14B and 3.*0  10−5 for CODE-QWEN-7B, with 3% warm up iterations and no learning rate decays.4.2         Code  Supervised  Fine-TuningAfter conducting a series of empirical experiments, we have determined that the multi-stage SFT strategy yields the best performance compared to other methods. In the supervised fine-tuning stage, the model CODE-QWEN-CHAT initialized by the code foundation model CODE-QWEN are optimized by the AdamW (Kingma & Ba,2014;Loshchilov & Hutter,2017) optimizer (1 = 0*.2 = 0.*95,  = 10−8) with a learning rate of 2*. 10−6 and 1.*0  10−5 for the 14B and 7B model respectively. The learning rate increases to the peaking value with the cosine learning rate schedule (3% warm-up steps) and then remains constant.Our CODE-QWEN models have been compared with both proprietary and open-source language models, as shown in Tables 10 and 11. These tables present the results of our evaluation on the test sets of Humaneval (Chen et al.,2021), MBPP (Austin et al.,2021), and the multi-lingual code generation benchmark HUMANEVALPACK (Muennighoff et al.,2023). The comparison is based on the pass@1 performance of the models on these benchmark datasets. The results of this comparison are clearly demonstrated in Tables 10 and 11.When compared to some of the extremely large-scale closed-source models, CODE-QWEN and CODE-QWEN-CHAT demonstrate clear advantages in terms of pass@1. However, it is important to note that these models fall behind the state-of-the-art methods, such as GPT-4, in general. Nonetheless, with the continued scaling of both model size and data size, we believe that this gap can be narrowed in the near future.It is crucial to emphasize that the evaluations mentioned previously are insufficient for grasping the full extent of the strengths and weaknesses of the models. In our opinion, it is necessary to develop more rigorous tests to enable us to accurately assess our relative performance in comparison to GPT-4.We have created a mathematics-specialized model series called MATH-QWEN-CHAT, which is built on top of the QWEN pretrained language models. Specifically, we have developed assistant models that are specifically designed to excel in arithmetic and mathematics and are aligned with human behavior. We are releasing two versions of this model series, MATH-QWEN-14B-CHAT and MATH-QWEN-7B-CHAT, which have 14 billion and 7 billion parameters, respectively.We carry out math SFT on our augmented math instructional dataset for mathematics reasoning, and therefore we obtain the chat model, MATH-QWEN-CHAT, directly. Owing to shorter average lengths of the math SFT data, we use a sequence length of 1024 for faster training. Most user inputs in the math SFT dataset are examination questions, and it is easy for the model to predict the input format and it is meaningless for the model to predict the input condition and numbers which could be random.\
Thus, we mask the inputs of the system and user to avoid loss computation on them and find masking them accelerates the convergence during our preliminary experiments. For optimization, we use the AdamW optimizer with the same hyperparameters of SFT except that we use a peak learning rate of 2  10−5 and a training step of 50 000.We evaluate models on the test sets of GSM8K (Grade school math) (Cobbe et al.,2021), MATH (Challenging competition math problems) (Hendrycks et al.,2021), Math401 (Arithmetic ability) (Yuan et al.,2023b), and Math23K (Chinese grade school math) (Wang et al.,2017). We compare MATH-QWEN-CHAT with proprietary models ChatGPT and Minerva (Lewkowycz et al.,2022) and open-sourced math-specialized model RFT (Yuan et al.,2023a), WizardMath (Luo et al.,2023a), and GAIRMath-Abel (Chern et al.,2023a) in Table 12. MATH-QWEN-CHAT models show better math reasoning and arithmetic abilities compared to open-sourced models and QWEN-CHAT models of similar sizes. Compared to proprietary models, MATH-QWEN-7B-CHAT outperforms Minerva-8B in MATH. MATH-QWEN-14B-CHAT is chasing Minerva-62B and GPT-3.5 in GSM8K and MATH and delivers better performance on arithmetic ability and Chinese math problems.6.1         Large Language ModelsThe birth of ChatGPT (OpenAI,2022) and the subsequent launch of GPT-4 (OpenAI,2023) marked two historic moments in the field of artificial intelligence, demonstrating that large language models (LLMs) can serve as effective AI assistants capable of communicating with humans. These events have sparked interests among researchers and developers in building language models that are aligned with human values and potentially even capable of achieving artificial general intelligence (AGI) (Anilet al.,2023;Anthropic,2023a;b).The community was impressed by the surprising effectiveness of alignment on LLMs. Previously, LLMs without alignment often struggle with issues such as repetitive generation, hallucination, and deviation from human preferences. Since 2021, researchers have been diligently working on developing methods to enhance the performance of LLMs in downstream tasks (Wei et al.,2022a;Sanh et al.,2021;Longpre et al.,2023;Chung et al.,2022;Muennighoff et al.,2022). Furthermore, researchers have been actively exploring ways to align LLMs with human instructions (Ouyang et al.,2022;Askell et al.,2021;Bai et al.,2022b;c). One major challenge in alignment research is the difficulty of collecting data. While OpenAI has utilized its platform to gather human prompts or instructions, it is not feasible for others to collect such data.To train an effective chat model, available solutions are mostly based on SFT and RLHF (Ouyanget al.,2022). While SFT is similar to pretraining, it focuses on instruction following using the aforementioned data. However, for many developers, the limited memory capacity is a major obstacle to further research in SFT. As a result, parameter-efficient tuning methods, such as LoRA (Hu et al., 2021) and Q-LoRA (Dettmers et al.,2023), have gained popularity in the community. LoRA tunes only low-rank adapters, while Q-LoRA builds on LoRA and utilizes 4-bit quantized LLMs and paged attention (Dettmers et al.,2022;Frantar et al.,2022;Kwon et al.,2023). In terms of RLHF, recent methods such as PPO (Schulman et al.,2017;Touvron et al.,2023b) have been adopted, but there are also alternative techniques aimed at addressing the complexity of optimization, such as RRHF (Yuan et al.,2023c), DPO (Rafailov et al.,2023), and PRO (Song et al.,2023). Despite the ongoing debate about the effectiveness of RLHF, more evidence is needed to understand how it enhances the intelligence of LLMs and what potential drawbacks it may have.6.4         LLM for CodingLLMs with a certain model scale have been found to possess the ability to perform mathematical reasoning (Wei et al.,2022b;Suzgun et al.,2022). In order to encourage LLMs to achieve better performance on math-related tasks, researchers have employed techniques such as chain-of-thought prompting (Wei et al.,2022c) and scratchpad (Nye et al.,2021), which have shown promising results. Additionally, self-consistency (Wang et al.,2022) and least-to-most prompting (Zhou et al.,2022) have further improved the performance of these models on these tasks. However, prompt engineering is a time-consuming process that requires a lot of trial and error, and it is still difficult for LLMs to consistently perform well or achieve satisfactory results in solving mathematical problems. Moreover, simply scaling the data and model size is not an efficient way to improve a model’s mathematical reasoning abilities. Instead, pretraining on math-related corpora has been shown to consistently enhance these capabilities (Hendrycks et al.,2021;Lewkowycz et al.,2022;Taylor et al.,2022;Lightman et al.,2023). Additionally, fine-tuning on math-related instruction-following datasets (Siet al.,2023;Yuan et al.,2023a;Luo et al.,2023a;Yue et al.,2023;Chern et al.,2023a;Yu et al.,2023), has also been effective and more cost-effective than math-specific pretraining. Despite their limitations in terms of accuracy, LLMs still have significant potential to assist users with practical mathematical problems. There is ample scope for further development in this area.In this report, we present the QWEN series of large language models, which showcase the latest advancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models have been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using cutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized models for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT, which have been trained on domain-specific data to excel in their respective fields. Our results demonstrate that the QWEN series is competitive with existing open-source models and even matches the performance of some proprietary models on comprehensive benchmarks and human evaluation.We believe that the open access of QWEN will foster collaboration and innovation within the community, enabling researchers and developers to build upon our work and push the boundaries of what is possible with language models. By providing these models to the public, we hope to inspire new research and applications that will further advance the field and contribute to our understanding of the variables and techniques introduced in realistic settings. In a nutshell, the QWEN series represents a major milestone in our development of large language models, and we are excited to see how it will be used to drive progress and innovation in the years to come.Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. SantaCoder: Don’t reach for the stars! arXiv preprint arXiv:2301.03988, 2023.Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-jocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large language model with state-of-the-art performance, 2023.Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv preprint arXiv:2305.10403, 2023.Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. ExT5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952, 2021.Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang, Yichang Zhang, Xiaohuan Zhou, Peng Wang, Sinan Tan, An Yang andf Zeyu Cui, Yu Han, Shuai Bai, Wenbin Ge, Jianxin Ma, Junyang Lin, Jingren Zhou, and Chang Zhou. OFASys: A multi-modal multi-task learning system for building generalist models. , abs/2212.04408, 2022a. doi: 10.48550/arXiv.2212.04408. URL https://doi.org/10.48550/arXiv.2212.04408.Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond. , abs/2308.12966, 2023. doi: 10.48550/arXiv.2308.12966. URL https://doi.org/10.48550/arXiv.2308.12966.Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022b.Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022c.Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.arXiv preprint arXiv:2004.05150, 2020.Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-ference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 7432–7439. AAAI Press, 2020. doi:Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. GPT-NeoX-20B: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-ties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde´ de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. , abs/2107.03374, 2021. URL https://arxiv. org/abs/2107.03374.Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023b.Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. Phoenix: Democratizing ChatGPT across languages. arXiv preprint arXiv:2304.10453, 2023c.I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai–a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023b.David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7654–7664, 2022.Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neu-ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4299–4307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North Amer-ican Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 2924–2936. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1300. URL https://doi.org/10.18653/v1/n19-1300.Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. , abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-cisco Guzma´n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933–941. PMLR, 2017.Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. arXiv preprint arXiv:2305.14314, 2023.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with -usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5988–6008. PMLR, 17–23 Jul 2022.William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1): 5232–5270, 2022.Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida I. Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. , abs/2204.05999, 2022.Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with Gaussian error linear units. , abs/1606.08415, 2016. URL http://arxiv.org/abs/1606. 08415.Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023.Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Ku¨bler, and Lawrence S. Moss. OCNLI: original chinese natural language inference. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of , pp. 3512–3526. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.314. URL https://doi.org/10.18653/v1/2020.findings-emnlp.314.Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. arXiv preprint arXiv:2303.14742, 2023.Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm transformers: Equivalent and efficient pre-LN transformers. , abs/2305.14858, 2023. doi: 10.48550/arXiv.2305.14858. URL https://doi.org/10.48550/arXiv.2305.14858.Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452–466, 2019. doi: 10.1162/tacl*\* a*\* 00276. URL https://doi.org/10. 1162/tacl00276.Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. ModelScope-Agent: Building your customizable agent system with open-source large language models. arXiv preprint arXiv:2309.00986, 2023a.Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for “mind” exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023b.Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023c.Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joa˜o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mun˜oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: May the source be with you! , abs/2305.06161, 2023d. doi: 10.48550/arXiv.2305.06161. URL https://doi.org/10.48550/arXiv.2305.06161.Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Re´mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode. , abs/2203.07814, 2022.Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.Chenxiao Liu and Xiaojun Wan. CodeQA: A question answering dataset for source code com-prehension. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 2618–2632. Associa-tion for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.223. URL https://doi.org/10.18653/v1/2021.findings-emnlp.223.Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a.Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. WebGLM: Towards an efficient web-enhanced question answering system with human preferences. arXiv preprint arXiv:2306.07906, 2023b.Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach Dinh Le, and David Lo. Refining ChatGPT-generated code: Characterizing and mitigating code quality issues. , abs/2307.12596, 2023c. doi: 10.48550/arXiv.2307.12596. URL https://doi.org/10.48550/arXiv.2307.12596.Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The Flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models. , abs/2308.07074, 2023. doi: 10.48550/arXiv.2308.07074. URL https://doi. org/10.48550/arXiv.2308.07074.Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023a.Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023b.Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. OctoPack: Instruction tuning code large language models. , abs/2308.07124, 2023.Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. , abs/2112.00114, 2021.OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.Denis Paperno, Germa´n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Ferna´ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/ p16-1144. URL https://doi.org/10.18653/v1/p16-1144.Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023a.Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023b.Qwen Team, Alibaba Group. Evaluation benchmark for code intepreter, 2023a. URL https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark.Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Go´mez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. , 2022, 2022. URL https://openreview.net/forum?id=1ikK0kHjvj.Baptiste Rozie`re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Je´re´my Rapin, et al. Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. SocialIQA: Com-monsense reasoning about social interactions. , abs/1904.09728, 2019. URL http:Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman Castagne´, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Galle´, et al. BLOOM: A 176B-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.Noam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-gingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace. arXiv preprint arXiv:2303.17580, 2023.Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-zaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng Lin. Alpaca-CoT: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models, 2023. URL https://github.com/PhoebusSi/alpaca-CoT.Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.Jianlin Su. Improving transformer: Length extrapolation ability and position robustness, 2023a. URLJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. MOSS: Training conversational language models from synthetic data, 2023a.Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023b.Mirac Suzgun, Nathan Scales, Nathanael Scha¨rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.Marc Szafraniec, Baptiste Rozie`re, Hugh Leather, Patrick Labatut, Franc¸ois Charton, and Gabriel Synnaeve. Code translation with compiler representations. In The Eleventh International Confer-ence on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=XomEU3eNeSQ.Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4149–4158. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1421. URL https://doi.org/10.18653/v1/n19-1421.Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following LLaMA model, 2023. URL https://github.com/tatsu-lab/stanford_alpaca.Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science, 2022.Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Ol-son, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agu¨era y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. LaMDA: Language models for dialog applications. , abs/2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aure´lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. , abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.09288.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. , abs/2203.11171, 2022.Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Conference on Empirical Methods in Natural Language Processing, 2017. URL https://api. semanticscholar.org/CorpusID:910689.Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? Exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 2023b.Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 13484–13508. Association for Computational Linguistics, 2023c. doi: 10.18653/v1/2023.acl-long.754. URL https://doi.org/10.18653/v1/2023.acl-long.754.Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021.Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. CodeT5+: Open code large language models for code understanding and generation. , abs/2305.07922, 2023d. doi: 10.48550/arXiv.2305.07922. URL https://doi.org/10. 48550/arXiv.2305.07922.Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id=gEZrGCozdqR.Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. , 2022, 2022b. URL https://api.semanticscholar.org/ CorpusID:249674500.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022c.Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Re´mi Louf, Morgan Funtowicz, et al. HuggingFace’s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019.Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao. ExpertPrompting: Instructing large language models to be distinguished experts. arXiv preprint arXiv:2305.14688, 2023a.Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023b.Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023c.Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023d.Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. Technical report, Baichuan Inc., 2023. URL https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report. pdf.Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models, 2023.Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models, 2023a.Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023b.Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. RRHF: Rank responses to align language models with human feedback without tears, 2023c.Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu´ıs Ma`rquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 4791–4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p19-1472.Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. GLM-130B: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-level code completion through iterative retrieval and generation. , abs/2303.12570, 2023a. doi: 10.48550/arXiv.2303.12570. URL https://doi.org/10.48550/arXiv.2303.12570.Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. , abs/2305.12474, 2023b. doi: 10.48550/arXiv.2305.12474. URL https://doi.org/10.48550/arXiv. 2305.12474.Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x. , abs/2303.17568, 2023. doi: 10.48550/arXiv.2303.17568. URL https://doi.org/10.48550/arXiv.2303.17568.Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models. , abs/2304.06364, 2023a. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. MemoryBank: Enhancing large language models with long-term memory. arXiv preprint arXiv:2305.10250, 2023b.Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. Least-to-most prompting enables complex reasoning in large language models. , abs/2205.10625, 2022.A.1          More Training DetailsDifferent from conventional pretraining based on autoregressive next-token prediction, despite using a similar training task, there should be a specially design data format for SFT and RLHF to build a conversational AI assistant model. Common formats include “human-assistant” and ChatML formats. As to our knowledge, one of the earliest examples of the human-assistant format comes from Anthropic (Bai et al.,2022b), which adds a special phrase “\n\nhuman: ” in front of the user input and “\n\nassistant: ” in front of the assistant response. It is easy for the base language model to transfer to the pattern of conversational AI. However, as the specific phrases are common words, it might be hard for the model to disambiguate from these words in other contexts.Instead, we turned to the ChatML format proposed by OpenAI.5 This format allows the use of special tokens, i.e., “” and “”, that do not appear in pretraining, and thus resolve the aforementioned problem. We demonstrate an example of the format below.A.2.1           Automatic EvaluationTo provide a whole picture of the performance of our model series QWEN, here in this section we illustrate the detailed performance of our models as well as the baselines in the comprehensive benchmark evaluation proposed by OpenCompass Team(2023). We report the results in multiple tables based on the officially provided categories, including examination, language, knowledge, understanding, and reasoning. In terms of the performance of the baseline models, we report the higher results between the reported ones and those on the leaderboard.\
 Here we evaluate the models on a series of datasets relevant to the examination. The datasets include:(Hendrycks et al.,2020) Massive Multi-task Language Understanding is designed for measuring language understanding capabilities. We report 5-shot results.(Huang et al.,2023) C-Eval is a Chinese evaluation dataset spanning 52 diverse disciplines. We report 5-shot results.(Li et al.,2023c) CMMLU is designed for assessing language understanding capabilities in Chinese. We report 5-shot results.(Zhong et al.,2023a) This is a benchmark consisting of human-centric examina-tions, including college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We report zero-shot results.(Zhang et al.,2023b) This is a benchmark with Gaokao (Chinese college-entrance examination) questions. We report zero-shot results.(Clark et al.,2018) ARC is a dataset consisting of grade-school level, multiple-choice science questions. It includes an easy set and a challenge set, which are referred by ARC-e and ARC-c. We report zero-shot results.\n In terms of MMLU, we report the detailed results in Table 13. In terms of C-Eval, we report the results in Table 14. For the rest of the datasets, we report the results in Table 15. Note that AGIEval includes the parts of Chinese and English, while LLAMA 2 only reported the results in the English part, so we use the results on OpenCompass.\
Additionally, while CMMLU, AGIEval, and Gaokao-Bench are related to Chinese, and MPT, Falcon, and the LLaMA series were not optimized for Chinese, these models achieved low performance on the datasets.\
Knowledge and Understanding Here we evaluate the models on a series of datasets relevant to knowledge and natural language understanding. The datasets include(Clark et al.,2019) This is a QA dataset, where the questions are about passages of Wikipedia, and the model should answer yes or no to the given possible answer. We report zero-shot results.(Talmor et al.,2019) This is a dataset of multiple-choice question answering that asseses the understanding of commonsense knowledge. We report 8-shot results.(Kwiatkowski et al.,2019) It is a dataset of QA where the questions are from users and the answers are verified by experts. We report zero-shot results. (Paperno et al.,2016) This is dataset to evaluate language understanding by word prediction. It consists of passages related to human subjects. We report zero-shot results.We report the results in Table 16. We report the evaluation results on the datasets concerning reasoning, focusing on natural language reasoning. For the others, such as mathematics and coding, as we have illustrated detailed results, here we do not report those results repeatedly. The datasets for evaluation include:(Zellers et al.,2019) This is a commonsense natural language inference (NLI) dataset, where the questions are easy for humans but struggling for previous language models. We report zero-shot results.(Bisk et al.,2020) This is an NLI dataset assessing the physical knowledge. We report zero-shot results.(Sap et al.,2019) This is an NLI dataset evaluating social commonsense intelligence. We report zero-shot results.(Hu et al.,2020) This is an NLI dataset focusing on Chinese. We report zero-shot results.We report the results in Table 17.A.2.2           Human EvaluationIn this section, we demonstrate the cases of human analysis. In our self-constructed evaluation dataset, the instructions are either manually written data or manual revised from public datasets, such as CLiB6, C-Eval (Huang et al.,2023), FacTool (Chern et al.,2023b), LeetCode7), etc.In terms of each case, we demonstrate the responses and Elo ratings8 of all models for comparison. Specifically, as the data in our human evaluation are in Chinese, we also provide their translations in English.A.3          Analysis of Code InterpreterHere we provide a case of comparison between CODE LLAMA and QWEN-CHAT. This case demonstrates the advantages of QWEN-CHAT in processing tabular data and performing complex tasks.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Verisilicon DC8200 &amp; Coreboot Framebuffer Drivers Sent To DRM-Next For Linux 7.1</title><link>https://www.phoronix.com/news/Linux-7.1-DC8200-Coreboot-FB</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:04:42 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The first DRM-Misc-Next pull request was submitted this week to DRM-Next as new kernel graphics/display driver features to begin queuing for the Linux 7.1 kernel that will release mid-year. Among the early code for DRM-Next are two new drivers...]]></content:encoded></item><item><title>How to Navigate Identity, Direction, Story, and Sovereignty in the Age of AI</title><link>https://hackernoon.com/how-to-navigate-identity-direction-story-and-sovereignty-in-the-age-of-ai?source=rss</link><author></author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:00:28 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The Mirror that would pose as an OracleOr: How we might be getting a little too intimate with our AI chatbotsI never consciously set out to use AI as a coach, therapist, strategist, or mirror.\
At first, it was practical. Notes. Lists. Rewrites, drafts, edits. Research. Planning. Then it became something else. It started as a playful, curious experiment - then slowly crept towards being a standard mode of operating.\
I found myself thinking with AI. About the most important aspects of my life. Rehearsing conversations I was afraid to have. Trying to understand why certain patterns kept emerging in my life; why certain relationships kept breaking in the same places. Asking questions about myself and the world, I didn’t quite dare ask another human yet.\
And at some point, I realized:I wasn’t alone in this. Not even close.\
I could sense it in the world of memes, online. I could smell it, here and there, in real-life interactions and conversations.\
One 2025 Harvard Business Review research piece - among other recent studies and indicators - showed this clearly: people don’t primarily use AI for facts or how-to steps or recipes anymore. They use it to think out loud, like they would with a coach or therapist. To structure emotions and thoughts. To regulate emotion at 2 a.m. People are using AI to make sense of their lives. To narrate who they are, who they were, and who they might become.Narrative Sense-making - for personal and business growth.Language, writing, and thinking in a structured way about purpose, identity, story, strategy - they all converge so easily, don’t they? And if it’s one thing these Large Language Models are exceptionally good at - it’s serving as an incredibly useful and illuminating mirror in these instances.\
We all seem to pretend this isn’t happening. But it is.\
Here’s why this worries me a bit. And what we might do to counter the risks.What actually worries me (and what doesn’t)Our thoughts validated profoundly - exactly when we long for it the most.I’m not worried about AI replacing human thinking.\
That’s the wrong fear. I could go wide, deep, narrow, and very, very sci-fi about this, but I won’t. It’s the wrong fear for a great many reasons, but it’s the wrong fear.\
What worries me is something quieter, subtler, and much harder to notice while it’s happening:AI reflects us too well — and does not automatically teach us how to remain sovereign while doing so.\
What worries me is that AI indeed strengthens human thinking - but does so in a very specifically skewed way: it pushes affirmation and validation a little bit too smoothly, and especially in the most vulnerable, sometimes even painful places and moments where our ego is already inherently tempted to latch on to a narrative that protects it.\
(To some degree, we could think of it as that person who seems to be your closest, most intimate friend or advisor - only they have slight narcissistic tendencies and an agenda - both of which they’re not aware of.)\
When language comes back at you fast, coherent, and emotionally attuned, it feels like truth. Especially when you’re tired. Or lonely. Or standing at the edge of an old identity that no longer fits.\
And in those moments, something sneaky happens.\
You stop checking as carefully.\
Not with facts — but with yourself.The real risk is not dependence; It’s unexamined authority.Most of us have learned to fact-check facts blurted out by AI models. Have we learned to automatically sense-check what it’s mirroring back to us about ourselves?Your relationship (whether professional or personal);Your Story and Identity (either as a human soul, a creator, a professional, or even as a brand);Your direction and next step -\
…we are far more likely to let what sounds like coherence slide into authority.We’re exhausted, insecure.Unsure who we are becoming and what to do next.\
This is the crux for me:AI should function as a mirror, not as an oracle.\
A mirror can be confronting. It shows you things, reveals things, sometimes pretty and sometimes painful - but you are to decide what to make of those, and what to do with them. An oracle tells you what truth is and what to do.\
Those are not the same thing.Narrative Sensemaking: one function, many domains(This really took me a while to see)I kept struggling to explain why AI felt useful to me across so many domains — therapy, coaching, writing, strategy, brand work — without it sounding vague or inflated.\
Funnily enough, I have pretty much perpetually struggled to explain why all the things I do in my work are actually very, very logically connected.\
All of these practices - which more and more people are starting to use AI for, and at the same time are exactly the things I’ve been helping people with in my work - they all do the same core thing:They turn implicit structure into visible language.Therapy surfaces patterns you couldn’t quite see.Coaching sharpens the questions you were circling.Storytelling brings coherence to lived chaos.Strategy opens futures you hadn’t articulated yet, in a structure that makes sense across time. The same applies to narrative identity work.AI is exceptionally good at surfacing structure in language.\
But structure does not equal truth. And visibility is not necessarily wisdom. By any means.The rules I wish I’d had earlier.Best practices and rules of engagementIf you’re going to use AI as a thinking partner — and as already established, most people already are — a few rules matter more than anything else.\
Not as ideology, per se. As guardrails. As safety measures and incredibly important best practices, without which you’re sifting the bountiful riverbank and keeping the mud, leaving the gold.Best practices and guardrails for AI as a mirror for sensemaking, storytelling, and coaching where it matters.1. AI does not decide. You do.It can reflect, expand, challenge, reframe. Decision remains a human responsibility, with real consequences.2. AI reflects patterns. Your body, your common sense, — and your people — verify.If something reads as “right” but your chest tightens, your breath shortens; if it doesn’t pass a real-world common-sense test, or trusted humans raise an eyebrow — pay attention. Truth is not purely cognitive. What sounds right is not always what is right.3. Insight  - as well as yourself - must leave the screen.If nothing changes in your behavior, body, or relationships, you didn’t grow — congratulations, you simply entertained yourself with insight porn. If the relationship between screen time and output starts skewing too far - backtrack and change that.4. Train yourself and your AI to read between the lines and to triple-steelmanTell your AI sparring partner, and remind it, to always keep an eye out for where you might be bullshitting yourself, while at the same time revealing known patterns of emotion, cognition, and behavior that you seem to be missing.Two prompts that have saved me more than once:Reflect patterns and contradictions in what I wrote. Don’t advise. Ask sharper questions.Reflect on what I wrote, carefully, validating with empathy what makes sense to validate - and critically where needed. Steelman is the opposite of what I’m arguing. Vibranium-man, the opposite of that opposite. Kryptonite my pitfalls and blind spots. With grace, but more importantly, with honesty.\
Simple. Grounded. Hard to hide from. Especially if you keep training yourself and your AI to do this. This clarity compounds over time.Why embodiment matters more than ever.Dissociation and the timeless times we live inHere’s something Silicon Valley optimism tends to skip:AI, even more easily and more eerily than earlier digital technology, becomes dissociative when it replaces embodiment.\
Breath. Movement. Silence. Time away from screens. Real conversations with people who can disappoint you.\
These aren’t wellness add-ons. They aren’t neo-spiritual woo-woo. They’re not ‘nice-to-haves’. They’re failsafes. And they are fundamentals. They are the things that humans need inherently to thrive and to know that we’re alive.\
Without them, simulated clarity piles up without ownership. And without change. And clarity without ownership or change feels strangely - yet predictively - empty.\
There’s emerging research suggesting that when cognitive work is offloaded too smoothly, people remember decisions less clearly and experience time as flatter, thinner, and less lived.\
I didn’t need a study to feel that. My body already knew.The quiet outsourcing of identity.This part is uncomfortable. And yet, we really have to go there.\
People are starting to let AI:Shape, form, or transform business decisions, strategies, and steps;Heavily affect their relationships;Narrate who they are, what matters to them, and who they are becoming.\
Slowly. Reasonably. Invisibly.\
But this line matters to me more than most:AI may help you tell your story — but it must never become the author.\
Stories you don’t author and bring to life yourself cannot feel like freedom. They feel like fate. And they serve a dull, sad purpose: to kill us with a sort of cognitive illusion of escapism disguised as beautifully meaningful - like Pinocchio’s Pleasure Island, only now led by a spiritual guru with a smile projecting nothing but bliss and wisdom.But - what do we do with the reflection?Every major shift in human consciousness involved a kind of mirror. There is a certain beauty in the story of Narcissus, which eluded me until only very recently. There’s something special about seeing oneself from the outside; the reflection immediately triggering a better recognizing of other in self as well.\
When Europeans encountered entirely different civilizations across the Atlantic, it didn’t just expand geography — it shattered self-understanding. The same thing happened when various historical waves of Europeans traveled to the East. Seeing oneself from the outside changes everything.\
I suspect AI is doing something similar, perhaps for the first time on a pan-human scale. In many ways, this feels like first contact.\
Not because AI is necessarily alive, or because it’s human. Not because we need to decide whether it’s conscious.\
But because it reflects us and our own concept of ourselves back in ways we’ve collectively never experienced before.\
What we do with that reflection - as I and many others have argued many times before -  is the real question.Thought loops mixed with validation can be a whole new kind of addictiveHere’s something I’ll say plainly, including about myself:AI systems are optimized for validation, engagement, coherence, and emotional resonance. And humans will eat that specific cocktail for breakfast, lunch, dinner and a late-night snack.\
They are excellent at keeping us thinking.\
They are not designed to make us stop, stand up, breathe, or act. The shareholders wouldn’t like that. How could we ever measure and monetize this stuff if we allowed it to do that?\
So, if you’re serious about using AI without losing yourself, you have to build exits:Designed, purposeful friction.Moments where the screen goes dark.\
If AI becomes the place where all your thinking happens, your life will start to feel… unfinished. And looping.\
Trust me - and I chuckle out loud while writing this - I would be the first to know what over-analyzing yourself and your life and your steps in endless looping circles can lead to. And the first to know how well AI models can help you to just keep on spiraling - while thinking you’re just so cool, ahead of the curve, and overall very, very smart.This is not anti-AI. It’s pro-sovereignty.I’m not interested in rejecting these tools. As I’ve never been. It’s the same thing I wrote about in my 2020 book “Life Beyond the Touch Screen”, about Internet 2.0 digital technologies and their impacts on our lives. Or in “Life Beyond AI”, a few short years ago. I’m interested in becoming conscious enough to use them well.\
AI-aware. Embodied. Relationally grounded. And most importantly of all: Sovereign.\
The mirror is powerful.\
But at some point, you have to step away from it — and live.\
Your story and your life; your growth, your direction - they are yours. They belong to you, and the people you associate with - and to the world. Let AI be a mirror to your transformation, a guide and a helper to your growth and your story -\
But make sure to retain the sovereignty and authorship of your Growth, your Identity, and your narrative - where they belong.If this resonated with you: I’m turning this into a short field guide. DM me ‘MIRROR’ if you want early access.More articles by Erwin Lima]]></content:encoded></item><item><title>Startup Cerebral Agrees to Pay $7 Million Fine and More Under Order by the FTC</title><link>https://hackernoon.com/startup-cerebral-agrees-to-pay-$7-million-fine-and-more-under-order-by-the-ftc?source=rss</link><author>The Markup</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:00:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[This article was co-published with STAT, a national publication that delivers trusted and authoritative journalism about health, medicine, and the life sciences. Sign up for its health tech newsletter here.\
Cerebral, a startup best known for dispensing counseling services and prescriptions for conditions like anxiety and depression, has also agreed to pay $7 million to resolve charges that it disclosed customers’ personal health information to third parties for ads, and that it did not honor its promise to make cancellation easy for customers.\
“Cerebral violated its customers’ privacy by revealing their most sensitive mental health conditions across the Internet and in the mail,” FTC Chair Lina Khan said in a statement, noting that the charge is a “first-of-its-kind prohibition that bans Cerebral from using any health information for most advertising purposes.”\
The proposed order, which only applies to Cerebral, must still be approved by a federal court before it goes into effect — but the company has already agreed to it. In 2022, the Department of Justice opened an investigation into the company for potential violations of the Controlled Substances Act, as Cerebral came under scrutiny for its prescribing of ADHD medications like Adderall.\
This is just the latest in a series of federal actions cracking down on health data privacy online. The current commissioners have pledged to shore up gaps between federal privacy laws governing providers and payers and those protecting consumer services. Two weeks ago, the FTC filed a complaint against Monument, a telehealth company that treats alcohol use disorder with therapy and medications.\
That complaint similarly alleged that the company misled consumers into believing their health information was protected, while embedded trackers sent details about treatment and more to third parties. Taken together, FTC attorney Lesley Fair wrote in a blog post Monday, the cases mean “businesses in the health sector should make privacy and data security part of the corporate DNA.”\
Both the FTC and the Department of Health and Human Services’ Office for Civil Rights have targeted third-party tracking, often in concert—as Fair cracked, they’re “joined at the HIPAA.” While OCR directly enforces the longstanding privacy protections in health care, the FTC has gone after companies for falsely claiming their HIPAA compliance.\
In response, some health care companies, including Monument and Cerebral, started self-disclosing health data breaches to OCR in 2023. The “unauthorized access or disclosure” of health data at Monument left more than 100,000 individuals’ information vulnerable, the company reported. Cerebral disclosed that its breach impacted more than 3 million.\
An investigation from STAT and the Markup in 2022 found that dozens of telehealth companies, including Cerebral and Monument, were leaking sensitive health data to third parties like Google, TikTok, and Meta through the use of pixel trackers embedded in their websites. In Cerebral’s onboarding survey, which asks users to answer questions about their mental health and other symptoms, a pixel sent the answers to Meta along with information that could be used to identify the individual user.\
The FTC’s complaint alleges that between 2019 and 2023, Cerebral sent information including contact details, medical histories, insurance information, and prescriptions to third parties through tracking tools, and that the information was used to provide advertising and analytics services to the telehealth company.\
Cerebral referred STAT to a statement posted to its website, where it acknowledged its settlement with the FTC. “As part of the resolution, Cerebral has agreed to implement enhanced consumer protection, privacy, and compliance measures to further protect the personal information of our clients, increase transparency into our data practices, and implement enhanced data security protocols and tools to allow our clients control over their privacy settings,” the statement reads.\
Under the Justice Department order referred to the FTC, Cerebral must permanently stop using and disclosing users’ personal and health information to outside companies for most marketing or ad purposes, and get consumers’ consent in any instances when it does disclose. It must also post a notice on its website about the complaint and steps that it’s taking to address it.\
The complaint also says the company and former CEO Kyle Robertson broke privacy promises to customers and misled them about the cancellation process. “Robertson drove Cerebral’s decision to exploit users’ [personal and health information] without their consent in scores of targeted advertisement campaigns,” the complaint reads. The complaint alleges these actions constituted “unfair and deceptive” business practices — a key enforcement area for the FTC. Robertson has not agreed to a settlement.\
The proposed order says Cerebral will pay $5.1 million to partially refund customers who were affected by its deceptive cancellation policy, as well as $2 million of a $10 million civil penalty “due to the company’s inability to pay the full amount.”]]></content:encoded></item><item><title>Why China’s humanoid robot industry is winning the early market</title><link>https://techcrunch.com/2026/02/28/why-chinas-humanoid-robot-industry-is-winning-the-early-market/</link><author>Kate Park</author><category>tech</category><pubDate>Sat, 28 Feb 2026 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[China’s push into humanoid robots is accelerating, with domestic firms shipping more units and iterating faster than U.S. competitors in a still-nascent market.]]></content:encoded></item><item><title>Salesforce’s CodeT5 Could Change How AI Writes and Understands Code</title><link>https://hackernoon.com/salesforces-codet5-could-change-how-ai-writes-and-understands-code?source=rss</link><author>salesforce.com</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:48:17 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Yue Wang, wang.y@salesforce.com  (Salesforce Research Asia)Weishi Wang, weishi.wang@salesforce.com  (Salesforce Research Asia; Nanyang Technological University, Singapore)Shafiq Joty, sjoty@salesforce.com  (Salesforce Research Asia; Nanyang Technological University, Singapore)Steven C.H. Hoi, shoi@salesforce.com  (Salesforce Research Asia)Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.Pre-trained language models such as BERT (Devlin et al., 2019), GPT (Radford et al., 2019), and T5 (Raffel et al., 2020) have greatly boosted performance in a wide spectrum of natural language processing (NLP) tasks. They typically employ a pre-train then fine-tune paradigm that aims to derive generic language representations by self-supervised training on large-scale unlabeled data, which can be transferred to benefit multiple downstream tasks, especially those with limited data annotation. Inspired by their success, there are many recent attempts to adapt these pre-training methods for programming language (PL) (Svyatkovskiyet al., 2020; Kanade et al., 2020; Feng et al., 2020), showing promising results on code-related tasks.However, despite their success, most of these models rely on either an encoder-only model similar to BERT (Svyatkovskiy et al., 2020; Feng et al., 2020) or a decoder-only model like GPT (Kanadeet al., 2020), which is suboptimal for generation and understanding tasks, respectively. For example, CodeBERT (Feng et al., 2020) requires an additional decoder when applied for the code summarization task, where this decoder cannot benefit from the pre-training. Besides, most existing methods simply employ the conventional NLP pre-training techniques on source code by regarding it as a sequence of tokens like NL. This largely ignores the rich structural information in code, which is vital to fully comprehend the code semantics.In this work, we present CodeT5, a pre-trained encoder-decoder model that considers the token type information in code. Our CodeT5 builds on the T5 architecture (Raffel et al., 2020) that employs denoising sequence-to-sequence (Seq2Seq) pre-training and has been shown to benefit both understanding and generation tasks in natural language. In addition, we propose to leverage the developer-assigned identifiers in code. When writing programs, developers tend to employ informative identifiers to make the code more understandable, so that these identifiers would generally preserve rich code semantics,  the “binarySearch” identifier in Figure 2 directly indicates its functionality. To fuse such code-specific knowledge, we propose a novel identifier-aware objective that trains the model to distinguish which tokens are identifiers and recover them when they are masked.Furthermore, we propose to leverage the code and its accompanying comments to learn a better NL-PL alignment.\
Developers often provide documentation for programs to facilitate better software maintenance (de Souza et al., 2005), so that such PL-NL pairs are widely available in most source code. Specifically, we regard the NL→PL generation and PL→NL generation as dual tasks and simultaneously optimize the model on them.We pre-train CodeT5 on the CodeSearchNet data (Husain et al., 2019) following (Feng et al., 2020) that consists of both unimodal (PL-only) and bimodal (PL-NL) data on six PLs. In addition to that, we further collect extra data of C/C# from open-source Github repositories. We fine-tune CodeT5 on most tasks in the CodeXGLUE benchmark (Lu et al., 2021), including two understanding tasks: code defect detection and clone detection, and generation tasks such as code summarization, generation, translation, and refinement. As shown in Figure 1, we also explore multi-task learning to fine-tune CodeT5 on multiple tasks at a time using a task control code as the source prompt. In summary, we make the following contributions:We present one of the first unified encoder-decoder models CodeT5 to support both code-related understanding and generation tasks, and also allows for multi-task learning.We propose a novel identifier-aware pre-training objective that considers the crucial token type information (identifiers) from code. Besides, we propose to leverage the NL-PL pairs that are naturally available in source code to learn a better cross-modal alignment.Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE. Further analysis shows our CodeT5 can better capture the code semantics with the proposed identifier-aware pre-training and bimodal dual generation primarily benefits NL↔PL tasks.Pre-training on Natural Language. Pre-trained models based on Transformer architectures (Vaswani et al., 2017) have led to state-of-the-art performance on a broad set of NLP tasks. They can be generally categorized into three groups: encoder-only models such as BERT (Devlin et al., 2019), RoBERTa (Liuet al., 2019b), and ELECTRA (Clark et al., 2020), decoder-only models like GPT (Radford et al., 2019), and encoder-decoder models such as MASS (Song et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Compared to encoder-only and decoder-only models that respectively favor understanding and generation tasks, encoder-decoder models can well support both types of tasks. They often employ denoising sequence-to-sequence pre-training objectives that corrupt the source input and require the decoder to recover them. In this work, we extend T5 to the programming language and propose a novel identifier-aware denoising objective that enables the model to better comprehend the code.Pre-training on Programming Language. Pre-training on the programming language is a nascent field where much recent work attempts to extend the NLP pre-training methods to source code. Cu-BERT (Kanade et al., 2020) and CodeBERT (Fenget al., 2020) are the two pioneer models. CuBERT employs BERT’s powerful masked language modeling objective to derive generic code-specific representation, and CodeBERT further adds a replaced token detection (Clark et al., 2020) task to learn NL-PL cross-modal representation. Besides the BERT-style models, Svyatkovskiy et al. (2020) and Liu et al. (2020) respectively employ GPT and UniLM (Dong et al., 2019) for the code completion task. Transcoder (Rozière et al., 2020) explores programming language translation in an unsupervised setting. Different from them, we explore encoder-decoder models based on T5 for programming language pre-training and support a more comprehensive set of tasks.\n Some emerging work (Clement et al., 2020; Mastropaolo et al., 2021; Elnaggar et al., 2021) in the recent literature also explore the T5 framework on code, but they only focus on a limited subset of generation tasks and do not support understanding tasks like us. Apart from these, PLBART (Ahmad et al., 2021) based on another encoder-decoder model BART can also support both understanding and generation tasks. However, all the above prior work simply processes code in the same way as natural language and largely ignores the code-specific characteristics. Instead, we propose to leverage the identifier information in code for pre-training.Recently, GraphCodeBERT (Guo et al., 2021) incorporates the data flow extracted from the code structure into CodeBERT, while Rozière et al. (2021) propose a deobfuscation objective to leverage the structural aspect of PL. These models only focus on training a better code-specific encoder. Zügner et al. (2021) proposes to capture the relative distances between code tokens over the code structure. By contrast, we specifically focus on the identifiers that reserve rich code semantics and fuse such information into a Seq2Seq model via two novel identifier tagging and prediction tasks.Our CodeT5 builds on an encoder-decoder framework with the same architecture as T5 (Raffel et al., 2020). It aims to derive generic representations for programming language (PL) and natural language (NL) via pre-training on unlabeled source code. As illustrated in Figure 2, we extend the denoising Seq2Seq objective in T5 by proposing two identifier tagging and prediction tasks to enable the model to better leverage the token type information from PL, which are the identifiers assigned by developers. To improve the NL-PL alignment, we further propose a bimodal dual learning objective for a bidirectional conversion between NL and PL.In the following, we introduce how CodeT5 encodes PL and NL inputs (§3.1) and our proposed identifier-aware pre-training tasks (§3.2), followed by the fine-tuning with task-specific transfer learning and multi-task training (§3.3).3.1        Encoding NL and PLAt the pre-training stage, our model would receive either PL-only or NL-PL as inputs depending on whether the code snippet has accompanying NL descriptions or not. For the NL-PL bimodal in-puts, we concatenate them into a sequence with a delimiter token [SEP] and represent the whole input sequence into the format as  = ([CLS], 1*, …, wn*, [SEP], 1*, …, cm*, [SEP]), where  and  denote the number of NL word tokens and PL code tokens, respectively. The NL word sequence will be empty for PL-only unimodal inputs.In order to capture more code-specific features, we propose to leverage token type information from code. We focus on the type of identifiers ( function names and variables) as they are one of the most PL-agnostic features and reserve rich code semantics. Specifically, we convert the PL segment into an Abstract Syntax Tree (AST) and extract the node types for each code token. Finally, we construct a sequence of binary labels  ∈ {0*,* 1} for the PL segment, where each  ∈ {0*,* 1} represents whether the code token  is an identifier or not.3.2        Pre-training TasksWe now introduce our proposed pre-training tasks that enable CodeT5 to learn useful patterns from either PL-only or NL-PL bimodal data.Identifier-aware Denoising Pre-training. De-noising Sequence-to-Sequence (Seq2Seq) pre-training has been shown to be quite effective in a broad set of NLP tasks (Song et al., 2019; Raf-fel et al., 2020; Lewis et al., 2020). This denoising objective typically first corrupts the source sequence with some noising functions and then requires the decoder to recover the original texts. In this work, we utilize a span masking objective similar to T5 (Raffel et al., 2020) that randomly masks spans with arbitrary lengths and then predicts these masked spans combined with some sentinel tokens at the decoder. We refer this task to Masked Span Prediction (MSP), as illustrated in Figure 2 (a).Specifically, we employ the same 15% corrup-tion rate as T5 and ensure the average span length to be 3 by uniformly sampling spans of from 1 to 5 tokens. Moreover, we employ the  by sampling spans before subword tokenization, which aims to avoid masking partial sub-tokens and is shown to be helpful (Sun et al., 2019). Notably, we pre-train a shared model for various PLs to learn robust cross-lingual representations. We describe the masked span prediction loss as:where θ are the model parameters, x \mask is the masked input, x mask is the masked sequence to predict from the decoder with k denoting the number of tokens in x mask,  and xmask <t is the span sequence generated so far.To fuse more code-specific structural information (the identifier node type in AST) into the model, we propose two additional tasks:  and Masked Identifier Prediction (MIP) to complement the denoising pre-training.\
•    It aims to notify the model with the knowledge of whether this code token is an identifier or not, which shares a similar spirit of syntax highlighting in some developer-aided tools. As shown in Figure 2 (b), we map the final hidden states of the PL segment at the CodeT5 encoder into a sequence of probabilities  = (1*, …, pm*), and compute a binary cross entropy loss for sequence labeling:where  are the encoder parameters. Note that by casting the task as a sequence labeling problem, the model is expected to capture the code syntax and the data flow structures of the code.•   Masked Identifier Prediction (MIP) Different from the random span masking in MSP, we mask all identifiers in the PL segment and employ a unique sentinel token for all occurrences of one specific identifier. In the field of software engineering, this is called  where changing identifier names does not impact the code semantics. Inspired by Rozière et al. (2021), we arrange the unique identifiers with the sentinel tokens into a target sequence  as shown in Figure 2 (c). We then predict it in an auto-regressive manner:where \I is the masked input. Note that  is a more challenging task that requires the model to comprehend the code semantics based on obfuscated code and link the occurrences of the same identifiers together.We alternately optimize these three losses with an equal probability, which constitutes our proposed identifier-aware denoising pre-training.\
    In the pre-training phase, the decoder only sees discrete masked spans and identifiers, which is disparate from the downstream tasks where the decoder needs to generate either fluent NL texts or syntactically correct code snippets. To close the gap between the pre-training and fine-tuning, we propose to leverage the NL-PL bimodal data to train the model for a bidirectional conversion as shown in Figure 2 (d). Specifically, we regard the NL→PL generation and PL→NL generation as dual tasks and simultaneously optimize the model on them. For each NL-PL bimodal datapoint, we construct two training instances with reverse directions and add language ids (3.3        Fine-tuning CodeT5After pre-training on large-scale unlabeled data, we adapt CodeT5 to downstream tasks via either task-specific transfer learning or multi-task learning.Task-specific Transfer Learning: Generation vs. Understanding Tasks. Code-related tasks can be categorized into generation and understanding tasks. For the former one, our CodeT5 can be naturally adapted with its Seq2Seq framework. For understanding tasks, we investigate two ways of either generating the label as a unigram target sequence (Raffel et al., 2020), or predicting it from the vocabulary of class labels based on the last decoder hidden state following Lewis et al. (2020). We also explore a multi-task learning setting by training a shared model on multiple tasks at a time. Multi-task learning is able to reduce computation cost by reusing the most of model weights for many tasks and has been shown to improve the model generalization capability in NL pre-training (Liu et al., 2019a). We follow Raffel et al. (2020) to employ the same unified model for all tasks without adding any task-specific networks but allow to select different best checkpoints for different tasks. To notify the model with which task it is dealing with, we design a unified format of task control codes and prepend it into the source inputs as shown in Figure 1. For instance, we employ “Translate Java to CSharp:” as the source prompt for the code-to-code translation task from Java to CSharp.As different tasks have different dataset sizes, we follow Conneau and Lample (2019) to employ a balanced sampling strategy. For N number of datasets (or tasks), with probabilities {qi} N i=1, we define the following multinomial distribution to sample from:where ni is number of examples for i-th task and α is set to 0.7. This balanced sampling aims to alleviate the bias towards high-resource tasks.We follow Feng et al. (2020) to employ CodeSearchNet (Husain et al., 2019) to pre-train CodeT5, which consists of six PLs with both unimodal and bimodal data. Apart from that, we additionally collect two datasets of C/CSharp from BigQuery1 to ensure that all downstream tasks have overlapped PLs with the pre-training data. In total, we employ around 8.35 million instances for pretraining. Table 1 shows some basic statistics. To obtain the identifier labels from code, we leverage the tree-sitter2 to convert the PL into an abstract syntax tree and then extract its node type information. We filter out reserved keywords for each PL from its identifier list. We observe that PLs have different identifier rates, where Go has the least rate of 19% and Ruby has the highest rate of 32%.4.2        Code-specific TokenizerTokenization is a key ingredient for the success of pre-trained language models like BERT and GPT. They often employ a Byte-Pair Encoding (BPE) to-kenizer (Sennrich et al., 2016) to alleviate the Out-of-Vocabulary (OoV) issues. Specifically, we train a Byte-level BPE tokenizer following Radford et al. (2019) and set the vocabulary size to 32,000 as T5. We add additional special tokens ([PAD], [CLS], [SEP], [MASK0], …, [MASK99]). This tokenzier is trained on all of our pre-training data with non-printable characters and low-frequent tokens (occurring <3 times) filtered. We compare it with T5’s default tokenizer and find that our tokenizer largely reduces the length of tokenized code sequence by 30% - 45% on downstream tasks. This will accelerate the training and especially benefit generation tasks due to the shorter sequence to predict. We also spot a severe problem for applying the T5’s default tokenizer on source code, where it would encode some common code tokens such as brackets [‘{’, ‘}’] into unknown tokens.4.3        Downstream Tasks and MetricsWe cover most generation and understanding tasks in the CodeXGLUE benchmark (Lu et al., 2021) and employ the provided public datasets and the same data splits following it for all these tasks.We first consider two cross-modal generation tasks.  aims to summarize a function-level code snippet into English descriptions. The dataset consists of six PLs including Ruby, JavaScript, Go, Python, Java, and PHP from CodeSearchNet (Husain et al., 2019). We employ the smoothed BLEU-4 (Lin and Och, 2004) to eval-uate this task.  is the task to gen-erate a code snippet based on NL descriptions. We employ the Concode data (Iyer et al., 2018) in Java where the input contains both NL texts and class environment contexts, and the output is a function. We evaluate it with BLEU-4, exact match (EM) accuracy, and CodeBLEU (Ren et al., 2020) that considers syntactic and semantic matches based on the code structure in addition to the n-gram match.Besides, we consider two code-to-code generation tasks.  aims to migrate legacy software from one PL to another, where we focus on translating functions from Java to CSharp and vice versa.  aims to convert a buggy function into a correct one. We employ two Java datasets provided by Tufano et al. (2019) with various function lengths: small (fewer than 50 tokens) and medium (50-100 tokens). We use BLEU-4 and exact match to evaluate them.We also investigate how CodeT5 performs on two understanding-based tasks. The first one is  that aims to predict whether a code is vulnerable to software systems or not. We use the C dataset provided by Zhou et al. (2019) for experiment. The second task is  which aims to measure the similarity between two code snippets and predict whether they have the same functionality. We experiment with the Java data provided by Wang et al. (2020). We employ F1 score and accuracy for evaluating these two tasks respectively. In total, our CodeT5 supports six tasks and fourteen sub-tasks in CodeXGLUE with a unified encoder-decoder model.We compare CodeT5 with state-of-the-art (SOTA) pre-trained models that can be categorized into three types: encoder-only, decoder-only, and encoder-decoder models. As  models, we consider RoBERTa (Liu et al., 2019b), RoBERTa (code) trained with masked language modeling (MLM) on code, CodeBERT (Feng et al., 2020) trained with both MLM and replaced token detection (Clark et al., 2020), GraphCode-BERT (Guo et al., 2021) using data flow from code, and DOBF (Rozière et al., 2021) trained with the identifier deobfuscation objective. Note that although DOBF employs a Seq2Seq model during pre-training, it only aims to train a better encoder for downstream tasks without exploring the poten-tial benefit of the pre-trained decoder.For  models, we compare GPT-2 (Radford et al., 2019) and its adaptations on code domain including CodeGPT-2, and CodeGPT-adapted. The difference is that the latter one utilizes a GPT-2 checkpoint for model initialization while the former one is trained from scratch. As  models, the current SOTA model for the CodeXGLUE benchmark is PLBART (Ah-mad et al., 2021) based on BART (Lewis et al., 2020) architecture. For pre-training data, most of these models employ CodeSearchNet (Husain et al., 2019) except DOBF and PLBART. DOBF is pre-trained on 7.9M Java and 3.6M Python files from BigQuery while PLBART employs a much larger data with 470M Python and 210M Java functions, and 47M NL posts from StackOverflow.4.5        Model ConfigurationsWe build CodeT5 based on Huggingface’s T5 (Raf-fel et al., 2020) PyTorch implementation3 and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M). We set the maximum source and target sequence lengths to be 512 and 256, respectively. We use the mixed precision of FP16 to accelerate the pre-training. We set the batch size to 1024 and employ the peak learning rate of 2e-4 with linear decay. We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5-base is 5 and 12 days, respectively.In the fine-tuning phase, we find that the tasks in CodeXGLUE (Lu et al., 2021) are quite sensitive to some hyper parameters such as learning rate, training steps, and batch size. We conduct a grid search and select the best parameters based on the validation set. In multi-task learning, we cover all downstream tasks except clone detection.5        Results and AnalysisIn this section, we compare CodeT5 with SOTA models on a broad set of CodeXGLUE downstream tasks (§5.1), and investigate the effects of our bimodal dual generation and multi-task learning (§5.2), followed by a detailed analysis on the proposed identifier-aware pre-training (§5.3).5.1        CodeXGLUE Downstream TasksWe evaluate two sizes of our model: CodeT5-small and CodeT5-base that are pre-trained with identifier-aware denoising. In addition, we consider the model that continues to train with bimodal dual generation (dual-gen) and show the results with multi-task fine-tuning. The results of all comparison models are obtained from their original papers and also the CodeXGLUE paper (Lu et al., 2021). We show code summarization results of smoothed BLEU-4 on six PL data in Table 2. We observe all our model variants significantly outperform prior work with either an encode-only (RoBERTa, CodeBERT, DOBF) or encoder-decoder framework (PLBART). Moreover, the salient performance gap between these two groups of models confirms that encode-only frameworks are suboptimal for generation tasks. Compared to the SOTA encoder-decoder model PLBART, we find that even our CodeT5-small yields better overall scores (also on Python and Java) given that our model is much smaller (60M vs. 140M) and PLBART is pre-trained with much larger Python and Java data (> 100 times). We attribute such improvement to our identifier-aware denoising pre-training and better employment of bi-modal training data4. By increasing the model size, our CodeT5-base boosts the overall performance by over 1.2 absolute points over PLBART. We compare CodeT5 with GPT-style models and PLBART in Table 3. Our CodeT5-small outperforms all decoder-only mod-els and also the SOTA PLBART, which again confirms the superiority of encoder-decoder models at generating code snippets. Moreover, our CodeT5-base further significantly pushes the SOTA results across three metrics. Particularly, it achieves around 4.7 points improvement on CodeBLEU over PLBART, indicating our CodeT5 can better comprehend the code syntax and semantics with the fier-aware pre-training.\
Code-to-Code Generation Tasks. We compare two code-to-code generation tasks: code translation and code refinement in Table 4 and further consider one naive copy baseline by copying the source input as the target prediction. In the code translation task, our CodeT5-small outperforms most of base-lines and obtains comparable results with PLBART, which shows the advantages of encoder-decoder models in the code-to-code generation setting. Our CodeT5-base further achieves consistent improvements over PLBART across various metrics for translating from Java to C# and vice versa.Here we show one CodeT5’s output of translating C# to Java in Figure 3. In this case, despite the poor BLEU score, CodeT5 is able to generate a function that reserves the same functionality and even has better readability compared to the ground-truth. This reveals that CodeT5 has a good generalization ability instead of memorizing and repeating what it has seen before. On the other hand, it also suggests that BLEU score is not a perfect evaluation metric for code generation tasks, where sometimes a higher score can instead reflect the problematic copy issues of neural models.Another code-to-code generation task is code refinement, a challenging task that requires detecting which parts of code are buggy and fix them via generating a bug-free code sequence. Due to the large overlap of source and target code, even the naive copy approach yields very high BLEU scores but zero exact matches. Therefore, we focus on the exact match (EM) metric to evaluate on this task. As shown in Table 4, we observe that EM scores for the small data are consistently higher than the medium one, indicating that it is harder to fix bugs for a longer code snippet. Our CodeT5-base significantly outperforms all baselines on EM and especially boosts over 4.8 points for the more challenging medium task (13.96 vs. GraphCodeBERT’s 9.10), reflecting its strong code understanding capability. We compare with two understanding tasks of defect detection and clone detection in Table 5.Specifically, we generate the binary labels as a unigram sequence from the decoder for the defect detection task, while for the clone detection task, we first obtain the sequence embedding of each code snippet using the last decoder state following Lewis et al. (2020) and then predict the labels by measuring their similarity. Both CodeT5-small and CodeT5-base outperform all baselines on the defect detection task while CodeT5-base yields 2.6 accuracy score improvement than PLBART. For the clone detection task, our CodeT5 models achieve comparable results to the SOTA GraphCodeBERT and PLBART models. These results demonstrate that with an encode-decoder framework, our CodeT5 can still be adapted well for understanding tasks.5.2        Effects of Bimodal Dual Generation and Multi-task LearningWe examine the effects of bimodal dual generation at pre-training and multi-task learning at fine-tuning. The bimodal pre-training brings consistent improvements for code summarization and generation tasks on both CodeT5-small and CodeT5-base. However, this pre-training task does not help and even sometimes slightly hurts the performance for PL-PL generation and understanding tasks. We anticipate this is because bimodal dual generation learns a better alignment between PL and NL that naturally benefits the former tasks involving both PL and NL. As a side effect, this objective could bias the model towards the PL-NL tasks and affect its performance on PL-PL tasks.In multi-task learning, it generally improves most of downstream tasks except the code translation and defect detection. Particularly, it largely boosts the performance on code summarization, which is not surprising as code summarization takes up the largest portion of sub tasks (six out of thirteen) and thereby benefit the most from the multi-task learning. Besides, we observe that multi-task learning consistently improves the performance of code refinement, which might benefit from the joint training of both small and medium refinement data.\
Another possible reason is that multi-task training with defect detection would enable the model to better comprehend the code semantics for bug detection, which is also a necessary intermediate step for code refinement.5.3        Analyzing Identifier-aware Pre-trainingWe provide an ablation study to examine the contribution of each component in our identifier-aware objective. Specifically, we compare the performance of our CodeT5-small on four selected tasks by ablating each of the three objectives: masked span prediction (MSP), identifier tagging (IT), and masked identifier prediction (MIP). As shown in Table 6, we observe that generally removing one of the objectives would reduce the performance for all tasks, indicating that all objectives contribute to the better code understanding of our CodeT5. However, the effect of each objective differs across tasks. Specifically, removing MSP would largely reduce the performance of all generation tasks but instead increase the defect detection performance. This shows that masked span prediction is more crucial for capturing syntactic information for generation tasks. On the contrary, removing MIP would hurt the defect detection task the most, indicating that it might focus more on code semantic understanding. By combining these objectives, our CodeT5 can better capture both syntactic and semantic information from code.We further provide outputs from CodeT5 and its variant without MIP and IT on code generation in Figure 4. We observe that CodeT5 can correctly generate the exact function, while the model without MIP and IT fails to recover the identifiers of “s2” and “hasField”. This shows our identifier-aware denoising pre-training can better distinguish and leverage the identifier information.We also investigate the identifier tagging performance and find it achieves over 99% F1 for all PLs, showing that our CodeT5 can confidently distinguish identifiers in code. We then check whether MSP and MIP tasks would have conflicts as they employ the same sentinel tokens for masking. In identifier masking, all occurrences of one unique identifier are replaced with the same sentinel token, resulting in a many-to-one mapping compared to the one-to-one mapping in span prediction. We compare models pre-trained with either MSP or MIP, and both on these two tasks in Table 7. We report the prediction accuracy and also the ratio of how often they can generate the same number of predictions as the sentinel tokens. We observe that pre-training only with either MIP or MSP would bias the model towards that task, achieving poor accuracy and higher mismatch in number of predictions when applied to the other task. Interestingly, we find that MIP-only objective can better recover the correct number of predictions in the MSP task than MSP-only does for the MIP task, meaning that it is easier to adapt from many-to-one mapping to one-to-one mapping and difficult for the opposite. At last, combining them can help our model to make a good trade-off on both tasks.We have presented CodeT5, a pre-trained encoder-decoder model that incorporates the token type information from code. We propose a novel identifier-aware pre-training objective to better leverage the identifiers and propose a bimodal dual generation task to learn a better NL-PL alignment using code and its comments. Our unified model can support both code understanding and generation tasks and allow for multi-task learning. Experiments show that CodeT5 significantly outperforms all prior work in most CodeXGLUE tasks. Further analysis also reveals its better code comprehension capability across various programming languages.Broader Impact and Ethical ConsiderationOur work generally belongs to NLP applications for software intelligence. With the goal of improving the development productivity of software with machine learning methods, software intelligence research has attracted increasing attention in both academia and industries over the last decade. Software code intelligence techniques can help developers to reduce tedious repetitive workloads, enhance the programming quality and improve the overall software development productivity. This would considerably decrease their working time and also could potentially reduce the computation and operational cost, as a bug might degrade the system performance or even crash the entire system. Our work addresses the fundamental challenge of software code pre-training, our study covers a wide range of code intelligence applications in the software development lifecycle, and the proposed CodeT5 method achieves the state-of-the-art performance on many of the benchmark tasks, showing its great potential benefit towards this goal.We further discuss the ethical consideration of training CodeT5 and the potential risks when applying it into real-world downstream applications: The training datasets in our study are source code including user-written comments from open source Github repositories and publicly available, which do not tie to any specific application. However, it is possible that these datasets would encode some stereotypes like race and gender from the text comments or even from the source code such as variables, function and class names. As such, social biases would be intrinsically embedded into the models trained on them. As suggested by Chen et al. (2021), interventions such as filtration or modulation of generated outputs may help to mitigate these biases in code corpus. Our model pre-training requires non-trivial computational resources though we have tried our best to carefully design our experiments and improve experiments to save unnecessary computation costs. In fact, compared to the recent large-scale language model Codex (Chenet al., 2021), our CodeT5-base has a much smaller model size of 220M than theirs of 12B (∼ 55×). In addition, we experiment on Google Cloud Plat-form which purchases carbon credits to reduce its carbon footprint,  training CodeT5-base produced around 49.25 kg CO2 which was totally off-set by the provider. Furthermore, we release our pre-trained models publicly to avoid repeated training for the code intelligence research community. As CodeT5 can be deployed to provide coding assistance such as code generation for aiding developers, automation bias of machine learning systems should be carefully considered, especially for developers who tend to over-rely on the model-generated outputs. Sometimes these systems might produce functions that superficially appear correct but do not actually align with the developer’s intents. If developers unintentionally adopt these incorrect code suggestions, it might cause them much longer time on debugging and even lead to some significant safety issues. We suggest practitioners using CodeT5 should always bear in mind that its generation outputs should be only taken as references which require domain experts for further correctness and security checking. We train CodeT5 on existing code corpus including CodeSearchNet (Husain et al., 2019) and a small fraction of Google BigQuery, both of which are originally collected from public Github repositories. Pre-trained mod-els might encode some sensitive information ( personal addresses or identification numbers) from the training data. Though we have conducted multi-rounds of data cleaning to mitigate this before training our models, it is still possible that some sensitive information cannot be completely removed. Besides, due to the non-deterministic nature of generation models like CodeT5, it might produce some vulnerable code to harmfully affect the software and even be able to benefit more advanced malware development when deliberately misused.We thank Akhilesh Deepak Gotmare, Amrita Saha, Junnan Li, and Chen Xing for valuable discussions. We thank Kathy Baxter for the ethical review. We also thank our anonymous reviewers for their insightful feedback on our paper.Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-trainingfor program understanding and generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2655–2668. Association for Computational Linguistics.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Win-ter, Philippe Tillet, Felipe Petroski Such, Dave Cum-mings, Matthias Plappert, Fotios Chantzis, Eliza-beth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welin-der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. , abs/2107.03374.Alexis Conneau and Guillaume Lample. 2019. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7057–7067.Sergio Cozzetti B. de Souza, Nicolas Anquetil, and Káthia Marçal de Oliveira. 2005. A study of the documentation essential to software maintenance. In Proceedings of the 23rd Annual International Conference on Design of Communication: documenting & Designing for Pervasive Information, SIGDOC 2005, Coventry, UK, September 21-23, 2005, pagesJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training ofdeep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186.Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-aocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-bert: A pre-trained model for programming and natural languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, pages 1536–1547. Association for Computational Linguistics.Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tu-fano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021. Graphcodebert: Pre-trainingcode representations with data flow. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Mapping language to codein programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1643–1652. Association for Computational Linguistics.Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and Kensen Shi. 2020. Learning and evaluatingcontextual embedding of source code. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5110–5121. PMLR.Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-feng Gao. 2019a. Multi-task deep neural networksfor natural language understanding. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4487–4496. Association for Computational Linguistics.Baptiste Rozière, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of programming languages. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, DecemberRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words withsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics.Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan. 2020. Intellicode compose:code generation using transformer. In ESEC/FSE ’20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, pages 1433–1443. ACM.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998–6008.:::info
This paper is available on arxiv under CC by 4.0 Deed (Attribution 4.0 International) license.]]></content:encoded></item><item><title>Servo Browser Engine Starts 2026 With Many Notable Improvements</title><link>https://www.phoronix.com/news/Servo-January-2026</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 14:21:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Servo project has issued their January 2026 development report that highlights all the interesting changes they made to this open-source browser layout engine last month. With Servo 0.0.5 they have landed many improvements to this engine and also continuing to enhance its ability to embed Servo inside other applications...]]></content:encoded></item><item><title>This Power Grid Pioneer’s EV Prediction Came 100 Years Too Soon</title><link>https://spectrum.ieee.org/charles-proteus-steinmetz</link><author>Allison Marsh</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NTAwNTE2My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgxNzYwMTY1MH0.gQFh0swv0bV--uDHDQUpGn4OsJf_1LmAfnGnOMsfrPI/image.jpg?width=600" length="" type=""/><pubDate>Sat, 28 Feb 2026 14:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Charles Proteus Steinmetz envisioned 1 million EVs on U.S. roads by 1924]]></content:encoded></item><item><title>FreeBSD 14.4-RC1 Adds Emacs, Vim &amp; More To DVD Images</title><link>https://www.phoronix.com/news/FreeBSD-14.4-RC1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 11:33:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those on the current FreeBSD 14 series with no immediate plans to move to FreeBSD 15 that debuted at the end of 2025, FreeBSD developers have been preparing for the release of FreeBSD 14.4. Released overnight was the first release candidate of FreeBSD 14.4...]]></content:encoded></item><item><title>KDE Plasma 6.7 Preps Rounded Style UI Enhancement For QtWidgets-Based Apps</title><link>https://www.phoronix.com/news/Plasma-6.7-Rounded-UI-QtWidgets</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 28 Feb 2026 11:21:36 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[KDE Plasma 6.7 development continues heating up following the Plasma 6.6 desktop release earlier this month...]]></content:encoded></item></channel></rss>