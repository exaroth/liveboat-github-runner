<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>New Neural Network Slashes Sensor Data Overload</title><link>https://spectrum.ieee.org/sensor-data</link><author>Matthew S. Smith</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTg2MTgyMi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1NzI2MDQxMX0.osAojwMNokNC5tC0mpe1FUdPUcrN-sD3iLfyDeoZ0MQ/image.jpg?width=600" length="" type=""/><pubDate>Mon, 27 Jan 2025 12:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Researchers say factories on Mars could benefit from the efficient approach]]></content:encoded></item><item><title>Laptop Improvements &amp; More AMD Driver Features Merged For Linux 6.14</title><link>https://www.phoronix.com/news/Linux-6.14-x86-Platform-Drivers</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 27 Jan 2025 11:58:37 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The x86 platform driver updates have been merged for the ongoing Linux 6.14 merge window. As usual the x86 platform driver updates are predominantly to benefit the many different Intel/AMD laptops out there with various OEM vendor features/functionality. Plus within the platform-drivers-x86 space is a growing number of AMD SoC drivers for not only laptops but also desktops/servers...]]></content:encoded></item><item><title>NAMD Molecular Dynamics Performance Improves Well With NVIDIA Blackwell / RTX 5090</title><link>https://www.phoronix.com/news/NAMD-NVIDIA-GeForce-RTX-5090</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 27 Jan 2025 11:46:14 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[With now having a Linux driver for running GPU compute workloads on the GeForce RTX 5090 (as mentioned, Linux gaming benchmarks will come following the formal R570 Linux driver release in the coming days that is better optimized for gaming), I ran some additional GPU compute benchmarks on the GeForce RTX 5090 "Blackwell" graphics card over the weekend...]]></content:encoded></item><item><title>Reduced SquashFS Memory Use With The Linux 6.14 Kernel, More NILFS2 Fixes</title><link>https://www.phoronix.com/news/Linux-6.14-Non-MM</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 27 Jan 2025 11:32:25 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In addition to all of the exciting "MM" changes for Linux 6.14 that were submitted by Andrew Morton's pull request, he also sent out the set of "non-MM" updates for the Linux 6.14 merge window...]]></content:encoded></item><item><title>Desktop Motherboards Continue Playing Catch-Up For Linux Monitoring Support</title><link>https://www.phoronix.com/news/Linux-6.14-HWMON</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 27 Jan 2025 11:20:49 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The hardware monitoring "HWMON" subsystem updates have been merged for the Linux 6.14 kernel. As happens with most kernel releases, there are a number of already-launched desktop motherboards beginning to see working sensor monitoring support under Linux...]]></content:encoded></item><item><title>DeepSeek ‘punctures’ AI leaders’ spending plans, and what analysts are saying</title><link>https://techcrunch.com/2025/01/27/deepseek-punctures-tech-spending-plans-and-what-analysts-are-saying/</link><author>Manish Singh</author><category>tech</category><pubDate>Mon, 27 Jan 2025 10:59:43 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Chinese AI firm DeepSeek has emerged as a potential challenger to U.S. AI companies, demonstrating breakthrough models that claim to offer performance comparable to leading offerings at a fraction of the cost. The company’s mobile app, released in early January, has lately topped the App Store charts across major markets including the U.S., UK, and […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Most Concerns About Bitcoin Are Deeply Human</title><link>https://hackernoon.com/most-concerns-about-bitcoin-are-deeply-human?source=rss</link><author>Edwin Liava&apos;a</author><category>tech</category><pubDate>Mon, 27 Jan 2025 10:55:34 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[              liavaa.space/blog/20250124/20250124.md at main · EdwinLiavaa/liavaa.space · GitHub                                         
The conversation began on LinkedIn after I shared by piece Most Web3 Projects Are Hiding Behind a Centralized Corporate Structure, whereby a fellow Pacific Islander and blockchain startup entrepreneur shared a profound set of questions that cut to the heart of our digital financial transformation. His concerns were not just technical, they were deeply human, rooted in the collective memory of economic vulnerabilities that have long challenged our region.\
Satoshi Nakamoto's vision wasn't just about creating a new currency. It was about dismantling the old power structures, about giving individuals a financial system that doesn't bow to corporate whims or governmental control. Yet here we are, watching institutions accumulate Bitcoin like modern-day treasure hunters.\
But here's the remarkable truth, despite the institutional interest, Bitcoin remains fundamentally unchanged. The protocol stands as a sentinel, unmoved by the machinations of corporate strategy. No single entity can alter its core code, print more coins, or manipulate its fundamental rules. The network continues to operate exactly as it was designed, decentralized, transparent and resilient.\
The fears are not without merit. Market manipulation has been the tragic hallmark of traditional financial systems. We've seen how the powerful can crush the dreams of everyday people, as witnessed in the 2008 financial crisis. But Bitcoin offers a different narrative. Every transaction is visible, every movement tracked on a public ledger. The distributed nature of its network makes coordinated manipulation an exponentially complex challenge.\
Rumors about changing Bitcoin's supply are just that, rumors. The 21 million coin limit isn't a suggestion, it's a fundamental law of the Bitcoin universe. Changing this would require an unprecedented, practically impossible consensus from millions of global participants.\
For the retail investor feeling overwhelmed, the message is simple, you don't need to be a technical wizard to participate. Dollar-cost averaging, holding, understanding the basic principles, these are powerful tools of engagement. Bitcoin's democratizing power lies not in complex trading strategies, but in its fundamental accessibility.\
Looking at alternative blockchain platforms like Ethereum reveals the true strength of Bitcoin's approach. While others chase rapid innovation and get entangled in internal debates, Bitcoin remains focused on its core mission i.e. creating a decentralized, censorship-resistant form of money.\
Bitcoin's decentralization is not just a theoretical concept, but a tangible reality demonstrated by its robust network infrastructure. As of the latest data, Bitcoin boasts approximately 21,500 nodes globally, compared to Ethereum's mere 4,625 nodes. This expansive network underscores Bitcoin's true distributed nature. Critically, anyone can run a node and participate in consensus, with nodes geographically dispersed across continents. There's no central legal body, and no government can access or control this network. While this approach means a larger network with lower transaction throughput, it ensures an unprecedented level of resilience and democratic participation in the financial ecosystem.\
This isn't a perfect system. No financial innovation is. But it represents our best current prototype of a truly democratic monetary system, a system that puts power back into the hands of individuals, not institutions.\
In the end, Bitcoin asks us to do something radical, to trust a system powered by cryptography and mathematical proof, not people. To believe in code, not corporate promises. To see financial sovereignty not as a luxury, but as a fundamental right.]]></content:encoded></item><item><title>They Know You More Than You Know Yourself - Surveillance Capitalism and It&apos;s Ethical Impacts</title><link>https://hackernoon.com/they-know-you-more-than-you-know-yourself-surveillance-capitalism-and-its-ethical-impacts?source=rss</link><author>Philosophical</author><category>tech</category><pubDate>Mon, 27 Jan 2025 10:52:38 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Angelica Sofia Valeriani, Ethics of Information Technology, Politecnico di Milano, Milan, Italy.\
Surveillance capitalism is a concept that describes the practice of collecting and analyzing massive amounts of user data for the purpose of targeted advertising and other forms of monetization. The phenomenon has become increasingly prevalent in recent years, with tech companies like Google and Facebook using users’ personal information to deliver personalized content and advertisements. Another example of surveillance capitalism is the use of military technology to collect and analyze data for national security purposes. In this context, surveillance capitalism involves the use of technologies like facial recognition and social media monitoring to gather information on individuals and groups deemed to be potential threats to national security. This information is then used to inform military operations and decision-making. This paper wants to analyze in a critical way the phenomenon of surveillance capitalism, proposed under two different ethical framework perspectives. Utilitarianism, a consequentialist ethical theory that judges actions based on their ability to bring about the greatest amount of happiness or pleasure for the greatest number of people, and Kantian deontology, a non-consequentialist ethical theory that emphasizes the importance of individual autonomy, freedom, and dignity. On one side, the utilitarian framework enlightens how Information Technology (IT) and the features provided offer, at first sight, all the positive perceptions to the majority of people, happiness, entertainment, and pleasure. On the other side, the Kantian deontology framework mostly focuses on the aspect of freedom and free will of the individual. This topic is particularly related to the concession of permissions to access data in change of services and the degree of influence that manipulation performed by surveillance capitalism can generate.To increase consumerism, the business of major companies and the power of countries, social networks and political parties are performing a form of inner and intimate manipulation that is located in the idea of Surveillance Capitalism. In this paper, I will try to show this new phenomenon, which can be realized utilizing targeted advertising and military technology, under two different ethical frameworks, the utilitarianist and the Kantian perspective. The scope is to enlighten justifications and implications according to these different frameworks. In particular, I will start by defining the concept of Surveillance Capitalism and the way it is perceived nowadays. I will analyze in depth some of the most common real-world situations in which this phenomenon can be seen in action, i.e. targeted advertisement and coercive manipulation in both totalitarian regime and democratic countries. I will then analyze these aspects in relation to the ethical frameworks proposed, from one side “the end justifies the means” and from the other the duty framework as the core, introducing other philosophical works as a support of my analysis.\
The paper is organized in the following way. Section 2 will describe Surveillance Capitalism, defining its characteristics and important features. Section 3 and 4 will be devoted to the analysis of the realization of Surveillance Capitalism in the form of targeted advertisement and manipulation of people in politics and military fields. In particular, these real scenarios of analysis were chosen because they are very representative of the main needs in the nowadays society. From one side the target advertisement represents an incentive for consumerism and the possibility of easily realizing every desire (e.g. the easy discovery of new places, occasions, and assets). From the other side, the social and national security, which is connected to both politics and military war is a topic to which people are very sensitive nowadays, because of the incredible ease with which some crimes can be performed today. Section 5.1 recalls these phenomena, in particular the one of target advertisement, which is more connected to pleasure and amusement, under a utilitarian perspective, after having described the key points of the framework. Section 5.2 will have the same approach as section 5.1, applied to the duty framework proposed by Kant and focusing more on the phenomena related to the political sphere, as it is more representative of the concepts of responsibility and duty towards people and the country. Section 5.3 will complete the analysis, deepening the study of both phenomena under the two ethical frameworks. It will also enlighten the main difference between the two frameworks and their values, while conclusions are drawn in section 6.2 The new Empire of Surveillance CapitalismThe wide spread of the Internet brought to the current society in which the major means of generating wealth on the Internet and through proprietary platforms (as apps) is the surveillance of the population. This phenomenon allows to increase exponentially gains from the digitalized companies that have the monopoly of society. Digitalization of surveillance has radically changed the nature of advertising. Now, the implication of the system is the absence of effective privacy. Revelations of Edward Snowden on NSA’s Prism of 2013 are an example of the pattern of a tight interweaving of the military with giant computer-Internet corporations. There are many examples of partly cooperative, partly legally coerced sharing of data that can be found in the case of Microsoft, Google, Yahoo, Facebook, and others. These companies turned over the data from tens of thousands of their accounts on individuals every six months both to the NSA and other intelligence agencies, with a fast rise in the number of accounts turned over to the secret government [1]. In practice, according to revelations, NSA gained access to data from mobile phones emanating from hundreds of millions of Americans as well as populations abroad—operating thorough Boundless Informant, Prism, and other secret projects. The final goal was to capitalize on new military technology and create larger global Internet monopolies while expanding the military-digital interchange system. In the context of capitalization, an example of the trend that well represents the centralized structure of monopoly-finance capital in the age of digital surveillance is given by the practice of “securitization” increasingly standing simultaneously for a world dominated by the elements identified in [1, 3]. In detail:Financial derivatives tradingA network of public and private surveillanceThe militarization of security-control systemsThe removal of judicial processes from effective civilian control\
Surveillance can be simply seen as a collection of techniques that together potentially constitute subjects to regulation (first interpretation), as well as attention purposeful, systematic, and oriented to social control (second interpretation). While totalitarian regimes may embrace the second definition, seeing Surveillance as an instrument to strengthen their control over people, capitalist and democratic systems may value Surveillance more for reasons connected to its rationality, potentialities in terms of welfare, and good sense. On the whole, it must be noticed anyway that Surveillance must be contextualized also in its economic implications and this is more than a way of social control [2]. One of the greatest issues that are connected to Surveillance Capitalism, as also shown in the core phenomenons of the paper, i.e. target advertising and coercive manipulation, is the asymmetry in\
the distribution of power that is generated by the retrieval of data of people unaware of this process (known and discussed following some whistle-blowers revelations). The weight of this power is concentrated in the hands of the actors that have access and can manage the information that is retrieved [3].]]></content:encoded></item><item><title>Movemaker: Aptos Growing Chinese-Speaking Region With Multi-Million-Dollar Support Via Its Community</title><link>https://hackernoon.com/movemaker-aptos-growing-chinese-speaking-region-with-multi-million-dollar-support-via-its-community?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Mon, 27 Jan 2025 10:30:05 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[HONG KONG, Hong Kong, January 27th, 2025/Chainwire/--The Aptos Foundation proudly announces the establishment of Movemaker, its official community organization. Co-initiated by Ankaa and BlockBooster, Movemaker is dedicated to advancing and building the Aptos ecosystem in the Chinese-speaking region. \
By integrating resources, empowering communities, and providing technical support, Movemaker aims to accelerate the adoption and prosperity of Aptos in this key market. As a decentralized community organization, Movemaker has secured multi-million-dollar funding and resource support from the Aptos Foundation.\
With autonomous decision-making authority, Movemaker is designed to efficiently address the needs of developers and ecosystem builders in the Chinese-speaking region, while further expanding Aptos’ reach and influence in the global Web3 landscape. “The Chinese-speaking community is an invaluable part of the Aptos ecosystem, and a massive driver of growth across all of Web3. The launch of Movemaker highlights a deep commitment to this community and its role in driving Web3 innovation,” said Avery Ching, CEO and cofounder of Aptos Labs “Alongside Ankaa, the teams at Aptos Foundation, Aptos Labs and BlockBooster, I look forward to empowering developers, fostering groundbreaking projects, and establishing a thriving environment for innovation and growth in this key market.”"Movemaker is an ode to the Chinese-speaking Aptos community and represents our commitment to unlocking the limitless potential of incredible builders to launch global products,” said Ash Pampati, Head of Ecosystem at Aptos. “We are excited to accelerate Web3 entrepreneurship through the deep talent pool in this market.” Multi-Million-Dollar Funding and Resources: Empowering Ecosystem GrowthThe launch of Movemaker marks a new phase in the Aptos Foundation’s strategic expansion within the Chinese-speaking region.\
With the comprehensive support of the Aptos Foundation, Movemaker will focus on the following key areas: Establishing an Ecosystem Grant Program to Support Project Incubation and GrowthMovemaker will manage and oversee the allocation of a dedicated ecosystem grant fund to drive the sustainable development of the Aptos blockchain ecosystem. This fund will prioritize innovative projects in the following core areas: Integration of AI and BlockchainInnovative Payment SolutionsStablecoins and Real-World Assets (RWA)\
Movemaker will focus on supporting developers and projects from the Chinese-speaking region while also welcoming global teams interested in this market to join the Aptos ecosystem. The aim is to ensure that resources are deployed effectively to enable rapid iteration of both technology and applications.Movemaker will take on the responsibility of community development and outreach for Aptos in the Chinese-speaking market. Using Hong Kong as a regional hub, Movemaker will employ a multi-dimensional approach to enhance Aptos’ visibility and influence.\
Establishing the Aptos Space: \
Movemaker will set up an Aptos Space in Hong Kong to provide an open collaboration platform for developers and startups.\
Developer Engagement and Community Building:\
By hosting hackathons, hacker houses, technical workshops, and community meet-ups, Movemaker will offer resources and technical support to developers, creating a vibrant and diverse interaction platform to draw top talent to the Aptos ecosystem. \
Public Relations and Government Collaboration:\
Movemaker will work closely with government agencies, industry associations, and business organizations across Hong Kong and the broader Chinese-speaking region to ensure the lawful and compliant adoption of Aptos ecosystem projects.\
Brand and Market Promotion:\
Through precise marketing strategies and collaborations with mainstream media and industry community leaders, Movemaker will expand Aptos’ brand recognition and influence among users and developers in the Chinese-speaking region.\
Annual Goals: Driving Ecosystem Growth with Tangible Results\
Movemaker’s first year of operations will focus on elevating Aptos’ ecosystem in the Chinese-speaking region, with the following core objectives: \
Onboarding 1 Million New Users: Engaging new users through community activities and educational initiatives to expand the Aptos ecosystem. \
5x Increase in On-Chain Activity: Supporting high-quality projects and enhancing user experiences to boost on-chain interactions. \
5x Expansion of Community Influence: Strengthening Aptos’ presence in Web3 communities across the region through extensive online and offline activities. Movemaker’s establishment represents a step forward in the Aptos Foundation’s strategic vision for the Chinese-speaking region. Moving forward, Movemaker will foster open collaboration to create a robust ecosystem network, driving Aptos’ regional and global expansion. Movemaker will prioritize building partnerships in the following areas: Investment Institutions: Collaborating closely with Web2 and Web3 investment institutions to provide funding opportunities for promising projects, enabling rapid growth and bridging traditional capital with the Aptos ecosystem. User and Developer Communities: Continuously empowering and supporting users and developers to further strengthen the Aptos ecosystem. \
Industry Community Leaders and Media:\
Leveraging content creation and brand promotion to enhance Aptos’ recognition and influence in the Chinese-speaking region.\
\
Driving the Integration of On-Chain and Off-Chain Resources with the Aptos Ecosystem to Expand Its Boundaries. \
Movemaker aims to become a key driving force behind Aptos’ ecosystem development in the Chinese-speaking region. Through collaboration and continuous innovation, Movemaker will unlock the full potential of Aptos technology in this market. \
As more partners join the initiative, Movemaker will solidify Aptos’ position at the core of the Chinese-speaking Web3 ecosystem, paving the way for greater growth and success. Interested parties can contact Movemaker at contact@movemaker.xyz and X @MovemakerHQ Ankaa Accelerator, co-launched by Aptos OKX Ventures and ALCOVE, is committed to fostering innovation and helping founders thrive in Web3. Their mission is to support the next generation of Web3 killer consumer-facing applications in Web3 for billions of users.\
They provide unfair advantages to early-staged founders to solve real-world problems empowered by blockchain technologies. For more information, users can visit ankaa.pro or follow on X @AnkaaLabs.BlockBooster is a leading Asian Web3 venture studio. Its mission is to lead the advancement of the Web3 industry through strategic investment and deep incubation of promising Web3 projects.\
BlockBooster aims to empower builders in the space and to be the trusted bridge between Web2 and Web3. For more information, users can visit blockbooster.io or follow on X @0xBlockBooster.Samuel Gu Movemaker contact@movemaker.xyz:::tip
This story was distributed as a release by Chainwire under HackerNoon’s Business Blogging Program. Learn more about the program ]]></content:encoded></item><item><title>Overwrite Bad Image Alt Tags via My Alt Tag Generation App Built with OpenAI API</title><link>https://hackernoon.com/overwrite-bad-image-alt-tags-via-my-alt-tag-generation-app-built-with-openai-api?source=rss</link><author>Danielle Ford</author><category>tech</category><pubDate>Mon, 27 Jan 2025 10:15:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Alt tags are the most well known aspect of building accessible web pages, but they are unfortunately often neglected or poorly implemented. Alt tags are brief text descriptions added to images. Screen readers read the contents of a web page to users, and the image descriptions are what they read to communicate what is in images on the page to visually impaired users, since they can’t see them.\
Unfortunately, it’s common for images to be completely missing any alt tags. I have also seen alt tags misused in ways that make things harder for a visually impaired user, such as tags that just say “picture” or “image,” tags that are cutesy captions an author added without any reference to what’s in the image (ie, an image of a coffee and a laptop on a blogging page, with the caption “dear diary, I’d love to be selected as a guest writer”). I’ve also seen alt tags that include 3 lines of SEO keywords. Can you imagine trying to listen to what’s on a website only to hear “image of picture” or a long list of SEO keywords?\
This is a Chrome extension designed to empower visually impaired users by allowing them to overwrite bad alt tags, and leverage Open AI to insert AI generated descriptions. This allows a visually impaired user to actually access all the content on a webpage that a visually unimpaired user can access (or at least not get slowed down by a long list of SEO keywords).\
If you just want the extension, you are welcome to download this repo and follow the instructions in the README.\n However, if you’re interested in a step-by-step guide of how to build a Chrome extension with OpenAI, following is a walk through.Get a basic ‘Hello World’ Chrome Extension Up and Running\
First, let’s get a basic Chrome boilerplate up and running. Clone this repository and follow the instructions in the README:\
Once you get that up and installed, you should have an image icon in your extension bar (I recommend pinning it to make testing quicker), and when you click on it, should see a popup with “hello world.”\
Let’s open the boilerplate code, and walk through the existing files. This will also cover some Chrome Extension basics:\
 - Every Chrome extension has a manifest.json file. It includes basic information and setup about the extension. In our Manifest file, we have a name, description, a background file set to src/background.js, an icon set to image-icon.png (this is the icon that will show up representing the extension on the extensions menu), and it sets popup.html as the file source for our popup. - A background.js file set up in our manifest. The code in this file will run in the background and monitor for events that trigger functionality in the extension. - Any script that is run in the context of the webpage or modifying the webpage gets put in a content script.src/popup.js, static/popup.css, and static/popup.html - These files control the popup you see when you click the extension icon\
Let’s get some basics set up - open static/manifest.json and change the name and description to “Screen Reader Image Description Generator” (or whatever you’d prefer).Enable interact with web pages using a content scriptOur extension is going to overwrite alt tags on the website the user is on, which means we need access to the page html. The way to do this in Chrome Extensions is via content scripts. Our content script will be in our src/content.js file.\
The simplest way to inject a content script is by adding a “scripts” field to the manifest with a reference to a js file. When you set up a content script this way,, the linked script will be run whenever the extension is loaded. However, in our case, we don’t want our extension to automatically run when a user opens the extension. Some websites have perfectly fine alt tags set on images, so we want to only run the code when the user decides it’s necessary.\
We’re going to add a button in our popup and a console log in our content script, so that when the user clicks the button, the content script is loaded, and we can confirm that by seeing our statement printed in the Chrome console.<button id="generate-alt-tags-button">Generate image descriptions</button>
console.log(‘hello console’)
\
The way to connect that button click on the popup to the content script involves both popup.js and background.js.\
In popup.js, we will grab the button from the DOM and add an event listener. When a user clicks that button, we will send a message signifying the content script should be injected. We’ll name the message “injectContentScript”const generateAltTagButton = document.body.querySelector('#generate-alt-tags-button');

generateAltTagButton.addEventListener('click', async () => {
   chrome.runtime.sendMessage({action: 'injectContentScript'})
});
\
In background.js, we have the code that monitors events and reacts to them. Here, we’re setting up an event listener, and if the message received is ‘injectContentScript’, it will execute the content script in the active tab (the user’s current web page).chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
   if (message.action === 'injectContentScript') {
       chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => {
           chrome.scripting.executeScript({
               target: { tabId: tabs[0].id },
               files: ['content.js']
           });
       });
   }
});
\
The last step to set this up is to add “activeTab” and “scripting” permissions to our manifest. The “scripting” permission is required to run any content script. We also have to add permissions for the pages we inject the script into. In this case, we’ll be injecting the script into the user’s current website, aka their active tab, and that is what the activeTab permission allows."permissions": [
 "activeTab",
 "scripting"
],
\
At this point, you may need to remove the extension from Chrome and reload it for it to run correctly. Once it is running, we should see our console log in our Chrome console.Gathering page images and inserting test alt tagsOur next step is to use our content script file to grab all of the images on the page, so we have that info ready to send in our API calls to get image descriptions. We also want to make sure we’re only making calls for images where it’s helpful to have descriptions. Some images are purely decorative and don’t have any need to slow down screen readers with their descriptions. For example, if you have a search bar that has both the label “search” and a magnifying glass icon. If an image has its alt tag set to an empty string, or has aria-hidden set to true, that means the image does not need to be included in a screen reader, and we can skip generating a description for it.\
So first, in content.js, we will gather all of the images on the page. I’m adding a console.log so I can quickly confirm it’s working correctly:const images = document.querySelectorAll("img");
console.log(images)
\
Then we will loop through the images, and check for images that we need to generate an alt tag for. That includes all images that don’t have an alt tag, and images that have an alt tag that isn’t an empty string, and images that haven’t been explicitly hidden from screen readers with the aria-hidden attribute.for (let image of images) {
   const imageHasAltTag = image.hasAttribute('alt');
   const imageAltTagIsEmptyString = image.hasAttribute('alt') && image.alt === "";
   const isAriaHidden = image.ariaHidden ?? false;
   if (!imageHasAltTag || !imageAltTagIsEmptyString || !isAriaHidden) {
// this is an image we want to generate an alt tag for!
   }
}
\
Then we can add a test string to set the alt tags to, so we can confirm we have a functional way to set them before we move on to our OpenAI calls. Our content.js now looks like:function scanPhotos() {
   const images = document.querySelectorAll("img");
console.log(images)

   for (let image of images) {
       const imageHasAltTag = image.hasAttribute('alt');
       const imageAltTagIsEmptyString = image.hasAttribute('alt') && image.alt === "";
       const isAriaHidden = image.ariaHidden ?? false;
       if (!imageHasAltTag || !imageAltTagIsEmptyString || !isAriaHidden) {

           image.alt = 'Test Alt Text'
       }
   }
}
scanPhotos()
\
At this point, if we open Chrome dev tools Elements, click on an image, we should see “Test Alt Text” set as the alt tag.Install OpenAI and generate image descriptionsIn order to use OpenAI, you will need to generate an OpenAI key and also add credit to your account. To generate an OpenAI key:Follow the instructions to create an OpenAI accountOn the "Make Your First Call" screen, enter a preferred name for your API key, such as "Chrome Image Description Generator Key"Save this key. Also, keep it private - make sure not to push it into any public git repos.\
Now, back in our repo, first we want to install OpenAi. In the terminal inside the project directory, run:\
Now in content.js, we’ll import OpenAI by adding this code at the top of the file, with your OpenAI key pasted in line 1:const openAiSecretKey = 'YOUR_KEY_GOES_HERE'
import OpenAI from "openai";
const openai = new OpenAI({ apiKey: openAiSecretKey, dangerouslyAllowBrowser: true  });
\
“DangerouslyAllowBrowser” allows the call to be made with your key from the browser. Generally, this is an unsafe practice. Since we are only running this project locally, we will leave it like this, rather than set up a back-end retrieval. If you do use OpenAI in other projects, make sure you follow best practices regarding keeping the key secret.\
We have to write our own prompt and also pass in the image’s src URL (more info on AI prompt engineering). You can adapt the prompt as you’d like. I chose to limit the descriptions to 20 works because OpenAI was returning long descriptions. Additionally, I noticed it was fully describing logos like Yelp or Facebook logos (ie, ‘a big blue box with a white lowercase f inside’), which weren’t helpful. In case it’s an infographic, I ask that the word limit is ignored and the full image text is shared.\
Here is the full call, which returns the content of the first AI response and also passes the error into a “handleError” function. I’ve included a console.log of each response so we can get quicker feedback on if the call is successful or not:async function generateDescription(imageSrcUrl) {
   const response = await openai.chat.completions.create({
       model: "gpt-4o-mini",
       messages: [
           {
               role: "user",
               content: [
                   { type: "text", text: "Describe this image in 20 words or less. If the image looks like the logo of a large company, just say the company name and then the word logo. If the image has text, share the text. If the image has text and it is more than 20 words, ignore the earlier instruction to limit the words and share the full text."},
                   {
                       type: "image_url",
                       image_url: {
                           "url": imageSrcUrl,
                       },
                   },
               ],
           },
       ],
   }).catch(handleError);

    console.log(response)
   if (response) {  return response.choices[0].message.content;}
}

function handleError(err) {
   console.log(err);
}
\
We add a call to this function into the if statement we wrote before (we also have to add an async keyword at the beginning of the scanImages function to include this asynchronous call):const imageDescription = await generateDescription(image.src)
if (!imageDescription) {
   return;
}
image.alt = imageDescription
Next, we want to build out our UI so that the user knows what is happening after they click the button to generate the tags. It takes a few seconds for the tags to load, so we want a ‘loading’ message so the user knows it’s working. Additionally, we want to let them know that it’s successful, or if there’s an error. In order to keep things simple, we’ll have a general user message div in the html, and then use popup.js to dynamically insert the appropriate message to the user based on what is happening in the extension.\
The way Chrome extensions are set up, our content script (content.js) is separated from our popup.js, and they aren’t able to share variables the way typical JavaScript files are. The way the content script can let the popup know that the tags are loading, or are successfully loaded, is via message passing. We already used message passing when we let the background worker know to inject the content script when a user clicked on the original button.\
First, in our html, we’ll add a div with the id ‘user-message’ under our button. I’ve added a little more description for the initial message, as well.<div id="user-message">
   <img src="image-icon.png" width="40" class="icon" alt=""/>
   This extension uses OpenAI to generate alternative image descriptions for screen readers.
</div>
\
Then, in our popup.js, we’ll add a listener that listens to any messages sent that may contain an update to the extension state. We’ll also write some html to inject based on whatever state result we get back from the content script.const userMessage = document.body.querySelector('#user-message');

chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
   renderUI(message.action)
   }
);

function renderUI(extensionState) {
   generateAltTagButton.disabled=true;
   if (extensionState === 'loading') {
       userMessage.innerHTML = '<img src="loading-icon.png" width="50" class="icon" alt=""/> New image descriptions are loading... <br> <br>Please wait. We will update you when the descriptions have loaded.'
   } else if (extensionState === 'success') {
       userMessage.innerHTML = '<img src="success-icon.png" width="50" class="icon" alt=""/> New image descriptions have been loaded! <br> <br> If you would like to return to the original image descriptions set by the web page author, please refresh the page.'
   } else if (extensionState === 'errorGeneric') {
       userMessage.innerHTML = '<img src="error-icon.png" width="50" class="icon"alt=""/> There was an error generating new image descriptions. <br> <br> Please refresh the page and try again.'
   } else if (extensionState === 'errorAuthentication') {
       userMessage.innerHTML = '<img src="error-icon.png" width="50" class="icon"alt=""/> There was an error generating new image descriptions. <br> <br> Your OpenAI key is not valid. Please double check your key and try again.'
   } else if (extensionState === 'errorMaxQuota') {
       userMessage.innerHTML = '<img src="error-icon.png" width="50" class="icon"alt=""/> There was an error generating new image descriptions. <br> <br> You\'ve either used up your current OpenAI plan and need to add more credit, or you\'ve made too many requests too quickly. Please check your plan, add funds if needed, or slow down the requests.'
   }
}
\
Within our content script, we’ll define a new variable called ‘extensionState’, which can either be ‘initial’ (the extension is loaded but nothing has happened yet), ‘loading’, ‘success’, or ‘error’ (we will add some other error states as well based on OpenAI error messages). We’ll also update the extension state variable and send a message to popup.js every time the state changes changes.​​let extensionState = 'initial';
\
Our error handler for becomes:function handleError(err) {
 if (JSON.stringify(err).includes('401')) {
   extensionState = 'errorAuthentication'
   chrome.runtime.sendMessage({action: extensionState})
 } else if (JSON.stringify(err).includes('429')) {
   extensionState = 'errorMaxQuota'
   chrome.runtime.sendMessage({action: extensionState})
 } else {
   extensionState = 'errorGeneric'
   chrome.runtime.sendMessage({action: extensionState})
 }
 console.log(err);
}
\
And within our scanPhotos function, we set the state to ‘loading’ at the beginning of the function, and to ‘success’ if it fully runs without errors.async function scanPhotos() {
 extensionState = 'loading'
 chrome.runtime.sendMessage({action: extensionState})
 const images = document.querySelectorAll("img");

 for (let image of images) {
   const imageHasAltTag = image.hasAttribute('alt');
   const imageAltTagIsEmptyString = image.hasAttribute('alt') && image.alt === "";
   const isAriaHidden = image.ariaHidden ?? false;
   if (!imageHasAltTag || !imageAltTagIsEmptyString || !isAriaHidden) {
     const imageDescription = await generateDescription(image.src)
     if (!imageDescription) {
       return;
     }
     image.alt = imageDescription
   }
 }
 extensionState = 'success'
 chrome.runtime.sendMessage({action: extensionState})
}
Fixing confusing popup behavior - persisting extension state when popups close and reopenYou may notice at this point that if you generate alt tags, get a success message, and close and reopen the popup, it will display the initial message prompting the user to generate new alt tags. Even though the generated alt tags are in the code now!\
In Chrome, every time you open an extension popup, it is a brand new popup. It won’t remember anything previously done by the extension, or what is running in the content script. However, we can make sure any newly opened popup is rendering the accurate state of the extension by having it call and check the extension state when it’s opened. To do that, we will have a popup pass another message, this time requesting the extension state, and we’ll add a message listener in our content.js that listens for that message and sends back the current state.chrome.tabs.query({active: true, currentWindow: true}, function(tabs) {
   chrome.tabs.sendMessage(tabs[0].id, {action: "getExtensionState"}, function(response) {
       // if the content script hasn't been injected, then the code in that script hasn't been run, and we'll get an error or no response
       if (chrome.runtime.lastError || !response) {
           return;
       } else if (response) {
           // if the code in content script HAS been injected, we'll get a response which tells us what state the code is at (loading, success, error, etc)
           renderUI(response.extensionState)
       }
   });
});
chrome.runtime.onMessage.addListener(
   function(request, sender, sendResponse) {
     if (request.action === "getExtensionState")
       sendResponse({extensionState});
   });
\
If the content script has never been run (aka the user never clicked the button to generate alt tags), there will be no extension state variable or event listener. In this instance, chrome will return a runtime error in response. So we include a check for an error, and if we receive one, leave the default UI as is.Extension accessibility - aria-live, color contrast, and close buttonThis extension is designed for people who use screen readers, so now we have to make sure it’s actually usable with a screen reader! Now is a good time to turn on your screen reader and see if it all works well.\
There are a few things we want to clean up for accessibility. First of all, we want to make sure all text is a high enough contrast level. For the button, I’ve decided to set the background to #0250C5 and the font to white bold. This has a contrast ratio of 7.1 and is WCAG compliant at both AA and AAA levels. You can check contrast ratios for whatever colors you’d like to use here at the WebAim Contrast Checker.\
Second, when using my screen reader, I notice that the screen reader doesn’t automatically read out the updates when the user message changes to a loading, success, or error message. In order to fix this, we will use an html attribute called aria-live. Aria-live allows developers to let the screen readers know to update users of changes. You can set aria-live to either assertive or polite - if it’s set to assertive, updates will be read immediately, regardless of if there are other items waiting to be read in the screen reader queue. If it’s set to polite, the update will be read at the end of everything the screen reader is in the process of reading. In our case, we want to update the user as soon as possible. So in popup-container, the parent element of our user-message element, we will add that attribute.<div class="popup-container" aria-live="assertive">
\
Last of all, using the screen reader, I’m noticing there isn’t an easy way to close the popup. When using a mouse, you just click anywhere outside of the popup and it closes, but I can’t quite figure out how to close it using the keyboard. So we will add a ‘close’ button at the bottom of the popup, so users can easily close it and return to the web page.<div>
   <button id="close-button">Close</button>
</div>
\
In popup.js, we add the close function to the onclick:const closeButton = document.body.querySelector('#close-button');

closeButton.addEventListener('click', async () => {
   window.close()
});
\
And that’s it! If you have any questions or suggestions, please reach out.]]></content:encoded></item><item><title>Researchers Invent Lightning-Fast AI Boost for Small, Complex Datasets</title><link>https://hackernoon.com/researchers-invent-lightning-fast-ai-boost-for-small-complex-datasets?source=rss</link><author>Procrustes</author><category>tech</category><pubDate>Mon, 27 Jan 2025 10:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sergey Kucheryavskiy, Department of Chemistry and Bioscience, Aalborg University and a Corresponding author (svk@bio.aau.dk);(2) Sergei Zhilin, CSort, LLC., Germana Titova st. 7, Barnaul, 656023, Russia and Contributing authors0 (szhilin@gmail.com).:::tip
Editor's note: This is Part 4 of 4 of a study detailing a new method for the augmentation of numeric and mixed datasets. Read the rest below.The experimental results confirm the benefits of PV-set augmentation, however optimization of ANN learning parameters is needed to make the benefits significant. At the same time, optimization of the PV-set generation algorithm is not necessary for\
most of the cases. Based on our experiments, we advise using cross-validation resampling with 5 or 10 splits and a number of latent variables large enough to capture the majority of variation in X. In some specific cases one can use tools for quality control of generated PV-sets described in [10].\
It must also be noted that the use of PV-sets for data augmentation is not always beneficial. Thus, according to our experiments, which are not reported here, in methods that are robust to overfitting, such as, for example, random forest (RF), increasing the training set artificially does not have a significant effect on the model performance. In the case of eXtreme Gradient Boosting changing training parameters, which regulate the overfitting, such as a learning rate, maximum depth and minimum sum of instance weight, can have an effect, but most of the time the effect observed in our experiments was marginal.This paper proposes a new method for data augmentation. The method is beneficial specifically for datasets with moderate to high degree of collinearity as it directly utilizes this feature in the generation algorithm.\
Two proposed implementations of the method (SVD and PLS based) cover most of the common data analysis tasks, such as regression, discrimination and one-class classification (authentication). Both implementations are very fast — the generation of a PV-set for X of 200×500 with 20 latent variables and 10 segments splits requires several seconds (less than a second on a powerful PC), much less than the training of an ANN model with several layers.\
The method can work with datasets of small size (from tens observations) and can be used for both numeric and mixed datasets, where one or several variables are categorical.1] Ratner, A. J., Ehrenberg, H. R., Hussain, Z., Dunnmon, J. & R´e, C. Learning to compose domain-specific transformations for data augmentation (2017). 1709. 01643.\
[2] Goodfellow, I. J. et al. Generative adversarial networks (2014). 1406.2661.\
[3] Dao, T. et al. A kernel theory of modern data augmentation (2019). 1803.06084.\
[4] Perez, E. & Ventura, S. Progressive growing of generative adversarial networks for improving data augmentation and skin cancer diagnosis. Artificial Intelligence in Medicine 141, 102556 (2023). URL https://www.sciencedirect.com/science/ article/pii/S0933365723000702.\
[5] Perez, F., Vasconcelos, C., Avila, S. & Valle, E. Stoyanov, D. et al. (eds) Data augmentation for skin lesion analysis. (eds Stoyanov, D. et al.) OR 2.0 Context-Aware Operating Theaters, Computer Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image Analysis, 303–311 (Springer International Publishing, Cham, 2018).\
[6] Iglesias, G., Talavera, E., Gonzalez-Prieto, A., Mozo, A. & Gomez-Canaval, S. Data augmentation techniques in time series domain: a survey and taxonomy. Neural Computing and Applications 35, 10123–10145 (2023). URL https://doi. org/10.1007/s00521-023-08459-3.\
[7] Saiz-Abajo, M., Mevik, B.-H., Segtnan, V. & Næs, T. Ensemble methods and data augmentation by noise addition applied to the analysis of spectroscopic data. Analytica Chimica Acta 533, 147–159 (2005). URL https://www.sciencedirect. com/science/article/pii/S000326700401428X.\
[8] Chadebec, C. & Allassonniere, S. Data augmentation with variational autoencoders and manifold sampling (2021). 2103.13751.\
[9] Kucheryavskiy, S., Zhilin, S., Rodionova, O. & Pomerantsev, A. Procrustes cross-validation—a bridge between cross-validation and independent validation sets. Analytical Chemistry 92, 11842–11850 (2020).\
[10] Kucheryavskiy, S., Rodionova, O. & Pomerantsev, A. Procrustes cross-validation of multivariate regression models. Analytica Chimica Acta 1255, 341096 (2023). URL https://www.sciencedirect.com/science/article/pii/S0003267023003173.\
[11] de Jong, S. Simpls: An alternative approach to partial least squares regression. Chemometrics and Intelligent Laboratory Systems 18, 251–263 (1993).\
[12] Paszke, A. et al. in Pytorch: An imperative style, highperformance deep learning library 8024–8035 (Curran Associates, Inc., 2019). URL http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.\
[13] Harris, C. R. et al. Array programming with NumPy. Nature 585, 357–362 (2020). URL https://doi.org/10.1038/s41586-020-2649-2.\
[14] Borggaard, C. & Thodberg, H. H. Optimal minimal neural interpretation of spectra. Analytical Chemistry 64, 545–551 (1992).\
[15] Detrano, R. et al. International application of a new probability algorithm for the diagnosis of coronary artery disease. The American Journal of Cardiology 64, 304–310 (1989). URL https://www.sciencedirect.com/science/article/ pii/0002914989905249.\
[16] Detrano, R. et al. Bayesian probability analysis: a prospective demonstration of its clinical utility in diagnosing coronary disease. Circulation 69, 541–547 (1984).\
[17] Janosi, A., Steinbrunn, W., Pfisterer, M. & Detrano, R. Heart Disease Dataset (1988).]]></content:encoded></item><item><title>How Meat Fat and Heart Data Prove AI Gets Smarter with the Right Boost</title><link>https://hackernoon.com/how-meat-fat-and-heart-data-prove-ai-gets-smarter-with-the-right-boost?source=rss</link><author>Procrustes</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:45:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sergey Kucheryavskiy, Department of Chemistry and Bioscience, Aalborg University and a Corresponding author (svk@bio.aau.dk);(2) Sergei Zhilin, CSort, LLC., Germana Titova st. 7, Barnaul, 656023, Russia and Contributing authors0 (szhilin@gmail.com).:::tip
Editor's note: This is Part 3 of 4 of a study detailing a new method for the augmentation of numeric and mixed datasets. Read the rest below.Two datasets were selected to demonstrate the capabilities of PV-sets to be used for data augmentation. Since both SVD and PLS decompositions model the variance-covariance structure of X, the proposed augmentation approach will work best for datasets with moderate to high degree of collinearity, which was taken into account for the selection of the datasets.\
It must be noted, that optimization of the ANN architecture was outside the scope of this work. In both examples a very simple architecture with several layers was used.\
All calculations were carried out using Python 3.10.12 supplemented with packages PyTorch[12] 2.0.1, NumPy[13] 1.26.0 and prcv 1.1.0. Statistical analysis of the results was performed in R 4.3.1.\
Python scripts used to obtain the results, presented in this chapter, are freely available so everyone can reproduce the reported results.The  dataset consists of spectroscopic measurements of finely minced meat samples with different moisture, protein and fat content. The measurements were taken by Tecator Infratec Food and Feed Analyzer working by the Near Infrared Transmission (NIT) principle. The dataset was downloaded from the StatLib public dataset archive (http://lib.stat.cmu.edu/datasets/).\
The dataset contains the spectra and the response values for 215 meat samples in total (subsets labeled as C, M and T in the archive), of which 170 samples are used as the training set and 45 are used as the independent test set to assess the performance of the final optimized models. Fat content (as % w/w) was selected as the response variable for this research.\
The matrix of predictors X contains absorbance values measured at 100 wavelengths in the range of 850–1050 nm.\
According to the Beer-Lambert law, there is a linear relationship between the concentration of chemical components and the absorbance of the electromagnetic radiation. Therefore, the prediction of fat content can in theory be performed by fitting the dataset with a multiple linear regression model. However, the shape of the real spectra suffers from various side effects, including noise, light scattering from an uneven surface of the samples, and many others.\
Figure 1 shows the spectra from the training set in the form of line plots colored according to the corresponding fat content using a color gradient from blue (low content) to red (high content) colors. As one can see, there is no clear association between the shape of the spectra and the response value.\
Handling such data necessitates the careful selection and optimization of preprocessing methods to eliminate undesirable effects and reveal the information of interest. However, the use of nonlinear models, such as artificial neural networks, has the capability to automatically unveil the needed information [14], bypassing the need for extensive preprocessing steps.\
Nonlinearity is not the sole factor in play. Preprocessing can be considered as a feature extraction procedure. It is known that in networks with many hidden layers, the initial layers act as feature extractors. The greater the number of layers, the more intricate features can be derived from raw data. However, it is worth noting that such models demand a substantial number of measurements or observations to achieve satisfactory performance.\
The  dataset will be employed for a thorough investigation of how the use of PV-set based data augmentation can attack the problem and how the two main parameters of the PV-set generation procedure (number of latent variables, number of segments) as well as the number of generated PV-sets influence the performance of the ANN regression models.\
The  dataset came from a study of 303 patients referred for coronary angiography at the Cleveland Clinic. The patients were examined by a set of tests, including physical examination, electrocardiogram at rest, serum cholesterol determination and fasting blood sugar determination. The dataset consists of the examination results combined with historical records. More details about the data can be found elsewhere [15] [16]. The dataset is publicly available from the UC Irvine Machine Learning Repository [17]. The original data include records from several hospitals, in this research only data from the Cleveland Clinic are used.\
Eleven records with missing values were removed from the original data, resulting in 292 rows. The dataset consists of 14 numeric and categorical variables, the overview is given in Table 1. The  variable is used as a response, and the rest are attributed to predictors\
This dataset is used to demonstrate that PV-set augmentation can also be applied to mixed datasets with categorical variables, and to show how SVD and PLS versions work for solving binary classification tasks.3.2 ANN regression of Tecator dataAs with most of the spectroscopic measurements, the  data are highly collinear, and SVD decomposition of the matrix with spectra resulted in the first eigenvalue of 25.6, while starting from the 6th all eigenvalues are significantly below 0.001. PLS decomposition of the training set resulted in similar outcomes, therefore, one can assume that using 5-6 latent variables is enough to capture the majority of the predictors’ variation.\
At the same time, preliminary investigation has shown that ck/c values are within the desired limit of [0, 2] for all latent variables up to A = 50. Thus, using any number of latent variables between 5 and 50 will produce a reasonable PV-set.\
To predict fat content a simple ANN model was employed. The model consisted of six fully connected (linear) layers of the following sizes: (100, 150), (150, 200), (200, 150), (150, 100), (100, 50), and (50, 1). The first value in the parenthesis denotes the number of inputs and the second value denotes the number of outputs in each layer. The first five layers were supplemented with the ReLU activation function, while the last layer was used as the output of the model. All other characteristics of the model are shown in Table 2.\
The model contains approximately 95000 tunable parameters in total, which is much larger than the number of samples in the training set (170). However, as already noted, first several hidden layers of ANN usually serve as feature extractors, so not all parameters will be a part of the regression model itself.\
The columns of the predictors were standardized using mean and standard deviation values computed for the original training set. The response values were mean centered only to obtain errors directly comparable in the original units (%w/w).\
First, the ANN training procedure was run 30 times using the original training set without augmentation. Repeated experiments are necessary because the ANN training procedure is not deterministic, as explained in the introduction, and the performance varies from model to model.\
After that, a full factorial experiment was set up to determine how the augmentation of the training data with PV-sets influences the performance of the ANN model and which parameters for PV-set generation as well as the number of generated sets have the largest effect on the performance. The PV-sets were generated using PLS decomposition. The generation parameters are shown in Table 3.\
All possible combinations of the values from the table were tested (60 combinations in total). For every combination the training/test procedure was repeated 5 times with full reinitialization, resulting in 300 outcomes.\
Figure 2 graphically illustrates the outcomes of the experiment in the form of boxplots. Every plot shows the variation in the RMSEP for a given value of one of the three tested parameters. The black point inside each box supplemented with text shows the corresponding median value.\
It is clear that the number of PV-sets used for data augmentation (first plot in the figure) has the largest effect on the RMSEP. Thus, augmenting the original training set with one PV-set reduces the median test set error by approximately 18% (from 4.17 to 3.41 %w/w). Adding five PV-sets reduces the median RMSEP down to 1.58 %w/w (62% reduction). Using 10 PV-sets further reduces the RMSEP down to 1.36 %w/w; however, the effect is smaller.\
Changing the number of latent variables does not give a significant difference in the performance of the models, while the number of segments in cross-validation resampling shows a small effect. The best model was obtained by using data augmented with 20 PV-sets generated with 10 latent variables and 10 segments. It could predict fat content in the test set with RMSEP = 0.94 %w/w (R2 = 0.995) which is more than 3 times smaller compared to the best model obtained without augmentation.\
Statistical analysis of the outcomes carried out by N-way analysis of variance (ANOVA) and Tukey’s honest significance difference (HSD) test confirmed the significant difference between average RMSE values obtained using different numbers of PV-sets (ANOVA p-value ≪ 0.01). However, pairwise comparison shows no significant difference between 10 vs. 20, 10 vs. 50, and 20 vs 50 PV-sets (p-value > 0.20 in for all three pairs).\
The number of latent variables has no significant effect (ANOVA p-value ≈ 0.71). The number of segments shows only a significant difference between 2 segments and other choices, favoring the use of 4 segments or above (p-values for all pairs with 2 segments ≪ 0.01, for 4 segments vs. 10 segments p-value ≈ 0.26)\
Optimization of the ANN learning parameters (by varying the learning rate and the batch size) toward reducing RMSEP values for the models trained without augmentation, made this gap smaller. Figure 3 shows the results obtained using a learning rate = 0.001. Training the model without augmentation results with median RMSEP = 2.84 %w/w. Training the model on the augmented data with 20 PV-sets resulted in a median RMSEP = 1.80 %w/w. This is still approximately 37% smaller; however, the overall performance of the model trained with this learning rate is worse.\
This effect is understandable as changing the learning rate and batch size to improve the performance of the model trained on the original data makes the model less sensitive to overfitting and local minima problems but also less flexible, which, in turn, makes the use of augmented data less efficient.\
Figure 4 further illustrates this effect. The plot shows the variation in RMSEP values for ANN models trained using different learning rates. Each learning rate is represented by a pair of box and whiskers series. The left (blue) series in each pair illustrates the results obtained using 30 models trained with the original data. The right (red) series in each pair shows the results for 30 models trained using the augmented data (20 PV-sets computed using 10 LVs and 4 segments). New PV-sets were computed at each run to eliminate possible random effects.\
This experiment was also repeated using other ANN architectures, including networks with convolution layers. The gap between the performance varied depending on the architecture and the learning parameters; however, in all experiments, a minimum of 20% remained, clearly indicating the benefits of the PV-set augmentation in this particular case.3.3 ANN classification of Heart dataAll categorical variables from the Heart dataset with L levels were converted to L − 1 dummy values [0, 1], so the matrix with predictors, X contained 17 columns in total. The columns of the matrix were mean centered and standardized. SVD decomposition of the matrix indicates moderate collinearity with eigenvalues ranging from 3.43 to 0.77 for the first 10 latent variables.\
ANN model for classification of the patient conditions used in this research included six linear layers and one sigmoid layer. The first five layers were supplemented with the ReLU activation function, while the last layer was used as the output of the model. The main characteristics of the model are shown in Table 4. The model has approximately 14700 tunable parameters, while the original dataset has only 292 objects.\
Since this dataset does not contain a dedicated test set, the following procedure was employed. At each run, the data objects were split into two groups with healthy persons in one group and sick persons in the other. Then, 75% of records were selected randomly from each group and merged together to form a training set. The remaining 25% of records formed the test set for the run. Classification accuracy, which was computed as a ratio of all correctly classified records to the total number of records in the test set, was used as the performance indicator.\
The experimental design was similar to the  experiments. First, the training/test procedure was repeated 30 times using the data without augmentation. Then the ANN models were trained using the augmented data and tested using the randomly selected test set. The PV-sets for augmentation were computed using an algorithm based on PLS decomposition applied to the randomly selected training set. As in the previous chapter, all possible combinations of PV-set generation parameters from Table 2 were tested with 5 replications (300 runs in total).\
Figure 5 presents the outcomes of the experiment in the form of boxplots, which show a variation of the test set accuracy computed at each run. One can clearly notice that the accuracy of the models trained on the data without augmentation is very low (median accuracy is 0.50), while augmenting the training data with 20 PV-sets raises the median accuracy to 0.84. The best model obtained in these experiments is also trained on the augmented data and has an accuracy of 0.91.\
N-way ANOVA and Tukey’s tests for the outcomes have shown that two parameters — number of PV-sets as well as number of segments in cross-validation resampling have a significant influence on the accuracy (p-value for both factors was ≪ 0.001 in ANOVA test). However, a significant difference was observed only for the number of segments equal to two, similar to the Tecator results. Using four or more segments for the generation of PV-sets shows statistically similar performance to the models trained on the augmented data.\
The results are very similar to those obtained using the PLS-based algorithm. However, in this case, the overall performance of the models trained on the augmented data was a slightly higher, with a median accuracy of 0.84 for the models trained with on data augmented with 10, 20 and 50 PV-sets. The best model had an accuracy of 0.95.\
Reducing the learning rate down to 0.001 had the same effect as for the data augmented using the PLS-based algorithm — the gap between the models trained with and without augmented data is eliminated however the overall performance also gets lower with, median a accuracy of approximately 0.79.]]></content:encoded></item><item><title>Starknet: Unlocking Better Performance With Cairo-Native Execution</title><link>https://hackernoon.com/starknet-unlocking-better-performance-with-cairo-native-execution?source=rss</link><author>2077 Research</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:41:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Starknet's Cairo-native execution utilizes its custom programming language, Cairo, to boost scalability, cut costs, and improve smart contract performance. However, challenges arise from the need for broader developer adoption, tooling compatibility, and balancing innovation with accessibility across the ecosystem.Starknet is an Ethereum Layer 2 (L2) protocol that leverages cryptographic STARK (Succinct Transparent Arguments of Knowledge) proofs to power verifiable computation at scale. Like other L2 blockchains, Starknet is able to offer meaningful scalability and security for L2 transactions while preserving decentralization.\
At the core of Starknet lies the Cairo virtual machine (VM), a purpose-built execution environment designed to leverage the cryptographic proof capabilities of STARKs. Unlike other VMs, Starknet’s Cairo VM is designed to run provable programs—which reduces the overhead associated with generating proofs to verify computation. This allows Starknet to process a vast number of transactions while maintaining trustless verification.\
That said, the true measure of Starknet’s performance—as experienced by end-users—depends on the speed at which its sequencer processes transactions and confirms execution results. Also known as “soft confirmations”, the sequencer’s confirmation of transaction success (or failure) are useful for time-sensitive users who want to avoid waiting for transactions to finalize on Ethereum L1 before knowing a transaction’s status.\
As it stands today, Starknet’s transaction throughput relies heavily on the efficiency of the sequencer. The sequencer is responsible for receiving, organizing, and proving transactions before submitting them to Ethereum for final verification. Although Starknet uses a single sequencer currently, it plans to decentralize the sequencer for better censorship resistance and higher failure tolerance (one offline sequencer cannot take down the network).\
On October 29, Starknet recorded an average of 127 transactions per second (TPS) over a continuous 24-hour period. This was not just a one-time peak but a sustained throughput milestone achieved during normal operations, showcasing the rollup's ability to handle high transaction volumes under real-world conditions. Alongside this achievement, Starknet’s performance stands out for its lightning-fast transaction confirmations (averaging under two seconds) and ultra-low fees ($0.002 per transaction)—making it a cost-effective and efficient solution for scaling Ethereum.\
Maintaining this level of performance requires constant innovation, however, especially as user demand grows. This is where Starknet’s upcoming enhancements to its execution model come into play, introducing optimizations that promise not only faster transaction processing but also more efficient resource usage. These developments mark a critical step toward elevating Starknet’s execution environment to meet the demands of a rapidly evolving ecosystem.\
Starknet has already introduced transformative upgrades to its architecture to improve performance and bolster UX (e.g., parallel execution and block-packing via Bolt upgrade). These innovations have drastically reduced transaction processing times and gas costs by enabling the sequencer to handle transactions more efficiently. To learn more about the Bolt upgrade and how it improves Starknet’s scalability, read our Starknet Bolt explainer.\
Building on this foundation, Starknet is now focusing on further optimizing its execution environment with Cairo-Native. The upcoming enhancements will redefine how transactions are processed, transitioning from virtual machine-based emulation to native execution. These changes promise significant improvements in processing speed and resource efficiency, ensuring that Starknet can meet the growing demands of users.\
In the following section, we’ll take a closer look at Starknet’s execution environment and dive deeper into how the Cairo VM and the broader architecture are being reimagined to support the next phase of scalability and efficiency. Let’s dive in!The execution environment of Starknet has evolved significantly over time to improve performance and ensure safety. Initially, Starknet relied on a straightforward pipeline where high-level Cairo code was compiled directly into CASM (Cairo Assembly) for execution. While this approach worked, it left the network vulnerable to inefficiencies and security risks. For example, invalid code could not be proven, and the sequencer was exposed to potential DoS attacks due to the lack of safeguards in gas accounting.\
To address these issues, the Cairo 1.0 upgrade introduced Sierra, an intermediate representation (IR) that enforces safety by design. Inspired by languages like Rust, Sierra ensures developers follow stricter coding patterns, effectively eliminating runtime errors and enabling accurate gas accounting. This upgrade is already live on Starknet’s mainnet and marks a crucial step toward a more robust and secure execution environment.\
Cairo-Native builds on this progress and further enhances Starknet’s performance by enabling native execution. Native execution is a departure from the previous pattern of executing programs in the Cairo VM and represents the next leap forward in terms of performance for Starknet.How does Cairo-Native execution work?Previously, the Starknet sequencer executed programs using a virtual machine that emulated code, which introduced significant overhead and slowed down transaction processing. Emulation in this context involves programming the CPU to behave like a theoretical Cairo CPU, interpreting instructions step by step. While functional, this process is inherently inefficient because it prevents the CPU from fully utilizing its native capabilities and execution styles.\
In contrast, native execution eliminates this awkwardness, allowing programs to run directly on the operating system in the CPU’s native instruction set. This change dramatically improves execution speed and resource efficiency, enabling Starknet to process transactions faster than ever.\
Cairo-Native takes Sierra programs and organizes them into an intermediate form, MLIR, that also allows for sophisticated optimizations, while LLVM further compiles this into native executables tailored for the underlying operating system. This dual-layer compilation process ensures that the generated executables are fast, efficient, and ready for direct execution without any additional overhead.\
The impact of this upgrade on the sequencer’s performance (and the network in general) is profound:Increased speed: By removing the need for VM emulation, native execution allows the sequencer to process transactions significantly faster, reducing latency across the network.Better resource utilization: The optimized executables generated by LLVM make better use of system resources, enabling the sequencer to handle higher transaction volumes.Lower costs: While faster execution alone doesn't immediately translate to reduced gas fees due to the absence of a fee mechanism, Starknet continues to demonstrate cost efficiency. For instance, during Starknet's record-breaking performance, gas fees averaged around $0.002 per transaction. The 0.13.4 update, which introduces Cairo-Native, also includes pricing-related improvements. However, these improvements are not directly related to Cairo-Native itself. While the full potential of Cairo-Native’s scalability on transaction fees has yet to be realized, combining this scalability with future fee-related enhancements positions Starknet as a highly affordable and scalable solution for Ethereum.Enhanced innovation: Performance gains from native execution enables more complex and dynamic applications to thrive on Starknet, opening the door to innovative use cases and opportunities for developers.\
Although native execution brings significant speed improvements, a natural concern arises for developers regarding the dual-execution workflow between the sequencer and the prover. While the sequencer benefits from running transactions using native artifacts for speed, the prover still relies on the Cairo VM to execute transactions step-by-step, ensuring they can be proven to Ethereum’s STARK verifier.\
CASM is still used in this process, as it is the representation validated by the STARK verifier on Ethereum. This duality—executing the same transactions through two different systems—can raise valid questions about consistency and correctness. Specifically, how can we be certain that the outcomes from native execution and Cairo VM execution will always match?\
The short answer lies in rigorous testing and validation. Specifically, the LambdaClass team conducted extensive replay tests, a process where historical Starknet blocks are re-executed under the new system to ensure every transaction produces the same results.\
Replay tests are a critical method for verifying that even with two distinct execution paths, the outputs remain consistent. Thus developers can rest assured that this extensive validation process significantly minimizes the risk of discrepancies.\
Additionally, it’s worth noting that Starknet has always operated with two separate execution flows. Even before Cairo-Native, the sequencer did not fully rely on the Cairo VM for tasks like syscalls or state access. These were already handled differently, with the prover focusing on producing cryptographic proofs to update Starknet’s state on Ethereum.\
In this sense, the introduction of native execution represents an evolution—not a departure—from Starknet’s established architecture. This also means developers should have few issues designing applications to work with Starknet’s new execution model.\
Overall, Cairo-Native unlocks a new level of scalability and speed for Starknet. By transitioning from VM-based emulation to native execution, the network can support more users, handle larger transaction volumes, and deliver a seamless experience for developers and end-users alike. This transformation solidifies Starknet’s position as a leading L2 solution and paves way for greater innovation in the Ethereum ecosystem.Cairo-Native marks a groundbreaking improvement in Starknet’s execution environment—one that delivers significant speedups over the traditional Cairo VM. Through extensive benchmarks conducted by LambdaClass and Nethermind, the performance gains from native execution have been quantified, highlighting Cairo-Native’s ability to drastically enhance transaction and contract processing speeds.\
The benchmarks revealed that Cairo-Native achieves an average speedup of 5x over the Cairo VM, with performance gains ranging from 1.5x to 20x, depending on the specific contract. These results are more than just numbers—they illustrate how native execution directly benefits users and developers.\
For example, a simple STRK transfer between two Argent accounts not only completed 2.3x faster but also showed 4.8x speedup in ERC-20 contract logic. For users, this means lower waiting times for transactions and smoother interactions with decentralized applications (dApps). For developers, these improvements ensure their applications perform faster and more reliably, even under heavy network load.\
In more demanding scenarios like STRK/ETH swaps or gaming applications such as Influenceth, the benefits are even more apparent. Influenceth transactions recorded a 10.5x transaction speedup, with some contract executions improving by as much as 70x. These advancements empower developers to create complex, high-performance decentralized applications, paving the way for richer gaming experiences, advanced DeFi protocols, and other computationally intensive use cases​\
The transition from emulation to native execution underpins these advancements. By compiling programs directly into machine-native code, Cairo-Native eliminates the inefficiencies of emulating Cairo instructions, where the CPU essentially mimics another virtual CPU. Native execution allows the hardware to operate at full capacity, delivering unmatched performance improvements across all transaction types.\
Cairo-Native introduces a more advanced compilation pipeline, adding several steps to translate Sierra code into native executables. This includes processes such as converting Sierra to MLIR, optimizing the code, and finally compiling it into machine-native instructions. While this more complex pipeline might intuitively seem like it would increase overall compilation time, benchmarks show that the additional steps contribute to less than 15% of total compilation time, demonstrating that the process remains efficient.\
It’s also worth noting that native executables are larger in size compared to CASM artifacts. This increase reflects the added detail and optimization required for machine-native code but does not affect network performance. Importantly, compilation is only performed once, at the time the contract is deployed on the network, ensuring that this step does not impact the real-time performance of the sequencer​What are the implications of Cairo-native execution for Starknet?The implications of Cairo-Native for Starknet’s ecosystem are far-reaching. By enabling faster, more efficient transaction execution, this upgrade is set to massively enhance the user experience for all applications on the network. With the network projected to handle nearly 1000 transactions per second (TPS) by the end of 2024, Starknet will become one of the most scalable and responsive L2 chains in the Ethereum ecosystem.\
For users, the Cairo-Native upgrade means quicker transaction confirmation and even lower costs. During Starknet’s record-breaking performance earlier this year, gas fees averaged around $0.002 per transaction. With Cairo-Native, these costs are expected to decrease even further, solidifying Starknet as one of the most affordable Layer-2 networks.\
The introduction of Cairo-Native execution will also supercharge the following use cases:Onchain gaming applicationsStarknet has already gained traction as a platform for blockchain-based games, and Cairo-Native takes this to the next level. Games like Influenceth, which rely on intricate logic and fast-paced interactions, will benefit significantly from the speed improvements. Faster execution times reduce in-game latency, enabling smoother experiences for players.\
Developers can also introduce more complex game mechanics, such as real-time multiplayer capabilities and dynamic NFT interactions, without worrying about performance bottlenecks. This can bring about a new class of onchain gaming applications whose gameplay can rival that of traditional competitors.Decentralized social media platforms demand high transaction throughput and low latency to ensure seamless user interactions, such as posting, commenting, and voting. With Cairo-Native, these platforms can scale to support millions of users, offering real-time feeds and instantaneous reactions, rivaling the performance of traditional Web2 counterparts. Lower gas costs further enhance accessibility, making decentralized social media a more viable alternative to centralized platforms and preparing it for mainstream adoption.Decentralized finance (DeFi)The DeFi ecosystem on Starknet will also reap the benefits of Cairo-Native. Complex financial operations like swaps, lending, and yield farming require high throughput and precision. Faster execution times ensure that trades and liquidations happen in real-time, minimizing slippage and providing users with better outcomes. Additionally, lower gas fees enable micro-transactions, making DeFi protocols more accessible to a broader audience.Cairo-Native execution is a critical milestone in the Starknet roadmap. By introducing a new level of execution, Starknet not only solidifies its position as a leading Layer-2 platform but also sets a precedent for the broader Ethereum rollup ecosystem. Starknet’s innovation showcases how rollups can achieve unparalleled scalability and efficiency, helping Ethereum move closer to its vision of mass adoption while maintaining its principles of decentralization and security.\
Cairo-Native provides developers with a robust, high-performance environment that supports more dynamic and innovative applications. By reducing execution times and resource overhead, developers can focus on creating richer, more interactive experiences that were previously constrained by network limitations.\
As noted in a tweet, there is still potential for further optimization—meaning Cairo-Native’s performance may continue to improve over time. In this sense, the upgrade to Cairo-Native execution represents a crucial step toward realizing Starknet’s full potential as a fast, scalable, and developer-friendly platform and advancing the capabilities of rollup technology across Ethereum’s ecosystem.A version of this article was originally published here.]]></content:encoded></item><item><title>TalkTalk investigating data breach after hacker claims theft of customer data</title><link>https://techcrunch.com/2025/01/27/talktalk-investigating-data-breach-after-hacker-claims-theft-of-customer-data/</link><author>Carly Page</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:38:53 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A hacker claims to be selling the data of 18.8 million TalkTalk customers, but the telecoms giant says this figure is ‘significantly overstated’© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>INE Security Alert: Expediting CMMC 2.0 Compliance</title><link>https://hackernoon.com/ine-security-alert-expediting-cmmc-20-compliance?source=rss</link><author>CyberNewswire</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:30:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[CARY, North Carolina, January 26th, 2025/CyberNewsWire/--INE Security, a leading global provider of cybersecurity training and certifications, today announced a new initiative designed to accelerate compliance with the Department of Defense's (DoD) newly streamlined \
This initiative aims to assist Defense Industry Base (DIB) contractors in swiftly adapting to the updated certification standards, which are critical to securing and maintaining defense contracts.\
With the DoD's reduction of CMMC levels from five to three, the path to compliance has become more direct but not less demanding. Recognizing the urgency for contractors to comply without delay, INE Security is offering a guide to strategic compliance acceleration. This includes a comprehensive checklist and guidance on how to implement the compliance requirements. \
“The DoD’s updated framework requires greater clarity and speed in the compliance process than ever before,” said Dara Warn, CEO of . “At INE Security, we recognize the challenges organizations face in navigating the complexities of CMMC compliance. Our goal is to empower organizations to not only meet but exceed their compliance objectives by providing them with the tools and strategies needed for a faster and smoother journey. We are committed to simplifying the path to compliance, enabling our clients to focus on what they do best: securing their operations and contributing to our national defense.”Certification RequirementsEach level carries its own stringent requirements, ranging from broad in scope at Level 1 to highly specialized at Level 3. Organizations can use this checklist to track progress and identify areas requiring attention before assessment. Level 1 Certification RequirementsBasic password managementAccess control implementationInformation integrity checksBasic endpoint protectionAccess control documentationBasic security proceduresSelf-assessment documentationLevel 2 Certification RequirementsMulti-factor authenticationSecurity monitoring toolsIncident response capabilitiesSystem Security Plan (SSP)Configuration management plansIncident response proceduresRisk assessment documentationThird-party assessment readinessStaff interview preparationControl validation testingLevel 3 Certification RequirementsAdvanced threat detectionZero-trust implementationThreat modeling documentationAdvanced security proceduresRisk management frameworkContinuous monitoring planGovernment assessment readinessAdvanced evidence compilationPersonnel training recordsProgram effectiveness metricsSuccessfully navigating the compliance requirements of CMMC 2.0 demands a structured approach to implementation and preparation. Each step, from initial technical review to mock assessments, is designed to build upon the previous, ensuring a seamless path to CMMC certification. Technical Control ImplementationReviewing current architectureIdentifying gaps in controlsDeveloping implementation planTesting controls in stagingDocumentation Best PracticesIncluding revision historyMaintaining clear proceduresDocumenting configurationsINE Security's  provides hands-on experience through practical labs focused on control implementation and security tool configuration. Structured learning paths cover essential skills in network security implementation and monitoring system setup, giving users real-world experience with the tools and techniques required for CMMC compliance. is the premier provider of online networking and cybersecurity training and certification. Harnessing a powerful hands-on lab platform, cutting-edge technology, a global video distribution network, and world-class instructors, INE Security is the top training choice for Fortune 500 companies worldwide for cybersecurity training in business and for IT professionals looking to advance their careers.\
INE Security’s suite of learning paths offers an incomparable depth of expertise across cybersecurity and is committed to delivering advanced technical training while also lowering the barriers worldwide for those looking to enter and excel in an IT career.]]></content:encoded></item><item><title>This Smart Method Makes Small Datasets Punch Above Their Weight</title><link>https://hackernoon.com/this-smart-method-makes-small-datasets-punch-above-their-weight?source=rss</link><author>Procrustes</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:30:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sergey Kucheryavskiy, Department of Chemistry and Bioscience, Aalborg University and a Corresponding author (svk@bio.aau.dk);(2) Sergei Zhilin, CSort, LLC., Germana Titova st. 7, Barnaul, 656023, Russia and Contributing authors0 (szhilin@gmail.com).:::tip
Editor's note: This is Part 2 of 4 of a study detailing a new method for the augmentation of numeric and mixed datasets. Read the rest below.This chapter presents the general idea behind the Procrustes cross-validation approach, provides the necessary mathematical background and describes two implementations of the approach in detail.\
Let {X, Y} be two matrices of size I × J and I × M correspondingly, representing the original training set to be augmented. Columns of matrix X are predictors — variables measured or observed directly, for example, light absorbance at different wavelengths in the case of spectra, abundance of operational taxonomic units (OTU) in the case of 16S RNA sequencing, results of clinical tests for patients etc. If the original dataset has qualitative predictors, they should be replaced by corresponding dummy variables, which are combined with quantitative predictors to form X.\
Columns of matrix Y are responses — variables whose values should be predicted. There are situations when matrix Y is obsolete, for example, in the case of developing of one class classifier, only matrix with predictors is needed.\
To augment the data a specific model should be developed. It aims at capturing the variance-covariance structure of X, or its part, directly related to the variance of values in Y. This model will be referred to as the PV-model and denoted as M.\
The selection of the methods for creating the PV-model depends on the objectives. Thus, if data augmentation is needed for developing a one class classifier, when the matrix with responses, Y, is obsolete, we suggest using singular value decomposition (SVD), which is efficient in capturing the whole variance-covariance structure of X.\
In the case of augmenting data for discrimination models, there can be two solutions. One solution is to consider each class as an independent population and use SVD decomposition to capture the variance-covariance structure of X within each class. The second solution will be to create a matrix with dummy predictors, Y, where each column corresponds to a particular class, and employ PLS decomposition. More details about using SVD and PLS decompositions are provided in the next two sections.\
A general definition of the approach can be formulated as follows.\
Let us create a global PV-model M by fitting all data points from the training set {X, Y}. If this model is then applied to a new dataset, {X∗ , Y∗}, it will result in a set of outcomes R∗:\
The nature of the outcomes depends on the method used for developing of M, and can include, for example, predicted response values in the case of PLS decomposition and scores and distances in the case of SVD decomposition.\
Using cross-validation with random splits enables the generation of a very large number of the PV-sets, which will hold the properties described above, but will be different from each other. Combining the generated PV-sets with the original training data will result in the augmented training set we are aiming at.\
The next two sections describe the details of PV-set generation using SVD and PLS decompositions. More information about the approach can be found in [10].In the case of truncated SVD with A latent variables, the PV-model M consists of:\
• number of latent variables (singular vectors), A\
• 1 × J row-vector for mean centering, m\
• 1 × J row-vector for standardization (if required), s\
• J × A matrix with right singular vectors, V\
• 1 × A vector with corresponding singular values, σ.\
The SVD decomposition of X is defined as:\
Here, U is an I × A matrix with left singular vectors and Σ is an A × A diagonal matrix with singular values σ = {σ1, …, σA}. Element E is an I × J matrix with residuals, which can be used to estimate the lack of fit both for individual data points and for the whole dataset. For the sake of simplicity we assume that columns of X are already mean centered and standardized.\
The right singular vectors V act as the orthonormal basis of the A-dimensional latent variable space located inside the original J-dimensional variable space (A ≤ J). The product of the left singular vectors and the singular values, T = UΣ, forms scores — coordinates of data points from X being projected to the latent variable space.\
The expression can also be written as:Any new data point, x = {x1, …, xJ } can be projected to the model M by mean centering and (if required) standardizing its values using m and s, and then projecting the resulting values to the latent variable space defined by the columns of V:\
The score vector t can be normalized to obtain the corresponding left singular vector, u:The explained part of x can be computed as:And a squared Mahalanobis distance, h, between the projection of the point in the latent variable space and the origin:The first distance, q, is a measure of lack of fit for the point, while the second distance, h, is a measure of extremeness of the point (as the majority of the data points will be located around the origin due to mean centering).\
Hence we meet the general Procrustes rule requirement defined by Equation 4 with the outcomes R consisting of the two distances. Moreover, this rule holds true for the distances computed using any number of latent variables a = 1, …, A.\
The generation of the PV-set in the case when A < rank(X) is similar but requires an additional step to hold the rule defined by Equation 14, further details can be found in [10].\
Repeating the generation using random cross-validation splits provides a large number of the unique PV-sets that can be used to augment the original training set. Therefore, the SVD based data augmentation procedure has two parameters:\
• number of segments, K\
• number of latent variables, A\
As it will be shown in the results section, neither parameter significantly influences the quality of the augmented data. In the case of the number of latent variables, A, any number large enough for capturing the systematic variation in X, will work well. The SVD based PV-set generation does not suffer from overfitting, and optimization of A is not needed.Partial least squares is a decomposition used for solving multiple linear regression problem in case when predictors (and responses if they are multiple) are collinear. The decomposition can be expressed in the following set of equations:\
In the case of one response, matrix Y can be replaced by a column vector y containing I response values. Then the Equation 18 can be rewritten in the following compact form:\
In contrast to SVD, the latent variables in PLS are oriented along directions in the column space of X, which give the largest covariance between the scores (columns of matrix T) and the values of y. Hence, by using PLS, we can prioritize the variance-covariance structure of X directly related to y.\
The difference between the original and estimated response values can be defined as:\
We can define the Procrustean rule for the regression as:\
Taking into account the equations above, this equation can be simplified to:\
Therefore, PV-set generation in the case of PLS has the same two parameters as the SVD-based procedure:\
• number of segments, K\
• number of latent variables, A\
It must also be noted that the number of latent variables, A, in case of PLS must be limited (in contrast to the SVD based algorithm). If too many latent variables are selected, the variation in scalar values, ck/c, can be very large, which leads to noisy PV-sets. This happens because higher latent variables do not have any covariance with the response variable, y; hence, ck values vary chaotically. It is recommended to keep all latent variables with absolute values of ck/c within the [0, 2] range.\
As it is shown in the experimental part, if this limit is introduced, the number of latent variables does not have a substantial influence on the PV-sets quality and does not require specific optimization. Same can be concluded about the number of segments, K.]]></content:encoded></item><item><title>HUGO Meme Coin Unveils AI-Driven Transparency Tool As Multi-Chain Expansion Begins</title><link>https://hackernoon.com/hugo-meme-coin-unveils-ai-driven-transparency-tool-as-multi-chain-expansion-begins?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:21:18 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[LONDON, United Kingdom, January 26th, 2025/Chainwire/--HUGO Meme Coin, a newly launched cryptocurrency project, has officially debuted on the Base blockchain, with plans to expand to Ethereum (ETH) and Solana (SOL) in the coming weeks.\
The project introduces FOMA AI, an AI-driven tool designed to provide accurate and transparent insights into the cryptocurrency space, with a focus on meme coins.Introducing HUGO Meme CoinHUGO Meme Coin positions itself as a community-driven initiative aimed at promoting transparency and informed decision-making within the cryptocurrency ecosystem.\
Central to the project is "Hugo," a thematic character representing the project's mission to address misinformation in the crypto space through advanced AI solutions.\
At the core of the initiative is FOMA AI, an artificial intelligence platform designed to deliver unbiased and data-driven insights into meme coins, providing users with tools to navigate the often speculative market with greater clarity.Expanding to Ethereum and SolanaCurrently operating on the Base blockchain, HUGO Meme Coin plans to adopt a multi-chain strategy by integrating with Ethereum and Solana in the near term.\
This expansion aims to enhance accessibility and interoperability, ensuring a broader reach within the global cryptocurrency community.HUGO Meme Coin has outlined a comprehensive marketing plan aimed at increasing visibility and fostering community participation. The strategy includes partnerships with influencers across platforms such as TikTok, YouTube, and Instagram, alongside community-focused initiatives such as meme creation challenges, giveaways, and exclusive merchandise.Key marketing highlights include:Collaborations with global influencers to expand awareness.Interactive community events and contests.Partnerships with strategic media outlets to enhance project visibility.A Focus on Transparency and InnovationThe HUGO Meme Coin project seeks to distinguish itself in the cryptocurrency space by combining the widespread appeal of meme culture with AI-driven solutions. The initiative emphasizes transparency and knowledge-sharing, aiming to foster a more informed and engaged community. is a blockchain-based cryptocurrency project dedicated to promoting transparency, informed decision-making, and community engagement within the digital asset space. Featuring FOMA AI, an advanced AI-driven platform, the project focuses on providing accurate insights into meme coin markets.\
By combining technology, transparency, and a community-centered approach, HUGO Meme Coin aims to redefine how individuals interact with the cryptocurrency ecosystem.\
For more information about HUGO Meme Coin, including updates on the multi-chain expansion and community initiatives, users can visit::::tip
This story was distributed as a release by Chainwire under HackerNoon’s Business Blogging Program. Learn more about the program ]]></content:encoded></item><item><title>Researchers Introduce Clever Math Trick to Beef Up Tiny Datasets Without Frying Your GPU</title><link>https://hackernoon.com/researchers-introduce-clever-math-trick-to-beef-up-tiny-datasets-without-frying-your-gpu?source=rss</link><author>Procrustes</author><category>tech</category><pubDate>Mon, 27 Jan 2025 09:14:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Sergey Kucheryavskiy, Department of Chemistry and Bioscience, Aalborg University and a Corresponding author (svk@bio.aau.dk);(2) Sergei Zhilin, CSort, LLC., Germana Titova st. 7, Barnaul, 656023, Russia and Contributing authors0 (szhilin@gmail.com).:::tip
Editor's note: This is Part 1 of 4 of a study detailing a new method for the augmentation of numeric and mixed datasets. Read the rest below.In this paper, we propose a new method for the augmentation of numeric and mixed datasets. The method generates additional data points by utilizing cross-validation resampling and latent variable modeling. It is particularly efficient for datasets with moderate to high degrees of collinearity, as it directly utilizes this property for generation. The method is simple, fast, and has very few parameters, which, as shown in the paper, do not require specific tuning. It has been tested on several real datasets; here, we report detailed results for two cases, prediction of protein in minced meat based on near infrared spectra (fully numeric data with high degree of collinearity) and discrimination of patients referred for coronary angiography (mixed data, with both numeric and categorical variables, and moderate collinearity). In both cases, artificial neural networks were employed for developing the regression and the discrimination models. The results show a clear improvement in the performance of the models; thus for the prediction of meat protein, fitting the model to the augmented data resulted in a reduction in the root mean squared error computed for the independent test set by 1.5 to 3 times.\
: data augmentation, artificial neural networks, Procrustes cross-validation, latent variables, collinearityModern machine learning methods that rely on high complexity models, such as artificial neural networks (ANN), require a large amount of data to train and optimize the models. Insufficient training data often lead to overfitting problems, as the number of model hyperparameters to tune is much larger than the number of degrees of freedom in the dataset.\
Another common issue in this case is the lack of reproducibility because the ANN training procedure is not deterministic, given the random selection of initial model parameters and the stochastic nature of their optimization. Consequently, it never leads to a model with the same parameters and performance, as different training trials can result in different models. This variability becomes large if the training set is too small.\
This problem is particularly urgent in the case of fitting the experimental data, as it is often expensive and time-consuming to run many experimental trials, making it simply impossible to collect thousands of measurements needed for proper training and optimization. There can also be other obstacles, such as paperwork related to permissions in medical research.\
One way to overcome the problem of insufficient training data is to artificially augment it by either simulating new data points or making small modifications to existing ones. This technique is often referred to as “data augmentation”. Data augmentation has proved to be particularly efficient in image analysis and classification, with a large body of research reporting both versatile augmentation methods [1] [2], [3] and methods that are particularly effective for specific cases [4] [5]. Augmentation methods for time series data are also relatively well developed [6].\
However, there is a lack of efficient methods that can provide decent data augmentation for numeric datasets with a moderate to high degree of collinearity. Such datasets are widespread in experimental research, including various types of spectroscopic data, results of genome sequencing (e.g., 16S RNA), and many others. Many tabulated datasets also exhibit internal structures where variables are mutually correlated. Currently available methods for augmentation of such data mostly rely on adding various forms of noise [7] to the existing measurements, which is not always sufficient. There are also promising methods that utilize variational autoencoders by random sampling from their latent variable space [8], or methods based on generative adversarial networks [4]. The downsides are that both approaches require building and tuning a specific neural network model for the data augmentation and hence need a thorough and resource demanding optimization process and a relatively large initial training set.\
In this paper, we propose a simple, fast, versatile, yet efficient method for augmenting numeric and mixed collinear datasets. The method is based on an approach that was initially developed for other purposes, specifically for generating validation sets, and hence is known as Procrustes cross-validation [9] [10]. However, as demonstrated in this paper, it effectively addresses the data augmentation problem, resulting in models with significantly improved prediction or classification performance.\
Our method directly leverages collinearity in the generation procedure. It fits the training data with a set of latent variables and then employs cross-validation resampling to measure variations in the orientation of the variables. This variation is then introduced to the training set as sampling error, resulting in a new set of data points.\
Two fitting models can be employed — singular value decomposition (SVD) and partial least squares (PLS) decomposition. The choice of the fitting model allows the user to prioritize a part of covariance structure, which will be used for generation of the new data.\
Both fitting models have two parameters — the number of latent variables and the number of segments used for cross-validation resampling. The experiments show though that the parameters do not require specific tuning. Any number of latent variables large enough to capture the systematic variation of the training set values serve equally well. As well as any number of segments starting from three.\
The proposed method is versatile and can be applied to both fully numeric data as well as to tabulated data where one or several variables are qualitative. This opens another perspective, namely data mocking, which can be useful, e.g., for testing of high loaded software systems, although we do not consider this aspect here.\
The paper describes the theoretical foundations of the method and illustrates its practical application and performance based on two datasets of different nature. It provides comprehensive details on how the method can be effectively applied to diverse datasets in real-world scenarios.\
We have implemented the method in several programming languages, including Python, R, MATLAB, and JavaScript, and all implementations are freely available in the GitHub repository (https://github.com/svkucheryavski/pcv). Additionally, we provide an online version where one can generate new data points directly in a browser (https://mda.tools/pcv).]]></content:encoded></item><item><title>Internet-Connected &apos;Smart&apos; Products for Babies Suddenly Start Charging Subscription Fees</title><link>https://slashdot.org/story/25/01/27/0455205/internet-connected-smart-products-for-babies-suddenly-start-charging-subscription-fees?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 27 Jan 2025 08:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The EFF has complained that in general "smart" products for babies "collect a ton of information about you and your baby on an ongoing basis". (For this year's "worst in privacy" product at CES they chose a $1,200 baby bassinet equipped with a camera, a microphone, and a radar sensor...) 

But today the Washington Post reported on a $1,700 bassinet that surprised the mother of a one-month-old when it "abruptly demanded money for a feature she relied on to soothe her baby to sleep."

The internet-connected bassinet... reliably comforted her 1-month-old — just as it had her first child — until it started charging $20 a month for some abilities, including one that keeps the bassinet's motion and sounds at one level all night. The level-lock feature previously was available without a fee. "It all felt really intrusive — like they went into our bedroom and clawed back this feature that we've been depending on...." When the Snoo's maker, Happiest Baby, introduced a premium subscription for some of the bassinet's most popular features in July, owners filed dozens of complaints to the Federal Trade Commission and the Better Business Bureau, coordinated review bombs and vented on social media — saying the company took advantage of their desperation for sleep to bait-and-switch them... 

Happiest Baby isn't the only baby gear company that has rolled out a subscription. In 2023, makers of the Miku baby monitor, which retails for up to $400, elicited similar fury from parents when it introduced a $10 monthly subscription for most features. A growing number of internet-connected products have lost software support or functionality after purchase in recent years, such as Spotify's Car Thing — a $90 Bluetooth streaming device that the company announced in May it plans to discontinue — and Levi's $350 smart jacket, which let users control their phones by swiping sensors on its sleeve... 

Seventeen consumer protection and tech advocacy groups cited Happiest Baby and Car Thing in a letter urging the FTC to create guidelines that ensure products retain core functionality without the imposition of fees that did not exist when the items were originally bought. 

The Times notes that the bassinets are often resold, so the subscription fees are partly to cover the costs of supporting new owners, according to Happiest Baby's vice president for marketing and communications. But the article three additional perspectives:

"This new technology is actually allowing manufacturers to change the way the status quo has been for decades, which is that once you buy something, you own it and you can do whatever you want. Right now, consumers have no trust that what they're buying is actually going to keep working." — Lucas Gutterman, who leads the Public Interest Research Group's "Design to Last" campaign.
 "It's a shame to be beholden to companies' goodwill, to require that they make good decisions about which settings to put behind a paywall. That doesn't feel good, and you can't always trust that, and there's no guarantee that next week Happiest Baby isn't going to announce that all of the features are behind a paywall." — Elizabeth Chamberlain, sustainability director at iFixit.
"It's no longer just an out-and-out purchase of something. It's a continuous rental, and people don't know that." — Natasha Tusikov, an associate professor at York University]]></content:encoded></item><item><title>Log-normal Work Function Distribution Vs. Gaussian Work Function Distribution</title><link>https://hackernoon.com/log-normal-work-function-distribution-vs-gaussian-work-function-distribution?source=rss</link><author>Distributed Work</author><category>tech</category><pubDate>Mon, 27 Jan 2025 07:46:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Nandan Pakhira, Department of Physics, Kazi Nazrul University, Asansol, West Bengal 713340, India;(2) Rajib Mahato, Department of Physics, Kazi Nazrul University, Asansol, West Bengal 713340, India and Central Electronics Engineering Research Institute, Pilani, Rajasthan 333031, India.B. Log-normal work function distributionNext we consider log-normal distribution for the work function. In Fig. 8 we show the histogram plot of the work function, sampled over log-normal distribution for four choice of median work function M ≡ e µ = 3.0, 3.5, 4.0 and 4.5 eV with σ = 0.05. From this plot we can see that our choice of random variables for Φ are well sampled over the log-normal distribution.We would like to thank Nei Lopes, Arghya Taraphder for many valuable discussions. One of us (N. P) would like to thank IIT, Kharagpur for local hospitality where part of the work was done. One of us (R. M) would like to thank Cetral Electronics Engineering Research Institute, Pilani for providing local hospitality and research support where part of the work was done.1 R. H. Fowler and L. Nordheim, Proc. Roy. Soc. London A, 119, 173 (1928).\
2 E. L. Murphy and R. H. Good, Phys. Rev., 102, 1464 (1956).\
3 T. C. Choy, A. H. Harker and A. M. Stoneham, J. Phys. Cond. Matt., 17, 1505 (2005).\
4 P. H. Cutler, J. He, J. Miller, N. M. Miskovsky, B. Weiss and T. E. Sullivan, Progress in Surface Science, 42,169185 (1993).\
5 R. G. Forbes, K. L. Jensen, Ultramicroscopy, 89, 1722 (2001).\
6 C. J. Edgcombbe, Phys. Rev. B, 72, 045420 (2005).\
7 K. L. Jensen and M. Cahay, Appl. Phys. Lett., 88, 154105 (2006).\
8 A. Fischer, M. S. Mousa, and R. G. Forbes, J. Vac. Sci. Technol. B, 31, 032201 (2013).\
9 R. G. Forbes, A. Fischer, and M. S. Mousa, J. Vac. Sci. Technol. B,31, 02B103 (2013).\
10 A. Kyritsakis and J. P. Xanthakis, Proc. R. Soc. A, 471, 20140811 (2015)\
11 J. T. Holgate and M. Coppins, Phys. Rev. Appl., 7(4), 044019 (2017).\
12 XLiana Gamez, Maxwell Terban, Simon Billinge and Maria Martinez-Inesta, 50, 741 (2017).\
13 Nei Lopes and A. V. Andrade-Neto, arXiv:1408.3663v3\
14 Nei Lopes and A. V. Andrade-Neto, Phys. Lett. A, 384, 126399 (2020).\
15 D. Biswas and R. Ramachandran, Phys. Plasmas 24, 073107 (2017).\
16 A. Haug, Theoretical Solid State Physics, Volume 1, (Pergamon Press, Oxford, 1975.)\
17 R. G. Forbes, Appl. Phys. Lett. 89, 113122 (2006).\
18 R. G. Forbes and J. H. B. Deane, J. Vac. Sci. Technol. B, 28, C2A33 (2010).\
19 W. W. Dolan, Phys. Rev. 91, 510 (1953).\
20 I. S. Gradshteyn and I. M. Ryzhik, Tables of Integrals, Series and Products. Academic, New York (1965).]]></content:encoded></item><item><title>What Is Gaussian Work Function Distribution?</title><link>https://hackernoon.com/what-is-gaussian-work-function-distribution?source=rss</link><author>Distributed Work</author><category>tech</category><pubDate>Mon, 27 Jan 2025 07:37:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Nandan Pakhira, Department of Physics, Kazi Nazrul University, Asansol, West Bengal 713340, India;(2) Rajib Mahato, Department of Physics, Kazi Nazrul University, Asansol, West Bengal 713340, India and Central Electronics Engineering Research Institute, Pilani, Rajasthan 333031, India.III. WORK FUNCTION DISTRIBUTIONIn the previous section we have summarized field emission current for a given local work function Φ. For a system of nano-particles or systems with inhomogeneity the work function will be different over the length scale of the size of the collector for emitted electrons. In such a situation we need to average the field emission current over the distribution of work function as follows:\
where P(Φ) is the distribution of the work function, Φ. We choose two widely used distribution functions, namely (i) Gaussian and (ii) log-normal distribution. It is well known that systems with bulk disorder follows Gaussian distribution and the work function distribution function, P(Φ), is given by\
where σ is the  of the distribution and Φ0 is the known bulk value for a given material.\
We also use log-normal distribution for the work function :\
It is important to mention that we were inspired by the experimental result of Gamez et. al.[12]. They showed that for a system of Pt nano-particles the pair distribution function (PDF) for the radius of nano-particles follows log-normal distribution\
Various statistical properties of this distribution are summarized in the table I :In this section we show our results for current density averaged over two choice of probability distributions as have been discussed in the previous section.A. Gaussian work function distributionWe first consider the case of Gaussian distribution for the work function. In Fig. 1 we show the histogram plot of the work function, sampled over Gaussian distribution for four choices of bulk work function Φ0 = 3.0, 3.5, 4.0 and 4.5 eV with σ = 0.05. In each case we also fit the histogram plot to Gaussian distribution. From this fit we can see that our choice of random variables for Φ are well sampled over Gaussian distribution.]]></content:encoded></item><item><title>Unlocking the Mystery of Nano-Powered Field Emission</title><link>https://hackernoon.com/unlocking-the-mystery-of-nano-powered-field-emission?source=rss</link><author>Distributed Work</author><category>tech</category><pubDate>Mon, 27 Jan 2025 07:23:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Nandan Pakhira, Department of Physics, Kazi Nazrul University, Asansol, West Bengal 713340, India;(2) Rajib Mahato, Department of Physics, Kazi Nazrul University, Asansol, West Bengal 713340, India and Central Electronics Engineering Research Institute, Pilani, Rajasthan 333031, India.Field emission is the process in which electrons from cold surfaces are emitted in the presence of applied strong electric field. This process should be compared agaist the thermo-ionic process in which electrons are emitted from hot metal surface. The field emission forms the back bone of modern semiconductor devices. This effect was first described by Fowler and Nordheim[1]. They considered quantum mechanical tunneling through a triangular potential energy (PE) barrier, created by the application of a constant electric field. Much later Murphy and Good[2] (MG) introduced a more realistic PE barrier by taking into account the induced image charge formed in the presence of emitted electrons. MG calculated the barrier transmission coefficient under semi-classical WKB approximation. More recently, essentially an exact solution of the problem was obtained by Choy et. al.[3]. Also, various cases including finite temperature (thermal emission), tunneling, curvature of th.\
To the best of our knowledge in all of those studies authors have considered constant local work function. The work function of a material depends on the composition, structure, geometry, local charge distribution etc. of the emitting surface. Assumption of constant work function is only suitable for an atomistic smooth homogeneous surface. For surfaces with inhomogeneities over nano-scale (much smaller than the size of the collectors) assumption of constant work function is no longer valid. Also it has been shown[12] that in a system of nanoparticles there is a distribution of the size of the nanoparticles. Since the work function is more of a property of the surface we naturally can expect that the work function of nano-particles will also have a distribution. The actual microscopic model for work function distribution for a system of nano-particles is beyond the scope of this work. Interestingly, Gamez et. al.[12] using scanning tunneling microscope (STM) have measured the pair distribution function (PDF) for Pt nano-particles and they found that it follows log-normal distribution.\
In this work, purely as a mathematical model, we choose Gaussian and log-normal distribution for the work function. We then study the field emision current averaged over work function distribution. The organization of the rest of the paper is as follows. In Sec. II we describe the mathematical formalism used to calculate field emission current. In Sec. III we describe the work function distribution used to calculate average current. In Sec. IV we present our results for both the case of Gaussian distribution and log-normal distribution. Finally in Sec. V we conclude.We closely follow and summarize the results obtained by Lopes et. al.[13],[14] for field-emission current density, J. In the standard FN-type MG theory the field emission current density is given by the well-known expression[15–18],\
K(λ) and E(λ) are the complete elliptic integral of the first and second kind, with\
From the relations above it is quite evident that calculation of transmission current requires evaluation of numerical integrals for complete elliptic integrals. Due to the singularities present in complete elliptic integrals, it is very hard to extract meaningful results purely numerically[19]. Under this circumstances we can consider series expansion for complete elliptic integrals[20] as follows\
ls A detailed calculation gives the following form for the field emission current density]]></content:encoded></item><item><title>The TechBeat: EIP-7002: A Better Way To Stake On Ethereum (1/27/2025)</title><link>https://hackernoon.com/1-27-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Mon, 27 Jan 2025 07:11:48 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @lumoz [ 6 Min read ] 
 Lumoz SVM Stack delivers ultra-high transaction throughput and processing speeds for the SVM chain. Read More.By @maken8 [ 3 Min read ] 
 Donald Trump has already withdrawn $500m from his memecoin $TRUMP. Read More.By @infinity [ 6 Min read ] 
 Discover 15 types of databases, from relational to vector, and explore their unique use cases in this comprehensive guide for developers. Read More.By @axotion [ 11 Min read ] 
 A long time ago, I had to create a scalable system that could be capable of handling hundreds of simultaneous connections at not very high cost. Read More.By @decoded [ 5 Min read ] 
 See which HackerNoon 2024 polls were the community favorites and received the most votes. The top 10 voters are included. Read More.By @hackernooncontests [ 4 Min read ] 
 Join the Spacecoin Writing Contest, today! Win from a 15,000 USDT prize pool by writing on decentralized internet, space tech, blockchain use cases & more.  Read More.By @awsmarketplace [ 10 Min read ] 
 Meet Fortinet FortiGate: AI-Powered Firewall Ranked by Gartner in the Leader’s Quadrant 13 Years in a Row. Read More.By @marutitechlabs [ 11 Min read ] 
 Maruti Techlabs offers end-to-end product development. Innovate, validate, and launch with our expert team guiding you every step.
 Read More.By @2077research [ 54 Min read ] 
 EIP-6110 simplifies Ethereum validator deposits by moving them on-chain, reducing technical debt, simplifying block validation, and enhancing network efficiency Read More.By @aisweatshop [ 3 Min read ] 
 AiSweat.Shop's unprecedented airdrop without any prior announcement strengthens its commitment to building a robust DefAI ecosystem. Read More.By @2077research [ 34 Min read ] 
 EIP-7002 enhances Ethereum staking by decoupling validator signing keys from withdrawal keys, boosting security, trustlessness, and user experience post-Merge. Read More.By @gleams [ 8 Min read ] 
 While AI offers efficient shortcuts to problem-solving, it changes the nature of the journey that shapes our genius. Read More.By @zacamos [ 5 Min read ] 
 Bites from venomous snakes can be deadly — but AI may be able to help. Here's how scientists used AI to design new proteins to counteract snake venom. Read More.By @atulthosar [ 3 Min read ] 
 Explore Network Namespaces and Linux Bridges in container networking. Learn about veth pairs and network isolation in this detailed guide Read More.By @dmytrospilka [ 4 Min read ] 
 Let’s take a deeper look into what could become a watershed moment for the cryptocurrency landscape. Read More.By @aswinbarath [ 4 Min read ] 
 Today AI is rapidly becoming part of everyday life, and NVIDIA has once again taken center stage with a new development: Project DIGITS.   Read More.]]></content:encoded></item><item><title>Should Big Tech Plug Its Data Centers Directly Into Power Plants?</title><link>https://hardware.slashdot.org/story/25/01/27/0539219/should-big-tech-plug-its-data-centers-directly-into-power-plants?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 27 Jan 2025 05:56:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["Looking for a quick fix for their fast-growing electricity diets, tech giants are increasingly looking to strike deals with power plant owners to plug in directly," reports the Associated Press, "avoiding a potentially longer and more expensive process of hooking into a fraying electric grid that serves everyone else." (It can take up to four years to connect a data center to the grid, one data center trade group says in the article — years longer than it takes to build a new data center.) 

But the idea of bypassing the grid is "raising questions over whether diverting power to higher-paying customers will leave enough for others and whether it's fair to excuse big power users from paying for the grid."

Front and center is the data center that Amazon's cloud computing subsidiary, Amazon Web Services, is building next to the Susquehanna nuclear plant in eastern Pennsylvania. The arrangement between the plant's owners and AWS — called a "behind the meter" connection — is the first such to come before the Federal Energy Regulatory Commission. For now, FERC has rejected a deal that could eventually send 960 megawatts — about 40% of the plant's capacity — to the data center. That's enough to power more than a half-million homes... [But the FERC's 2-1 rejection "was procedural. Recent comments by commissioners suggest they weren't ready to decide how to regulate such a novel matter without more study."] 

In theory, the AWS deal would let Susquehanna sell power for more than they get by selling into the grid... The profit potential is one that other nuclear plant operators, in particular, are embracing after years of financial distress and frustration with how they are paid in the broader electricity markets. Many say they have been forced to compete in some markets against a flood of cheap natural gas as well as state-subsidized solar and wind energy. Power plant owners also say the arrangement benefits the wider public, by bypassing the costly buildout of long power lines and leaving more transmission capacity on the grid for everyone else... 

Monitoring Analytics, the market watchdog in the mid-Atlantic grid, wrote in a filing to FERC that the impact would be "extreme" if the Susquehanna-AWS model were extended to all nuclear power plants in the territory. Energy prices would increase significantly and there's no explanation for how rising demand for power will be met even before big power plants drop out of the supply mix, it said.]]></content:encoded></item><item><title>The &apos;Super Bowl for Nerds&apos;: Scenes from the Microsoft Excel World Championship</title><link>https://tech.slashdot.org/story/25/01/27/0355223/the-super-bowl-for-nerds-scenes-from-the-microsoft-excel-world-championship?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 27 Jan 2025 03:58:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[At December's "Microsoft Excel World Championship" in Las Vegas, "finance professionals fluent in spreadsheets were treated like minor celebrities," writes the New York Times, "as they gathered to solve devilishly complex Excel puzzles in front of an audience of about 400 people, and more watching an ESPN3 livestream." 

The Times notes that "many fans find out about the Excel championship through ESPN's annual obscure sports showcase, where it is sandwiched between competitions like speed chess and the World Dog Surfing Championships." But the contest's organizer envisions tournaments with "more spectators, bigger sponsors and a million-dollar prize" — even though this year's prize was $5,000 and a pro wrestling-style championship belt.

The format for the finals was a mock-up of World of Warcraft, an online role-playing game. It required the 12 men (this particular nerdfest was mostly a guy thing) to design Excel formulas for tracking 20 avatars and their vital signs... To prepare, [competitor Diarmuid] Early adjusted the width of his Excel columns with the precision of a point guard lining up a 3-point shot. [Andrew] Ngai queued up a YouTube compilation of "focus music". After an announcer kicked off the 40-minute event — "Five, four, three, two, one, and Excel!" — the 12 players leaned over their keyboards and began plugging in formulas. One example: "=CountChar (Lower (D5),"W")" allowed one competitor, Michael Jarman, to figure out how many times the letter "W" appeared in a spreadsheet. 


ZDNet points out that there's a seven-hour livestream of the event that's "worth checking out for the opening theme song alone." 

The New York Times closes their article with a quote from super-fan Erik Oehm, a software developer from San Francisco who called the event "the Super Bowl for Excel nerds". Oehm watched excitedly from the front row as this year's winner — Michael Jarman — finally raised the championship belt overhead while someone dumped glitter on him. And then he said... 

"You'd never see this with Google Sheets. You'd never get this level of passion."]]></content:encoded></item><item><title>Why Scenario Planning Is An Effective Strategy Tool</title><link>https://hackernoon.com/why-scenario-planning-is-an-effective-strategy-tool?source=rss</link><author>Olga Kirgizova</author><category>tech</category><pubDate>Mon, 27 Jan 2025 03:24:21 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
We live in an era of turbulence — even before Covid, rapid technological change, economic crises, and political instability made strategy development complex. Managers constantly asked themselves, "What’s next?" and "How do we prepare for the future?". In many cases, the past can guide us in predicting the future through analysis. However, when something unprecedented occurs, that approach fails.Scenario planning is a strategic planning tool that allows to manage uncertainty effectively. This tool helps not only to anticipate possible futures, but also to develop an actionable plan for each scenario. In this article, we’ll explore the fundamentals of scenario planning, its stages, advantages, and disadvantages.In general, strategy is a high-level plan for achieving complex long-term goals. To develop a strategy, one must first understand the current position (Point A), define where they want to go (Point B), and set interrim objectives.Developing a strategy begins with analyzing the current situation, which includes:Formulation of a value propositionOnce you have analyzed the current situation (Point A), the next step is defining the destination—Point B. Point B is essentially the vision, the long-term goal to achieve. Often it comes down to finding a BHAG (Big Hairy Audacious Goal).The next step is to build a bridge between the current state and the desired future through the formulation of strategic goals.Issues with the Traditional Approach to StrategyStrategic Goal ≠ StrategyIt is not enough to assess the current state, imagine a BHAG, and set strategic objectives and OKRs. Strategy is not the same as strategic goals or vision. You often hear “strategies” like this: “become an industry leader” or “to have a 20% revenue growth every year while maintaining a 20% margin”. Without an answer to how we can achieve this, it will be impossible to do this. Ambition does not equal strategy; if neither you nor your team understand exactly how to achieve these goals, then nothing will work.Strategy is not only about what the organization will do, but also about what it will not do. When there are too many strategic objectives, priorities become unclear. Given that resources are always limited, achieving all goals simultaneously is unrealistic.Imagine you’ve established your current state, vision, mission, values, goals, and initiatives. Then, an unexpected change occurs in the external environment — like, for example, COVID-19, and your plans are no longer relevant. A change in the external or internal context often leads to the fact that traditional strategies are unprepared for new challenges.If the strategy does not include an understanding of the key factors that can influence your company, you haven’t identified the problem, and you do not have a strategy. Instead, you have goals, a budget, or a list of initiatives that you would like to do.Scenario Planning as a Way to Make Strategy RobustScenario planning is a strategic planning method that allows you to manage uncertainty. The concept of scenario planning appeared in the 50-60s of the twentieth century as a way to anticipate and plan potential events. One of the founders of the method is considered to be Herman Kahn, who conducted an analysis of possible military conflicts for the American center for global policy RAND Corporation after World War II. In the business environment, the idea of scenario planning was first used by the oil company Royal Dutch Shell, which began studying the possible scenarios in case of oil prices increase by exporting countries. Through this analysis, Shell predicted the energy crises of 1973 and 1979, prepared accordingly, and became one of the top five oil companies in the world.\
The tool gained popularity in the era that researchers called VUCA (volatility, uncertainty, complexity and ambiguity). Even before COVID-19, rapid technological changes, economic crises and political instability made predicting the future difficult. Managers have always wondered what will happen next and how to prepare for the future. In many cases, you can rely on the past to predict the future, analyzing it. But when something happens that has never happened, this becomes impossible. The most striking example was COVID-19.The Difference Between Scenario Planning and Forecasts: Predictions about future events based on current trends, using statistical or economic methods. Forecasts focus on quantitative outcomes and often and does not take risks into account.: Descriptions of possible future developments based on specific conditions and assumptions. Scenarios aren’t precise predictions but frameworks for considering risks and opportunities.Strategic planning does not answer the question of "what" to think about the future. It helps us understand "how" to think about it.Scenario planning involves five key steps:Identify the focus and constraintsIdentify key uncertaintiesCreating a list of actionsImplementation of scenario planning1. Identify the Focus and ConstraintsDefine the key question you would like to address in the planning (e.g., "Should we invest in AI?" or "Should we enter new markets?").Set time and geographic boundaries.Assemble a team with different perspectives and experiences.2. Identify Key UncertaintiesSelect 10-20 factors that could significantly impact the business. For example, changes in demand, competitor actions, or emerging technologies.Assess the influence and uncertainty of each factor on a 0–10 scaleSelect key factors and set polar values for them: will it happen or notCreate a 2x2 matrix and develop scenarios for each quadrantFor example, this set of scenarios developed for one of the Detroit Three automakers back in 1984, contained the “official future” in the lower left quadrant: cheap fuel with neo-traditional consumer values. This was the future Detroit had been planning for decades. But higher fuel prices and changing consumer values required them to think about new types of vehicles that would fit different environments. As a result, the customer began to think more about smaller cars, as well as minivans and SUVs that would appeal to consumers with nontraditional values.All scenario options must meet these criteria: The stories in each quadrant must be different from each other. If this is not the case, then one of the factors does not have a significant impact on the course of the story and was chosen incorrectly. The developed scenarios have a minimal but still a probability of occurring.Potential for decision-making. Each of the scenarios should contain events on the basis of which management decisions can be made.. Each scenario should be internally consistent and have no internal contradictions.4. Creating a List of ActionsSelect the most favorable and the most negative scenario.Define actions to achieve the favorable scenario and avoid the negative one.==If you already have a strategy:====Sometimes scenarios are developed after the strategy has been defined. In this case, the scenarios serve as a stress test for the strategy. Look at what projects and plans you have at the moment and assess how relevant they are in the developed scenarios.==5. Implementation of Scenario PlanningStrategic sessions should be held regularly to adjust scenarios, maintain flexibility and adapt the company to external changes. Today, the use of scenarios is widespread. But too often, organizations conduct only one exercise and then shelve everything they have learned. If companies want to develop effective strategy in the face of uncertainty, they need to set up an iterative scenario planning process, conduct such sessions regularly, and adjust scenarios as the situation changes.Advantages and Disadvantages of Scenario PlanningScenarios are easy to understand.Teamwork helps create a big picture.Reduces uncertainty to a manageable level.Based on assumptions rather than precise data.Does not provide specific forecasts.Scenario planning is a powerful tool that helps companies to navigate uncertainty. It is based on identifying critical factors and creating an action plan, which makes strategies more flexible and effective.In a rapidly changing world, the ability to foresee potential scenarios and adapt accordingly is a key success factor. Modern flexible strategies, including regular scenario updates, help companies successfully go ahead and minimize risks.]]></content:encoded></item><item><title>Another Undersea Cable Damaged in Baltic Sea. Criminal Sabotage Investigation Launched</title><link>https://tech.slashdot.org/story/25/01/27/0140243/another-undersea-cable-damaged-in-baltic-sea-criminal-sabotage-investigation-launched?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 27 Jan 2025 01:47:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA["An underwater data cable between Sweden and Latvia was damaged early on Sunday," reports the Financial Times, "in at least the fourth episode of potential sabotage in the Baltic Sea that has caused concern in Nato about the vulnerability of critical infrastructure..."

	Criminal investigations have started in Latvia and Sweden, and a ship has been seized as part of the probes, according to Swedish prosecutors, who did not identify the vessel. Previous incidents have been linked to Russian and Chinese ships... 

	The latest incident comes as the three Baltic states are preparing to disconnect their electricity systems from the former Soviet network in early February and integrate themselves into the continental European grid, with some fearing further potential disruption ahead of that. Estonia, Latvia and Lithuania have joined the EU and Nato since regaining their independence after their forced annexation by the Soviet Union, and see their switch to the European electricity system as their final integration into the west. KÄ(TM)stutis Budrys, Lithuania's foreign minister, said navigation rules in the Baltic Sea needed to be reviewed "especially when it comes to the use of anchors" and added there were now so many incidents that there was little chance they could all be accidents. 

	Repair of data cables has tended to take much less time than that for gas or electricity connections, and the Latvian state radio and television centre said it had found alternative routes for its communications.]]></content:encoded></item><item><title>Microsoft Announces Open-Source DocumentDB NoSQL Database</title><link>https://www.phoronix.com/news/Microsoft-OpenSource-DocumentDB</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 27 Jan 2025 01:41:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[In a blog post dated for this past Thursday but only being made public on Sunday night, Microsoft issued an announcement open-sourcing their new NoSQL database... Where it gets weirder is that it's named DocumentDB. Amazon also has a database offering named DocumentDB albeit proprietary...]]></content:encoded></item><item><title>Linux 6.14 To Switch From SHA1 To SHA512 For Module Signing By Default</title><link>https://www.phoronix.com/news/Linux-6.14-Modules</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 27 Jan 2025 01:27:31 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While many Linux distribution vendor kernels are already using SHA-512 for signing modules by default rather than the default SHA-1,  the upstream Linux 6.14 kernel is also now switching the default over to using SHA-512 for better security...]]></content:encoded></item><item><title>A New Bid for TikTok from Perplexity AI Would Give the US Government a 50% Stake</title><link>https://tech.slashdot.org/story/25/01/26/2316243/a-new-bid-for-tiktok-from-perplexity-ai-would-give-the-us-government-a-50-stake?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Mon, 27 Jan 2025 00:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shared this report from the Associated Press:

 Perplexity AI has presented a new proposal to TikTok's parent company that would allow the U.S. government to own up to 50% of a new entity that merges Perplexity with TikTok's U.S. business, according to a person familiar with the matter... The new proposal would allow the U.S. government to own up to half of that new structure once it makes an initial public offering of at least $300 billion, said the person, who was not authorized to speak about the proposal. The person said Perplexity's proposal was revised based off of feedback from the Trump administration. If the plan is successful, the shares owned by the government would not have voting power, the person said. The government also would not get a seat on the new company's board. 

Under the plan, ByteDance would not have to completely cut ties with TikTok, a favorable outcome for its investors. But it would have to allow a "full U.S. board control," the person said. 

Under the proposal, the China-based tech company would contribute TikTok's U.S. business without the proprietary algorithm that fuels what users see on the app, according to a document seen by the Associated Press.]]></content:encoded></item><item><title>Biometrics, Windmills, and VHS tapes: The Winners of &apos;Rest of World&apos; International Tech Photo Contest</title><link>https://tech.slashdot.org/story/25/01/26/231212/biometrics-windmills-and-vhs-tapes-the-winners-of-rest-of-world-international-tech-photo-contest?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 23:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Since launching in 2020, the nonprofit site RestofWorld.org has been covering tech news from 100 countries. And they've just announced the winners in their 2024 international photography contest. 

"From Cape Verde to Bhutan, we received 227 entries from over 45 countries around the world, featuring everything from sprawling mines to biometric facial scans."

Like last year, the majority of the entries in our 2024 photography contest captured on-the-ground realities of how technology is transforming lives in every corner of the world. We received submissions from over 45 countries, showcasing a stunning variety of perspectives on the intersection of technology and daily life. 

Beyond striking visuals, the photographs tell us stories of how tech plays a role in local communities, from iris-scanning payment systems inside refugee camps to EV battery-powered music gatherings. The 227 entries we received from contestants — including from Mongolia, the Philippines, Argentina, and Jordan — not only celebrate these stories but reaffirm our commitment at Rest of World to challenge stereotypes about how people use technology in their daily lives. 

An "honorable mention" photo shows immigrants from Africa arriving on the Italian island of Lampedusa after a perilous boat journey. ("Upon their arrival, these refugees borrowed a smartphone from a bystander and started a video call to let their relatives know they survived the journey.") And the top photo shows a U.S. Customs and Border Protection agent using a cellphone to collect facial scans from migrants entering the country from Mexico. ("After they make the crossing into the U.S., migrants are subjected to further data collection, including DNA samples.") 

Biometric data collection was a recurring theme. A photo from Jordan shows a Syrian boy paying for groceries with an iris scanner at a supermarket "run jointly by the World Food Programme and the U.N. High Commissioner for Refugees." Eye-scanning technology is being used there "to ensure people use only their own credit and not borrowed or stolen cards. After having their iris scanned, Syrian refugees living in the camp can make use of services such as health care and shopping, using just their eyes." 

Another recurring theme was energy. There's a lovely "honorable mention" photo from the Philippines showing two young people on a beach playing basketball "under the towering blades of the windmills in Bangu... Renewable energy has transformed this community, cutting household expenses and powering opportunities once thought to be out of reach." The third-place photo shows six children in a distant tent in "a mountainous, subarctic forest" in Mongolia" — all gathered around a laptop "to watch a documentary about a Norwegian reindeer herder" who had visited their region. ("Modern technology such as solar panels, car batteries, and the occasional Wi-Fi connection allows these families to stay connected with the world.") One photo shows a young boy carrying a solar panel down from the roof in a remote village in Jharkhand, India. 

Another photo documents the largest salt flat in Argentina, part of the so-called "lithium triangle" with parts of Chile and Bolivia. A salt miner says "They started looking for lithium there in 2010. We made them stop; it was hurting the environment and affecting the water. But now they are back and I am afraid. Everything we have could be lost." 

And a photo from Nigeria shows two people wearing traditional African attire but adorned with "goggles crafted from repurposed VHS tapes". RestofWorld says the goggles "represent how individuals and communities reclaim and reinterpret technology for art, commentary, and resilience. This practice reflects a community's ability to find new life in what others might discard, highlighting a deep relationship with both old and new technologies."]]></content:encoded></item><item><title>How a Gamification Engine Integrates Game Mechanics and Player Features in Software Environments</title><link>https://hackernoon.com/how-a-gamification-engine-integrates-game-mechanics-and-player-features-in-software-environments?source=rss</link><author>Gamifications FTW Publications</author><category>tech</category><pubDate>Sun, 26 Jan 2025 23:02:45 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Oscar Pedreira, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(2) Felix García, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(3) Mario Piattini, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(4) Alejandro Cortinas, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(5) Ana Cerdeira-Pena, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica.4.4 Support of game mechanics and elementsA popular question in gamification is what game mechanics we can apply in gamified environments in order to foster motivation and engagement in the users. In our previous systematic mapping about gamification in software engineering[8], the game mechanics and elements which have been considered previously were identified. Table 3 shows that list of game mechanics, and how they are supported in our gamification engine. As we can see in Table 3, only one, namely “betting”, is not currently supported.\
activities in the gamified environment. Figure 8 displays a screenshot of the home page of the player’s site in a real setting of the engine (real logos have been removed from the head of the page). This application allows the players to see all the information about their activity in the gamified environment. The home page shows them their profile information, the experience points they have accumulated, the level, the percentage of points obtained until the next level is reached, a chart for experience points, a list of the badges obtained, and two rankings, one of them considering all the players, and the other one taking into account only the players immediately above and below the player. The site also allows the players to access other information, such as a map with their locations, the projects they are involved in, social networks (“Friends” option in the menu), messages, notifications, challenges, and access to the virtual assistant.\
The players can thus access all the information of the gamified environment in a single place. Of course, this does not prevent us from showing information about rewards in the gamified tools.]]></content:encoded></item><item><title>What Does a Gamification Engine Do for Software Development Teams?</title><link>https://hackernoon.com/what-does-a-gamification-engine-do-for-software-development-teams?source=rss</link><author>Gamifications FTW Publications</author><category>tech</category><pubDate>Sun, 26 Jan 2025 23:02:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Oscar Pedreira, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(2) Felix García, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(3) Mario Piattini, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(4) Alejandro Cortinas, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(5) Ana Cerdeira-Pena, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica.4 Gamification Engine for SE EnvironmentsIn addition to proposing the generic software architecture and gamification model presented in the previous section, we have also implemented a gamification engine based on them. This implementation has allowed us to carry out a case study on the gamification of the work environment of a real software organization using our proposal. In addition, we have incorporated to our gamification engine functionalities that can be of interest for a real organization, and that go beyond the gamification architecture and model we have presented. These functionalities include a social network for the players, messaging, system notifications, challenges between players, and a virtual assistant based on dialog generation technologies. It also contributes tools for the analysis of the activities carried out by the users; in particular, there is a tool for analyzing the interaction of players, and community detection based on the interactions of the users in the workplace, along with a sentiment analysis module. This module enables detection of positive and negative polarities in the texts introduced by the players.\
In this section we present the details of the implementation of the gamification engine, how it supports the software architecture and gamification model presented in the previous section, and the additional advanced functionalities we have added.4.1 System architecture and designThe gamification engine has been designed following a three-layer architecture (see Fig. 7). The first layer is devoted to data persistence, and has been implemented as a relational database in PostgreSQL. The engine model contains the data access layer and the business logic we have described, comprising the management of users, behaviors, achievements, and gamification rules. A third layer provides two different interfaces.\
● The administrator of the gamified environment accesses the configuration of the engine though a web application that provides an interface from which the administrator can manage everything: users, tool credentials, behaviors, achievements, game rules, etc.\
● REST API provides a complete interface for all the tools of the gamified environment. This interface provides those tools with a large set of operations that allows them to access all the information in the gamified environment, and not just the communication of behaviors. For example, the player’s site does not have its own database, since it accesses all the information stored in the engine through the API.\
The engine has been developed in the Java EE platform, using technologies such as Hibernate, Spring, Spring MVC, and AngularJS.The REST API provides an interface for the rest of the tools in the gamified environment. By implementing this interface as a REST web service, we ensure that the platform or technology will not be an impediment for integrating any tool into the engine. In addition, since many tools that could be integrated into the engine have been developed in Java, we have created a client library for the REST API; this simplifies its use.\
The gamified tools cannot communicate freely with the engine. If a tool has to communicate behaviors, it must be registered in the engine with a tool ID and a password that will be used in every transaction.\
So far, we have focused on the data sent by the working tools to the gamification engine for the purpose of registering the player’s actions and evaluating them according to the gamification rules. However, data flow in the opposite direction is also possible, since the REST API allows all gamified tools to access all the data related to players, their actions, rewards and, in general, all gamification information (even the rules of the game). In this way, the gamified tools could also show the results of the gamification live to the players. For example, a programmer could see the result of a just completed development task in the Integrated Development Environment (IDE) he/she is using.4.3 Other functionalitiesThe engine completes the basic gamification model we have described with other functionalities present in classic games that are also used in gamified applications.\
 Most collaborative games allow players to communicate with their friends, or to even have an explicit social network. The engine supports this concept by providing a social network among the players, who can explicitly create friendship relationships.\
This allows us to, for example, show different rankings to the users, as rankings comparing their results with that of the rest of the players, or as a ranking comparing the results obtained by one user with those of their friends. Although the gamification engine currently provides its own social network, the data about the player’s relationships could be obtained from an external social network if the company is already using one.\
 This is a feature present in most collaborative games, allowing the players to communicate with their peers instantly.\
● Profile information and rankings: One of the important game elements used in gamification is continuous feedback on the actions of a user; that is, the users can immediately see the results of their actions in the games. The engine covers this need in two ways. First, the achievements assigned to each received behavior are returned to the application that communicated the behavior, so they can be immediately shown to the user. Secondly, the engine provides all the tools with the whole set of information making up the user profile (personal data, level, and achievements obtained to date), also giving rankings that allow users to compare their performance in the gamified environment with the performance of the rest of the users (all users, or only their friends).\
 Quests allow users to challenge other users to achieve a certain goal in a given period. That goal can be expressed as a certain number of points or badges of a given type.]]></content:encoded></item><item><title>What Gamification Means for Software Projects</title><link>https://hackernoon.com/what-gamification-means-for-software-projects?source=rss</link><author>Gamifications FTW Publications</author><category>tech</category><pubDate>Sun, 26 Jan 2025 23:02:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Oscar Pedreira, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(2) Felix García, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(3) Mario Piattini, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(4) Alejandro Cortinas, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(5) Ana Cerdeira-Pena, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica.The software architecture and the gamification engine are based on a model composed of three main elements: behaviors, achievements, and game rules. The gamification engine will receive behaviors carried out by the users in their respective tools, and will evaluate these according to the game rules defined by a designer (administrator), to assign the corresponding achievements to those behaviors if the game rules consider them successful.\
This model is a central component of our architecture, since it allows the designers of the gamified environments to define behaviors, achievements, and evaluation rules using concepts that are independent of any particular SE work environment we would consider. Although the details of the gamification engine we have implemented are presented in Section 4, some screenshots are included in this section, as they may clarify how the designer can use the concepts of the gamification model in a real case.\
In this section we will use a simple guiding example. For the sake of simplicity, let us assume that a generic software development organization wants to gamify its SE environment, focusing on the areas of project management, requirements, and testing. Employee actions receiving awards would include those, such as finishing development tasks, registering requirements in the system, commenting on existing requirements, creating test cases, writing unit tests, or closing the project.\
The rest of this section presents the details of the gamification model.\
Different types of behavior can occur in a software development environment. Instead of trying to identify and model all those particular and specific behaviors, however, we have extracted the features they have in common, and have aggregated them in three types of behavior (summarized in the diagram shown in Fig. 2):\
 This type of behavior is designed for those behaviors where we are only interested in knowing that they have actually happened, as well as who has carried out the behavior, and when there is no need for any other data about the action.\
For example, we could define simple behaviors for requirement management actions as being those of registering a new requirement into the system, commenting on an existing requirement to clarify its description, changing its state, or labeling the requirement as completed. These are simple behaviors if we assume that we would not need other data from those actions, apart from the fact that they have happened and who carried them out.\
 They are those behaviors in which we are also interested in parameters related to the development and completion of typical tasks in an SE environment, such as the effort, cost, quality, or the completion date. More specifically, the task behaviors currently include the following attributes:\
– Planned completion date: completion date for the task in the project plan.\
 date on which the task was actually completed.\
 estimated effort, in hours, to complete the task. \
 real number of hours needed to complete the task. \
 work units refer to tangible results of the task, such as lines of code, classes, or requirements, for example. This attribute corresponds to the estimated number of work units for the task, if this has been estimated. \
 real number of work units completed during the task. – Unit type: the name of the work units that are being considered. \
 this attribute, which takes values between 0 and 100, allows us to take into account the quality of the results obtained during the task.\
As we will see later, these attributes of a task behavior can be used in the definition of the gamification rules. For example, we could reward the finishing of a task only if it has been completed by the planned completion date, with the estimated effort, and with a given quality level. None of these attributes is mandatory, so we can use just those ones that are of interest in each particular case.\
Task behaviors are designed mainly for those tasks that would appear in a project plan, or in a product backlog, for example. The most obvious example for task behaviors is development tasks. That is, when a developer marks a task as completed, the system would notify that action to the gamification engine, indicating the estimated and real dates and effort. However, task behaviors may also apply to other actions.\
 They represent actions in which two people have collaborated in some way. This type of behaviors is concerned with rewarding the collaboration in the workplace. For example, we could use it to record that two people have interacted because one of them has created a task and has assigned it to the other, or because one of them has registered a requirement in the system, and the other person has commented on that requirement.\
As we will see in the next section, these classes of behaviors will also allow us to derive an interaction graph from which important information can be extracted, such as the interaction network of each user, relevant users that act as hubs or links, and the existence of communities that can be automatically identified from this information.\
Figure 2 shows a class diagram summarizing the behavior types. As we can see in the diagram, the model also considers maintaining who has carried out the task, the tool from which the behavior was received, the project in which it has been carried out, and the date and hour on which the action has taken place. These attributes allow the gamification engine to keep a persistent log of all the actions carried out by team members in each project, which is a valuable information.\
Notice also that all behavior types include two more attributes: artifactId and artifactName. Most tasks in an SE environment give as a result a project artifact, such as a document, or a task in the project plan, for example. These attributes allow us to include in each behavior the identifier and name of the resulting artifact. For example, in the behavior “Task completed”, we could indicate the identifier and name of the task. As we will see in the presentation of the gamification rules, the attributes can also be used in the definition of the rules, as well as in the messages that will be shown to the user when receiving a reward. These three types of behavior cover most actions that could take place in an SE development environment. Although the model currently considers these types of behaviors, it could be easily extended to support new ones, if we detected a kind of action that does not fit in to these three types.\
When configuring the gamified environment, the administrator will start by defining the behaviors that are subject to being evaluated and rewarded. For each of them, the administrator will have to indicate for each behavior only its identifier (a string), its type (simple, task, or interaction behavior), its name, its description, and its category. The identifier is a key point, since it will be used by the gamified tools when communicating behaviors to indicate what action they are communicating. \
Example: Figure 3 shows a screenshot of the behavior definition screen in the gamification engine we have implemented. In this example, we have defined just four behaviors: create a task (GSE CREATE TASK), complete a task (GSE TASK COMPLETED), detect an error (GSE ERROR DETECTED), and comment on a project requirement (GSE COMMENT REQ).\
When the rules of the game determine that a user has successfully completed a behavior, the system will reward that user with an achievement. The model has been designed to provide a flexible range of achievements. Three classes of achievements are currently supported:\
 (also called experience points): They are the basic reward mechanism, with a role analogous to what this type of achievement has in classic games. The number of points is a measure of the amount of successful behaviors completed by each user. In addition, the experience points also determine the level of the player\
The environment designer could even define more than one type of points (in order to distinguish between clearly different groups of behaviors). However, one of them must be used as the basis for computing the level of each player. \
● : They are a classical achievement type in gamification. Badges are usually granted when a significant milestone in the gamified environment is reached.\
The designer of the gamified environment can define as many badges as needed. For example, we could grant a badge on a developer’s first 100K line of code, or\
establish badges for the best analyst, best developer, and best tester of the month. Other badges could be created, depending on the design of the gamified environment.\
● : They meant to represent real-world rewards for the players. For example, resources could be used to reward the players with physical gifts, or time packages they can devote to personal projects or training, for example.\
This set of achievements will allow us to apply the most typical game mechanics used in gamification. Experience points, badges, and resources are all direct rewards, but we can also use them to implement levels, leaderboards, social status, and even quests.\
The diagram shown in Fig. 4 summarizes the achievement classes currently considered in our gamification model. The model allows the environment designer to define as many achievement types as needed, each of them belonging to one of the achievement classes we have just established. That is, although the gamification model currently provides the three types of achievement we have presented, it allows the designer of the gamified environment to define new types of achievements, like currencies.\
: although levels are not a particular class of achievement, they are directly derived from the experience points of the players through an exponential function that can be customized by the environment administrator,\
where  is the level, and  returns the number of experience points necessary to achieve level . For example, with values a = 1, b =1.4, and c = 2, the number of points necessary to achieve the first nine levels are shown in Table1.\
In this way, the difficulty of getting to the next level is completely customizable. It can be made linear, or\
exponential, as in our example, making it increasingly difficult to get to the next level.\
\
The link between the user’s behaviors and the achievements is established by the gamification rules. The model provides a gamification rule system that allows the environment designer to define a complete set of rules in a flexible way. This is the most important component of the model, since it removes the logic of gamification from the gamified tools, and it allows centralizing it in a gamification engine.\
A game rule maps behaviors to achievements. Each rule has a source type of behavior and many target types of achievement. Every time a behavior from the source type is received at the engine, all game rules with that source type of behavior are activated and evaluated by the gamification engine. Each rule is associated to its types of achievement through an achievement modifier, which represents the condition that uses the behavior’s attributes to define the criteria determining whether the achievement is granted or not.\
: In the example we are using for presentation of the gamification model, the organization could be interested in defining a rule for the behavior “Task completed”, which is a task behavior. On receiving such a behavior, we would like to reward the user in different ways depending on whether or not the task has been completed within the parameters of estimated effort. The definition of such a rule is shown in Table 2.\
As we can see in the example shown in Table 2, the rule “Task completion” is activated when a “Task\
completed” behavior is received, and three possible achievements are evaluated. In the first one, if the user has completed the task with an effort less than the estimated one, he or she is rewarded with as many experience points as the effort estimation of the task. In the second achievement, if the real effort is greater than or equal to that estimated, the user is rewarded with a number of points equal to what was estimated, minus a penalization for the number of hours he/she has exceeded the estimation. Finally, in the third achievement, if the user has completed the task in less than half the estimated effort, he is rewarded with an extra badge of “Star performer”. It is important to notice that all the conditions and modifiers used in this example can be specified in the gamification engine using the behaviors attributes.\
The gamification engine could now receive “Task completed” behaviors from any tool, such as Jira, or Redmine, for example, which would communicate those behaviors with the real attributes of how a user has completed a task in that tool. Let us look now at what would happen in the following three cases:\
 John completes a task with 20 estimated hours in just 18. In this case, he is rewarded with the Achievement 1; that is, 20 experience points. \
 John completes that task in 22 hours. In this case he receives the Achievement 2, that is, 18 experience points (20 – (22 – 20)). \
 John completes the same task in just 8 hours. In this case, John will receive two Achievements, the first and the third. For the first one he receives 20 experience points, and for the third one he receives a “Star performer” badge, since he has completed the task in less than half the estimated time.\
Figure 5 shows a screenshot of the rule definition screen in our gamification engine implementation, with the same example we have just presented. As we can see in the screenshot, this rule, “Task completion”, will be activated when a “Task completed” behavior is received, with three possible achievements for the\
\
user who has completed the task. In Fig. 5, we can also see that, in addition to the definition of the conditions for each achievement, the administrator of the gamified environment can introduce messages that will be shown to the user who completed the task, when obtaining each of the achievements. Notice that the messages like “Congrats! You’ve completed a task! (Task #id, #name)” can also use the attributes of the behavior that has been evaluated. In this example, #id and #name correspond to the attributes artifact and artifactName of the task behavior. When evaluating a particular behavior, the message template would be transformed into a real message, such as “Congrats! You’ve completed a task! (Task 45, User authentication)”. These messages can be shown on the player’s site or in the tools that have communicated the behavior to the gamification engine. The example shown in Fig. 5 demonstrates that the designer also has the option of awarding a given achievement to a type of behavior only the first time that a behavior of that type is evaluated. This would allow us to define a rule that, for example, awards a “First task completed!” badge to a player only the first time he/she carries out a task in the system. It is worth highlighting that the conditions of the rules are established through the graphical interface of the engine, that is, without modifying its source code.\
Although this example is simple, it shows the flexibility of the rule system of the gamification model. As we have just seen, the designer of the gamified environment can establish any condition on the received behaviors, and can also use its attributes when awarding achievements. This provides us with a high degree of flexibility in rule definition. We should also point out that the tool in which the user carries out the behavior knows nothing about how the action is gamified; it simply has to communicate it to the engine. This allows us to integrate and therefore to gamify as many heterogeneous SE tools as required. The example we have just shown considers the simplest type of rule supported by the engine. Actually, we distinguish between three types of rules:\
 They are gamification rules that evaluate just the condition of each achievement on the received behaviors, determining if an achievement must be awarded to the player. \
 These rules award the achievements only when the conditions are evaluated successfully a given number of times; in other words, a number of behaviors that fulfill the required condition were received. Besides, it can be specified that the behaviors must be received within a closed period of time, defined by start and end dates.\
● Interval repetitive rules: These are also repetitive rules, but instead of defining start and end date, a generic interval of time (i.e., week, month) is selected, so a number of behaviors that fulfill the condition must be received within this period.\
These types of rules allow us to reward behaviors not just when they happen, but when they happen repeatedly in time. For example, we could reward a developer for completing one hundred tasks, or for completing those one hundred tasks in a month.\
Figure 6 shows a class diagram summarizing the design of the gamification rules in the model. Since a complete gamified environment can have a large set of rules; these can be grouped into games. In this context, therefore, a game is defined as a set of related rules. The designer can even configure that only some particular games are played in a project.\
Notice that although the examples we have used in the description of the rules involved mainly task completion time, the types of rules we have considered allow us to reward behaviors more than just finishing on time or in cost. In addition, that the TaskBehavior type of behavior includes a grade attribute, intended to reflect the quality of the work.]]></content:encoded></item><item><title>A Software Architecture for the Gamification of SE Environments</title><link>https://hackernoon.com/a-software-architecture-for-the-gamification-of-se-environments?source=rss</link><author>Gamifications FTW Publications</author><category>tech</category><pubDate>Sun, 26 Jan 2025 23:02:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[(1) Oscar Pedreira, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(2) Felix García, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(3) Mario Piattini, Universidad de Castilla-La Mancha, Grupo Alarcos, Escuela Superior de Informatica, Paseo de la Universidad;(4) Alejandro Cortinas, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica;(5) Ana Cerdeira-Pena, Universidade da Coruna, Centro de Investigacion CITIC, Laboratorio de Bases de Datos, Facultade de Informatica.3 A Software Architecture for the Gamification of SE EnvironmentsIn this section, we present our software architecture for the gamification of SE environments. The proposal has two parts. First, we present the software architecture and its main components: the gamification engine and the software mechanisms to integrate the gamification engine with the organization’s Computer-Assisted Software Engineering (CASE) tools. Second, we present the gamification model that has guided the design and implementation of the gamification engine. This gamification model defines the gamification concepts, elements, and techniques supported by the gamification engine, such as (1) behaviors (that represent people’s actions in the work environment), (2) achievements (that represent rewards such as points, badges, or resources), and (3) the rules that establish the relationship between behaviors and their corresponding achievements.3.1 Software architectureThe purpose of the architecture is to make the task of gamifying the complete tool suite of a company easier. In order to do this, the business logic related to gamification is moved from the CASE tools to a gamification engine that centralizes and integrates it for all the tools. The basic idea of the architecture is the following: the gamified tools (SE tools covering any software lifecycle activity, such as development, requirements management, project management, or testing, for example) only have to communicate the actions (behaviors) carried out by their users to a central gamification engine. When those behaviors are received in the gamification engine, they are evaluated according to a set of gamification rules defined by the designer of the gamified environment. If a behavior is evaluated as successful according to those rules, the engine will generate the corresponding achievements for the user responsible for that behavior.\
Figure 1 shows a high-level view of the architecture. As we can see in the diagram, the gamification engine is the central element of the architecture, since it receives all the behaviors carried out by the software engineers and evaluates them. The engine provides an integration REST API that allows any other tools to communicate with it. This integration API includes a large list of operations that allow those tools to access all the information from the gamified environment, including those operations for communicating the player’s behaviors. Another important part of the architecture is the player’s site, which allows players to visualize all the information of the gamified environment, including the user’s actions and achievements, and also other gamification elements, such as rankings or progress charts.\
The main advantage of this architecture is that many tools can be included in the same gamified environment. For example, we could gamify tools, such as Jira (https://atlassian.com/software/jira), Eclipse (https://eclipse.org), Redmine (http://www.redmine. org/), or TestLink (http://testlink.org/), the rewards obtained by the players as a consequence of their actions in one of these tools would be added onto the rewards obtained from their actions in any of the other tools. If these tools were gamified separately, it would be difficult to integrate all the rewards obtained by each player. In addition, the logic of gamification would have to be repeated in all of them. However, our gamification engine provides the designer of the gamified environment with generic types of gamification rules that are tool-independent, and which can therefore fit all of them. This design choice simplifies greatly the introduction of gamification in the tools used by the software engineers.]]></content:encoded></item><item><title>Perplexity submits a new bid for TikTok</title><link>https://techcrunch.com/2025/01/26/perplexity-submits-a-new-bid-for-tiktok/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 26 Jan 2025 22:16:03 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Perplexity AI has submitted a revised proposal to merge with TikTok, in an arrangement that would give the U.S. government up to 50 percent ownership of the new entity. The Associated Press first reported on the new proposal. A source with knowledge of the bid confirmed to TechCrunch that the AP’s reporting is accurate. The […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Bad Week for Unoccupied Waymo Cars: One Hit in Fatal Collision, One Vandalized by Mob</title><link>https://tech.slashdot.org/story/25/01/26/2150209/bad-week-for-unoccupied-waymo-cars-one-hit-in-fatal-collision-one-vandalized-by-mob?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 21:52:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[For the first time in America, an empty self-driving car has been involved in a fatal collision. But it was "hit from behind by a speeding car that was going about 98 miles per hour," a local news site reports, citing comments from Waymo. ("Two other victims were taken to the hospital with life-threatening injuries. A dog also died in the crash, according to the San Francisco Fire Department.") 

Waymo's self-driving car "is not being blamed," notes NBC Bay Area. Instead the Waymo car was one of six vehicles "struck when a fast-moving vehicle slammed into a line of cars stopped at a traffic light..."


 The National Highway Traffic Safety Administration requires self-driving car companies, like Waymo, to report each time their vehicles are involved in an accident, regardless of whether the autonomous vehicle was at fault. According to NHTSA, which began collecting such data in July 2021, Waymo's driverless vehicles have been involved in about 30 different collisions resulting in some type of injury. Waymo, however, has noted that nearly all those crashes, like Sunday's collision, were the fault of other cars driven by humans. While NHTSA's crash data doesn't note whether self-driving vehicles may have been to blame, Waymo has previously noted that it only expects to pay out insurance liability claims for two previous collisions involving its driverless vehicles that resulted in injuries. 


In December, Waymo touted the findings of its latest safety analysis, which determined its fleet of driverless cars continue to outperform human drivers across major safety metrics. The report, authored by Waymo and its partners at the Swiss Reinsurance Company, reviewed insurance claim data to explore how often human drivers and autonomous vehicles are found to be liable in car collisions. According to the study, Waymo's self-driving vehicles faced about 90% fewer insurance claims relating to property damage and bodily injuries compared to human drivers... The company's fleet of autonomous vehicles have traveled more than 33 million miles and have provided more than five million rides across San Francisco, Los Angeles, Phoenix and Austin... 

In California, there are more than 30 companies currently permitted by the DMV to test driverless cars on the open road. While most are still required to have safety drivers sitting in the front seat who can take over when needed, Waymo remains the only fleet of robotaxis in California to move past the state's testing phase to, now, regularly offer paid rides to passengers. 

Their article adds that while Sunday's collision marks the first fatal crash involving a driverless car, "it was nearly seven years ago when another autonomous vehicle was involved in a deadly collision with a pedestrian in Tempe, Arizona, though that self-driving car had a human safety driver behind the wheel. The accident, which occurred in March 2018, involved an autonomous car from Uber, which sold off its self-driving division two years later to a competitor." 

In other news, an unoccupied Waymo vehicle was attacked by a mob in Los Angeles last night, according to local news reports. "Video footage of the incident appears to show the vehicle being stripped of its door, windows shattered, and its Jaguar emblems removed. The license plate was also damaged, and the extent of the vandalism required the vehicle to be towed from the scene." 
The Los Angeles Times reminds its readers that "Last year, a crowd in San Francisco's Chinatown surrounded a Waymo car, vandalized it and then set it ablaze..."]]></content:encoded></item><item><title>ARCTIC Freezer 4U-M Cooler For Ampere Altra 4U Servers/Workstations</title><link>https://www.phoronix.com/review/arctic-freezer-4u-m-ampere</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 21:46:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[For those looking for a heatsink fan to cool a custom build of an Ampere Altra / Altra Max server or workstation, the ARCTIC Freezer 4U-M ends up being a very potent option that offers similar performance to more expensive Ampere Altra heatsinks while providing similar performance.]]></content:encoded></item><item><title>DeepSeek gets Silicon Valley talking</title><link>https://techcrunch.com/2025/01/26/deepseek-gets-silicon-valley-talking/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 26 Jan 2025 20:49:37 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Since Chinese AI company DeepSeek released an open version of its reasoning model R1 at the beginning of this week, many in the tech industry have been making grand pronouncements about what the company achieved, and what it means for the state of AI. Venture capitalist Marc Andreessen, for example, posted that DeepSeek is “one […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Cory Doctorow Asks: Can Interoperability End &apos;Enshittification&apos; and Fix Social Media?</title><link>https://tech.slashdot.org/story/25/01/26/2043253/cory-doctorow-asks-can-interoperability-end-enshittification-and-fix-social-media?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 20:46:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[This weekend Cory Doctorow delved into "the two factors that make services terrible: captive users, and no constraints."


If your users can't leave, and if you face no consequences for making them miserable (not solely their departure to a competitor, but also fines, criminal charges, worker revolts, and guerrilla warfare with interoperators), then you have the means, motive and opportunity to turn your service into a giant pile of shit... Every economy is forever a-crawl with parasites and monsters like these, but they don't get to burrow into the system and colonize it until policymakers create rips they can pass through. 

Doctorow argues that "more and more critics are coming to understand that lock-in is the root of the problem, and that anti-lock-in measures like interoperability can address it."

Even more important than market discipline is government discipline, in the form of regulation. If Zuckerberg feared fines for privacy violations, or moderation failures, or illegal anticompetitive mergers, or fraudulent advertising systems that rip off publishers and advertisers, or other forms of fraud (like the "pivot to video"), he would treat his users better. But Facebook's rise to power took place during the second half of the neoliberal era, when the last shreds of regulatory muscle that survived the Reagan revolution were being devoured... But it's worse than that, because Zuckerberg and other tech monopolists figured out how to harness "IP" law to get the government to shut down third-party technology that might help users resist enshittification... [Doctorow says this is "why companies are so desperate to get you to use their apps rather than the open web"] IP law is why you can't make an alternative client that blocks algorithmic recommendations. IP law is why you can't leave Facebook for a new service and run a scraper that imports your waiting Facebook messages into a different inbox. IP law is why you can't scrape Facebook to catalog the paid political disinformation the company allows on the platform...
 
But then Doctorow argues that "Legacy social media is at a turning point," citing as "a credible threat" new systems built on open standards like Mastodon (built on Activitypub) and Bluesky (built on Atproto):

I believe strongly in improving the Fediverse, and I believe in adding the long-overdue federation to Bluesky. That's because my goal isn't the success of the Fediverse — it's the defeat of enshtitification. My answer to "why spend money fixing Bluesky?" is "why leave 20 million people at risk of enshittification when we could not only make them safe, but also create the toolchain to allow many, many organizations to operate a whole federation of Bluesky servers?" If you care about a better internet — and not just the Fediverse — then you should share this goal, too... Mastodon has one feature that Bluesky sorely lacks — the federation that imposes antienshittificatory discipline on companies and offers an enshittification fire-exit for users if the discipline fails. It's long past time that someone copied that feature over to Bluesky. 
Doctorow argues that federated and "federatable" social media "disciplines enshittifiers" by freeing social media's captive audiences. 
"Any user can go to any server at any time and stay in touch with everyone else."]]></content:encoded></item><item><title>Why Reid Hoffman feels optimistic about our AI future</title><link>https://techcrunch.com/2025/01/26/why-reid-hoffman-feels-optimistic-about-our-ai-future/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sun, 26 Jan 2025 18:54:39 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In Reid Hoffman’s new book Superagency: What Could Possibly Go Right With Our AI Future, the LinkedIn co-founder makes the case that AI can extend human agency — giving us more knowledge, better jobs, and improved lives — rather than reducing it. That doesn’t mean he’s ignoring the technology’s potential downsides. In fact, Hoffman (who […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>California&apos;s Battery Plant Fire Sparks Call for Investigation, New Regulations</title><link>https://hardware.slashdot.org/story/25/01/26/1841249/californias-battery-plant-fire-sparks-call-for-investigation-new-regulations?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 18:43:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Earlier this month a major fire erupted at a California battery plant. But several factors contributed to its rapid spread, the fire district's chief told the Los Angeles Times:
A fire suppression system that is part of every battery rack at the plant failed and led to a chain reaction of batteries catching on fire, he said at a news conference last week. Then, a broken camera system in the plant and superheated gases made it challenging for firefighters to intervene. Once the fire began spreading, firefighters were not able to use water, because doing so can trigger a violent chemical reaction in lithium-ion batteries, potentially causing more to ignite or explode. 
The county's Board of Supervisors has now requested that the plant remain offline until an investigation is completed. A county supervisor told the newspaper "What we're doing with this technology is way ahead of government regulations and ahead of the industry's ability to control it." 

And plans for a new battery storage site nearby are now being questioned, with an online petition to halt all new battery-storage facilities in the county drawing over 3,200 signatures.

The fire earlier this month was the fourth at Moss Landing since 2019, and the third at buildings owned by Texas-based Vistra Energy... Already, the fire has prompted calls for additional safety regulations around battery storage, and more local control over where storage sites are located... 
California Assemblymember Dawn Addis (D-Morro Bay) has introduced Assembly Bill 303 — the Battery Energy Safety & Accountability Act — which would require local engagement in the permitting process for battery or energy storage facilities, and establish a buffer to keep such sites a set distance away from sensitive areas like schools, hospitals and natural habitats... Gov. Gavin Newsom, a fierce advocate of clean energy, agrees an investigation is needed to determine the fire's cause and supports taking steps to make Moss Landing and similar facilities safer, his spokesperson Daniel Villaseñor said in a statement. Addis and two other state legislators sent a letter to the California Public Utilities Commission Thursday requesting an investigation. 

"The Moss Landing facility has represented a pivotal piece of our state's energy future, however this disastrous fire has undermined the public's trust in utility scale lithium-ion battery energy storage systems," states the letter. "If we are to ensure California moves its climate and energy goals forward, we must demonstrate a steadfast commitment to safety..." 

initial testing from the U.S. Environmental Protection Agency ruled that the levels of toxic gases released by the batteries, including hydrogen fluoride, did not pose a threat to public health during the fire. [The EPA says their monitoring "showed concentrations of particulate matter to be consistent with the air quality index throughout the Monterey Bay and San Francisco Bay regions, with no measurements exceeding the moderate air quality level... In addition to EPA's monitoring, Vistra Energy brought in a third-party environmental consultant with air monitoring expertise, right after the fire started"] 

Still, many residents remain on edge about potential long-term impacts on the nearby communities of Watsonville, Castroville, Salinas and the ecologically sensitive Elkhorn Slough estuary.]]></content:encoded></item><item><title>2025 will likely be another brutal year of failed startups, data suggests</title><link>https://techcrunch.com/2025/01/26/2025-will-likely-be-another-brutal-year-of-failed-startups-data-suggests/</link><author>Mary Ann Azevedo</author><category>tech</category><pubDate>Sun, 26 Jan 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[More startups shut down in 2024 than the year prior and a high volume of startup failures will continue, data from multiple sources.© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Europe Made More Electricity from Solar Than Coal In 2024</title><link>https://hardware.slashdot.org/story/25/01/25/0119258/europe-made-more-electricity-from-solar-than-coal-in-2024?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 15:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Long-time Slashdot reader AmiMoJo shared this report from the Guardian:
More electricity was made from sunshine than coal in the EU last year, a report has found, in what analysts called a "milestone" for the clean energy transition. Solar panels generated 11% of the EU's electricity in 2024, while coal-burning power plants generated 10%, according to data from climate thinktank Ember... 

Coal-burning in the EU power sector peaked in 2003 and has fallen by 68% since then. At the same time, clean sources of electricity have boomed. Wind and solar energy rose to 29% of EU electricity generation in 2024, while hydropower and nuclear energy continued to rebound from the 2022 lows... 

The report found the share of coal fell in 16 of the 17 countries that still used it in 2024. It said the fuel has become "marginal or absent" in most systems. Germany and Poland, the two countries that burn most of the EU's coal, were among those where there was a shift to cleaner sources of energy. The share of coal in Germany's electricity grid fell 17% year-on-year, while in Poland it dropped8%, the report found.
 
Fossil gas also fell for the fifth year in a row, declining in 14 of the 26 countries, according to the article, and now accounting for just 16% of the electricity mix. 

"The findings come despite a small increase in electricity demand after two years of steep decline brought on by Russia's full-scale invasion of Ukraine."]]></content:encoded></item><item><title>AMD Squeezes In More RDNA4 Changes For Linux 6.14 - Enables Cleaner Shader On GFX12</title><link>https://www.phoronix.com/news/AMDGPU-More-GFX12-Linux-6.14</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 15:25:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While the main feature pull of new and updated kernel graphics/accelerator drivers were merged already for the ongoing Linux 6.14 merge window, an additional set of AMDGPU changes were sent out this week for squeezing into this next kernel release...]]></content:encoded></item><item><title>Just How Many Robots Can One Person Control at Once?</title><link>https://spectrum.ieee.org/darpa-robot</link><author>Michelle Hampson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTY2OTkzMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1OTQwODEzNH0.qNbYE2RR9k_cPwEibfDENekp0ofA_li3CMU8nHx_qds/image.jpg?width=600" length="" type=""/><pubDate>Sun, 26 Jan 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[A DARPA project overturns longstanding assumptions ]]></content:encoded></item><item><title>New CIA Director Touts &apos;Low Confidence&apos; Assessment About Covid Lab Leak Theory</title><link>https://news.slashdot.org/story/25/01/26/0351245/new-cia-director-touts-low-confidence-assessment-about-covid-lab-leak-theory?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 12:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Slashdot reader DevNull127 writes: "Every US intelligence agency still unanimously maintains that Covid-19 was not developed as a biological weapon," CNN reported today. 

But what about the possibility of an accidental leak (rather than Covid-19 originating in wild animal meat from the Wuhan Market)? "The agency has for years said it did not have enough information to determine which origin theory was more likely." 

CNN notes there's suddenly been a new announcement "just days" after the CIA's new director took the reins — former lawyer turned Republican House Representative John Ratcliffe. While the market-origin theory remains a possibility according to the CIA, CNN notes that Ratcliffe himself "has long favored the theory that the pandemic originated from research being done in China and vowed in an interview published in Breitbart on Thursday that he would make the issue a Day 1 priority." 

"We have low confidence in this judgement," the CIA says in the complete text of its announcement, "and will continue to evaluate any available credible new intelligence reporting or open-source information that could change CIA's assessment." 


After speaking to a U.S. official, CNN added these details about the assessment:
It was not made based on new intelligence gathered by the US government — officials have long said such intelligence is unlikely to surface so many years later — and instead was reached after a review of existing information. 
"CIA continues to assess that both research-related and natural origin scenarios of the COVID-19 pandemic remain plausible," a CIA spokesperson said in a statement Saturday. 

CNN adds that "Many scientists believe the virus occurred naturally in animals and spread to humans in an outbreak at a market in Wuhan, China...."]]></content:encoded></item><item><title>Linux Patches Allow Sharing PTEs Between Processes - Can Mean Significant RAM Savings</title><link>https://www.phoronix.com/news/Linux-Sharing-PTEs-Processes</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 12:10:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A set of patches being worked on by Oracle engineers allow for optionally sharing page table entries (PTEs) between processes. For some workloads this can equate to very significant memory savings...]]></content:encoded></item><item><title>Mesa 25.0 Gets A New Vulkan Layer For Limiting The Amount Of Reported vRAM</title><link>https://www.phoronix.com/news/Mesa-Vulkan-vRAM-Report-Limit</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 11:49:20 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A half-year-old merge request from Igalia's Karmjit Mahil has been merged for Mesa 25.0 that is a Vulkan layer allowing for optionally limiting the amount of video memory reported to games/applications...]]></content:encoded></item><item><title>New Sound Hardware Supported By The Linux 6.14 Kernel</title><link>https://www.phoronix.com/news/Linux-6.14-New-Sound-Hardware</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 11:40:46 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged this week for the Linux 6.14 kernel were the various sound/audio driver updates. In addition to core API enhancements for better supporting the MIDI 2.0 specification, there is also support for some new audio hardware...]]></content:encoded></item><item><title>Linux 6.14 Adds ROCEv2 Support For The Alibaba Cloud</title><link>https://www.phoronix.com/news/Linux-6.14-RDMA</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 11:21:17 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The RDMA subsystem updates have been merged for the Linux 6.14 kernel with a light set of changes overall and the most significant being support for the ROCEv2 protocol within the ERDMA driver...]]></content:encoded></item><item><title>FSF: Meta&apos;s License for Its Llama 3.1 AI Model &apos;is Not a Free Software License&apos;</title><link>https://news.slashdot.org/story/25/01/25/2311217/fsf-metas-license-for-its-llama-31-ai-model-is-not-a-free-software-license?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 08:34:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[July saw the news that Meta had launched a powerful open-source AI model, Llama 3.1. 

But the Free Software Foundation evaluated Llama 3.1's license agreement, and announced this week that "this is not a free software license and you should not use it, nor any software released under it."


Not only does it deny users their freedom, but it also purports to hand over powers to the licensors that should only be exercised through lawmaking by democratically-elected governments. 

Moreover, it has been applied by Meta to a machine-learning (ML) application, even though the license completely fails to address software freedom challenges inherent in such applications.... 

We decided to review the Llama license because it is being applied to an ML application and model, while at the same time being presented by Meta as if it grants users a degree of software freedom. This is certainly not the case, and we want the free software community to have clarity on this.
 

In other news, the FSF also announced the winner of the logo contest for their big upcoming 40th anniversary celebration.]]></content:encoded></item><item><title>Bill Gates Began the Altair BASIC Code in His Head While Hiking as a Teenager</title><link>https://news.slashdot.org/story/25/01/26/0249204/bill-gates-began-the-altair-basic-code-in-his-head-while-hiking-as-a-teenager?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>tech</category><pubDate>Sun, 26 Jan 2025 05:04:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Friday Bill Gates shared an excerpt from his upcoming memoir Source Code: My Beginnings. Published in the Wall Street Journal, the excerpt includes pictures of young Bill Gates when he was 12 (dressed for a hike) and 14 (studying a teletype machine). 

Gates remembers forming "a sort of splinter group" from the Boy Scouts when he was 13 with a group of boys who "wanted more freedom and more risk" and took long hikes around Seattle, travelling hundreds of miles together on hikes as long as "seven days or more." (His favorite breakfast dish was Oscar Mayer Smokie Links.) But he also remembers another group of friends — Kent, Rick, and... Paul — who connected to a mainframe computer from a phone line at their private school. Both hiking and programming "felt like an adventure... exploring new worlds, traveling to places even most adults couldn't reach." 

Like hiking, programming fit me because it allowed me to define my own measure of success, and it seemed limitless, not determined by how fast I could run or how far I could throw. The logic, focus and stamina needed to write long, complicated programs came naturally to me. Unlike in hiking, among that group of friends, I was the leader. 
When Gates' school got a (DEC) PDP-8 — which cost $8,500 — "For a challenge, I decided I would try to write a version of the Basic programming language for the new computer..." And Gates remembers a long hike where "I silently honed my code" for its formula evaluator:

 I slimmed it down more, like whittling little pieces off a stick to sharpen the point. What I made seemed efficient and pleasingly simple. It was by far the best code I had ever written... 
By the time school started again in the fall, whoever had lent us the PDP-8 had reclaimed it. I never finished my Basic project. But the code I wrote on that hike, my formula evaluator — and its beauty — stayed with me. Three and a half years later, I was a sophomore in college not sure of my path in life when Paul Allen, one of my Lakeside friends, burst into my dorm room with news of a groundbreaking computer. I knew we could write a Basic language for it; we had a head start. 
Gates typed his code from that hike, "and with that planted the seed of what would become one of the world's largest companies and the beginning of a new industry." 

Gates cites Richard Feynman's description of the excitement and pleasure of "finding the thing out" — the reward for "all of the disciplined thinking and hard work." And he remembers his teenaged years as "intensely driven by the love of what I was learning, accruing expertise just when it was needed: at the dawn of the personal computer."]]></content:encoded></item><item><title>Shotcut 25.01 Open-Source Video Editor Brings New Features</title><link>https://www.phoronix.com/news/Shotcut-25.01-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Sun, 26 Jan 2025 01:49:12 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Shotcut open-source video editor is out with its first new release of 2025...]]></content:encoded></item><item><title>Real estate firms pivot to energy development amid booming data center demand</title><link>https://techcrunch.com/2025/01/25/amid-soaring-demand-for-data-centers-real-estate-companies-look-to-become-energy-developers/</link><author>Connie Loizos</author><category>tech</category><pubDate>Sun, 26 Jan 2025 00:33:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Brendan Wallace has a lot on his mind lately. Wallace is the co-founder of Fifth Wall Ventures, a nine-year-old proptech venture firm with $3.2 billion in assets under management. He’s also a homeowner in L.A., which continues to battle raging wildfires. While his place remains intact, many of his friends haven’t been so lucky.  Wallace […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Trump administration reportedly negotiating an Oracle takeover of TikTok</title><link>https://techcrunch.com/2025/01/25/trump-administration-reportedly-negotiating-an-oracle-takeover-of-tiktok/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 25 Jan 2025 22:56:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The Trump administration is negotiating a deal that would see Oracle take over TikTok alongside new U.S. investors, according to a report in NPR. Lawmakers passed a bill last year forcing Chinese parent company ByteDance to either sell TikTok or see it banned in the U.S. The app briefly went dark before the law took […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Will states lead the way on AI regulation?</title><link>https://techcrunch.com/2025/01/25/will-states-lead-the-way-on-ai-regulation/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 25 Jan 2025 21:37:14 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Last year was a busy time for lawmakers and lobbyists concerned about AI — most notably in California, where Gavin Newsom signed 18 new AI laws while also vetoing high-profile AI legislation. And 2025 could see just as much activity, especially on the state level, according to Mark Weatherford. Weatherford has, in his words, seen […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Intel Media Driver 2024Q4 Released With Battlemage Video Encode</title><link>https://www.phoronix.com/news/Intel-Media-Driver-2024Q4</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 25 Jan 2025 20:27:53 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Intel Media Driver 2024Q4 release was tagged this Saturday as the quarterly update to Intel's open-source Video Acceleration API (VA-API) driver for Linux systems...]]></content:encoded></item><item><title>OpenAI wants to take over your browser</title><link>https://techcrunch.com/2025/01/25/openai-wants-to-take-over-your-browser/</link><author>Cody Corrall</author><category>tech</category><pubDate>Sat, 25 Jan 2025 18:05:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Welcome back to Week in Review. This week we’re diving into OpenAI’s newly released AI agent, called Operator. We also look at where TikTok stands after being resuscitated, whether it’s time to go back to Tumblr, and more! Let’s get into it. OpenAI launched a research preview of Operator, a general-purpose AI agent that can […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Paul McCartney calls on UK government to protect artists from AI</title><link>https://techcrunch.com/2025/01/25/paul-mccartney-calls-on-uk-government-to-protect-artists-from-ai/</link><author>Anthony Ha</author><category>tech</category><pubDate>Sat, 25 Jan 2025 17:18:47 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Legendary musician Paul McCartney is warning against proposed changes to UK copyright law that would allow tech companies to freely train their models on online content unless the copyright holders actively opt out. In excerpts of an interview with the BBC, McCartney said the government needs to do more to protect musicians and other artists. […]© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>ISD: A New Interactive Way For systemd Management</title><link>https://www.phoronix.com/news/ISD-Interactive-Systemd</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 25 Jan 2025 14:28:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[ISD is a new open-source project aiming to provide a more "Interactive SystemD" for simplifying management of Linux systems with systemd...]]></content:encoded></item><item><title>Experts Weigh in on $500B Stargate Project for AI</title><link>https://spectrum.ieee.org/stargate</link><author>Eliza Strickland</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTgwNDk3NS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc1MzUyNjUyNn0.zn1Tvmzpar7av_olM9Ej9Up6yFLrkWwVGvcD0CRKabA/image.jpg?width=600" length="" type=""/><pubDate>Sat, 25 Jan 2025 14:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The Trump-announced initiative to build data centers will have big impacts]]></content:encoded></item><item><title>Much Faster Suspend &amp; Resume For Some Systems With Linux 6.14</title><link>https://www.phoronix.com/news/Linux-6.14-ACPI</link><author>Michael Larabel</author><category>tech</category><pubDate>Sat, 25 Jan 2025 12:19:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Alongside the power management and thermal driver updates this week for the ongoing Linux 6.14 kernel cycle were also the ACPI updates. The ACPI pull request was worth calling out on its own thanks to a change that will allow for faster suspend and resume cycles on some systems with this new kernel...]]></content:encoded></item></channel></rss>