<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>arXiv No Longer Accepts Computer Science Position or Review Papers Due to LLMs</title><link>https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</link><author>dw64</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 14:58:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go&apos;s Context Logger</title><link>https://github.com/pablovarg/contextlogger?tab=readme-ov-file#examples</link><author>/u/PurityHeadHunter</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 1 Nov 2025 14:24:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello Gophers! A while ago, I started using contextual logging in my projects and found it made debugging significantly easier. Being able to trace request context through your entire call stack is a game-changer for understanding what's happening in your system.This project started as a collection of utility functions I copy-pasted between projects. Eventually, it grew too large to maintain that way, so I decided to turn it into a proper library and share it with the community. https://github.com/PabloVarg/contextloggerContext Logger is a library that makes it easy to propagate your logging context through Go's  and integrates seamlessly with Go's standard library, mainly  and . If this is something that you usually use or you're interested on using it for your projects, take a look at some Usage Examples.For a very simple example, here you can see how to:Embed a logger into your contextUpdate the context (this can be done many times before logging)Log everything that you have included in your context so farctx = contextlogger.EmbedLogger(ctx) contextlogger.UpdateContext(ctx, "userID", user.ID) contextlogger.LogWithContext(ctx, slog.LevelInfo, "done") ]]></content:encoded></item><item><title>Async Rust explained without Tokio or Smol</title><link>https://youtu.be/_x61dSP4ZKM?si=XPDtuH13Du-s5KTD</link><author>/u/Gisleburt</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 14:00:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>unsupportedConfigOverrides USAGE</title><link>https://www.reddit.com/r/kubernetes/comments/1olodfm/unsupportedconfigoverrides_usage/</link><author>/u/BigBprofessional</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 13:52:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Need Advice: Bitbucket Helm Repo Structure for Multi-Service K8s Project + Shared Infra (ArgoCD, Vault, Cert-Manager, etc.)</title><link>https://www.reddit.com/r/kubernetes/comments/1olnp4b/need_advice_bitbucket_helm_repo_structure_for/</link><author>/u/Dependent_Concert446</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 13:22:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’m looking for some advice on how to organize our Helm charts and Bitbucket repos for a growing  setup.We currently have  that contains everything — about  several  (like ArgoCD, Vault, Cert-Manager, etc.).For our , we created  that’s used for microservices. We don’t have separate repos for each microservice — all are managed under the same project.Here’s a simplified view of the repo structure:app/ ├── project-argocd/ │ ├── charts/ │ └── values.yaml ├── project-vault/ │ ├── charts/ │ └── values.yaml │ ├── project-chart/ # Base chart used only for microservices │ ├── basechart/ │ │ ├── templates/ │ │ └── Chart.yaml │ ├── templates/ │ ├── Chart.yaml # Defines multiple services as dependencies using │ └── values/ │ ├── cluster1/ │ │ ├── service1/ │ │ │ └── values.yaml │ │ └── service2/ │ │ └── values.yaml │ └── values.yaml │ │ # Each values file under 'values/' is synced to clusters via ArgoCD │ # using an ApplicationSet for automated multi-cluster deployments The following  are also in the same repo right now:Project Contour (Ingress)(and other cluster-level tools like k3s, Longhorn, etc.)These are not tied to the application project — they’re might shared and deployed across multiple clusters and environments.Should I move these shared infra components into a separate “infra” Bitbucket repo (including their Helm charts, Terraform, and Ansible configs)?For GitOps with , would it make more sense to split things like this:  → all microservices + base Helm chart → cluster-level services (ArgoCD, Vault, Cert-Manager, Longhorn, etc.)How do other teams structure and manage their repositories, and what are the best practices for this in DevOps and GitOps? Used AI to help write and format this post for grammar and readability.]]></content:encoded></item><item><title>SQLite concurrency and why you should care about it</title><link>https://jellyfin.org/posts/SQLite-locking/</link><author>HunOL</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 12:59:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.SQLite is a file-based database engine running within your application and allows you to store data in a relational structure.
Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so.
Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.So an application that wants to use SQLite as its database needs to be the only one accessing it.
Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL).
The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file.
This allows multiple parallel writes to take place and get enqueued into the WAL.
When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly.
This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.A transaction is supposed to ensure two things.
Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction.
This is where it gets spicy and we come to the real reason why I am writing this blog post.
For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes.
This seems to be a not uncommon issue and there are many reports to be found on the issue.The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug.
From the reports this issue happens across all operating systems, drive speeds and with or without virtualization.
So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite.
During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal.
In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle.
While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error.
That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level.
EF Core supports a way of hooking into every command execution or transaction by creating Interceptors.
With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller.
The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary.
So, I decided on three locking strategies:As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override  in .Optimistic locking behavior​Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely.
This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.Jellyfin uses the  library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.Pessimistic locking behavior​Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed.
This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode.
While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.The future Smart locking behavior​In the future we might also consider combining both modes, to get the best of both worlds.Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here.
There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query.
Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.]]></content:encoded></item><item><title>Abandonware of the web: do you know that there is an HTML tables API?</title><link>https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/</link><author>begoon</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 12:58:21 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[When people turn data into  tables using JavaScript, they either use the  methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that  tables also have an old, forgotten  ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes,  tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the Codepen to see how you can create a table from a nested array:let table 
let b  document.
let t  document.
b.t
table.rowri
  let r  t.ri
  row.li
    let c  r.i
    c. llet table = [
  ['one','two','three'],
  ['four','five','six']
];
let b = document.body;
let t = document.createElement('table');
b.appendChild(t);
table.forEach((row,ri) => {
  let r = t.insertRow(ri);
  row.forEach((l,i) => {
    let c = r.insertCell(i);
    c.innerText = l;  
  })
});You can then access each table cell with an index (with t being a reference to the table):console.t..console.log(t.rows[1].cells[1]);
// => <td>five</td>You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:t.
t..
t...t.insertRow(-1);
t.rows[2].insertCell(0);
t.rows[2].cells[0].innerText = 'foo';There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.However, seeing how much of a pain it is to create tables, it would be fun to re-visit this  and add more functionality to it. We did add a lot of things to  forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.]]></content:encoded></item><item><title>We open-sourced a minimal NASDAQ ITCH parser in Rust. Built for clarity, not just speed. Here&apos;s how we pushed it to 107M msg/sec.</title><link>https://www.reddit.com/r/rust/comments/1oln5mw/we_opensourced_a_minimal_nasdaq_itch_parser_in/</link><author>/u/capitanturkiye</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 12:57:12 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[We just released Lunyn ITCH Lite, a minimal NASDAQ ITCH parser designed to be readable, reproducible, and easy to build on top of. And we published a technical deep dive on how the optimized version hits 107 million messages per second.We were frustrated with how opaque ITCH parsing was. Existing implementations either hide complexity behind vendor APIs or get bogged down in micro-optimization that obscures the core logic. So we built a version that strips everything away and focuses on clarity.Clean, readable, baseline Rust code to get startedNo SIMD intrinsics, no lock-free queues, no vendor tweaksTargets about 6-10M messages per second on commodity hardwareIntentionally leaves optimization and building as an exercise for youValidates message boundaries but doesn't decode fields (fast baseline)You can clone it right now and reproduce the numbers on your own hardware. Memory-mapped files, length-prefixed message scanning, full benchmark harness included.use lunyn_itch_lite::{Parser, ParseStats}; let buf = std::fs::read("/path/to/itch.bin")?; let mut parser = Parser::default(); let stats = parser.parse(&buf)?; println!("{} msgs in {:?} ({:.2}M/s)", stats.messages, stats.elapsed, stats.mps() / 1_000_000.0); That's it. The whole API.Then we built the optimized version.The blog post walks through every optimization decision:Zero-copy parsing (no allocations per message)SIMD vectorization (8x parallel field extraction)Lock-free concurrency (linear scaling to 16+ cores)Cache-aligned memory layouts (eliminate cache misses)Production-hardened error handlingEach section explains why the optimization matters and what the performance impact actually is. Numbers backed by real benchmarks against official NASDAQ data.The lite version is your baseline. Fork it, add SIMD, benchmark again, see the difference. That's how you learn where performance actually comes from instead of just cargo-culting optimizations.Binary protocol parsing shouldn't be a black box. Neither should high-performance systems design. If you want to build fast infrastructure, you need to understand these patterns. So we're open-sourcing the simple version and publishing the deep dive.The lite parser is good for education, research, and as a foundation for your own optimization work. The blog post is for anyone who wants to understand the decisions that take you from 10M to 107M messages per second.Happy to answer technical questions about the architecture, benchmarking methodology, or specific optimization decisions in the comments.]]></content:encoded></item><item><title>Hard Rust requirements from May onward for all Debian ports</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/pyeri</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:32:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Hard Rust requirements from May onward (for Debian&apos;s package manager, APT)</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/DeleeciousCheeps</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 12:24:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Python refuses $1.5M grant, Unity&apos;s in trouble, AUR attacked again - Linux Weekly News</title><link>https://tilvids.com/videos/watch/02a038db-fdd0-46d4-8cb2-1f0b1b0bd04d</link><author>/u/Pure_Toe6636</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:08:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>what exactly is open-sourced in grokipedia?</title><link>https://www.reddit.com/r/linux/comments/1olk43q/what_exactly_is_opensourced_in_grokipedia/</link><author>/u/nix-solves-that-2317</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:05:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Monthly: Certification help requests, vents, and brags</title><link>https://www.reddit.com/r/kubernetes/comments/1olk1no/monthly_certification_help_requests_vents_and/</link><author>/u/thockin</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 10:01:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Did you pass a cert? Congratulations, tell us about it!Did you bomb a cert exam and want help? This is the thread for you.Do you just hate the process? Complain here.(Note: other certification related posts will be removed)]]></content:encoded></item><item><title>Monthly: Who is hiring?</title><link>https://www.reddit.com/r/kubernetes/comments/1olk17i/monthly_who_is_hiring/</link><author>/u/gctaylor</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 10:00:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[This monthly post can be used to share Kubernetes-related job openings within  company. Please include:Location requirements (or lack thereof)At least one of: a link to a job posting/application page or contact detailsIf you are interested in a job, please contact the poster directly. Common reasons for comment removal:Not meeting the above requirementsRecruiter post / recruiter listingsNegative, inflammatory, or abrasive tone   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code (WASM 45% slower)</title><link>https://ar5iv.labs.arxiv.org/html/1901.09056</link><author>/u/Zomgnerfenigma</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 09:27:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Challenge of Benchmarking WebAssemblyThe aforementioned suite of 24 benchmarks is the PolybenchC benchmark
suite , which is designed to measure the effect of
polyhedral loop optimizations in compilers. All the benchmarks in the
suite are small scientific computing kernels rather than full
applications (e.g., matrix multiplication and LU Decomposition); each is
roughly 100 LOC. While WebAssembly is designed to accelerate scientific
kernels on the Web, it is also explicitly designed for a much richer set
of full applications.The WebAssembly documentation highlights several intended use
cases , including scientific kernels, image editing,
video editing, image recognition, scientific visualization, simulations,
programming language interpreters, virtual machines, and POSIX applications.
Therefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC
do not imply that it will perform well given a different kind of application.We argue that a more comprehensive evaluation of WebAssembly should rely on an
established benchmark suite of large programs, such as the SPEC CPU benchmark
suites. In fact, the SPEC CPU 2006 and 2017 suite of
benchmarks include several applications that fall under the intended use cases of
WebAssembly: eight benchmarks are scientific applications (e.g., ,
, , , and
), two benchmarks involve image and video processing
( and ), and all of the benchmarks are POSIX
applications.Unfortunately, it is not possible to simply compile a sophisticated
native program to WebAssembly. Native programs, including the programs in
the SPEC CPU suites, require operating system services, such as a
filesystem, synchronous I/O, and processes, which WebAssembly and the
browser do not provide. The SPEC benchmarking harness itself requires
a file system, a shell, the ability to spawn processes, and other Unix
facilities. To overcome these limitations when porting native
applications to the web, many programmers painstakingly modify their
programs to avoid or mimic missing operating system
services. Modifying well-known benchmarks, such as SPEC CPU, would not
only be time consuming but would also pose a serious threat to
validity.The standard approach to running these applications today is to use
Emscripten, a toolchain for compiling C and C++ to
WebAssembly . Unfortunately, Emscripten only supports
the most trivial system calls and does not scale up to large-scale
applications. For example, to enable applications to use synchronous
I/O, the default Emscripten  filesystem loads the entire
filesystem image into memory before the program begins executing. For
SPEC, these files are too large to fit into memory.A promising alternative is to use , a framework that enables
running unmodified, full-featured Unix applications in the
browser .  implements
a Unix-compatible kernel in JavaScript, with full support for
processes, files, pipes, blocking I/O, and other Unix features.
Moreover, it includes a C/C++ compiler (based on Emscripten)
that allows programs to run in the browser
unmodified. The  case studies include complex applications,
such as , which runs entirely in the browser without any
source code modifications.Unfortunately,  is a JavaScript-only solution, since it was
built before the release of
WebAssembly. Moreover,  suffers from high performance overhead,
which would be a significant confounder while benchmarking. Using ,
it would be difficult to tease apart the poorly performing benchmarks
from performance degradation introduced by .]]></content:encoded></item><item><title>You can&apos;t refuse to be scanned by ICE&apos;s facial recognition app, DHS document say</title><link>https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/</link><author>nh43215rgb</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 08:58:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Photos captured by Mobile Fortify will be stored for 15 years, regardless of immigration or citizenship status, the document says.]]></content:encoded></item><item><title>Hard Rust requirements from May onward</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>rkta</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 07:31:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Programming Language Agnostic Naming Conventions</title><link>https://codedrivendevelopment.com/posts/programmatic-naming-conventions-guide</link><author>/u/Distinct-Panic-246</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 07:30:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There is a famous quote when it comes to naming things in programming, which is attributed to Phil Karlton"There are only two hard things in Computer Science: cache invalidation and naming things"(Or the slight variation of "There are only two hard things in Computer Science: cache invalidation, naming things, and off by one errors")But over the last few decades there are definitely a few common conventions. Using standard names for things frees up time to work on tougher problems than naming, and means future readers of your code can probably understand the concept better.Why we spend time naming things correctlyIf you see a variable called , you can probably assume it is a boolean.  or  are not clear.Avoid Negative variable names:Negative names can lead to double negatives, which are confusing.Abbreviations can be ambiguous - not everyone will interpret it as the same meaning. Just use the full word, it is clearer.(Although  is probably an exception where it should always be used over ).Pick a language and always use thatIf you work in a modern company then it is likely you work with people originally from various countries around the world. It can be easy to end up with a codebase with a mix of words like  and .I'd recommend just picking US spelling in your code (even if the app is localised only for a UK or AU audiece)Make booleans obvious by using is/has prefixIf you name something , it is quite obvious that it is a boolean. Try to always do this, as something like  could read as if it wasn't a booleanWords like , ,  are too generic. Try to avoid these termsPick a convention for naming things, and use those everywhere.calculateAmountBad 👎:  and Good 👍:  and Also pick a style for casing, and be sure you're consistent with it. Here are some examples (there might be other typical conventions for your library/language of choice) for class names for most other variables for static constantsCommon names for specific thingsIf you're taking some data and  it to a different shape or different values then  is a common and accurate name.If you need to check if data is valid/correct, then its almost always called a validator.Used when describing the shape of some data structure. Often used with database designs.When you need to take some data (e.g. some string) and understand its own data structure. They are quite different things, parsers and transformers can  be very relatedFor code that runs 'between' different parts of your application. A typical use for middleware is in HTTP servers the incoming HTTP request can go through multiple middlewares to either transform the incoming data (before passing to next one or final endpoint handler function) or to do something with that dataWhen you have some functionality with a specific interface, and you need to convert it to another interface/shape.It is also known as a 'wrapper' (or a bridge, although that is technically a slightly different thing)When you need to make data uniform in scale, format, or structure]]></content:encoded></item><item><title>Well a old school flex i guess</title><link>https://www.reddit.com/r/linux/comments/1olftbt/well_a_old_school_flex_i_guess/</link><author>/u/Puzzleheaded-Car4883</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 05:16:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This old Red Hat Linux 8.0 manual’s been gathering dust on my shelf. I used to read it as a kid — didn’t understand a single word back then. Fast forward to age 19, 3 years into using Linux daily... and everything suddenly makes sense.Btw this is one of those first thing that introduced me to linux ]]></content:encoded></item><item><title>Happy Halloween, nerds</title><link>https://www.reddit.com/r/linux/comments/1olf21x/happy_halloween_nerds/</link><author>/u/feelingsupersonic</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 04:29:17 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Java Virtual Threads VS GO routines</title><link>https://www.reddit.com/r/golang/comments/1oldyoo/java_virtual_threads_vs_go_routines/</link><author>/u/gamecrow77</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 1 Nov 2025 03:25:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I recently had a argument with my tech lead about this , my push was for Go since its a new stack , new learning for the team and Go is evolving , my assumption is that we will find newer gen of devs who specialise in Go. Was i wrong here ? the argument was java with virtual threads is as efficient as go ]]></content:encoded></item><item><title>The profitable startup</title><link>https://linear.app/now/the-profitable-startup</link><author>doppp</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 03:18:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?But that thinking was always flawed.Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.Paul Graham famously wrote about "ramen profitability" – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.Understand Your Risk ProfileAre you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.Hire Intentionally and SlowerFor most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.]]></content:encoded></item><item><title>IRS open-sourced the fact graph it uses for tax law</title><link>https://github.com/IRS-Public/fact-graph</link><author>/u/R2_SWE2</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:48:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>shift left approach for requests and limits</title><link>https://www.reddit.com/r/kubernetes/comments/1olaat1/shift_left_approach_for_requests_and_limits/</link><author>/u/Containertester</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 00:10:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[We’re trying to solve the classic requests & limits guessing game; instead of setting CPU/memory by gut feeling or by copying defaults (which either wastes resources or causes throttling/OOM), we started experimenting with a benchmark-driven approach: we benchmark workloads in CI/CD and derive the optimal requests/limits based on http_requests_per_second (load testing).In our latest write-up, we share:Why manual tuning doesn’t scale for dynamic workloadsHow benchmarking actual CPU/memory under realistic load helps predict good limitsHow to feed those results back into Kubernetes manifestsSome gotchas around autoscaling & metrics pipelinesCurious if anyone here has tried a similar “shift-left” approach for resource optimization or integrated benchmarking into their pipelines and how that worked out.]]></content:encoded></item><item><title>Steinberg, creators of VST technology and the ASIO protocol, have released the SDKs for VST 3 and ASIO as Open Source.</title><link>https://www.reddit.com/r/linux/comments/1ola786/steinberg_creators_of_vst_technology_and_the_asio/</link><author>/u/fenix0000000</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:05:42 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/fenix0000000 ]]></content:encoded></item><item><title>Show HN: Strange Attractors</title><link>https://blog.shashanktomar.com/posts/strange-attractors</link><author>shashanktomar</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 23:23:59 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A few months back, while playing around with Three.js, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.The Basics: Dynamical Systems and Chaos TheoryDynamical Systems are a mathematical way to understand how things . Imagine you have a system, which
could be anything from the movement of planets to the growth of a population. In this system, there are rules that
determine how it evolves from one moment to the next. These rules tell you what will happen next based on what is
happening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a
population (we are all too familiar with this one), and stock market.There are two primary things to understand about this system:: This is like a big collection of all the possible states the system can be in. Each state is like a
snapshot of the system at a specific time. This is also called the  or the .: These are the rules that takes one state of the system and moves it to the next state. It can be
represented as a function that transforms the system from now to later.For instance, when studying population growth, a phase-space (world-state) might consist of the current population size
and the rate of growth or decline at a specific time. The dynamics would then be derived from models of population
dynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate
the changes in population size over time.Another way of saying this is that the dynamical systems describe how things change over time, in a space of
possibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics,
study systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in
relevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.There are different ways of classifying dynamical systems, and one of the most interesting is the classification into
chaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to
chaotic systems which exhibit randomness and unpredictability. is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional
deterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop
of ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets,
pendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow
deterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability,
might seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of
the time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex
systems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information
to make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially
observe) and which are very sensitive to small changes. A small change in the initial conditions can lead to a
completely different outcome, a phenomenon known as the . In this post, we will try to see the
butterfly effect in action but before that, let's talk about .To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems
are all about . During this change, the system moves through different possible states (remember the
phase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say,
towards which it is . It's like a magnet that pulls the system towards it.For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the
bottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of
the reasons why different states get attracted towards attractors:: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay
there. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This
loss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is
what happens in the case of the pendulum.: In some regions of the phase space, the system's dynamics cause trajectories to converge. This
contraction effect means that nearby states will tend to come closer together over time, eventually being drawn
towards the attractor.Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These
nonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These
complex attractors are categorised as . They are called "strange" due to their unique
characteristics.: Strange attractors often have a fractal-like structure, meaning they display intricate
patterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.Sensitive Dependence on Initial Conditions: Systems with strange attractors are highly sensitive to their initial
conditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known
as the "butterfly effect".Unpredictable Trajectories: The trajectories on a strange attractor never repeat themselves, exhibiting
non-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by
deterministic rules.Emergent Order from Chaos: Despite their chaotic nature, strange attractors exhibit a form of underlying order.
Patterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the
butterfly effect.A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small
changes in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the
. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to
drastically different outcomes.In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:A small change in the parameter  can lead to vastly different particle trajectories and the overall shape of the
attractor. Change this value in the control panel and observe the butterfly effect in action.There is another way of observing the butterfly effect in this visualisation. Change the  from  to
 in the control panel and observe how the particles move differently in the two cases. The particles
eventually get attracted to the same states but have different trajectories.This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a
technique called . This method handles iterative updates of particle systems directly on the GPU,
minimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One
stores the current state of particles and render them on the screen, while the other calculates the next state.Setting Up Frame Buffer Objects (FBOs): We start by creating two FBOs,  and , to hold the current and
next state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use
of GPU resources.Shader Programs for Particle Dynamics: The shader programs execute on the GPU and apply attractor dynamics to
each particle. Following is the attractor function which update the particle positions based on the attractor equation.Rendering and Buffer Swapping: In each frame, the shader computes the new positions based on the attractor's
equations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously
inactive buffer becomes active, and vice versa.This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.If you have any comments, please leave them on this GitHub discussions topic. Sooner or later, I will integrate it with the blog. The  discussion can be found here.]]></content:encoded></item><item><title>S.A.R.C.A.S.M: Slightly Annoying Rubik&apos;s Cube Automatic Solving Machine</title><link>https://github.com/vindar/SARCASM</link><author>chris_overseas</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 23:03:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[GoGreement] A new linter that can help enforce interface implementation and immutability</title><link>https://www.reddit.com/r/golang/comments/1ol8das/gogreement_a_new_linter_that_can_help_enforce/</link><author>/u/Green-Sympathy-2198</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 22:38:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey guys! I wrote this linter mainly for myself, but I hope some of you find it useful.I came to golang from JVM world and I was missing some things like explicit implementation declaration and immutability.But I see gophers love their linters, so I thought I could solve this with a linter.How does it work? You just add annotations to your types like: go // @immutable type User struct { id string name string } And run the linter and it will give you an error if you try to change fields like this: I also added annotations that let you check interface implementation: go // @implements io.Reader This lets you check that a struct actually implements an interface without all this stuff: go var _ MyInterface = (*MyStruct)(nil) And many other annotations (testonly, packageonly, ...). Would love to hear what you think!]]></content:encoded></item><item><title>Borrow checker says “No”! An error that scares me every single time!</title><link>https://polymonster.co.uk/blog/borow-checker-says-no</link><author>/u/__shufb</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 31 Oct 2025 22:25:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[It’s Halloween and I have just been caught out by a spooky borrow checker error that caught me by surprise. It feels as though it is the single most time consuming issue to fix and always seems to catch me unaware. The issue in particular is “cannot borrow x immutably as it is already borrowed mutably” - it manifests itself in different ways under different circumstances, but I find myself hitting it often when refactoring. It happened again recently so I did some investigating and thought I would discuss it in more detail.The issue last hit me when I was refactoring some code in my graphics engine hotline, I have been creating some content on YouTube and, after a little bit of a slog to fix the issue, I recorded a video of me going through the scenario of how it occurred and some patterns to use that I have adopted in the past to get around it. You can check out the video if you are that way inclined, the rest of this post will mostly echo what is in the video, but it might be a bit easier to follow code snippets and description in text.I have a generic graphics API, which consists of traits called gfx. This is there to allow different platform backends to implement the trait; currently I have a fully implemented Direct3D12 backend and I recently began to port macOS using Metal.The gfx backend wraps underlying graphics API primitives; in this case we are mostly concerned about  which is a command buffer. Command buffers are used to submit commands to the GPU. They do things like  or , amongst other things. For the purposes of this blog post, what the command buffer does is not really that important, just that is does , which at the starting point when the code was working is a trait method that takes an immutable self and another immutable parameter ie. fn do_something(&self, param: &Param).In the rest of the code base I have a higher level rendering system called . This is graphics engine code that is not platform specific but implements shared functionality. So where  is a low level abstraction layer,  implements concepts of a  that is a view of a scene that we can render from. A  has a camera that can look at the scene and is then passed to a render function, which can build a command buffer to render the scene from that camera’s perspective. The engine is designed to be multithreaded and render functions are dispatched through  systems, so a view gets passed into a render system but it is wrapped in an .I made a small cutdown example of this code to be able to demonstrate the problem I encounter, so let’s start with the initial working version:I tried to simplify it as much as possible so these snippets should compile if you copy and paste them, they won’t run thanks to  macro (which I absolutely love using, it is so handy!) but we only care about the borrow checker anyway.All we really need to think about is that a  can  and it also gets passed in a , which is also contained as part of ‘view’. Coming from a C/C++ background I landed on my personal preference being procedural C code with context passing, so I tend to group things together into a single struct. It makes sense to me in this case and I wanted to group everything inside , and we fetch the view from elsewhere in the engine.So the code in the snippet compiles fine and I was working with this setup for some time. I began work on macOS and it turned out that the  method needed to mutate the command buffer so that I could mutate some internal state and make the Metal graphics API behave similarly to Direct3D12. This is common for graphics API plumbing.The specific example in this case was that in Direct3D we call a function  to bind an index buffer before we make a call to , but in Metal there is no equivalent to bind an index buffer. Instead you pass a pointer to your index buffer when calling the equivalent draw indexed. So to fix this, when we call  we can store some extra state in the command buffer so we can pass it in the later call to .In hindsight any method on the command buffer trait that does anything, like set anything or write into the command buffer, should take a  because it is mutating the command buffer after all. In my case since I am calling through to methods on  , which is unsafe code and does not require any mutable references.In our simplified example, in order to store, state  now needs to change and take a mutable self: do_something(&mut self, param: &Param) it should be noted that  itself was already .Borrow checker now kicks in…my heart sinks. In the real code base not only did I have to modify a single call site, but I had hundreds of places where this error was happening, I made the decision here and now to make any methods that write to the command buffer also be mutable and make the mutabilityerror[E0502]: cannot borrow `view` as immutable because it is also borrowed as mutable
  --> src/main.rs:30:28
   |
30 |     view.cmd.do_something(&view.param);
   |     ----     ------------  ^^^^ immutable borrow occurs here
   |     |        |
   |     |        mutable borrow later used by call
   |     mutable borrow occurs here

For more information about this error, try `rustc --explain E0502`.
error: could not compile due to 1 previous error
This is not the first time I have encountered this problem and I doubt it will be the last. There are a number of ways to resolve it and they aren’t too complicated. The frustrating thing is that it seems to occur always when you are doing something else and not just when you decide to refactor, so you end up having a mountain of errors to solve before you can get back to the original task. I suppose you could call it a symptom of bad design or lack of experience, but when writing code things inevitably change and bend with new requirements, and Rust throws these unexpected issues up for me more often than I find with C, and often the required refactor takes more effort as well. But that is the cost you pay, hopefully more upfront effort to get past the borrow checker means fewer nasty debugging stages later. So let’s look at some patterns to fix the issue!The one I actually went for in this case was using . We take the  out of view so we no longer need to borrow a ‘view’ to use , and then when finished return the cmd into ‘view’. It is important to note here that  needs to derive default in order for this to work, as when we take the  in  will become This approach is the simplest I could think of at the time because any existing code using  doesn’t need updating, everything stays the same and we just separate the references. In this case it was easy to derive the default for  .You need to remember to set the  back on  here, which could be a pitfall and cause unexpected behaviour if you didn’t.If you can’t easily derive default on a struct there are some other options. If the struct is clonable or you can easily derive a clone, you can clone to achieve a similar effect.Cloning might be considered a heavier operation than ‘take’ depending on the circumstances, but this method has the same benefit as the take version whereby unaffected code that is using  elsewhere doesn’t need to be changed.Another approach would be to use  this allows for interior mutability and again we do not need to worry about default or clone.We also need to update any code that ever used  and do the same. Not ideal but it allows us to get around the need for a default or clone. I have had to resort to this in other places in the code base.There are more options; quite literally  here can help. If we make  an  then this gives us the ability to use  as the default and we can use the  approach. We can also use  and swap with . Swapping works similar to ‘take’, where we take mem and swap with the default.The  approach also requires more effort as we need to now take a reference and unwrap the option and update any code that ever used  to do the same. Not ideal, but it allows us to get around the need for a default or clone, and if your type is already optional then this will fit easily.There is one final approach that could save a lot of time, and that would be to not change the  function at all in the first place. That is to keep it as do_something(&self, param: &Param). So how do we mutate the interior state without requiring the self to be mutable?This can be done with  in single threaded code or  in multithreaded code. Since we already looked at  I will do an example of .I decided to make the mutability explicit to the trait and that was based on how the command buffers are used in the engine, in other places I have taken other approaches favouring interior mutability. For this case a view can be dispatched in parallel with other views, but the engine is designed such that 1 thread per view and no work happens to a single view on multiple threads at the same time. Command buffers are submitted in a queue in order and dispatched on the GPU.Here it made sense to me to avoid locking interior mutability for each time we call a method on a  and it works with the engine’s design. We lock a view at the start of a render thread, fill it with commands and then hand it back to the graphics engineer for submission to the GPU. The usage is explicit, we just needed to appease the borrow checker!I hope you enjoyed this article, please check out my YouTube channel for more videos or more articles on my blog, let me know what you think and if you have any other strategies or approaches I would love to hear about them. I would also like to hear about compiler and borrow checker errors you find particularly time consuming or frustrating to deal with.]]></content:encoded></item><item><title>What kind of debug tools are available that are cloud native?</title><link>https://www.reddit.com/r/kubernetes/comments/1ol64df/what_kind_of_debug_tools_are_available_that_are/</link><author>/u/lickety-split1800</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 31 Oct 2025 20:59:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm an SRE and a longtime Linux & automation person, starting in the late 90s.With the advent of apps on containers, there are fewer and fewer tools to perform debugging.Taking a look at the types of debug tools one has used to diagnose issues.even basic tools such as find, grep, ls and others are used in debugging.The Linux OS used to be under the control of the system administrator, who would put the tools required to meet operational debugging requirements, increasingly since it is the developer that maintains the container image and none of these tools end up on the image, citing most of the time startup time as the main requirement.Now a container is a slice of the operating system so I argue that the container base image should still be maintained by those who maintain Linux, because it's their role to have these tools to diagnose issues. That should be DevOps/SRE teams but many organisations don't see it this way.So what tools does Kubernetes provide that fulfil the needs I've listed above?]]></content:encoded></item><item><title>My Must-Have Apps Since Switching to Linux</title><link>https://www.reddit.com/r/linux/comments/1ol5a1k/my_musthave_apps_since_switching_to_linux/</link><author>/u/Overflow_Nuts</author><category>dev</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:24:34 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[OnlyOffice → If you’re used to MS Office, the interface feels almost identical — super easy to adapt.Brave / Zen → When I need a Chromium-based browser, I use Brave; when I need a Firefox-based one, Zen. Both are top-tier.Okular → Opens everything from PDFs to EPUBs.yt-dlp → Downloads videos and audio straight from the terminal — and not just from YouTube, it supports tons of platforms.Qbittorrent → Clean, simple, and easily the best torrent client out there.Stremio + Add-ons → The best torrent-based media player, hands down.KeepassXC → A simple yet powerful password manager with browser integration.LocalSend → Transfers files across all your devices locally, no internet needed.KDE Connect → Perfect bridge between your phone and computer.Bottles → Makes using Wine more stable and user-friendly.Espanso → Expands text shortcuts automatically — a real time-saver.Tmux → Lets you split your terminal and run multiple sessions at once.Btop / ytop / glances → Displays system resource usage right from the terminal.Fastfetch → A faster Neofetch alternative for system info.Syncthing → Syncs your files seamlessly between devices.Czkawka → Finds duplicate or junk files on your disk.Mpv + Plugins → Lightweight, scriptable video player.Input Leap → Control multiple computers with one keyboard and mouse.Zapret → Bypasses DPI-based network restrictions.Moonlight / Sunshine → Stream your games locally across your network.Heroic Games Launcher → Great alternative for Epic Games.Lutris → Customizable launcher supporting multiple game libraries.Prism Launcher → Clean, mod- and shader-friendly Minecraft launcher.Ente Auth → The best 2FA app I’ve tried — encrypted sync between devices.GDU → Visual disk usage analyzer.Newsboat → Read RSS feeds directly in the terminal.Neovim → Fast, lightweight text editor.Waypaper / Swaybg / Hyprpaper → Manage your wallpapers easily.Easy Effects → Lets you tweak and filter your system’s audio.Waybar (+ eww + rofi) → Build a fully customizable system bar.scrcpy → The simplest way to mirror your Android screen on your PC.Podman / Distrobox → Run another Linux environment inside a container.Wireshark / mitmproxy → Monitor and analyze your network traffic.Opensnitch → See which apps are making network connections.qutebrowser → A minimalist, keyboard-driven browser.fail2ban → The most satisfying way to troll persistent brute-forcers.qemu + Virt-Manager → Create and manage virtual machines easily.Waydroid → Run Android apps directly on Linux.Lf → Terminal-based file manager.These are the tools I’ve discovered and personally enjoy using on Linux. What about yours what are your must-have apps?]]></content:encoded></item><item><title>A theoretical way to circumvent Android developer verification</title><link>https://enaix.github.io/2025/10/30/developer-verification.html</link><author>sleirsgoevy</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 20:20:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Futurelock - Subtle Risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>/u/-Y0-</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 31 Oct 2025 20:20:15 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>/u/iamkeyur</author><category>dev</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 19:26:29 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Write PostgreSQL functions in Go Golang example</title><link>https://www.reddit.com/r/golang/comments/1ol2tqv/write_postgresql_functions_in_go_golang_example/</link><author>/u/WinProfessional4958</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 18:45:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It took me a while to figure this out. Go compiles the C files automatically.#include "postgres.h" #include "fmgr.h" PG_MODULE_MAGIC; extern int32 Adder(int32); PG_FUNCTION_INFO_V1(add_two); Datum add_two(PG_FUNCTION_ARGS) { int32 arg = PG_GETARG_INT32(0); PG_RETURN_INT32(Adder(arg)); } package main /* #cgo CFLAGS: -DWIN32 -ID:/pg18headers -ID:/pg18headers/port/win32 #cgo LDFLAGS: -LD:/pg18lib #include "postgres.h" #include "fmgr.h" // Forward declare the C function so cgo compiles add_two.c too. extern void init_add_two(); */ import "C" //export Adder func Adder(a C.int32) C.int32 { return a + 3 } func main() {} PS D:\C\myextension> go build -o add_two.dll -buildmode=c-sharedIn PostgreSQL: open the query window (adjust path to your generated dynamically loaded library and header file (.dll, .h).CREATE FUNCTION add_two(int4) RETURNS int4AS 'D:/C/myextension/add_two.dll', 'add_two']]></content:encoded></item><item><title>Go vs Kotlin: Server throughput</title><link>https://www.reddit.com/r/golang/comments/1ol1upp/go_vs_kotlin_server_throughput/</link><author>/u/iG0tB00ts</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 18:08:15 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Let me start off by saying I'm a big fan of Go. Go is my side love while Kotlin is my official (work-enforced) love. I recognize benchmarks do not translate to real world performance & I also acknowledge this is the first benchmark I've made, so mistakes are possible.That being said, I was recently tasked with evaluating Kotlin vs Go for a small service we're building. This service is a wrapper around Redis providing a REST API for checking the existence of a key.With a load of 30,000 RPS in mind, I ran a benchmark using  (the workload is a list of newline separated 40chars string) and saw to my surprise Kotlin outperforming Go by ~35% RPS. Surprise because my thoughts, few online searches as well as AI prompts led me to believe Go would be the winner due to its lightweight and performant goroutines.Go + net/http + go-redis Text Thread Stats Avg Stdev Max +/- Stdev Latency 4.82ms 810.59us 38.38ms 97.05% Req/Sec 5.22k 449.62 10.29k 95.57% 105459 requests in 5.08s, 7.90MB read Non-2xx or 3xx responses: 53529 Requests/sec: 20767.19  Kotlin + ktor + lettuce  Thread Stats Avg Stdev Max +/- Stdev Latency 3.63ms 1.66ms 52.25ms 97.24% Req/Sec 7.05k 0.94k 13.07k 92.65% 143105 requests in 5.10s, 5.67MB read Non-2xx or 3xx responses: 72138 Requests/sec: 28057.91 I am in no way an expert with the Go ecosystem, so I was wondering if anyone had an explanation for the results or suggestions on improving my Go code. ```Go package mainimport ( "context" "net/http" "runtime" "time""github.com/redis/go-redis/v9" var ( redisClient *redis.Client )func main() { redisClient = redis.NewClient(&redis.Options{ Addr: "localhost:6379", Password: "", DB: 0, PoolSize: runtime.NumCPU() * 10, MinIdleConns: runtime.NumCPU() * 2, MaxRetries: 1, PoolTimeout: 2 * time.Second, ReadTimeout: 1 * time.Second, WriteTimeout: 1 * time.Second, }) defer redisClient.Close()mux := http.NewServeMux() mux.HandleFunc("/", handleKey) server := &http.Server{ Addr: ":8080", Handler: mux, } server.ListenAndServe() // some code for quitting on exit signal // handleKey handles GET requests to /{key} func handleKey(w http.ResponseWriter, r *http.Request) { path := r.URL.Pathkey := path[1:] exists, _ := redisClient.Exists(context.Background(), key).Result() if exists == 0 { w.WriteHeader(http.StatusNotFound) return } Kotlin code for reference ```Kotlin // applicationfun main(args: Array<String>) { io.ktor.server.netty.EngineMain.main(args) }fun Application.module() { val redis = RedisClient.create("redis://localhost/"); val conn = redis.connect() configureRouting(conn) }fun Application.configureRouting(connection: StatefulRedisConnection<String, String>) { val api = connection.async()routing { get("/{key}") { val key = call.parameters["key"]!! val exists = api.exists(key).await() > 0 if (exists) { call.respond(HttpStatusCode.OK) } else { call.respond(HttpStatusCode.NotFound) } } } ]]></content:encoded></item><item><title>html/template: Why does it escape opening angle bracket?</title><link>https://www.reddit.com/r/golang/comments/1ol1cyi/htmltemplate_why_does_it_escape_opening_angle/</link><author>/u/cvilsmeier</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 17:49:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hi, html/template escapes input data, but why does it escape an angle bracket character ("<") in the template? Here is an example:package main import ( "fmt" "html/template" "strings" ) func main() { text := "<{{.tag}}>" tp := template.Must(template.New("sample").Parse(text)) var buf strings.Builder template.Must(nil, tp.Execute(&buf, map[string]any{"tag": template.HTML("p")})) fmt.Println(buf.String()) // Expected output: <p> // Actual output: &lt;p> }    submitted by    /u/cvilsmeier ]]></content:encoded></item><item><title>Addiction Markets</title><link>https://www.thebignewsletter.com/p/addiction-markets-abolish-corporate</link><author>toomuchtodo</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 17:42:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Use DuckDB-WASM to query TB of data in browser</title><link>https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/</link><author>mlissner</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 17:37:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Libraries, digital humanities projects, and cultural heritage organizations have long had to perform a balancing act when sharing their collections online, negotiating between access and affordability. Providing robust features for data discovery, such as browsing, filtering, and search, has traditionally required dedicated computing infrastructure such as servers and databases. Ongoing server hosting, regular security and software updates, and consistent operational oversight are expensive and require skilled staff. Over years or decades, budget changes and staff turnover often strand these projects in an unmaintained or nonfunctioning state.The alternative, static file hosting, requires minimal maintenance and reduces expenses dramatically. For example, storing gigabytes of data on Amazon S3 may cost $1/month or less. However, static hosting often diminishes the capacity for rich data discovery. Without a dynamic computing layer between the user’s web browser and the source files, data access may be restricted to brittle pre-rendered browsing hierarchies or search functionality that is impeded by client memory limits. Under such barriers, the collection’s discoverability suffers.For years, online collection discovery has been stuck between a rock and a hard place: accept the complexity and expense required for a good user experience, or opt for simplicity and leave users to contend with the blunt limitations of a static discovery layer.Why We Explored a New ApproachWhen LIL began thinking about how to provide discovery for the Data.gov Archive, we decided that building a lightweight and easily maintained access point from the beginning would be worth our team’s effort. We wanted to provide low-effort discovery with minimal impact on our resources. We also wanted to ensure that whatever path we chose would encourage, rather than impede, long-term access.This approach builds on our recent experience when the Caselaw Access Project (CAP) hit a transition moment. At that time, we elected to switch case.law to a static site and to partner with others dedicated to open legal data to provide more feature-rich access.CAP includes some 11 TB of data; the Data.gov Archive represents nearly 18 TB, with the catalog metadata alone accounting for about 1 GB. Manually browsing the archive data in its repository, even for a user who knows what she’s looking for, is laborious and time-consuming. Thus we faced a challenge. Could we enable dynamic, scalable discovery of the Data.gov Archive while enjoying the frugality, simplicity, and maintainability of static hosting?Our Experiment: Rich Discovery, No Server RequiredRecent advancements in client-side data analysis led us to try something new. Tools like DuckDB-Wasm, sql.js-httpvfs, and Protomaps, powered by standards such as WebAssembly, web workers, and HTTP range requests, allow users to efficiently query large remote datasets in the browser. Rather than downloading a 2 GB data file into memory, these tools can incrementally retrieve only the relevant parts of the file and process query results locally.We developed Data.gov Archive Search on the same model. Here’s how it works: We store Data.gov Archive catalog metadata as sorted, compressed Parquet files on Source.coop, taking advantage of performant static file hosting. Our client-side web application loads DuckDB-Wasm, a fully functional database engine running inside the user’s browser. When a user navigates to a resource or submits a search, our DuckDB-Wasm client executes a targeted retrieval of the data needed to fulfill the request. No dedicated server is required; queries run entirely in the browser.This experiment has not been without obstacles. Getting good performance out of this model demands careful data engineering, and the large DuckDB-Wasm binary imposes a considerable latency cost. As of this writing, we’re continuing to explore speedy alternatives like hyparquet and Arquero to further improve performance.Still, we’re pleased with the result: an inexpensive, low-maintenance static discovery platform that allows users to browse, search, and filter Data.gov Archive records entirely in the browser.Why This Matters for Libraries, Digital Humanities Projects, and BeyondThis new pattern offers a compelling model for libraries, academic archives, and DH projects of all sizes: By shifting from an expensive server to lower cost static storage, projects can sustainably offer their users access to data.Reduced technical overhead: With no dedicated backend server, security risks are reduced, no patching or upgrades are needed, and crashing servers are not a concern. Projects can be set up with care, but without demanding constant attention. Organizations can be more confident that their archive and discovery interfaces remain usable and accessible, even as staffing or funding changes over time.Knowing that we are not the only group interested in approaching access in this way, we’re sharing our generalized learnings. We see a few ways forward for others in the knowledge and information world: If your organization has large, relatively static datasets, consider experimenting with a browser-based search tool using static hosting. Template applications, workflows, and lessons learned can help this new pattern gain adoption and maturity across the community.This project is still evolving, and we invite others—particularly those in libraries and digital cultural heritage—to explore these possibilities with us. We’re committed to open sharing as we refine our tools, and we welcome collaboration or feedback at lil@law.harvard.edu.]]></content:encoded></item><item><title>Just use a button</title><link>https://gomakethings.com/just-use-a-button/</link><author>moebrowne</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 16:59:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[One of the weirdest “debates” I seem to perpetually have with framework-enthusiastic developers is whether or not a  is “just as good” as a . it’s not. Let’s dig in.Among the React crowd, and also among people who seem to enjoy HTMX, I see a lot this…
	Open Modal
This element does not announce itself as an interactive element to screen reader users.You can’t focus on a  with a keyboard.The event only fires on , not when the  or  keys are pressed (again, keyboard users).I’ve had arguments with a very prominent React thought leader whose name starts with R who insisted that using a  was “more accessible” than using a , and that Twitter made the right decision in using this pattern in their app.It’s wrong. It’s all wrong.Many HTML elements have  that tell assistive tech like screen readers what they do.The  element is one of them. It has an implicit  of , which tells screen reader users it can be interacted with and will trigger some type of behavior in the app.The HTML  attribute can be used to add or modify the role of an element. And so, folks like React Ry–thought-leader-guy will say stuff like (I’m paraphrasing)…That attribute exists for a reason. You can add  to a  to give it the correct semantics.OK, that addresses one issue.That role doesn’t affect focusability (or lack thereof) or keyboard behavior. Visually impaired users and people who navigate with a keyboard still can’t use it.“No worries!” they say. “We can fix that, too!”You can make the element focusable with the  attribute.
	Open Modal
You , though! Seriously, just don’t fuck with focus order.It’s way too easy to go down this path and then fuck it up and have folks jumping all over the page instead of navigating through in the normal and expected order.And again, still no keyboard interactivity.But don’t fear! You can add that, too. You just need to listen for all  events, and then filter them out by  so that you only run your code if the  or  keys were pressed (the latter means checking for a literal space: ).That can’t run on the element, either. You’ve got to attach that even to the  and figure out which element has focus.So um… ok, I guess it is technically a fix, but…You’ve just recreated all of the functionality a  gives you for freeSeriously, WTF would you do that?!?All of these hoops to write this HTML…
	Open Modal
When you could write this HTML instead…
	Open Modal
Has the correct  implicitly.Is automatically focusable.Fires a  event in response to  and  presses when it has focus.Look, I’m a lazy developer.And I suspect, if you’re someone who loves tools like React, you probably are, too. It’s cool, I get it! The best code is the code you didn’t write and all that.Use the correct element for the job, and avoid writing a bunch of extra code!]]></content:encoded></item><item><title>Futurelock: A subtle risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>bcantrill</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 16:49:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>Another European agency shifts off US Tech as digital sovereignty gains steam</title><link>https://www.zdnet.com/article/another-european-agency-ditches-big-tech-as-digital-sovereignty-movement-gains-steam/</link><author>CrankyBear</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 16:39:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Austria's Ministry of Economy has migrated to a Nextcloud platform.It's the latest move in a European trend to shift away from Big Tech.European governments and agencies want to control sensitive data.This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers. European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is " to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European." What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, "We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information."All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance. Open-source software is seen as combining the virtues of faster development and better security, while providing companies and governments with more control, as general manager Thierry Carrez of the OpenInfra Foundation recently suggested: "Open infrastructure allows nations and organizations to maintain control over their applications, their data, and their destiny while benefiting from global collaboration."  While the US may not like it, with NextCloud's help, BMWET completed its migration in just four months. Although BMWET had already begun adopting Microsoft 365 and Teams before the project's start, the shift was still considered a success. That's because instead of reversing its path, the ministry implemented a hybrid architecture: Nextcloud handles internal collaboration and secure data management, while Teams remains available for external meetings.The project emphasized integration with existing workflows, including seamless integration with Outlook email and calendar via Sendent's Outlook app. This approach minimized disruption and ensured user acceptance. However, not all migrations progress so well. For example, in Austria, the Ministry of Justice decided to replace Office with LibreOffice. Yet the transition has run into trouble. It appears that the move of 20,000 desktops, which was prompted by a desire to reduce spending on Microsoft licenses, has been, as one person reported, an "unprofessional, rushed operation." Some offices are still on Office, others on LibreOffice, and they're running into incompatible document format problems and misfires in e-mail systems. The moral of the story is that any switch from one software suite to another requires careful handling by the IT department and helpdesk staff. Otherwise, you end up with unhappy users.That said, BMWET's bold shift to Nextcloud appears to have gone well. This initiative demonstrates that adopting sovereign cloud solutions can be practical, user-friendly, and rapid in the public sector. However, as Austria's Justice Ministry experience has shown, simply shifting to an open-source approach without careful planning can get in the way of getting work done. ]]></content:encoded></item><item><title>C3 0.7.7 Vector ABI changes, RISC-V improvements and more</title><link>https://c3-lang.org/blog/c3-language-at-0-7-7-vector-abi,-riscv-improvements-and-more/</link><author>/u/Nuoji</author><category>dev</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 16:37:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[0.7.7 is a major advance in C3 usability with vector ABI changes. It also contains several small quality-of-life additions, such as the ability to splat structs into an initializer, and implicit subscript dereferencing. Fairly few bugs were discovered during this development cycle, which is why the fixed bugs are unusually low.Let’s look at what 0.7.7 brings in more detail:The most significant change in this release is the ABI change for vectors, which now store and pass vectors as arrays in function calls and structs. While vectors still use SIMD, their equality to arrays on the ABI level means that C graphical libraries will directly match vector types.Where before you needed to work with C structs defining vectors and then converting them to SIMD vectors for actual computation, it now works out of the box. Another problem with vectors prior to 0.7.7 was their space and alignment requirements over structs. From 0.7.7 alignment matches that of structs and arrays, making them extremely convenient to work with.For cases where SIMD vectors are actually expected, it’s possible to create distinct types using  with a new  attribute to exactly match standard C SIMD vectors, e.g. typedef V4si = int[<4>] @simd;. This then exactly matches the corresponding C SIMD type.This makes it easier than ever to use SIMD with C3.Struct initializer splatsThis feature enables using the splat operator  to give a designated initializer default values that are overridden by the following arguments.When passing arrays or lists by reference, the  operator tend to behave in an undesirable way, dereferencing the pointer instead of the underlying array/list:Subscript deref addresses this. Using  will dereference :This is helpful when writing macros and such that will want to accept both elements by reference and by value:A new feature for  is to allow creating a type with a specific alignment without wrapping it in a struct. We may, for example, create an integer that is 16 bit aligned using typedef Int2 = int @align(2);. This is an alternative way to safely work with references to under-aligned members in packed structs., ,  and  macros are added to modify strings at compile time efficiently for certain macro manipulation at compile time.Small but important changesAliases that refer to  variables must themselves have local visibility.  is renamed  as it was frequently misunderstood. Generic inference now works better in initializers. For slices with the  syntax, it’s now possible to have the end index be one less than the starting index, so that zero size slices can be expressed with the  syntax as well.This release significantly strengthens C3C’s cross-platform capabilities, particularly for RISC-V architecture support. It’s now possible to set individual CPU features using , e.g. . For RISC-V,  has been added, as well as renaming the RISC-V abi flag to the more correct .The sorting macros accidentally only took non-slices by value, which would work in some cases but not in others. This has been fixed, but might mean that some code needs to update as well. TcpSocketPair was added to the tcp module to create a bidirectional local socket pair, and using sockets on Windows should now implicitly initialize the underlying socket subsystem.0.7.7 has only about 11 fixes, which reflects the relatively few bugs encountered in the 0.7.7 cycle. There are outstanding bugs on the inline asm, which has a significant update planned. The most important fix is patching a regression for MacOS which prevented backtrace printing.With the updated Vector ABI and the change from  to  there are a lot of vendor libraries that will need a refresh. There is also a new matrix library in development that hopefully might get included in the next release. There is more functionality to add for fine-tuning processor capabilities for both RISC-V, but also AArch64. There have also been requests for 32-bit Arm support, but the lack of CI tests for different Arm processors is blocking it at the moment.This release wouldn’t have been possible without the C3 community. I’d like to extend a deep thank you to all who have contributed, both through filed issues, PRs and just plain discussions.Have questions? Come and chat with us on Discord.Discuss this article on Reddit.]]></content:encoded></item><item><title>Project goals for 2025H2 | Rust Blog</title><link>https://blog.rust-lang.org/2025/10/28/project-goals-2025h2/</link><author>/u/Kobzol</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 31 Oct 2025 16:23:45 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[On Sep 9, we merged RFC 3849, declaring our goals for the "second half" of 2025H2 -- well, the last 3 months, at least, since "yours truly" ran a bit behind getting the goals program organized.In prior goals programs, we had a few major flagship goals, but since many of these goals were multi-year programs, it was hard to see what progress had been made. This time we decided to organize things a bit differently. We established four flagship , each of which covers a number of more specific goals. These themes cover the goals we expect to be the most impactful and constitute our major focus as a Project for the remainder of the year. The four themes identified in the RFC are as follows:, making it possible to create user-defined smart pointers that are as ergonomic as Rust's built-in references .Unblocking dormant traits, extending the core capabilities of Rust's trait system to unblock long-desired features for language interop, lending iteration, and more.Flexible, fast(er) compilation, making it faster to build Rust programs and improving support for specialized build scenarios like embedded usage and sanitizers., making higher-level usage patterns in Rust easier.One of Rust's core value propositions is that it's a "library-based language"—libraries can build abstractions that feel built-in to the language even when they're not. Smart pointer types like  and  are prime examples, implemented purely in the standard library yet feeling like native language features. However, Rust's built-in reference types ( and ) have special capabilities that user-defined smart pointers cannot replicate. This creates a "second-class citizen" problem where custom pointer types can't provide the same ergonomic experience as built-in references.The "Beyond the " initiative aims to share the special capabilities of , allowing library authors to create smart pointers that are truly indistinguishable from built-in references in terms of syntax and ergonomics. This will enable more ergonomic smart pointers for use in cross-language interop (e.g., references to objects in other languages like C++ or Python) and for low-level projects like Rust for Linux that use smart pointers to express particular data structures.
"Unblocking dormant traits"Rust's trait system is one of its most powerful features, but it has a number of longstanding limitations that are preventing us from adopting new patterns. The goals in this category unblock a number of new capabilities:Polonius will enable new borrowing patterns, and in particular unblock "lending iterators". Over the last few goal periods, we have identified an "alpha" version of Polonius that addresses the most important cases while being relatively simple and optimizable. Our goal for 2025H2 is to implement this algorithm in a form that is ready for stabilization in 2026.The next-generation trait solver is a refactored trait solver that unblocks better support for numerous language features (implied bounds, negative impls, the list goes on) in addition to closing a number of existing bugs and sources of unsoundness. Over the last few goal periods, the trait solver went from being an early prototype to being in production use for coherence checking. The goal for 2025H2 is to prepare it for stabilization.The work on evolving trait hierarchies will make it possible to refactor some parts of an existing trait into a new supertrait so they can be used on their own. This unblocks a number of features where the existing trait is insufficiently general, in particular stabilizing support for custom receiver types, a prior Project goal that wound up blocked on this refactoring. This will also make it safer to provide stable traits in the standard library while preserving the ability to evolve them in the future.The work to expand Rust's  hierarchy will permit us to express types that are neither  nor , such as extern types (which have no size) or Arm's Scalable Vector Extension (which have a size that is known at runtime but not at compilation time). This goal builds on RFC #3729 and RFC #3838, authored in previous Project goal periods.In-place initialization allows creating structs and values that are tied to a particular place in memory. While useful directly for projects doing advanced C interop, it also unblocks expanding  to support  and  methods, as compiling such methods requires the ability for the callee to return a future whose size is not known to the caller.The "Flexible, fast(er) compilation" initiative focuses on improving Rust's build system to better serve both specialized use cases and everyday development workflows:People generally start using Rust for foundational use cases, where the requirements for performance or reliability make it an obvious choice. But once they get used to it, they often find themselves turning to Rust even for higher-level use cases, like scripting, web services, or even GUI applications. Rust is often "surprisingly tolerable" for these high-level use cases -- except for some specific pain points that, while they impact everyone using Rust, hit these use cases particularly hard. We plan two flagship goals this period in this area:We aim to stabilize cargo script, a feature that allows single-file Rust programs that embed their dependencies, making it much easier to write small utilities, share code examples, and create reproducible bug reports without the overhead of full Cargo projects.We aim to finalize the design of ergonomic ref-counting and to finalize the experimental impl feature so it is ready for beta testing. Ergonomic ref-counting makes it less cumbersome to work with ref-counted types like  and , particularly in closures.For the remainder of 2025 you can expect monthly blog posts covering the major progress on the Project goals.Looking at the broader picture, we have now done three iterations of the goals program, and we want to judge how it should be run going forward. To start, Nandini Sharma from CMU has been conducting interviews with various Project members to help us see what's working with the goals program and what could be improved. We expect to spend some time discussing what we should do and to be launching the next iteration of the goals program next year. Whatever form that winds up taking, Tomas Sedovic, the Rust program manager hired by the Leadership Council, will join me in running the program.]]></content:encoded></item><item><title>AI scrapers request commented scripts</title><link>https://cryptography.dog/blog/AI-scrapers-request-commented-scripts/</link><author>ColinWright</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 15:44:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop</title><link>https://news.ycombinator.com/item?id=45771870</link><author>threeturn</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 13:39:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Dear Hackers,
I’m interested in your real-world workflows for using open-source LLMs and open-source coding assistants on your laptop (not just cloud/enterprise SaaS). Specifically:Which model(s) are you running (e.g., Ollama, LM Studio, or others) and which open-source coding assistant/integration (for example, a VS Code plugin) you’re using?What laptop hardware do you have (CPU, GPU/NPU, memory, whether discrete GPU or integrated, OS) and how it performs for your workflow?What kinds of tasks you use it for (code completion, refactoring, debugging, code review) and how reliable it is (what works well / where it falls short).I'm conducting my own investigation, which I will be happy to share as well when over.]]></content:encoded></item><item><title>Attention lapses due to sleep deprivation due to flushing fluid from brain</title><link>https://news.mit.edu/2025/your-brain-without-sleep-1029</link><author>gmays</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 13:14:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Nearly everyone has experienced it: After a night of poor sleep, you don’t feel as alert as you should. Your brain might seem foggy, and your mind drifts off when you should be paying attention.A new study from MIT reveals what happens inside the brain as these momentary failures of attention occur. The scientists found that during these lapses, a wave of cerebrospinal fluid (CSF) flows out of the brain — a process that typically occurs during sleep and helps to wash away waste products that have built up during the day. This flushing is believed to be necessary for maintaining a healthy, normally functioning brain.When a person is sleep-deprived, it appears that their body attempts to catch up on this cleansing process by initiating pulses of CSF flow. However, this comes at a cost of dramatically impaired attention.“If you don’t sleep, the CSF waves start to intrude into wakefulness where normally you wouldn’t see them. However, they come with an attentional tradeoff, where attention fails during the moments that you have this wave of fluid flow,” says Laura Lewis, the Athinoula A. Martinos Associate Professor of Electrical Engineering and Computer Science, a member of MIT’s Institute for Medical Engineering and Science and the Research Laboratory of Electronics, and an associate member of the Picower Institute for Learning and Memory.Lewis is the senior author of the study, which appears today in . MIT visiting graduate student Zinong Yang is the lead author of the paper.Although sleep is a critical biological process, it’s not known exactly why it is so important. It appears to be essential for maintaining alertness, and it has been well-documented that sleep deprivation leads to impairments of attention and other cognitive functions.During sleep, the cerebrospinal fluid that cushions the brain helps to remove waste that has built up during the day. In a 2019 study, Lewis and colleagues showed that CSF flow during sleep follows a rhythmic pattern in and out of the brain, and that these flows are linked to changes in brain waves during sleep.That finding led Lewis to wonder what might happen to CSF flow after sleep deprivation. To explore that question, she and her colleagues recruited 26 volunteers who were tested twice — once following a night of sleep deprivation in the lab, and once when they were well-rested.In the morning, the researchers monitored several different measures of brain and body function as the participants performed a task that is commonly used to evaluate the effects of sleep deprivation.During the task, each participant wore an electroencephalogram (EEG) cap that could record brain waves while they were also in a functional magnetic resonance imaging (fMRI) scanner. The researchers used a modified version of fMRI that allowed them to measure not only blood oxygenation in the brain, but also the flow of CSF in and out of the brain. They also measured each subject’s heart rate, breathing rate, and pupil diameter.The participants performed two attentional tasks while in the fMRI scanner, one visual and one auditory. For the visual task, they had to look at a screen that had a fixed cross. At random intervals, the cross would turn into a square, and the participants were told to press a button whenever they saw this happen. For the auditory task, they would hear a beep instead of seeing a visual transformation.Sleep-deprived participants performed much worse than well-rested participants on these tasks, as expected. Their response times were slower, and for some of the stimuli, the participants never registered the change at all.During these momentary lapses of attention, the researchers identified several physiological changes that occurred at the same time. Most significantly, they found a flux of CSF out of the brain just as those lapses occurred. After each lapse, CSF flowed back into the brain.“The results are suggesting that at the moment that attention fails, this fluid is actually being expelled outward away from the brain. And when attention recovers, it’s drawn back in,” Lewis says.The researchers hypothesize that when the brain is sleep-deprived, it begins to compensate for the loss of the cleansing that normally occurs during sleep, even though these pulses of CSF flow come with the cost of attention loss.“One way to think about those events is because your brain is so in need of sleep, it tries its best to enter into a sleep-like state to restore some cognitive functions,” Yang says. “Your brain’s fluid system is trying to restore function by pushing the brain to iterate between high-attention and high-flow states.”The researchers also found several other physiological events linked to attentional lapses, including decreases in breathing and heart rate, along with constriction of the pupils. They found that pupil constriction began about 12 seconds before CSF flowed out of the brain, and pupils dilated again after the attentional lapse.“What’s interesting is it seems like this isn’t just a phenomenon in the brain, it’s also a body-wide event. It suggests that there’s a tight coordination of these systems, where when your attention fails, you might feel it perceptually and psychologically, but it’s also reflecting an event that’s happening throughout the brain and body,” Lewis says.This close linkage between disparate events may indicate that there is a single circuit that controls both attention and bodily functions such as fluid flow, heart rate, and arousal, according to the researchers.“These results suggest to us that there’s a unified circuit that’s governing both what we think of as very high-level functions of the brain — our attention, our ability to perceive and respond to the world — and then also really basic fundamental physiological processes like fluid dynamics of the brain, brain-wide blood flow, and blood vessel constriction,” Lewis says.In this study, the researchers did not explore what circuit might be controlling this switching, but one good candidate, they say, is the noradrenergic system. Recent research has shown that this system, which regulates many cognitive and bodily functions through the neurotransmitter norepinephrine, oscillates during normal sleep.The research was funded by the National Institutes of Health, a National Defense Science and Engineering Graduate Research Fellowship, a NAWA Fellowship, a McKnight Scholar Award, a Sloan Fellowship, a Pew Biomedical Scholar Award, a One Mind Rising Star Award, and the Simons Collaboration on Plasticity in the Aging Brain.]]></content:encoded></item><item><title>How OpenAI uses complex and circular deals to fuel its multibillion-dollar rise</title><link>https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html</link><author>reaperducer</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 13:03:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Sam Altman, the chief executive of OpenAI, says that technological revolutions are driven by more than just technology. They are also driven, he argues, by new ways of paying for them.“There is always a lot of focus on technological innovation. What really drives a lot of progress is when people also figure out how to innovate on the financial model,” he recently said at the site of a data center that OpenAI is building in Abilene, Texas.Over the last several years, Mr. Altman’s company has found unusual and creative ways of paying for the computing power needed to fuel its ambitions.Many of the deals OpenAI has struck — with chipmakers, cloud computing companies and others — are strangely circular. OpenAI receives billions from tech companies before sending those billions back to the same companies to pay for computing power and other services.Industry experts and financial analysts have welcomed the start-up’s creativity. But these unorthodox arrangements have also fueled concerns that OpenAI is helping to inflate a potential financial bubble as it builds what is still a highly speculative technology.Here are unusual financial agreements helping to drive the ambitions of OpenAI, the poster child of the artificial intelligence revolution.From 2019 through 2023, Microsoft was OpenAI’s primary investor. The tech giant pumped more than  into the start-up. Then OpenAI funneled most of those billions back into Microsoft, buying  needed to fuel the development of new A.I. technologies.(The New York Times has sued OpenAI and Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit’s claims.)By the summer of last year, OpenAI could not get all the computing power it wanted from Microsoft. So it started signing cloud computing contracts with other companies, including Oracle and little-known start-ups with names like CoreWeave.Across three different deals signed this year, OpenAI agreed to pay CoreWeave, a company that builds A.I. data centers, more than  for computing power. As part of these agreements, OpenAI received  in CoreWeave stock, which could ultimately help pay for this computing power.OpenAI also struggled to get the additional investment dollars it wanted from Microsoft. So, it turned to other investors. Earlier this year, the Japanese conglomerate SoftBank led a  investment in OpenAI.At the same time, OpenAI has been working with various companies to build its own computing data centers, rather than rely on cloud computing deals. This also includes SoftBank, which is known for highly speculative technological bets that don’t always pay off. The company is raising  to help OpenAI build data centers in Texas and Ohio.Similarly, Oracle, a software and cloud computing giant, has agreed to spend  building new data centers for OpenAI in Texas, New Mexico, Michigan and Wisconsin. OpenAI will then pay Oracle roughly the same amount to use these  over the next several years.The United Arab Emirates was part of an OpenAI’s fund-raising round in October 2024. Now, G42, a firm with close ties to the Emirati government, is building a roughly  for OpenAI in the Emirates.Last month, Nvidia announced that it intended to invest  in OpenAI over the next several years. This could help OpenAI pay for its new data centers. As OpenAI buys or leases specialized chips from Nvidia, Nvidia will pump billions back into OpenAI.Two weeks later, OpenAI signed an agreement with AMD that allows OpenAI to buy up to  in the chipmaker at a penny per share. That translates to roughly a 10 percent stake in the company. This stock could supply OpenAI with additional capital as it works to build new data centers.OpenAI pulls in billions of dollars in revenue each year from customers who pay for ChatGPT, computer programming tools and other technologies. But it still loses more money than it makes, according to a person familiar with the company’s finances.If the company can use its new data centers to significantly improve A.I. technologies and expand its revenue over the next several years, it can become a viable business, as Mr. Altman believes it will. If technology progress stalls, OpenAI – and its many partners – could lose enormous amounts of money. Smaller companies like CoreWeave, which are taking on enormous amounts of debt to build new data centers, could go bankrupt.In some cases, companies are hedging their bets. Nvidia and AMD, for instance, have the option of reducing the cash and stock they send to OpenAI if the A.I. market does not expand as quickly as expected. But others would be left with enormous debt, which could send ripples across the larger economy.]]></content:encoded></item><item><title>My Impressions of the MacBook Pro M4</title><link>https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/</link><author>secure</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 10:13:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I have been using a MacBook Pro M4 as my portable computer for the last half a
year and wanted to share a few short impressions. As always, I am not a
professional laptop reviewer, so in this article you won’t find benchmarks, just
subjective thoughts!Back in 2021, I wrote about the MacBook Air
M1, which was the first computer I used that
contained Apple’s own ARM-based CPU. Having a silent laptop with long battery
life was a game-changer, so I wanted to keep those properties.When the US government announced tariffs, I figured I would replace my 4-year
old MacBook Air M1 with a more recent model that should last a few more
years. Ultimately, Apple’s prices remained stable, so, in retrospect, I could
have stayed with the M1 for a few more years. Oh well.The nano-textured displayI went to the Apple Store to compare the different options in
person. Specifically, I was curious about the display and whether the increased
weight and form factor of the MacBook Pro (compared to a MacBook Air) would be
acceptable. Another downside of the Pro model is that it comes with a fan, and I
really like absolutely quiet computers. Online, I read from other MacBook Pro
owners that the fan mostly stays off.In general, I would have preferred to go with a MacBook Air because it has
enough compute power for my needs and I like the case better (no ventilation
slots), but unfortunately only the MacBook Pro line has the better displays.Why aren’t all displays nano-textured? The employee at the Apple Store presented
the trade-off as follows: The nano texture display is great at reducing
reflections, at the expense of also making the picture slightly less vibrant.I could immediately see the difference when placing two laptops side by side:
The bright Apple Store lights showed up very prominently on the normal display
(left), and were almost not visible at all on the nano texture display (right):Personally, I did not perceive a big difference in “vibrancy”, so my choice was
clear: I’ll pick the MacBook Pro over the MacBook Air (despite the weight) for
the nano texture display!After using the laptop in a number of situations, I am very happy with this
choice. In normal scenarios, I notice no reflections at all (where my previous
laptop did show reflections!). This includes using the laptop on a train (next
to the window), or using the laptop outside in daylight.(When I chose the new laptop, Apple’s M4 chips were current. By now, they have
released the first devices with M5 chips.)I decided to go with the MacBook Pro with M4 chip instead of the M4  chip
because I don’t need the extra compute, and the M4 needs less cooling — the M4
Pro apparently runs hotter. This increases the chance of the fan staying off.Here are the specs I ended up with:14" Liquid Retina XDR Display with nano textureApple M4 Chip (10 core CPU, 10 core GPU)32 GB RAM (this is the maximum!), 2 TB SSD (enough for this computer)One thing I noticed is that the MacBook Pro M4 sometimes gets warm, even when it
is connected to power, but is suspended to RAM (and has been fully charged for
hours). I’m not sure why.Luckily, the fan indeed stays silent. I think I might have heard it spin up once
in half a year or so?The battery life is amazing! The previous MacBook Air M1 had amazing all-day
battery life already, and this MacBook Pro M4 lasts even longer. For example,
watching videos on a train ride (with VLC) for 3 hours consumed only 10% of
battery life. I generally never even carry the charger.Because of that, Apple’s re-introduction of MagSafe, a magnetic power connector
(so you don’t damage the laptop when you trip over it), is nice-to-have but
doesn’t really make much of a difference anymore. In fact, it might be better to
pack a USB-C cable when traveling, as that makes you more flexible in how you
use the charger.I was curious whether the 120 Hz display would make a difference in practice. I
mostly notice the increased refresh rate when there are animations, but not,
for example, when scrolling.One surprising discovery (but obvious in retrospect) is that even non-animations
can become faster. For example, when running a Go web server on , I
noticed that navigating between pages by clicking links felt faster on the 120
Hz display!The following illustration shows why that is, using a page load that takes 6ms
of processing time. There are three cases (the illustration shows an average
case and the worst case):Best case: Page load finishes  the next frame is displayed: no delay.Worst case: Page load finishes  a frame is displayed: one frame of delay.Most page loads are somewhere in between. We’ll have 0.x to 1.0 frames of delayAs you can see, the waiting time becomes shorter when going from 60 Hz (one
frame every 16.6ms) to 120 Hz (one frame every 8.3ms). So if you’re working with
a system that has <8ms response times, you might observe actions completing (up
to) twice as fast!I don’t notice going back to 60 Hz displays on computers. However, on phones,
where a lot more animations are a key part of the user experience, I think 120
Hz displays are more interesting.My ideal MacBook would probably be a MacBook Air, but with the nano-texture display! :)I still don’t like macOS and would prefer to run Linux on this laptop. But
Asahi Linux still needs some work before it’s usable
for me (I need external display output, and M4 support). This doesn’t bother me
too much, though, as I don’t use this computer for serious work.]]></content:encoded></item><item><title>Reasoning models reason well, until they don&apos;t</title><link>https://arxiv.org/abs/2510.22371</link><author>optimalsolver</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 09:23:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD could enter ARM market with Sound Wave APU built on TSMC 3nm process</title><link>https://www.guru3d.com/story/amd-enters-arm-market-with-sound-wave-apu-built-on-tsmc-3nm-process/</link><author>walterbell</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 03:07:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[AMD is expanding its processor portfolio beyond the x86 architecture with its first ARM-based APU, internally known as “Sound Wave.” The chip’s existence was uncovered through customs import records, confirming several details about its design and purpose. Built with a BGA-1074 package measuring 32 mm × 27 mm, the processor fits within standard mobile SoC dimensions, making it suitable for thin and light computing platforms. It employs a 0.8 mm pitch and FF5 interface, replacing the FF3 socket previously used in Valve’s Steam handheld devices, further hinting at a new generation of compact AMD-powered hardware.
                                    According to leaks from industry insiders such as @Moore’s Law Is Dead and @KeplerL2, “Sound Wave” is manufactured on  and aims for a  range, positioning it directly against Qualcomm’s Snapdragon X Elite. The chip is expected to power future  products scheduled for release in 2026. four RDNA 3.5 compute unitsmachine learning accelerationMemory support is another highlight: the chip integrates a 128-bit LPDDR5X-9600 controller and will reportedly include , aligning with current trends in unified memory designs used in ARM SoCs. Additionally, the APU carries AMD’s fourth-generation AI engine, enabling on-device inference tasks and enhanced efficiency for workloads such as speech recognition, image analysis, and real-time translation.While AMD experimented with ARM over a decade ago through the abandoned “Project Skybridge,” this new effort represents a more mature and strategic approach. With industry interest in efficient, ARM-based computing accelerating, “Sound Wave” could help AMD diversify its portfolio while leveraging its strengths in graphics and AI acceleration. If reports are accurate, the processor will enter production in late 2025, with commercial devices expected the following year.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>azhenley</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 02:34:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICE and the Smartphone Panopticon</title><link>https://www.newyorker.com/culture/infinite-scroll/ice-and-the-smartphone-panopticon</link><author>fortran77</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 01:13:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Last week, as  raids ramped up in New York, city residents set about resisting in the ways they had available: confronting agents directly on sidewalks, haranguing them as they processed down blocks, and recording them on phone cameras held aloft. Relentless documentation has proved something of an effective tool against President Donald Trump’s empowerment of ; agents have taken to wearing masks in fear of exposure, and the proliferation of imagery showing armed police and mobilized National Guard troops in otherwise calm cities has underlined the cruel absurdity of their activities. Activist memes have been minted on social media: a woman on New York’s Canal Street, dressed in a polka-dotted office-casual dress, flipping  agents off; a man in Washington, D.C., throwing a Subway sandwich at a federal agent in August. The recent “No Kings” marches were filled with protesters in inflatable frog costumes, inspired by a similarly outfitted man who got pepper-sprayed protesting outside the U.S. Immigration and Customs Enforcement Building in Portland, Oregon. Some might write the memes off as resistance porn, but digital content is at least serving as a lively defense mechanism in the absence of functional politics.At the same time, social media has served as a reinvigorated source of transparency in recent weeks, harking back to the days when Twitter became an organizing tool during the Arab Spring, in the early twenty-tens, or when Facebook and Instagram helped fuel the Black Lives Matter marches of 2020. The grassroots optimism of that earlier social-media era is long gone, though, replaced by a sense of posting as a last resort. After Trump authorized the deployment of the National Guard in Chicago earlier this month, the governor of Illinois, J. B. Pritzker, told residents to “record and narrate what you see—put it on social media.” But, if the anti- opposition is taking advantage of the internet,  and the Trump Administration are, too. Right-wing creators have been using the same channels to identify and publicize targets for raids. According to reporting in Semafor, the Trump-friendly YouTuber Nick Shirley’s videos of African migrant vendors on Canal Street seemed to help drive recent  sweeps of the area.  itself is also working to monitor social media. The investigative outlet  found documents revealing that the agency has enlisted an A.I.-driven surveillance product called Zignal Labs that creates “curated detection feeds” to aid in criminal investigations. According to reporting in ,  also has plans to build out a team of dozens of analysts to monitor social media and identify targets. Recent videos, identified by 404 Media and other publications, have purportedly shown  agents using technology developed by the data-analytics firm Palantir, founded by Peter Thiel and others, to scan social-media accounts, government records, and biometrics data of those they detain. Social media has become a political panopticon in which your posts are a conduit for your politics, and what you post can increasingly be used against you.Meanwhile, a new wave of digital tools has emerged to help surveil the surveillants. The apps ICEBlock, Red Dot, and DEICER all allow users to pinpoint where  agents are active, forming an online version of a whisper network to alert potential targets. Eyes Up provides a way for users to record and upload footage of abusive law-enforcement activity, building an archive of potential evidence. Its creator is a software developer named Mark (who uses only his first name to separate the project from his professional work); he was inspired to create Eyes Up earlier this year, when he began seeing clips of  abductions and harassment circulating on social media and worried about their shelf life. As he put it to me, “They could disappear at any given moment, whether the platforms decide to moderate, whether the individual deletes their account or the post.”Ultimately, the app itself was also vulnerable to sudden disappearance. After launching, on September 1st, Eyes Up accumulated thousands of downloads and thousands of minutes of uploaded footage. Then, on October 3rd, Mark received a notice that Apple was removing the app from its store on the grounds that it may “harm a targeted individual or group.” Eyes Up is not alone. ICEBlock and Red Dot have been blocked from both Apple and Google’s app stores, the two largest marketplaces; DEICER, like Eyes Up, was removed by Apple. Pressure on the tech platforms seemed to come from the Trump Administration; after a deadly shooting at an  field office in Dallas in late September, the Attorney General, Pam Bondi, said in a statement to Fox News Digital that ICEBlock “put ICE agents at risk just for doing their jobs.” Mark is contesting Apple’s decision about Eyes Up through its official channels, and the creator of ICEBlock, Joshua Aaron, has argued that his app should be treated no differently than services, such as Google’s Waze, that allow users to warn one another of highway speed traps. But for now they must try to make do with a limited reach.]]></content:encoded></item><item><title>Myths Programmers Believe about CPU Caches (2018)</title><link>https://software.rajivprab.com/2018/04/29/myths-programmers-believe-about-cpu-caches/</link><author>whack</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 00:46:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[distributed-system-architecturedatabase-isolation-levelsstrong-vs-eventual consistencyvolatilesread/written all the way to main memoryeven single-core systems are at risk of concurrency bugsgreat wealth of nuancethis tutorialMESI protocolThe state of the cache-line is set to M, since it is now modifiedmulti-processor systemimmediately trigger cache reads/writes instead]]></content:encoded></item><item><title>Show HN: Quibbler – A critic for your coding agent that learns what you want</title><link>https://github.com/fulcrumresearch/quibbler</link><author>etherio</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 00:43:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kimi Linear: An Expressive, Efficient Attention Architecture</title><link>https://github.com/MoonshotAI/Kimi-Linear</link><author>blackcat201</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 00:07:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking</title><link>https://arstechnica.com/gadgets/2025/10/leaker-reveals-which-pixels-are-vulnerable-to-cellebrite-phone-hacking/</link><author>akyuu</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 23:12:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A change of address led to our Wise accounts being shut down</title><link>https://shaun.nz/why-were-never-using-wise-again-a-cautionary-tale-from-a-business-burned/</link><author>jemmyw</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 22:41:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[For years, one of my businesses has been a regular user of  (formerly TransferWise). Wise is a financial service that lets you send and receive money across currencies, often at a better rate and lower fee than traditional banks. Sounds great, right?This is our story – a sobering, frustrating, and frankly  experience that ended with our business and personal accounts being shut down, without any meaningful reason, support, or recourse.And all we did? We .🏢 A Routine Change Turned NightmareLike many businesses, we recently moved into a new office. Alongside the usual updates to suppliers and records, we updated our  with Wise. Not long after, we received an email requesting us to  the new address.Fair enough – we had no problem with that.Wise provided a dropdown list of acceptable documents: a lease agreement, rates notice, tax document, utilities bill, or telecommunications bill. Due to our company structure, most of those documents are in the name of our parent company or show our PO Box (which NZ Post requires, since they won’t deliver to our street address). But we had a  that ticked every box:Correct physical street address ✅Even detailed our fibre connection at the new premises ✅So we uploaded it – and assumed that would be the end of it.📞 The Call That Made No SenseDays later, we received an email: our document was . No clear reason. So, I called Wise and explained the situation to the customer service representative.Her response left me stunned.“The document was rejected because it was a , not a .”I paused, trying to process this. I politely explained that in New Zealand, a “tax invoice” is a legal form of a bill – even down to the name “tax invoice” being a legal requirement by IRD, and that’s how telecommunications companies issue invoices here. But she refused to accept it.“It needs to say  at the top,” she insisted.“A tax invoice isn’t acceptable.”This is simply , and completely out of touch with New Zealand’s business documentation standards. The rep wouldn’t budge.🧠 The “Solution” That Was Beyond BeliefStill trying to find a solution, I asked: what do you recommend I do then?“You should find a local shared workspace, lease a desk under your company name, change your registered office to that address, and use that lease document to verify your address with us.”Yes, you read that right.Wise’s advice was to artificially lease a desk we didn’t need, change our registered address, and use that document – just to verify an address we actually operate from.I asked to speak to a manager. That request was . She told me, flatly:“I  providing you with the correct information.”A bit more back and forth… then the call .📞 A Glimmer of Hope – Then The Hammer FallsLater that day, I received a call back from Wise – not from a manager (because apparently, Wise doesn’t have managers), but from a more “senior” representative.This rep was  and agreed the document should have been acceptable. She escalated the issue, resubmitted the document herself, and said she’d personally follow up if it was rejected again.🚫 “We’ve Restricted Your Account”I woke to an email with a stunning subject line:“We’ve restricted your account”Just like that, our  was locked. No warning. No reason. No discussion.We could no longer send or receive money, use our Wise cards, or even contact support. The email stated:“Due to our current risk policies, your account will be closed in a few months. You will not be able to use support channels.”Even worse? My  was locked too. The same personal account which did have its address fully verified, by a rates invoice for my personal address.🧾 An “Appeal” That Wasn’t an AppealThe email offered an option to . Naturally, I did.The appeal process asked for our articles of incorporation and . No problem.Then it asked us to provide our preferred currency, and bank account details to refund the balances.Wait… I thought this was an appeal? A chance to discuss and resolve the issue?That was the end. There was no opportunity to explain anything, no communication, no questions asked. The decision was made, and we were , permanently.To summarise the absurdity of this:We moved office, and updated our address with WiseWe provided a legal, NZ-compliant  showing our entity and addressIt was rejected because it was labelled a “Tax Invoice”A rep told us to lease a coworking desk elsewhere just to get a different documentA senior rep agreed we were right, and escalated itThen our accounts were shut down – with no explanation or recourseEven trying to call support now gets an automated message: “Because your account is restricted, we cannot connect you.”⚠️ Our Final Word: Be Very, Very CarefulWe had used Wise for . Regular monthly supplier payments. International stock orders. Five-figure transactions. Never a problem – until this. A minor change triggered a totally flawed process that , with no transparency or logical path to resolution.We’re not alone – a quick search shows many others facing similar horror stories with Wise.So this is my word of warning:💡 Don’t put all your eggs in the Wise basket.If you’re a business, don’t rely on them as your sole means of transferring funds. For us, it’s back to traditional banks – slower, yes, but at least they have humans you can talk to, and actual escalation paths.🧾 28th October update on our Wise debacle – it gets worse.Following the so-called “appeal” (which gave us no option to provide any information), we received the unsurprising outcome: Wise has decided to  as we had breached their acceptable use policy. 🤨What was surprising, however, was the  they gave after I queried what was breached in Wise’s Acceptable Use Policy:I was told my  was being closed for allegedly breaching their Acceptable Use Policy — specifically, section 1.4.e, which states “you may not use your personal Wise account to receive business payments.”I’ve  used my personal account for business transactions — in fact, over 99% of transfers were to overseas family members. When I asked for clarification or examples, I got none. Just a vague statement and the very strange line:“Just because we can’t offer you our services going forward doesn’t mean that we think your business activities are illegal or illegitimate — it just means that we don’t support those types of activities.”What activities?! Again, To make matters worse — our business account’s refund transfer . Why? Because it requires documentation — the  Wise previously rejected for address verification, claiming a telecommunications tax invoice isn’t a bill.After a few days, the transfer was then cancelled as of course, Wise was unable to “verify” us.So now our , their support ticket is marked “final response,” and our attempts to get clarity have gone nowhere. We’ve escalated the issue to Financial Services Complaints Ltd, Wise’s dispute resolution provider in New Zealand.Funds stuck. No clear reason. No accountability. Wise still gets a 0/10 from us.This isn’t just poor service — it’s unacceptable.Think twice before trusting Wise with your money.]]></content:encoded></item><item><title>Phone numbers for use in TV shows, films and creative works</title><link>https://www.acma.gov.au/phone-numbers-use-tv-shows-films-and-creative-works</link><author>nomilk</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 21:49:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Denmark reportedly withdraws Chat Control proposal following controversy</title><link>https://therecord.media/demark-reportedly-withdraws-chat-control-proposal</link><author>layer8</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 21:35:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ Denmark’s justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms.  Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM).  After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.  Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections.  "This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material," Hummelgaard said, according to local news reports.  The current model allowing for voluntary scanning expires in April, Hummelgaard said.  "Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children,” he said. "That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse."   Meredith Whittaker, the president of the Signal Foundation, lobbied hard against the original measure, saying the organization would leave the European market if the provision was adopted.  “What they propose is in effect a mass surveillance free-for-all, opening up everyone’s intimate and confidential communications, whether government officials, military, investigative journalists, or activists,” she said at the time. ]]></content:encoded></item><item><title>Minecraft HDL, an HDL for Redstone</title><link>https://github.com/itsfrank/MinecraftHDL</link><author>sleepingreset</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 18:59:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>TypeScript Overtakes Python and JavaScript To Claim Top Spot on GitHub</title><link>https://developers.slashdot.org/story/25/10/30/1753252/typescript-overtakes-python-and-javascript-to-claim-top-spot-on-github?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Thu, 30 Oct 2025 18:00:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[TypeScript overtook Python and JavaScript in August 2025 to become the most used language on GitHub. The shift marked the most significant language change in more than a decade. The language grew by over 1 million contributors in 2025, a 66% increase year over year, and finished August with 2,636,006 monthly contributors. 

Nearly every major frontend framework now scaffolds projects in TypeScript by default. Next.js 15, Astro 3, SvelteKit 2, Qwik, SolidStart, Angular 18, and Remix all generate TypeScript codebases when developers create new projects. Type systems reduce ambiguity and catch errors from large language models before production. A 2025 academic study found 94% of LLM-generated compilation errors were type-check failures. Tooling like Vite, ts-node, Bun, and I.D.E. autoconfig hide boilerplate setup. Among new repositories created in the past twelve months, TypeScript accounted for 5,394,256 projects. That represented a 78% increase from the prior year.]]></content:encoded></item><item><title>Cursor 2.0 is here... 5 things you didn&apos;t know it can do</title><link>https://www.youtube.com/watch?v=HIp8sFB2GGw</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/HIp8sFB2GGw?version=3" length="" type=""/><pubDate>Thu, 30 Oct 2025 17:07:19 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Build better apps with PostHog - https://posthog.com/fireship

Cursor, the IDE of choice for any self-respecting vibe engineer, just released version 2.0.

In today's video, we'll take a first look at 5 new features every developer should know about.

#coding #programming #ai

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources
- https://cursor.com/blog/2-0

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered
- Cursor 2.0 First Look
- Cursor Composer 1
- Agent Layout
- Native Browser
- Parallel Agents
- In-editor Bugbot]]></content:encoded></item><item><title>How the cochlea computes (2024)</title><link>https://www.dissonances.blog/p/the-ear-does-not-do-a-fourier-transform</link><author>izhak</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 17:01:20 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Let’s talk about how the cochlea computes!The hair cells on different parts of the basilar membrane wiggle back and forth at the frequency corresponding to their position on the membrane. But how do wiggling hair cells translate to electrical signals? This mechanoelectrical transduction process feels like it could be from a Dr. Seuss world: springs connected to the ends of hair cells open and close ion channels at the frequency of the vibration, which then cause neurotransmitter release. Bruno calls them “trapdoors”. Here’s a visualization:Wouldn’t it be convenient if the cochlea were doing a Fourier transform, which would fit cleanly into how we often analyze signals in engineering? But no 🙅🏻‍♀️! A Fourier transform has no explicit temporal precision, and resembles something closer to the waveforms on the right; this is not what the filters in the cochlea look like. Lewicki 2002It appears that human speech occupies a distinct time-frequency space. Some speculate that speech evolved to fill a time-frequency space that wasn’t yet occupied by other existing sounds.To drive the theory home, one that we have been hinting at since the outset: forming ecologically-relevant representations makes sense, as behavior is dependent on the environment. It appears that for audition, as well as other sensory modalities, we are doing this. This is a bit of a teaser for efficient coding, which we will get to soon.]]></content:encoded></item><item><title>Falling panel prices lead to global solar boom, except for the US</title><link>https://arstechnica.com/science/2025/10/theres-a-global-boom-in-solar-except-in-the-united-states/</link><author>Jtsummers</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 16:32:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>