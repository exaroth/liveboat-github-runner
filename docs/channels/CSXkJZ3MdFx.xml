<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>The missing cross-platform OS API for timers</title><link>https://gaultier.github.io/blog/the_missing_cross_platform_os_api_for_timers.html</link><author>/u/broken_broken_</author><category>dev</category><category>reddit</category><pubDate>Mon, 3 Feb 2025 06:06:30 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Resistance to Rust abstractions for DMA mapping [LWN.net]</title><link>https://lwn.net/SubscriberLink/1006805/f75d238e25728afe/</link><author>/u/OptimalFa</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Mon, 3 Feb 2025 03:07:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!

           By January 30, 2025
           
While the path toward the ability to write device drivers in Rust has been
anything but smooth, steady progress has been made and that goal is close
to being achieved ‚Äî for some types of drivers at least.  Device drivers
need to be able to set up memory areas for direct memory access (DMA)
transfers, though; that means Rust drivers will need a set of
abstractions to interface with the kernel's DMA-mapping subsystem.  Those
abstractions have run into resistance that has the potential to block
progress on the Rust-for-Linux project as a whole.

DMA transfers move data directly between RAM and the device of interest,
without involving the CPU.  It is difficult to get any sort of reasonable
I/O performance without DMA, so almost all devices support it.  Making DMA
work, though, is not just a matter of handing a memory address to a
peripheral device; there are many concerns that must be dealt with.  These
include maintaining cache coherency, ensuring that pages are resident in
RAM, handling device-specific addressing limitations, programming I/O
memory-management units, and more.  Plus, of course, every architecture
does things differently.  The DMA-mapping layer exists to hide most of
these problems from device drivers behind an architecture-independent
interface.

Drivers written in Rust will need to do DMA, so they will need access to
the mapping layer.  There have been patches to provide some of that access
in circulation for some time; Abdiel Janulgue posted a version
of this work in early January.  This series adds a small Rust module
with sufficient support to set up coherent mappings (long-term mappings in
cache-coherent memory) for drivers.  This work only covers part of the DMA
API, but it is sufficient for simpler devices.  Upcoming drivers will
require that these abstractions are in place.

But Christoph Hellwig, who does a lot of work with the DMA-mapping layer,
turned this submission
away with a message reading, in its entirety: "No rust code in
kernel/dma, please" (despite the fact that the patch did not put any
code in that directory).  When pressed, he added that developers
should keep these abstractions in their own code and said that he had no
interest in maintaining multi-language code.  Rust developers should keep
their wrapping code to themselves, he concluded.

Danilo Krummrich pointed out
that the proposed abstractions were doing exactly that ‚Äî keeping the Rust
code separate from the rest: "We wrote a single piece of Rust code that
abstracts the C API for all Rust drivers, which we offer to maintain
ourselves".  The conversation then went quiet for several days, after
which Krummrich said:
"Since there hasn't been a reply so far, I assume that we're good with
maintaining the DMA Rust abstractions separately".

Hellwig, though, made it
clear that he is not on board with that plan.  He does not want the
Rust code anywhere near the DMA layer, and that fact that somebody else
would be maintaining it does not change his view.  Adding another language
(he was clear that he was talking about any language, not Rust in
particular) would, he said, make Linux as a whole "".  That has, for now, brought the conversation to a halt.

Without DMA support, there can be no interesting drivers written in Rust.
So one option that the Rust-for-Linux developers have at this point is to
give up on the whole thing and find a less frustrating project to work on.
As appealing as this option might be, it still is probably not their
first choice, though.

An alternative would be to do what Hellwig is suggesting and put the
abstractions into each driver that needs them.  That, however, is not a
path toward a more maintainable kernel.  When the DMA API changes, as it
inevitably will, numerous drivers will have to be fixed, one by one, rather
than fixing a single set of abstractions that are used by all.  So this,
too, might not appear at the top of the list of options as seen by the
developers involved.

Yet another approach might be to stash the DMA abstractions somewhere out
of Hellwig's immediate sight ‚Äî not in the  directory, in
other words.  At that point it becomes just another user of the DMA API
that, in theory, is not subject to more scrutiny than any other driver.
The only problem with this idea is that Janulgue's patch already does that,
and it was not sufficient.

Someday, there will need to be a more decisive answer to this
question.  Krummrich has tried to bring this about with a note asking for Linus Torvalds
or Greg Kroah-Hartman to make a decision regarding these abstractions.
Other Rust developers have reiterated
that they would take responsibility for the maintenance of this code, and
that it would not affect the DMA subsystem.  Jason Gunthorpe questioned that last
claim, noting that a 6.14 pull request was delayed due to a Rust build
problem, but Kroah-Hartman answered that it was
"a tooling issue that people missed due to the holidays" rather than
an example of Rust code holding up development.  Neither he nor Torvalds
has made any decrees on whether the code in question will be merged, though.


By allowing the entry of Rust, the kernel community has decided
‚Äî on a provisional basis, at least ‚Äî that it is indeed willing to maintain
a multi-language code base.  Perhaps, for now, the desire to banish Rust
code to the periphery of the kernel makes some sense, while Rust is still
seen as an ongoing experiment.  If it is eventually decided that the Rust
experiment has failed, backing the existing Rust code out will be easier if
it's confined to the edges.

But it seems increasingly unlikely that the Rust experiment will be judged
that way.  Rust clearly can be used to write kernel code, and there would
appear to be some significant advantages to doing that.  If the experiment
has indeed succeeded then, at some point, the language will need to be
treated as a first-class citizen within the kernel.  Over time, "I don't
want to deal with more than one language" will be an increasingly weak
argument against a contribution written in Rust.

That day may be a while in coming yet.  Already overworked kernel
maintainers will have to find time to learn Rust well enough to manage it
within their subsystems.  Incoming Rust developers can shoulder some of
that burden, but they too will need time to acquire anywhere near the level of
experience that the current maintainers have ‚Äî experience that the kernel
community depends on heavily.  A change of this magnitude to a body of code
as large as the kernel was never going to be a quick or easy affair; it has
gone as well as could have been expected so far, but there will be more,
perhaps harder, obstacles to overcome in the future.]]></content:encoded></item><item><title>The perfect splash.png for PXE bootloader!</title><link>https://www.reddit.com/r/linux/comments/1igenct/the_perfect_splashpng_for_pxe_bootloader/</link><author>/u/ed_mercer</author><category>dev</category><category>reddit</category><pubDate>Mon, 3 Feb 2025 02:29:14 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Convincing Arguments for Go</title><link>https://www.reddit.com/r/golang/comments/1ige21s/convincing_arguments_for_go/</link><author>/u/ktoks</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Mon, 3 Feb 2025 01:58:57 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey all. I have a meeting coming up with mid-level managers. This meeting has been a long time coming, I've been pushing for it for years and I think I've finally gotten through to at least one of them. Wether he's onboard 100% or not is yet to be seen Short explanation of the situation: we're an old enterprise company, old code, old dependencies, old developers, old managers, and a (mostly) old mindset, except when it comes to security. We have used mainly Perl in the past, but a few devs are starting to use Python more.I'm trying to get them to add Go as a development option.Perl is ü§Æ and Python doesn't quite cut it sometimes need shorter processing times types would reduce bugs I see on the reg strict error handling to reduce missed errors current parallel processing is costlyReasons I think they would care:less bugs than other compiled languages faster processing than current languages type safety parallelism baked in dead simple syntax and readability backward compatibility is better than most great community support lower cost and less server loadOne additional problem is that most folks think Go is for web, I've made arguments against that. The top reason is true even for Rust because most of my division isn't computer science and would be unable to understand Rust(I write in Rust too).I need to flesh out some of these arguments and probably could add a few more, can you help me out?]]></content:encoded></item><item><title>Introducing deep research</title><link>https://openai.com/index/introducing-deep-research/</link><author>mfiguiere</author><category>dev</category><category>hn</category><pubDate>Mon, 3 Feb 2025 00:06:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GarminDB</title><link>https://github.com/tcgoetz/GarminDB</link><author>haltcatchfire</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 22:27:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>F-strings for C++26 proposal [pdf]</title><link>https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3412r0.pdf</link><author>HeliumHydride</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 22:19:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Emergence of a second law of thermodynamics in isolated quantum systems</title><link>https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.6.010309</link><author>westurner</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 22:15:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Steam breaks Go runtime</title><link>https://steamcommunity.com/discussions/forum/0/595138100650327297/</link><author>/u/TopAd8219</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 2 Feb 2025 21:49:26 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Praise for John Arundels books.</title><link>https://bitfieldconsulting.com/books</link><author>/u/ThatGuyWB03</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 2 Feb 2025 21:44:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Costa Rican supermarket wins trademark battle against Nintendo</title><link>https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court</link><author>type0</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 21:07:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Flathub adds ‚ÄúWe Love Games‚Äù section with games, emulators, and launchers</title><link>https://osna.social/@razze/113936280189606079</link><author>/u/BrageFuglseth</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 21:07:43 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to learn Kubernetes in 3 days</title><link>https://www.reddit.com/r/kubernetes/comments/1ig7iuk/how_to_learn_kubernetes_in_3_days/</link><author>/u/Traditional_Cap1587</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 2 Feb 2025 20:59:00 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have worked with Kubernetes but not extensively. I have a decent understanding of all the theory and have some hands on exposure but haven't done anything complex like deploying Microservices. Any recommendations on how to get my hands dirty with deploying Microservices apps on AWS EKS? ]]></content:encoded></item><item><title>Should We Sing the Praises of Agile, or Bury It?</title><link>https://it.slashdot.org/story/25/02/02/1948240/should-we-sing-the-praises-of-agile-or-bury-it?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 2 Feb 2025 20:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA["Stakeholders must be included" throughout an agile project "to ensure the evolving deliverables meet their expectations," according to an article this week in Communications of the ACM. 

But long-time Slashdot reader theodp complains it's a "gushing how-to-make-Agile-even-better opinion piece."

Like other pieces by Agile advocates, it's long on accolades for Agile, but short on hard evidence justifying why exactly Agile project management "has emerged as a critical component for firms looking to improve project delivery speed and flexibility" and the use of Agile approaches is being expanded across other departments beyond software development. Indeed, among the three examples of success offered in the piece to "highlight the effectiveness of agile methods in navigating complex stakeholder dynamics and achieving project success" is Atlassian's use of agile practices to market and develop its products, many of which are coincidentally designed to support Agile practices and teams (including Jira). How meta. 

Citing "recent studies," the piece concludes its call for stakeholder engagement by noting that "59% of organizations measure Agile success by customer or user satisfaction." But that is one of those metrics that can create perverse incentives. Empirical studies of user satisfaction and engagement have been published since the 1970's, and sadly one of the cruel lessons learned from them is that the easiest path to having satisfied users is to avoid working on difficult problems. Keep that in mind when you ponder why difficult user stories seem to languish forever in the Kanban and Scrum Board "Ice Box" column, while the "Complete" column is filled with low-hanging fruit. Sometimes success does come easy! So, are you in the Agile-is-Heaven or Agile-is-Hell camp?]]></content:encoded></item><item><title>Facebook Admits Linux-Post Crackdown Was &apos;In Error&apos;, Fixes Moderation Error</title><link>https://tech.slashdot.org/story/25/02/02/1837253/facebook-admits-linux-post-crackdown-was-in-error-fixes-moderation-error?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 2 Feb 2025 19:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[Tom's Hardware reports:

Facebook's heavy-handed censorship of Linux groups and topics was "in error," the social media juggernaut has admitted. Responding to reports earlier this week, sparked by the curious censorship of the eminently wholesome DistroWatch, Facebook contacted PCMag to say that it had made a mistake and that the underlying issue had been rectified. 

"This enforcement was in error and has since been addressed. Discussions of Linux are allowed on our services," said a Meta rep to PCMag. That is the full extent of the statement reproduced by the source... Copenhagen-hosted DistroWatch says it has appealed against the Community Standards-triggered ban shortly after it noticed it was in effect (January 19). PCMag received the Facebook admission of error on January 28. The latest statement from DistroWatch, which now prefers posting on Mastodon, indicates that Facebook has lifted the DistroWatch links ban. 


More details from PCMag:

Meta didn't say what caused the crackdown in the first place. But the company has been revamping some of its content moderation and plans to replace its fact-checking methodology with a user-driven Community Notes, similar to X. "We're also going to change how we enforce our policies to reduce the kind of mistakes that account for the vast majority of the censorship on our platforms," the company said earlier this month, in another irony. 
"Up until now, we have been using automated systems to scan for all policy violations, but this has resulted in too many mistakes and too much content being censored that shouldn't have been," Meta added in the same post.
]]></content:encoded></item><item><title>Waydroid ‚Äì Android in a Linux container</title><link>https://waydro.id/</link><author>birdculture</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 19:29:45 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ We have started creating a few fully-integrated distros in order to demonstrate some of the possibilities that Waydroid can help achieve.  Each of the distros we produce will also showcase some of the work from our growing community of contributors.  Our initial alpha releases of this integration started with Ubuntu 20.04 (focal) and is now on Ubuntu 22.04 (jammy) as well as Debian 12 (bookworm), and includes many added tools and scripts to help open up what is possible. ]]></content:encoded></item><item><title>What Okta Bcrypt incident can teach us about designing better APIs</title><link>https://n0rdy.foo/posts/20250121/okta-bcrypt-lessons-for-better-apis/</link><author>/u/_n0rdy_</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 19:15:35 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hello there! If you follow tech news, you might have heard about the Okta security incident that was reported on 1st of November. The TLDR of the incident was this:The Bcrypt algorithm was used to generate the cache key where we hash a  combined string of userId + username + password. Under a specific set of conditions, listed below, this could allow users to authenticate by  providing the username with the stored cache key of a previous  successful authentication.This means that if the user had a username above 52 chars, any password would suffice to log in. Also, if the username is, let‚Äôs say, 50 chars long, it means that the bad actor needs to guess only 3 first chars to get in, which is quite a trivial task for the computers these days. Too bad, isn‚Äôt it?On the other hand, such long usernames are not very usual, which I agree with. However, some companies like using the entire name of the employee as the email address. So, let‚Äôs say, Albus Percival Wulfric Brian Dumbledore, a headmaster of Hogwarts, should be concerned, as  is 55 chars. Ooops!This was possible due to the nature of Bcrypt hashing algorithm that has a maximum supported input length of 72 characters (read more here), so in Okta case the characters above the limit were ignored while computing the hash, and therefore, not used in the comparison operation. We can reverse engineer that: - user id with separators if anythis way, the password will be outside the 72 chars limit, and, therefore, ignored by the Bcrypt algorithmHowever, there was one thing that made me wonder: if there is a known limit of the algorithm, why is it not enforced by the crypto libraries as a form of input validation? A simple if input length > 72 -> return error will do the trick. I assumed that they might have used some custom library for Bcrypt implementation and simply forgotten about the input validation, which can happen. So, I decided to check how other programming languages behave.Let‚Äôs start with Go, and implement the Okta incident-like case with the help of the official golang.org/x/crypto/bcrypt library:All the code samples can be found heregenerates 18-chars long userIdgenerates 55-chars long usernameconcatenates them with each other and a dummy password super-duper-secure-password with the use of  as a separatorcomputes Bcrypt hash from the concatenated stringthen concatenates the same userId and username with a different password uses bcrypt API to compare whether the 2nd concatenated string matches the hash of the 1st oneLet‚Äôs run the code and see the result:Good job, Go! If we check the source code of the bcrypt.GenerateFromPassword(...) function, we‚Äôll see this piece of code at the very beginning:Perfect! At this point, I became even more suspicious about the tool Okta used, as it seemed like the industry figured that out based on this example. Spoiler alert: it‚Äôs not that simple.Btw, if you like my blog and don‚Äôt want to miss out on new posts, consider subscribing to my newsletter here. You‚Äôll receive an email once I publish a new post.Java doesn‚Äôt support Bcrypt from its core API, but my simple Google search showed that Spring Security library has implemented it. For those who are not into Java ecosystem, Spring is the most used and battle-tested frameworks out there, that has libraries for almost anything: Web, DBs, Cloud, Security, AI, etc. Pretty powerful tool, that I‚Äôve used a lot in the past, and still sometimes use for my side projects.So, I added the latest version of Spring Security to the project and reproduced the same scenario, as in Go example above:I ran the code, and to my great surprise, saw this outcome:I took a peak at the implementation code, and was disappointed: even though there are a bunch of checks on salt:if (saltLength < 28) {
	throw new IllegalArgumentException("Invalid salt");
}
...
if (salt.charAt(0) != '$' || salt.charAt(1) != '2') {
	throw new IllegalArgumentException("Invalid salt version");
}
...
minor = salt.charAt(2);
if ((minor != 'a' && minor != 'x' && minor != 'y' && minor != 'b') || salt.charAt(3) != '$') {
	throw new IllegalArgumentException("Invalid salt revision");
}
...
I didn‚Äôt see any validation of the input that will be hashed. Hm‚Ä¶I decided to check other Google results, and the next Java library in the list was  from Patrick Favre (link to GitHub repo) with 513 starts and the last release version 0.10.2 (so, not stable) from 12th of February 2023 (almost 2 years old). This suggested that I‚Äôd not use it in production, but why not to run our tests.Bcrypt from Patrick FavreNice, good job, Patrick, you saved the day for Java!After checking the source code, I found this piece:and the strict strategy that threw the exception we‚Äôve seen:We can see that this strict strategy is used as a part of the default configs:Let‚Äôs switch to JavaScript.Here I used the bcryptjs which has over 2 million weekly downloads based on the NPM stats.Not great. The source code reveals that similar to Spring Security, the library validates the saltbut not the input length.Let‚Äôs try if Python can do any better.Using bcrypt library with 1.3k starts and the latest release in November.The result is same as we observed for most of our test subjects:All right, but what about some newer and more safety-oriented language - let‚Äôs try Rust.Here I need to be honest: since I‚Äôm not a Rust expert at all, I used a help of a Claude AI to write this code. So, if you see any issues there, please, let me know in the comments section, so I can fix that.As a library, I used rust-bcrypt based on my AI friend advice.I can see the validation of the cost:but not of the input. And here is the place where the explicit truncation of 72 chars happens (the comment is from the library source code):That was my first question after seeing that the majority of the tools follow the pattern that leads to the vulnerability. Wikipedia article about Bcrypt gave a hint:Many implementations of bcrypt truncate the password to the first 72 bytes, following the OpenBSD implementationInteresting! Let‚Äôs check the OpenBSD implementation of this algorithm, and here is the link to it. The first point of interest lies here:And from that moment on,  is used as a limit to iterate over the input string within, for example:Where  is passed as a  parameter. So this piece of code:will make sure that no chars over the limit (72) will end up being processed.Git blame shows that the  line is 11 years oldwhile the if (j >= databytes) j = 0; is 28 years old (what were you busy with in 1997, ah?)So, it‚Äôs been a while since the API has been reiterated.Let me start with a short disclaimer: I have a huge respect for people who spend their free time and mental capacity on maintaining open-source projects. That‚Äôs a large amount of work, that is not paid, and, unfortunately, quite often not appreciated by the users of the tools. That‚Äôs why they have all the legal and ethical rights to build the project the way they see them. My opinions below are not targeted towards anyone in particular.My initial goal was to create issues for each of the mentioned library, but I noticed that this behavior has been already reported to each of them:Check the discussions and their outcomes by following those links.As a guy who spent a few years of my career on building tools and solutions to be used by other software engineers, I understand the frustration: you invested your time and effort into writing a clear documentation and guides, but a certain number of your users don‚Äôt bother checking it at all, and just use the tool the way they think it should be used. However, that‚Äôs the reality that I had to accept and started thinking about how can I make my tools handle those use cases. Here are a few principles I came up with in that process.Don‚Äôt let the people use your API incorrectlyIn my opinion, from the API perspective, the approach when the tool silently cuts the part of the input and processes the remaining one only, it is an extremely poor design choice. What makes things worse is the fact that Bcrypt is used in the domain of security and sensitive data, and, as we can see, most of the tools mentioned above, use  as the name of the input parameter of the hashing method. The good design should explicitly reject the invalid input with the error / exception / any other mechanism the platform uses. So, basically, exactly what Go and Patrick‚Äôs Java library did. This way, incidents like Okta one would be impossible by design (btw, I‚Äôm not shifting the blame away from Okta, considering the domain they operate in).It is ok, though, to offer the non-default unsafe option, that will let the users pass longer input that will be truncated if the user explicitly asks for that. A prefix/suffix like , , etc. can be a good addition to the names of the method that expose these options.If we take a step back from the Bcrypt case, imagine other examples, if such a pattern becomes common in the industry:We created a new user account on HBO to watch a new season of Rick and Morty, and there is a warning that the max size of the password should not exceed 18 chars. However, the password generator of your password manager tool uses 25 chars as a default length of the produced password. So, the password manager inserts that password while creating an account, but the server cuts the last 7 chars, hashes the rest, and saves the hash to the DB. How easy would it be for us to be able to log in to HBO next time and watch a new episode?The tech lead of the new project configured a linter tool, and set the max line length as 100 chars. While performing a check, linter removes the chars above the defined limit, and informs that the check has passed. How useful would it be?A good API design should remember that when it comes to tech, nobody likes surprises.While following a few online discussions about the Bcrypt Okta incident, I noticed something else: while the majority of comments agreed that we should design APIs like these better, there were a few folks that took a very defensive stance and exposed their ego: ‚ÄúRead a paper before using anything!‚Äù, ‚ÄúAPIs are only correcting the input after the stupid users!‚Äù, etc. Based on my experience, ego is a big enemy of engineering. And I wouldn‚Äôt be surprised if you have a story or two in that regard as well. So, yeah, let‚Äôs not bring our egos to our APIs.Don‚Äôt get me wrong, I do understand the gist that the users should have some basic knowledge before using any tool. But let‚Äôs get back to the reality: how many different tools, programming languages, databases, protocols, frameworks, libraries, algorithms, data structures, clouds, AI models, etc. does a software engineer use per week these days? I tried to count for my use case, but stopped after the number had reached 30. Is it possible to know all of them deep? To know all the edge cases and limits? For some of them and to some degree is a reasonable ask, as well as having an expertise in 1 or 2, but definitely not all. The hard truth is that on average, the industry today requires the wide spectrum of knowledge over the deep one (check any job opening to verify that claim). Therefore, while designing the tools, why not to help our fellow colleagues? For example, if our tool accepts only positive numbers, let‚Äôs add if num < 1 -> return error  to our solution, and make the life simpler for somebody out there.Especially, if the tool might be used in the security-sensitive context, where humans are usually the weak point in the thread modelling. The good API can help there.It‚Äôs not so often that the API we design is something completely new to the world. Most likely, there are other solutions like ours out there. And the chances are that they‚Äôve been already doing certain things the particular way. However, that doesn‚Äôt mean that we need to follow the same path. Kudos to the Go team and Patrick‚Äôs Java library for being brave to do things the different way as the industry does in the Bcrypt example. Let‚Äôs learn from them.Regardless of the original design choices and intentions, it‚Äôs never too late to reiterate on some of them if we see a need or have discovered new information. That‚Äôs, actually, a place where a lot of us fail due to different reasons, with some of them listed above.The Okta incident exposed large security issues out there. Our test showed, even 3 months after the incident, the industry is still vulnerable to the same outcome, so the chances are that more to come. However, we, as software engineers, can learn from that, and apply these lessons while designing APIs to make them predictable and easier to use.I hope that was useful, and triggered some thoughts. Thanks a lot for reading my post, and see you in the following ones, there are plenty of topics to discuss. Have fun! =)]]></content:encoded></item><item><title>The legacy of lies in Alzheimer&apos;s science</title><link>https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html</link><author>apsec112</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 19:00:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Medical advances have beaten back many relentless assassins in recent decades, such as cancer and heart disease. A wide range of treatments share credit: surgery, medicines, radiation, genetic therapies and healthful habits. Mortality rates for those two diseases, the top causes of death in the United States, have fallen sharply. But in an aging population, Alzheimer‚Äôs death rates have gone in the opposite direction.The disease afflicts nearly seven million Americans, about one in every nine people over the age of 65, making it a leading cause of death among older adults. Up to 420,000 adults in the prime of life ‚Äî including people as young as 30 ‚Äî suffer from early-onset Alzheimer‚Äôs. The annual number of new cases of dementia is expected to double by 2050.Yet despite decades of research, no treatment has been created that arrests Alzheimer‚Äôs cognitive deterioration, let alone reverses it. That dismal lack of progress is partly because of the infinite complexity of the human brain, which has posed insurmountable challenges so far. Scientists, funders and drug companies have struggled to justify billions in costs and careers pursuing dead-end paths. But there‚Äôs another, sinister, factor at play.Over the past 25 years, Alzheimer‚Äôs research has suffered a litany of ostensible fraud and other misconduct by world-famous researchers and obscure scientists alike, all trying to ascend in a brutally competitive field. During years of investigative reporting, I‚Äôve uncovered many such cases, including several detailed for the first time in my forthcoming book.]]></content:encoded></item><item><title>GNOME added HDR configuration merge requests</title><link>https://gitlab.gnome.org/GNOME/gnome-control-center/-/merge_requests/2991</link><author>/u/Existing-Code-1318</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 18:37:54 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I made the most chaotic Go package ever and somehow got 8 stars? What?</title><link>https://www.reddit.com/r/golang/comments/1ig42i3/i_made_the_most_chaotic_go_package_ever_and/</link><author>/u/a7madx7</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 2 Feb 2025 18:36:27 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey everyone, so I did something dumb.You know how we all have that "utils" package we copy-paste between projects? Well, I took all of mine, threw them in a GitHub repo, and called it . Yeah, really creative name, I know.The mess started like this:I got tired of writing the same error handling and logging stuff over and over. You know the drill: go if err != nil { log.Printf("something broke: %v", err) return nil, err } So I made some wrappers: ```go // Now it's someone else's problem config := it.Must(LoadConfig())// Or if you're scared of panics user := it.Should(GetUser()) ```Then things got out of handI started adding everything I commonly use: - Rate limiters (because hammering APIs isn't cool) - Pools (because making new objects is expensive) - Math stuff (turns out loops aren't always the answer) - Load balancers (for when one function isn't enough) - Circuit Breaker (whatever that might be) - A Deboucer - A really ( and I mean really ) bad clone of Rust's Result type. - Exponential Retrial stuff. - A benchmarker, a time keeper & a time measuring package. - Did I mention a graceful shutdown manager & a version tracking packge?People actually starred the repo. Like, 8 whole stars. That's 8 more than I expected. Either they really like bad ideas or they're as messy as I am."Well, if people are gonna use this train wreck, might as well make it a TESTED train wreck."So now it has: - Actually decent test coverage - Documentation (with bad jokes) - Examples that work - More features nobody asked forProbably not. But if you do, at least you'll get some laughs from the docs while your code catches fire.PS: If this post gets more upvotes than my repo has stars, I'll add whatever stupid feature gets the most upvotes in the comments.PS: I hope I made you smile.Edit 1: Yes, I know it breaks every Go package design principle. No, I won't fix it. Yes, I'll probably add more stuff.Edit 2: WoW, Just WoW, 102+ Stars up till now, I am in loss of words, r/golang is such a nice community to be part of, thanks guys.]]></content:encoded></item><item><title>Simple wgpu (24.0.1) + winit (0.30.8) template</title><link>https://www.reddit.com/r/rust/comments/1ig3ys7/simple_wgpu_2401_winit_0308_template/</link><author>/u/Foxicution</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 2 Feb 2025 18:32:10 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Recently I decided to try out wgpu and ran into issues trying to get a basic triangle render working.The official examples were very hard to gasp for me, as they contained a lot of abstractions.The standalone examples were a lot better, but they didn't contain instructions on how to compile for WebAssembly.Other resources were usually outdated and didn't work for the current versions of wgpu and winit.Because of this I made myself a minimal working example for the current version of winit and wgpu to use as a template for other projects and thought it might be useful to some other people.Feedback is highly appreciated, and hope this can help people get into wgpu quicker.   submitted by    /u/Foxicution ]]></content:encoded></item><item><title>[Media] Flashing own code to e-link price tag only using a pico</title><link>https://www.reddit.com/r/rust/comments/1ig37ah/media_flashing_own_code_to_elink_price_tag_only/</link><author>/u/tracyspacygo</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 2 Feb 2025 18:00:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Everyone knows your location: tracking myself down through in-app ads</title><link>https://timsh.org/tracking-myself-down-through-in-app-ads/</link><author>apokryptein</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 17:07:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Recently I read about a massive geolocation data leak from Gravy Analytics, which exposed more than 2000 apps, both in AppStore and Google Play, that secretly collect geolocation data without user consent. Oftentimes, even without developers` knowledge. I looked into the list (link here) and found at least 3 apps I have installed on my iPhone. Take a look for yourself! This made me come up with an idea to track myself down externally, e.g. to buy my geolocation data leaked by some application. After more than couple dozen hours of trying, here are the main takeaways: I found a couple requests sent by my phone with + 5 requests that leak , which can be turned into geolocation using reverse DNS. Learned a lot about the RTB (real-time bidding) auctions and OpenRTB protocol and was shocked by the amount and types of data sent with the bids to ad exchanges. Gave up on the idea to buy my location data from a data broker or a tracking service, because I don't have a big enough company to take a trial or $10-50k to buy a huge database with the data of millions of people + me. Well maybe I do, but such expense seems a bit irrational. Turns out that EU-based peoples` data is almost the most expensive. But still, I know my location data was collected and I know where to buy it! My setup for this research included:My old iPhone 11 restored to factory defaults + new apple id. Felt too uncomfortable to do all this on my current phone. Charles Proxy to record all traffic coming in and out. I set up the SSL certificate on the iPhone to decrypt all https traffic.A simple game called Stack by KetchApp - I remember playing it at school 10-12 years ago. Choosing it as a lab rat felt nostalgic. To my surprise, there were a lot of KetchApp games on the list. Ok, here we go: only 1 app installed without the default Apple ones, Charles on, launching Stack in 3, 2, 1.... These are the requests that the app sends in the first minute after launch. Take a look at the timing of the requests - almost every split second. Let's take a look at the contents of the requests. I actually checked every single one of them - but I'll leave out only the interesting ones here. Let's start with the juiciest request sent to https://o.isx.unity3d.com - the first one that included my geo, while I disabled Location Services on iPhone for all apps! If you are as naive as I was before this, you might be surprised - what does Unity, the 3D engine, have to do with the in-app advertisement or location tracking? Perhaps that's just some monitoring data to help improve the engine? Turns out that Unity's main revenue stream (they made $2 bln+ in 2023) is Unity Ads - "Mobile Game Ad Network". Sounds quite interesting.Below is the request body in json format sent to Unity Ads. I will only leave the  fields worth mentioning - the actual size is 200+ keys. {
  "ts": "2025-01-18T23:27:39Z", // Timestamp
  "c": "ES", // Country code,
  "d": "sports.bwin.es", // Domain; the app or website where the ad will be displayed.
  "bn": "molocoads-eu-banner", // WTF is moloco ads? We'll see!
  "cip": "181.41.[redacted]", // my IP !!
  "dm": "iPhone12,1", 
  "ct": "2", // Connection type; e.g., Wi-Fi
  "car": "Yoigo", // mobile network operator
  "ifv": "6B00D8E5-E37B-4EA0-BB58-[redacted]", // ID for Vendor. We'll get back to it!
  "lon": "2.[redacted]", // Longitude ... 
  "lat": "41.[redacted]", // Latitude ... 
  "sip": "34.227.224.225", // Server IP (Amazon AWS in US) 
  "uc": "1", // User consent for tracking = True; OK what ?!
}Ok, so my IP + location + timestamp + some  id are shared with Unity ‚Üí Moloco Ads ‚Üí Bwin, and then I see the actual Bwin ad in the game. Wonderful! As a quick note - location shared was not very precise (but still in the same postal index), I guess due to the fact that iPhone was connected to WiFi and had no SIM installed. If it was LTE, I bet the lat/lon would be much more precise. Hello Facebook... What are you doing here?Next interesting request that leaks my IP + timestamp (= geo-datapoint) is Facebook.What?!I don't have any Meta [Facebook] app installed on this iPhoneI didn't link the app nor my Apple ID to any Facebook accountI didn't consent to Facebook getting my IP address!{ 
	"bundles": {
		"bidder_token_info": {
			"data": {
				"bt_extras": {
                  "ip":"181.41.[redacted], // nice Extras, bro
                  "ts":1737244649
			},
			"fingerprint": null
		},
        {
          "a lot of data: yes a loooooooot"
         }We'll talk more about this one in the next section. Why do you need my screen brightness level? {
  "osVersion":"16.7.1",
  "connectionType":"wifi",
  "eventTimeStamp":1737244651,
  "vendorIdentifier":"6B00D8E5-E37B-[redacted]", // ifv once again 
  "wiredHeadset":false, // excuse me? 
  "volume":0.5,
  "cpuCount":6,
  "systemBootTime":1737215978,
  "batteryStatus":3,
  "screenBrightness":0.34999999403953552,
  "freeMemory":507888,
  "totalMemory":3550640, // is this RAM?
  "timeZone":"+0100",
  "deviceFreeSpace":112945148
  "networkOperator":"6553565535"
  "advertisingTrackingId":"00000000-0000....", // interesting ...
  }There's no "personal information" here, but honestly this amount of data shared with an arbitrary list of 3rd parties is scary. Why do they need to know my screen brightness, memory amount, current volume and if I'm wearing headphones? I know the "right" answer - to help companies target their audience better! For example, if you're promoting a mobile app that is 1 GB of size, and the user only has 500 MB of space left - don't show him the ad, right?But I also heard lots of controversies on this topic. Like Uber dynamically adjusting taxi price based on your battery level - because you're not waiting for a cheaper option with 4% left while standing in the street. I can't know if that or another one is true. But the fact that this data is available and accessible by advertisers suggests that they should at least think of using it. Ok, enough with the requests. We can already see the examples of different ip and geolocation leaks.  + timestamp was adjust.com - but the request body was too boring to include. You might've already noticed  and  ==  in the requests above - what are those? IFV, or IDFV, is "ID for Vendor". This is my id unique for each vendor, a.k.a developer - in this case, KetchApp. This checks out: I installed another KetchApp game to quickly record the requests, and the  value was the same for it. Advertising Tracking ID, on the other hand, is the cross-vendor value, the one that is shared with an app if you choose "Allow app to track your activity across ...". As you can see above, it was actually set to  because I "Asked app not to track". I checked this by manually disabling and enabling tracking option for the Stack app and comparing requests in both cases. And that's the only difference between allowing and disallowing trackingI understand there might be nothing shocking to you in it - this is not really kept secret, you can go and check the docs for Apple developers, for example. But I believe this is  communicated correctly to the end users, you and me, in any adequate way, shape or form: the free apps you install and use collect your precise location with timestamp and send it to some 3rd-party companies. The only thing that stops anyone with access to bid data (yet another ad buying agent, or ad exchange, or a dataset bought or rented from data broker, as you'll see later) from tracking you down with all trips you make daily is this  that is not shared when you disallow apps to "track you across apps" to "enhance and personalise your ads experience". By the way: if you're using 10 apps from the same vendor (Playrix, KetchApp or another 1000-app company) and allow  to track you ‚Äì it would mean that the data collected in all 10 apps will be enriched with your IDFA which can later be exchanged to your personal data. At the same time, there is so much data in the requests that I'd expect ad exchanges to find some loophole ID that would allow cross-app tracking without the need for IDFA. I found at least 20 ids like  and ,  and  (these 2 are shared with Facebook), and so on. By the way, the fact that Facebook collected my IP + timestamp without any adequate consent / app connection from my end is crazy. I think Facebook is more than capable of connecting the dots and my Meta Account to this hit as soon as I login to Instagram or Facebook app on the same IP address. Let's get back to the request that leaked my location for a second and look at its trace. We'll focus on the parties in the middle:stack ‚Üí  o.isx.unity3d.com ‚Üí molocoads ‚Üí bwin (advertiser)Unity [ads] is an SSP (supply-side platform) that acts as a collector of data from the app via SDK. As an app developer, you don't need to worry about gathering the right data, registering as a publisher on an ad exchange or whatever - just install the SDK and receive the money. Moloco ads is a DSP network that resells data from multiple SSPs (like Unity, Applovin, Chartboost). Basically, from almost every one of the requested hosts I've seen pop up in Charles Proxy.It then applies some "smart optimisation" and connects a vacant banner space on your phone screen with the advertiser.Sounds like moloco aggregates a lot of data and basically anyone (- any company that becomes an ad partner) can access the data by bidding lower than others. Or imagine a real ad exchange that bids normally and collects all of the data along the way "as a side gig". Basically, this is how intelligence companies and data brokers get their data. At this point I was looking for any mentions of Moloco on Telegram and Reddit, and I ran into this post that answered a lot of my questions:They access it if they integrate with the provider of bidstream, which would be the SSP. It's on the SSP to verify the vendor to whom they give access to bids. Usually, the requirement would be that you actually... bid. SSPs want you to spend money, that's how their business makes revenue. They might open up only part of the traffic to specific vendors (i.e.. if you don't bid worldwide, you won't get the bidstream worldwide, only in the regions in which you operate).Let's move further. When I found out how the data gets out, I started looking for any place where it's being sold. It was a quick search.I found a data marketplace called Datarade which is a panel with all sorts of data. When I searched for MAID-specific data, hundreds of options showed up, like these two: The price of the Redmob dataset surprised me, - $120k a year... for what?Let's now take a look at their promo:Check out the list of features on the right - do any of them look familiar? : "low latency" means they know your location from the last time any of the apps shared it. It can be as little as 5 seconds ago. What's even better is that Redmob provides a  of the data. I tried to request it from their website, but the sample never landed in my mailbox (surprise-surprise, timsh.org doesn't seem like a customer with high potential). Thankfully, this sample is public on Databricks Marketplace with this annotation:Enhance your products and services using our global location data covering over 1.5 billion devices. Using our extensive location dataset, you can unearth concealed patterns, conduct rapid analyses, and obtain profound knowledge.We can also provide region-specific data (MENA, Africa, APAC, etc.) based on your specific requirements. Our pricing model includes an annual licensing option, and we provide free sample data so that you can evaluate the quality of our dataset for yourself. To me, the most absurd part is the  column - the source of the data can't be more obvious. I'm also quite interested in the  column - if it's the birthyear, where did they get it from? Never mind, who cares about your birthyear.All right, imagine I bought the access to a huge stream of Redmob data. But my goal is to track and stalk people like myself or anyone else, so I need some way to exchange MAIDs () for the actual personal info: name, address, phone number... No problem! This kind of dataset is surprisingly also present on Datarade. Take a look at a sample table with  type that is provided by "AGR Marketing Solutions":Inside - all personal info (full name, email, phone number, physical address, property ownership... and IDFAs. Congrats, you have just reached the bottom of this rabbit hole. Let's wrap it up and make a couple of bold statements.How to track yourself down?Easy! Just follow this simple step-by-step guide:Use some free apps for a bit. Move around and commute - this makes the geo data more valuable. "Allow" or "ask not to track" - a combo of IP + location + User-agent + geolocation will still be leaked to hundreds of "3rd parties" regardless of your choice.Wait for a few seconds until fake DSPs and data brokers receive your data.Exchange your full name or phone number for an IDFA (if present), IP address and user-agent through the  data purchased somewhere.Now, access the "Mobility data" consisting of geolocation history, and filter it using the values from the previous step. Congratulations! You found yourself. I created a flowchart that includes almost all actors and data mentioned above - now you can see how it's all connected. This is the worst thing about these data trades that happen constantly around the world - each small part of it is (or seems) legit. It's the bigger picture that makes them look ugly. Thanks for reading this story until the end!My research was heavily influenced by these posts and investigations: ]]></content:encoded></item><item><title>DocumentDB: Open-Source MongoDB implementation based on PostgreSQL (from Microsoft)</title><link>https://opensource.microsoft.com/blog/2025/01/23/documentdb-open-source-announcement/</link><author>/u/mariuz</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 16:09:09 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How to hide restart count in kubectl get pod command.</title><link>https://www.reddit.com/r/kubernetes/comments/1ig0do9/how_to_hide_restart_count_in_kubectl_get_pod/</link><author>/u/Budget-Agent9524</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 2 Feb 2025 16:01:58 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I tried using custom column command but then ready cloumns print true/false instead of 1/1.   submitted by    /u/Budget-Agent9524 ]]></content:encoded></item><item><title>Ask HN: What is interviewing like now with everyone using AI?</title><link>https://news.ycombinator.com/item?id=42909166</link><author>ramesh31</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 15:19:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Have you gone back to in-person whiteboards? More focus on practical problems? I really have no idea how the traditional tech interview is supposed to work now when problems are trivially solvable by GPT.]]></content:encoded></item><item><title>What is PKCE Flow?</title><link>https://www.hemantasundaray.com/blog/pkce-flow</link><author>/u/sundaray</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 14:48:02 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Proof Key for Code Exchange (PKCE, pronounced "pixy") is an extension of the Authorization Code grant type.The Authorization Code grant type is used by OAuth 2.0 confidential and public clients to exchange an authorization code for an access token.However, OAuth 2.0 public clients using the Authorization Code grant type are susceptible to authorization code interception attacks. This occurs when an attacker intercepts the authorization code within the client's operating system, in communication paths that aren't protected by TLS. The PKCE flow mitigates this security risk.The PKCE flow has 5 main steps:Step 1: Client generates a code verifier and a code challengeThe client first creates a code verifier.A code verifier is a cryptographic random string with a minimum length of 43 characters and maximum length of 128 characters. The code verifier should have high entropy (a high degree of randomness), so that it‚Äôs practically impossible for anyone to guess its value.The client then derives a code challenge from the code verifier in 3 steps:Converts the code verifier string into its byte form.Converts the bytes into a fixed-length binary hash using the  algorithm (known as the code challenge method). This conversion is one-way, meaning you can't derive the original code verifier from the hash.Converts the binary hash into a string with only URL-safe characters using base64url encoding.Step 2: Client sends an Authorization RequestThe client sends an Authorization Request to the Authorization Server. In the request, it includes two important pieces of information: the code challenge and the code challenge method. The Authorization Server stores both these pieces of information.Step 3: Authorization Server issues an Authorization CodeThe Authorization Server responds by issuing an authorization code. Note that the server typically stores the code challenge and the code challenge method in an encrypted form in the authorization code itself.Step 4: Client sends an Access Token RequestThe client proceeds to request an access token. This request includes the authorization code and the original code verifier ().Step 5: Authorization Server issues an Access TokenBefore issuing an access token, the Authorization Server verifies that the request for access token has come from the same client that made the initial authorization request. This verification happens in two steps:The server takes the code verifier and transforms it into a code challenge using the code challenge method it had stored (alongside the original code challenge) in step 1.The server then compares this newly generated code challenge with the original code challenge. If they match, proving it's the same client, the server issues an access token.How PKCE flow prevents authorization code interception attacksLet's say an attacker intercepts the authorization code. However, the attacker would not be able to exchange the stolen authorization code for an access token, because the attacker wouldn‚Äôt have the corresponding code verifier. Note that the code verifier can‚Äôt be intercepted by the attacker because it‚Äôs sent over TLS.Important note about PKCEWhile PKCE was originally designed to secure native applications (applications that are installed directly on a user‚Äôs devices such as a smartphone, or desktop and run natively on the operating system), it's now recommended for all types of OAuth clients (including confidential clients like server-side web applications) because it provides strong protection against authorization code interception attacks.]]></content:encoded></item><item><title>Void is booting on Apple ARM-powered devices</title><link>https://voidlinux.org/news/2025/02/new-images.html</link><author>/u/daemonpenguin</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 14:09:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Feburary 2025 Image Release: Arm64 ExtravaganzaWe‚Äôre pleased to announce that the 20250202 image set has been promoted to
current and is now generally available.This release introduces support for several arm64 UEFI devices:Live ISOs for  and  should also support other arm64
devices that support UEFI and can run a mainline (standard) kernel.Additionally, this image release includes:Xfce 4.20 in -flavored live ISOsLinux 6.6.69 in Raspberry Pi PLATFORMFSes and images, a new script from 
to simplify generation of  for chroot installsand the following changes:Fixed issue where systems with Nvidia graphics cards would not boot without 
 (void-packages #52545)Raspberry Pi platform images are now smaller by default, but will grow the root partition to fit the storage
 device upon first boot using . See
 the handbook
 for more details
 (void-mklive #379) now includes a post-installation menu to enable services on the installed system
 (void-mklive #389) and  PLATFORMFSes and platform images should now
support the recently-released Raspberry Pi 500 and CM5.You may verify the authenticity of the images by following the instructions in
the handbook,
and using the following minisign key information:untrusted comment: minisign public key 4D56E70F102AF9F9
RWT5+SoQD+dWTeOdNuc4Q/jq2+3+jpql7+JJp4WukkxTdpsZlk2EGuPj
]]></content:encoded></item><item><title>This really doesn&apos;t help.</title><link>https://www.reddit.com/r/linux/comments/1ifxs7p/this_really_doesnt_help/</link><author>/u/Separate-Solution801</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 13:59:58 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I got tired of tracking software versions manually... So I built &quot;Veno&quot;</title><link>https://www.reddit.com/r/rust/comments/1ifxknr/i_got_tired_of_tracking_software_versions/</link><author>/u/Marekzan</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 2 Feb 2025 13:48:29 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[At work, I deal with multiple software artifacts‚Äîrepositories, container images, Helm charts. Some of them are critical, and keeping them up to date is non-negotiable. But tracking them manually? That‚Äôs a nightmare.At first, I tried the usual methods: setting up email notifications, following RSS feeds, even writing a few scripts to scrape version numbers. But each artifact lived in its own ecosystem‚Äîsome on GitHub, others on DockerHub or ArtifactHub. Some had built-in notification systems, some didn‚Äôt. And worst of all, I had no .I needed a way to track everything in one place. Not just track, but also get notified in the right way. Some updates needed an email. Others were better suited for a Slack message or a webhook trigger for automation, or all of them (just to be sure ;) ).So I built ‚Äîa simple tool that lets you define  (the things you care about) and attach  (the places you want updates to go). One configuration file, multiple sinks, and no more version-hunting.Right now, Veno supports GitHub, DockerHub, and ArtifactHub as sources, with notifications via Email, Webhooks, Google Chat, and Slack. A CLI version is up and running, and a web service is in the works. Eventually, I want to add scheduled tracking (daemon mode) so it runs in the background.If you're in the same boat‚Äîmanaging multiple software artifacts and tired of manually checking for updates‚ÄîVeno might be useful for you. I‚Äôd love feedback, feature requests, and contributors! Would this be useful for your workflow? Let me know what you'd like to see in a tool like this!: I started learning Rust about 6 months ago, so I'd appreciate some feedback about idiomatic Rust and or other tips how to improve! The version checking logic is still kind of janky and the GitHub logic only supports semver currently. I am going to improve upon the version logic in future releases!]]></content:encoded></item><item><title>I got Linux running in a PDF file via a RISC-V emulator compiled to JS</title><link>https://www.reddit.com/r/linux/comments/1ifwpl0/i_got_linux_running_in_a_pdf_file_via_a_riscv/</link><author>/u/vk6_</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 13:00:01 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Managing Secrets in Docker Compose ‚Äî A Developer&apos;s Guide | Phase Blog</title><link>https://phase.dev/blog/docker-compose-secrets/</link><author>/u/ascendence</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 12:57:16 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[It's truly remarkable how much the direction of software engineering is dictated by inertia. Brendan Eich in 1995 designed JavaScript to be a client side scripting language of choice for the Netscape browser, over the years it has evolved to be on the client, server and other technologies like serverless. Similarly, Docker Compose has evolved from a local development tool into a popular choice for deploying applications, even in production environments. While Docker has published guidelines for using Compose in production, one critical aspect often overlooked by users is secure secret management.In this guide, we'll explore the best practices for managing secrets in modern Docker Compose deployments and discuss common pitfalls to avoid. We'll progressively build up from basic approaches to more secure configurations.The Problem with Environment VariablesMost Docker Compose setups handle secrets in one of two ways: either by hardcoding them directly in the compose file or using a  file:While convenient, this approach has several security implications:Environment variables are accessible to all processes in a containerThey often appear in logs during debuggingThey can be exposed through application errorsThey make it difficult to maintain separation of concerns between servicesLet's demonstrate why this is problematic. With a basic Postgres container running:We can easily inspect all environment variables:We can also directly print all environment variables from within the container:This exposure of secrets through environment variables has led to numerous security incidents over the years:Although the first two examples presume an application misconfiguration and most modern web application frameworks try their best to censor secrets in error logs, this can be a pretty serious issue.Let's explore three progressively more secure approaches to managing secrets in Docker Compose.First, ensure you're running Docker Compose version 2.30.0 or later for full secrets support:Your application should also be configured to read secrets from files rather than environment variables. Here's a pattern we recommend for python, as an example:Prioritizes reading secrets from files using the  suffix conventionMaintains compatibility with environment variables as a fallbackFor a given secret  check if  environment variable containing a file path exists, if yes - read the  secret from the fileElse, read the secret from the  environment variableApproach 1: Environment Variables - Mount secrets inside your containers based on the values of host environment variablesThe following implementation uses Docker Compose's secrets feature to read environment variables from the host and mount them as files via a virtual filesystem in each of your services:This mounts secrets as read-only virtual filesystem under :Easy setup - Simply mount the values of environment variables in memory as a filesystemSecrets never written to disk - Secrets remain in memory, reducing attack surface from filesystem accessBetter isolation between services - Each service only receives the secrets provisioned to itRead-only mounting - Prevents accidental or malicious corruption of secrets by containersSecrets exposed as host environment variables - Secrets must exist as environment variables on the host system - This can be addresses by using runtime secret injection via a secret manager such as Phase.World-readable within container - Any user, user group or process within the container can read the secrets (addressed in the next section)Requires service restart for updates - Changes to secrets require restarting affected services to take effectUsing things like  on your host system to set secrets as environment can create other unwanted externalities like your secrets getting logged in your shell history. You can use the Phase CLI to to improve the overall secret management workflow by injecting secrets directly inside the docker compose process during runtime. Here's an example:Approach 2: File-Based Secrets - Mount secrets on the host system inside your containerThe following implementation uses Docker Compose's secrets feature to mount files containing secrets from the host in each of your services:Better isolation between services - Each service only receives the secrets provisioned to itDynamic secret updates without restarts - Services can read updated secrets without container restartsInherits host file permissions - Secret files maintain their permission attributes from the host systemRead-only mounting - Prevents accidental or malicious corruption of secrets by containersSecrets written to disk on the host system - Creates potential security risk from filesystem access or backupsRequires secure file management - Additional operational overhead to secure secret filesWorld-readable by default - All users/processes in container can read secrets unless explicitly restricted - Addressed in the next sectionTo make creation of secrets on the host system easier and to improve the overall secret management workflow you can use the Phase CLI. Here's an example:Controlling access to secrets supplied to your servicesNow that we have figured out how to supply secrets securely to your services, next let's take a look how at how we can better protect them once provisioned inside our containers:Docker Compose supports what they call a 'long syntax' for declaring how secrets are provisioned and controlling their access with more granularity within the respective service's containers.: The name of the secret as it exists on the platform.: The name of the file to be mounted in /run/secrets/ in the service's task container, or absolute path of the file if an alternate location is required. Defaults to source if not specified. and : The numeric UID or GID that owns the file within /run/secrets/ in the service's task containers. Default value is USER running container.: The permissions for the file to be mounted in /run/secrets/ in the service's task containers, in octal notation. The default value is world-readable permissions (mode 0444). The writable bit must be ignored if set. The executable bit may be set.You can find the  and the  of for a given image by looking at the Dockerfile or if it's your own image add one.Here's a postgresql example:Restricts secret access to specific users/groupsPrevents other users from reading secretsMaintains write protectionYou can verify the permissions:For more information on the Docker Compose secrets long syntax, please see the Docker docsWhile this is a good start for your docker compose secrets, below are some of the things that you should most consider when dealing with informational fissile materials like secrets:Keep secrets away from source code, container files and images. Ivory and ebony, AC/AD and secrets and source code and container images; never the two shall meet.Control access to secrets and keep them in sync and up to date with the rest of your team and deployments securely. Please do not add secrets to your git repository, drop your .env files over Slack or add them to your Notion docs as part of getting started with a project.Don't reuse secrets across different environments. Your production database password should never be the same as the  that you are using for local development. Compromise of one will mean de-facto compromise of all.Encrypt secrets at all times, whether they are in flight over a network making they way to your production deployment or waiting in a database patiently to be pulled. "Dance like no one is watching, but encrypt like everyone is" - Werner Vogels, Chief Technical Officer Amazon.Keep track of all changes and actions over your secrets. It's 4:30 in the morning, do you know where your secrets are? Keep tabs on the who, what, when, and where so you can infer the  if and when there is an incident. Given the outsized number of breaches that occur due to a secret compromise, you will need those audit logs during an investigation.Some or all of these points may seem obvious to most of you reading this, but what may not be as obvious is how tricky and tedious it can be to follow security best practices without losing development velocity. Consider using open-source secrets management platform like Phase which can help streamline the process. We are a bit biased plugging our own solution, but we think you'll find it useful.Docker Compose's secret management capabilities have matured significantly, offering features typically found in larger container orchestration systems. While there are still some areas of improvements and limitations around permission enforcement (see docker/compose#12362), the available options provide a solid foundation for securing secrets in both development and smaller production environments.]]></content:encoded></item><item><title>Configurature: Simple, flexible, and powerful application configuration for Go.</title><link>https://www.reddit.com/r/golang/comments/1ifwa5o/configurature_simple_flexible_and_powerful/</link><author>/u/PopAdministrative923</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 2 Feb 2025 12:33:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Configurature is a Go library that provides declarative app configuration using structs. Configuration values can be specified (in value precedence order) on the command line, using environment variables, and/or in a config file (yaml or json).]]></content:encoded></item><item><title>Was Building Something, But Now Having Self-Doubt</title><link>https://www.reddit.com/r/golang/comments/1ifvwi7/was_building_something_but_now_having_selfdoubt/</link><author>/u/unbeatable697</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 2 Feb 2025 12:08:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Initially, I started building a CLI file-sharing platform in Go. I am almost done with the project, with only some CLI and backend parts remaining. However, now I feel like it might not be worth completing since I doubt anyone will use or appreciate it.I originally started this project to learn Go, and through it, I have gained experience with CRUD operations and more. Now, I'm in a dilemma‚Äîshould I complete this project or move on to something else?By the way, I‚Äôm using GitHub as the storage backend, allowing users to upload files and download them on any PC by providing a name and password.also even if I complete it is it worth to add in resume otherwise i dont see any point of making it. as people are not going to use it]]></content:encoded></item><item><title>Lambdas/serverless functions/functions as a service - any opinions?</title><link>https://www.reddit.com/r/kubernetes/comments/1ifvqhv/lambdasserverless_functionsfunctions_as_a_service/</link><author>/u/leeliop</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 2 Feb 2025 11:58:01 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[If so what did you think? How much friction was there? Is it worth it rather than throwing a bunch of functions into a service and routing the ingress?   submitted by    /u/leeliop ]]></content:encoded></item><item><title>Show HN: Lume ‚Äì OS lightweight CLI for MacOS and Linux VMs on Apple Silicon</title><link>https://github.com/trycua/lume</link><author>sandropuppo</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 11:46:22 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[We just open-sourced Lume - a tool we built after hitting walls with existing virtualization options on Apple Silicon. No GUI, no complex stacks - just a single binary that lets you spin up macOS or Linux VMs via CLI or API.Why we built Lume:
- Run native macOS VMs in 1 command, using Apple Virtualization.Framework: `lume run macos-sequoia-vanilla:latest`- API server to manage VMs programmatically `POST /lume/vms`- A python SDK on github.com/trycua/pylumeRun prebuilt macOS images in just 1 step:
lume run macos-sequoia-vanilla:latestLocal API Server:
`lume` exposes a local HTTP API server that listens on `http://localhost:3000/lume`, enabling automated management of VMs.HN devs - would love raw feedback on the API design and whether this solves your Apple Silicon VM pain points. What would make you replace UTM/Multipass/Docker Desktop with this?]]></content:encoded></item><item><title>I thought TypeScript&apos;s type system was powerful. Until I tried Rust</title><link>https://www.reddit.com/r/rust/comments/1ifurb2/i_thought_typescripts_type_system_was_powerful/</link><author>/u/nikitarevenco</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 2 Feb 2025 10:49:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[My first language was JavaScript. Then TypeScript. TypeScript caught many bugs which I've experienced with JS, such as spending 2 hours debugging because I accidentally wrote "false" instead of false. TypeScript caught these bugs. I was amazed by how "powerful" its type system isThen I tried Rust. I've been programming in Rust for about 2 months now. Rust's type system to TypeScript is like TypeScript's type system to JavaScript. I wouldn't have believed these worlds before Rust. Having a type system which actually co-exists with your code is really, really delightful.When a value claims to have some type in TypeScript, you can't be sure that it actually is that type. Any of the 100s of function calls used to derive this value may have used an  assertion, and if this assertion is wrong then the type is invalid. If an invalid type is somewhere deep enough it can spread like the plague, but you won't know.TypeScript's type inference compared to Rust's is weaker. This means there are often situations where you're forced to use an  assertion. And that's pretty bad, because now you've introduced an extra chance for incorrect types to be introduces and proliferate in your codeWhen people said "haha using JavaScript on the backend", I thought TypeScript is the solution. But if you want to be certain your program will work correctly you'll have to be extra careful when using TypeScript, values can claim to be some type while not actually being that type.In some sense, using an  assertion is similar to using  in Rust. You're making a promise that your program upholds some invariants that the compiler can't infer itself. And the Rust community places a lot of importance on properly documenting why certain invariants are upheld for  blocks. But  are not treated nearly the same, they're often just used without a second thought. And that's a problemHaving a strong type system is one of the most important factors about a language, to me at least. Type systems allow you to express the purpose of your program in a way that your code never will.I am super grateful to Rust and Haskell for opening my eyes on type systems. I'm also super grateful for all the work done on the TypeScript compiler. While it's far from perfect, I understand it's the way it is and has to be that way. There's not much TS can do since it compiles to the same old JavaScript. TypeScript it is an essential layer of protection on top of your JavaScript. But I'm really looking forward to a future where we don't need to use TypeScript anymore, and Rust frameworks such as Dioxus can become popular like Next.js (I can dream...)]]></content:encoded></item><item><title>Unexpected Benefits of Building Your Own Tools</title><link>https://tiniuc.com/make-more-tools/</link><author>/u/tiniucIx</author><category>dev</category><category>reddit</category><pubDate>Sun, 2 Feb 2025 10:41:15 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Recently I've been thinking a lot about some of the tools I've made, and I have found an insight from game development that I think can apply to the software engineering industry as a whole.The origin of the insightI'm working on a hacking simulator, Botnet of Ares and part of the work involves designing and balancing the parameters for the various Exploits that the player can use. For context, these Exploits are kind of equivalent to items in an Role-Playing Game. They have stats similar to "Damage", "Attack Speed", "Mana Cost", except framed in the context of a computer thread performing work - so the rough equivalents in my game would be "Machine Speed", "Loss Wait Time" & the various "Per Machine" costs, respectively.This used to be done in a spreadsheet, and the process involved copy-pasting ~10 fields every time I wanted to move something into or out of the game.At some point I realized that I am spending an eternity staring at spreadsheets and it was just no fun for this sort of task. Furthermore, I was wasting a lot of time copy-pasting things into Godot (the game engine I am using), play-testing, noticing something is wrong, going back to the spreadsheet, editing, copy-pasting again etc. etc. etc.So I decided to implement my own editor which can save & load directly using the Godot  format. It's super quick & dirty & not optimized at all, but it's already made the process of creating content for the game much faster.All this is is a little editor that lets me input stats for the various exploits, shows a bit of info on the right side about its balance, and then outputs a scene file that can be loaded directly into the game.What this has made me realize is that speeding up part of a workflow can provide value far beyond the mere time savings: it also unlocks new ways of working that were not possible before. For example, with the Exploit Editor I can go over entire lines of devices and balance them in relation to each other. This used to be incredibly tedious to do with the spreadsheet because of all the tedious copy-pasting required to import from the editor. But now, I can edit dozens of scenes and remain in a state of flow throughout.The editor has made the creation of these Exploits at least an order of magnitude faster, and has also increased the quality of content I can make by letting me iterate much faster. Not too shabby for ~4 hours of work!Iteration velocity: the insight from game developmentI was motivated to build the Exploit Editor for Botnet of Ares thanks to a common principle in game development: to design a good game, you must iterate through as many versions of the game as you can, keeping the parts that are fun, getting rid of the ones that are not & improving the game at any step. Essentially, the more iterations of the design you can make, the higher quality your game will be - limited, of course, by your budget.I built the Exploit Editor because I could not iterate through various item designs quickly enough, and as a result I now have many more items in the game, and they are all higher quality than they would be without this tool.This is a well-known principle in the games development industry - in bigger studios there is often a team dedicated to building & supporting tools & editors for designers & other folks to use. And I think this principle has applications far beyond game development.Application from work - the chilictl utilityDuring my day job I have worked on an utility called "chilictl." It allows one to list embedded devices connected to the PC, see basic information about them & flash a new application.$ ./chilictl list
2020-12-16 12:53:35.897 NOTE:  Cascoda SDK v0.14 Dec 16 2020
Device Found:
        Device: Chili2
        App: mac-dongle
        Version: v0.21-71-g49212790
        Serial No: FBC647CDB300A0DA 
        Path: 0001:0051:00
        Available: Yes
        External Flash Chip Available: Yes
$ ./chilictl flash -s FBC647CDB300A0DA -df "~/sdk-chili2/bin/ldrom_hid.bin"
2020-12-16 12:56:46.510 NOTE:  Cascoda SDK v0.14 Dec 16 2020
Flasher [FBC647CDB300A0DA]: INIT -> REBOOT
Flasher [FBC647CDB300A0DA]: REBOOT -> ERASE
Flasher [FBC647CDB300A0DA]: ERASE -> FLASH
Flasher [FBC647CDB300A0DA]: FLASH -> VERIFY
Flasher [FBC647CDB300A0DA]: VERIFY -> VALIDATE
Flasher [FBC647CDB300A0DA]: VALIDATE -> COMPLETE
All of these things are things you could already do in other ways before the tool was created. For example, you could find the serial number by connecting to the USB debug API & looking for the right . Or you could flash devices by digging through your drawer, getting your JLink programmer out, making sure the flimsy 10-pin JTAG cable hasn't snapped a wire internally like the other 5 you've had to throw out this year, opening the correct programming utility & then finally flashing.You can see where I'm going with this. A single utility was created that removes 90% of the tedium. The tool would have been worth it for this reason alone, but now my colleagues have incorporated it in countless workflows. Here are some examples:automated Hardware-In-The-Loop nightly test harness that notifies people whenever someone's  broken prodmodifying serial numbers & security credentials en masse, for tests & development work that requires devices with different identities.unit tests for the parts of the code that were prone to bugs & hard to keep an eye on until nowtests and experiments involving different binaries across dozens of devicessimply programming the hardware you're working on without having to fiddle with wiresThe crucial insight is that none of these applications would have been obvious without being able to flash devices an order of magnitude faster. And  was not designed to be built into these! All of these applications were unlocked once we had experience with the new utility.One of the highest returns on investment I've had on workflow automation was the script below. It rebuilds an OpenWRT image from scratch with updated dependencies.#!/bin/sh
./scripts/feeds update -a
./scripts/feeds install -a
make clean
make -j12
# Write logs to file, for debugging
#make -j12 V=sc -k 2>&1 | tee build_log.txt
cmatrix -b
Ive lost count of how many times I ran  without first updating the feed I was working on! The call to  at the end plays a Matrix inspired stream of green characters the moment the build finishes. This solves the problem of starting the build, working on something else & not noticing that the build has completed even when it was a rather high priority task!Lets say this script took five minutes to write. If it saves one rebuild, it has already paid for itself four times over. It has saved  more time since. Thanks to this script, debugging certain things that used to take a whole day now can be done in just a few hours.I think many people could benefit from creating more tools for their own needs. Think of the fraction of your work that you hate the most, and see if there's any way you can remove some of the tedium from it. Even a little bit of automation helps a lot if you are automating a common frustrating task.And often, you will find that these tools will let you work in new and surprising ways. Ways in which would not have been obvious had you not built the tool in the first place. This leads not just to building your application faster, but building a higher quality application as well thanks to the benefits of iterative development. As the saying goes, quantity has a quality all its own.

    
    tagged
    
	Development]]></content:encoded></item><item><title>Reverse-engineering and analysis of SanDisk High Endurance microSDXC card (2020)</title><link>https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</link><author>userbinator</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 10:32:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Thank you for contacting SanDisk¬Æ Global customer care. We really appreciate you being a part of our SanDisk¬Æ family.I understand that you wish to know more about the SanDisk¬Æ High Endurance video monitoring card, as such please allow me to inform you that all our SanDisk¬Æ memory cards uses Multi level cell technology (MLC) flash. However, the read/write cycles for the flash memory is not published nor documented only the read and write speed in published as such they are 100 MB/S & 40 MB/s. The 64 GB card can record Full HD video up to 10,000 hours. To know more about the card you may refer to the link below:Best regards,
Allan B.
SanDisk¬Æ Global Customer CareI‚Äôll give them a silver star that says ‚ÄúYou Tried‚Äù at least.While (micro)SD cards feel like a solid monolithic piece of technology, they‚Äôre made up of multiple different chips, each performing a different role. A basic SD card will have a controller that manages the NAND Flash chips and communicates with the host (PC, camera, etc.), and the NAND Flash itself (made up of 1 or more Flash dies). Bunnie Huang‚Äôs blog, Bunnie Studios, has an excellent article on the internals of SD cards, including counterfeits and how they‚Äôre made ‚Äì check it out here.Block diagram of a typical SD card.MicroSD cards often (but not always!) include test pads, used to program/test the NAND Flash during manufacture. These can be exploited in the case of data recovery, or to reuse microSD cards that have a defective controller or firmware by turning the card into a piece of raw NAND Flash ‚Äì check out Gough Lui‚Äôs adventures here. Note that there is no set standard for these test pads (even for the same manufacturer!), but there are common patterns for some manufacturers like SanDisk that make reverse-engineering easier.microSD cards fall into a category of ‚Äúmonolithic‚Äù Flash devices, as they combine a controller and raw NAND Flash memory into a single, inseparable package. Many manufacturers break out the Flash‚Äôs data bus onto hidden (and nearly completely undocumented) test pads, which some other memory card and USB drive manufacturers take advantage of to make cheap storage products using failed parts; the controller can simply be electrically disabled and the Flash is then used as if it were a regular chip.In the case of SanDisk cards, there is very limited information on their cards‚Äô test pad pinouts. Each generation has slight differences, but the layout is mostly the same. CORRECTION (July 22, 2020):Upon further review, I might have accidentally created a discrepancy between the leaked pinouts online, versus my own documentation in terms of power polarity; see the ‚ÄúTest Pad Pinout‚Äù section.My card (and many of their higher-end cards ‚Äì that is, not their Ultra lineup) features test pads that aren‚Äôt covered by solder mask, but are instead covered by some sort of screen-printed epoxy with a laser-etched serial number on top. With a bit of heat and some scraping, I was able to remove the (very brittle) coating on top of the test pads; this also removed the serial number which I believe is an anti-tamper measure by SanDisk.After cleaning off any last traces of the epoxy coating, I was greeted with the familiar SanDisk test pad layout, plus a few extra on the bottom.The breakout board is relatively simple in concept: for each test pad, bring out a wire that goes to a bigger test point for easier access, and wire up the normal SD bus to an SD connector to let the controller do its work with twiddling the NAND Flash bus. Given how small each test pad is (and how many), things get a bit‚Ä¶ messy.I started by using double-side foam adhesive tape to secure the SD card to a piece of perfboard. I then tinned all of the pads and soldered a small 1uF ceramic capacitor across the card‚Äôs power (Vcc) and ground (GND) test pads. Using 40-gauge (0.1 mm, or 4-thousandths of an inch!) magnet wire, I mapped each test pad to its corresponding machine-pin socket on the perfboard. Including the extra test pads, that‚Äôs a total of 28 tiny wires!For the SD connector side of things, I used a flex cable for the XTC 2 Clip (a tool used to service HTC Android devices), as it acted as a flexible ‚Äúremote SD card‚Äù and broke out the signals to a small ribbon cable. I encased the flex cable with copper tape to act as a shield against electrical noise and to provide physical reinforcement, and soldered the tape to the outer pads on the perfboard for reinforcement. The ribbon cable end was then tinned and each SD card‚Äôs pin was wired up with magnet wire. The power lines were then broken out to an LED and resistor to indicate when the card was receiving power.With all of the test pads broken out to an array of test pins, it was time to make sense of what pins are responsible for accessing the NAND Flash inside the card.Diagram of the test pads on SanDisk‚Äôs High Endurance microSD card. (click to enlarge)The overall test pad pinout was the same for other microSD cards from SanDiskCORRECTION (July 22, 2020):I might actually have just gotten my own documentation mixed up in terms of the power and ground test pads. Regardless, one should always be careful to ensure the correct power polarity is sent to a device.I used my DSLogic Plus logic analyzer to analyze the signals on all of the pins. Since the data pinout was previously discovered, the hard part of figuring out what each line represented (data bus, control, address, command, write-protect, ready/busy status) was already done for me. However, not all of the lines were immediately evident as the pinouts I found online only included the bare minimum of lines to make the NAND Flash accessible, with one exception being a control line that places the controller into a reset state and releases its control of the data lines (this will be important later on).By sniffing the data bus at the DSLogic‚Äôs maximum speed (and using its 32 MB onboard buffer RAM), I was able to get a clear snapshot of the commands being sent to the NAND Flash from the controller during initialization.Bus Sniffing & NAND I/O 101 (writing commands, address, reading data)In particular, I was looking for two commands: RESET (0xFF), and READ ID (0x90). When looking for a command sequence, it‚Äôs important to know how and when the data and control lines change. I will try to explain it step-by-step, but if you‚Äôre interested there is an introductory white paper by Micron that explains all of the fundamentals of NAND Flash with much more information about how NAND Flash works.When a RESET command is sent to the NAND Flash, first the /CE (Chip Select, Active Low) line is pulled low. Then the CLE (Command Latch Enable) line is pulled high; the data bus is set to its intended value of 0xFF (all binary ones); then the /WE (Write Enable, Active Low) line is pulsed from high to low, then back to high again (the data bus‚Äô contents are committed to the chip when the /WE line goes from low to high, known as a ‚Äúrising edge‚Äù); the CLE line is pulled back low to return to its normal state. The Flash chip will then pull its R/B (Ready/Busy Status) line low to indicate it is busy resetting itself, then releases the line back to its high state when it‚Äôs finished.The READ ID command works similarly, except after writing the command with 0x90 (binary 1001 0000) on the data bus, it then pulls the ALE (Address Latch Enable) line high instead of CLE, and writes 0x00 (all binary zeroes) by pulsing the /WE line low. The chip transfers its internally programmed NAND Flash ID into its internal read register, and the data is read out from the device on each rising edge of the /RE (Read Enable, Active Low) line; for most devices this is 4 to 8 bytes of data.For each NAND Flash device, it has a (mostly) unique ID that identifies the manufacturer, and other functional data that is defined by that manufacturer; in other words, only the manufacturer ID, assigned by the JEDEC Technology Association, is well-defined.The first byte represents the Flash manufacturer, and the rest (2 to 6 bytes) define the device‚Äôs characteristics, as set out by the manufacturer themselves. Most NAND vendors are very tight-lipped when it comes to datasheets, and SanDisk (and by extension, Toshiba/Kioxia) maintain very strict control, save for some slightly outdated leaked Toshiba datasheets. Because the two aforementioned companies share their NAND fabrication facilities, we can reasonably presume the data structures in the vendor-defined bytes can be referenced against each other.In the case of the SanDisk High Endurance 128GB card, it has a NAND Flash ID of 0x45 48 9A B3 7E 72 0D 0E. Some of these values can be compared against a Toshiba datasheet:Description/InterpretationI/O voltage: Presumed 1.8 volts (measured with multimeter)Device capacity: Presumed 128 GB¬†(unable to confirm against datasheet)NAND type: TLC (Triple-Level Cell / 3 bits per cell)Flash dies per /CE: 4 (card uses four 32GB Flash chips internally)Block size: 12 MiB (768 pages per block) excluding spare area (determined outside datasheet)Page size: 16,384 bytes / 16 kiB excluding spare areaPlanes per /CE: 8 (2 planes per die)Interface type: AsynchronousProcess geometry: BiCS3 3D NAND (determined outside datasheet)Unknown (no information listed in datasheet)Unknown (no information listed in datasheet)Although not all byte values could be conclusively determined, I was able to determine that the , but at least it is  which improves endurance dramatically compared to traditional/planar NAND. Unfortunately, from this information alone, I cannot determine whether the card‚Äôs controller takes advantage of any SLC caching mechanisms for write operations.The chip‚Äôs process geometry was determined by looking up the first four bytes of the Flash ID, and cross-referencing it to a line from a configuration file in Silicon Motion‚Äôs mass production tools for their SM3271 USB Flash drive controller, and their SM2258XT DRAM-less SSD controller. These tools revealed supposed part numbers of SDTNAIAMA-256G and SDUNBIEMM-32G respectively, but I don‚Äôt think this is accurate for the specific Flash configuration in this card.I wanted to make sure that I was getting the correct ID from the NAND Flash, so I rigged up a Texas Instruments MSP430FR2433 development board and wrote some (very) rudimentary code to send the required RESET and READ ID commands, and attempt to extract any extra data from the chip‚Äôs hidden JEDEC Parameter Page along the way.My first roadblock was that the MSP430 would reset every time it attempted to send the RESET command, suggesting that too much current was being drawn from the MSP430 board‚Äôs limited power supply. This can occur during bus contention, where two devices ‚Äúfight‚Äù each other when trying to set a certain digital line both high and low at the same time. I was unsure what was going on, since publicly-available information didn‚Äôt mention how to disable the card‚Äôs built-in controller (doing so would cause it to tri-state the lines, effectively ‚Äúletting go‚Äù of the NAND bus and allowing another device to take control).I figured out that the A1 test pad (see diagram) was the controller‚Äôs reset line (pulsing this line low while the card was running forced my card reader to power-cycle it), and by holding the line low, the controller would release its control of the NAND Flash bus entirely. After this, my microcontroller code was able to read the Flash ID correctly and consistently.Reading out the card‚Äôs Flash ID with my own microcontroller code.JEDEC Parameter Page‚Ä¶ or at least what SanDisk made of it!The JEDEC Parameter Page, if present, contains detailed information on a Flash chip‚Äôs characteristics with far greater detail than the NAND Flash ID ‚Äì and it‚Äôs well-standardized so parsing it would be far easier. However, it turns out that SanDisk decided to ignore the standard format, and decided to use their own proprietary Parameter Page format! Normally the page starts with the ASCII string ‚ÄúJEDEC‚Äù, but I got a repeating string of ‚ÄúSNDK‚Äù (corresponding with their stock symbol) with other data that didn‚Äôt correspond to anything like the JEDEC specification! Oh well, it was worth a try.I collected the data with the same Arduino sketch as shown above, and pulled 1,536 bytes‚Äô worth of data; I wrote a quick program on Ideone to provide a nicely-formatted hex dump of the first 512 bytes of the Parameter Page data:Offset 00:01:02:03:04:05:06:07:08:09:0A:0B:0C:0D:0E:0F 0123456789ABCDEF
------ --+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-- ----------------
0x0000 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B SNDKSNDKSNDKSNDK
0x0010 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B SNDKSNDKSNDKSNDK
0x0020 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 ..... ...H.....A
0x0030 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 Hcj..... ...H...
0x0040 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 ..AHcj..... ...H
0x0050 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 .....AHcj..... .
0x0060 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 ..H.....AHcj....
0x0070 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 . ...H.....AHcj.
0x0080 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 .... ...H.....AH
0x0090 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 cj..... ...H....
0x00A0 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A .AHcj..... ...H.
0x00B0 B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 ....AHcj..... ..
0x00C0 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 .H.....AHcj.....
0x00D0 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08  ...H.....AHcj..
0x00E0 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 ... ...H.....AHc
0x00F0 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 j..... ...H.....
0x0100 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 AHcj..... ...H..
0x0110 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 ...AHcj..... ...
0x0120 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 H.....AHcj..... 
0x0130 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 ...H.....AHcj...
0x0140 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A .. ...H.....AHcj
0x0150 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 ..... ...H.....A
0x0160 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 Hcj..... ...H...
0x0170 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 ..AHcj..... ...H
0x0180 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 .....AHcj..... .
0x0190 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 ..H.....AHcj....
0x01A0 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 . ...H.....AHcj.
0x01B0 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 .... ...H.....AH
0x01C0 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 cj..... ...H....
0x01D0 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A .AHcj..... ...H.
0x01E0 B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 ....AHcj..... ..
0x01F0 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 .H.....AHcj.....Further analysis with my DSLogic showed that the controller itself requests a total of 4,128 bytes (4 kiB + 32 bytes) of Parameter Page data, which is filled with the same repeating data as shown above.When looking at the logic analyzer data, I noticed that the controller sends the READ ID command twice, but the first time it does so without resetting the Flash (which should normally be done as soon as the chip is powered up!). The data that the Flash returned was‚Ä¶ strange to say the least.I/O voltage: Unknown (no data)Device capacity: Unknown (no data)NAND type: SLC (Single-Level Cell / 1 bit per cell)Block size: 4 MB excluding spare areaPage size: 16,384 bytes / 16 kiB excluding spare areaInterface type: AsynchronousProcess geometry: 70 nm planarThis confused me initially when I was trying to find the ID from the logic capture alone; after talking to a contact who has experience in NAND Flash data recovery, they said this is expected for SanDisk devices, which make liberal use of vendor-proprietary commands and data structures. If the fourth byte is to be believed, it says the block size is 4 megabytes, which I think is plausible for a modern Flash device. The rest of the information doesn‚Äôt really make any sense to me apart from the first byte indicating the chip is made by Toshiba.I shouldn‚Äôt have to go this far in hardware reverse-engineering to just ask a simple question of what Flash SanDisk used in their high-endurance card. You‚Äôd think they would be proud to say they use 3D NAND for higher endurance and reliability, but I guess not!For those that are interested, I‚Äôve included the logic captures of the card‚Äôs activity shortly after power-up. I‚Äôve also included the (very crude) Arduino sketch that I used to read the NAND ID and Parameter Page data manually:]]></content:encoded></item><item><title>Show HN: Modest ‚Äì musical harmony library for Lua</title><link>https://github.com/esbudylin/modest</link><author>esbudylin</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 10:32:30 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[This is a project I've been building in my spare time over the past few months. It's a library that provides methods for working with musical harmony ‚Äí intervals, notes, chords. For example, it can parse almost any chord symbol (Fmaj7, CminMaj9, etc) and turn it into notes, or it can identify a chord from a given set of notes.I started this project with the idea of using formal grammar to parse chord symbols. I wanted to use it instead of a hand-written parser, which is the common approach among similar libraries. Lua caught my attention because of Lpeg, a Parsing Expression Grammar library that is both fast and easy to use. An additional motivation for using Lua was the lack of comparable libraries for it, even though the language is commonly used in audio programming.However, despite being a Lua library, the project itself is written in Fennel ‚Äî a "lispy" language that transpiles to Lua. Fennel has features that make writing code for the Lua platform much more pleasant: a concise syntax, macros, and destructuring ‚Äî a feature Lua sorely lacks!In the process, I definitely learned a lot about music theory, although my new knowledge is quite one-sided. By working on this library, I know a thing or two about types and structure of chords, but I learned almost nothing about their composition and transformation. Perhaps these will be the directions I explore next in the project.]]></content:encoded></item><item><title>Analyzing the codebase of Caffeine, a high performance caching library</title><link>https://adriacabeza.github.io/2024/07/12/caffeine-cache.html</link><author>synthc</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 09:37:05 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>When can I claim that I have little bit of knowledge about Kubernetes?</title><link>https://www.reddit.com/r/kubernetes/comments/1iftiic/when_can_i_claim_that_i_have_little_bit_of/</link><author>/u/Keeper-Name_2271</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 2 Feb 2025 09:17:22 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I've been learning kubernetes starting from last year. And I must have spent about 50hrs on udemy courses, labbing. However, I still can't do anything. As I said "I attempted labbing", I could not deploy what I want with kubernetes. Mostly, I was doing nginx deployment using k8s(:D).Now, I, as a 2yoe support engineer; whose job in k8s is basically restarting pods using rancher, wants to know what should I learn in order to be considered as a kubernetes beginner(as a person who primarily works with kubernetes)...]]></content:encoded></item><item><title>Python developer getting started with Rust</title><link>https://www.reddit.com/r/rust/comments/1ift6e5/python_developer_getting_started_with_rust/</link><author>/u/InfluenceFit478</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 2 Feb 2025 08:52:20 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I‚Äôm a Python developer (mostly backend and data related work, approx 12 years) and I‚Äôm seeing a big part of my toolchain transitioning to having a core built in Rust (polars, orjson, Pydantic, uv, ruff). All of these tools seem super exciting and I‚Äôm keen to contribute to some of them. However, I‚Äôd have to start from scratch learning Rust.Any suggestions on where to start? ]]></content:encoded></item><item><title>How to switch job?</title><link>https://www.reddit.com/r/kubernetes/comments/1ift1qm/how_to_switch_job/</link><author>/u/Localhost_notfound</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 2 Feb 2025 08:42:26 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I was in this perception if I clear CK-A I will be able to get good raise, I cleared my exam in june 2024. I have 3.5 yrs of experience in DevOps on prem and as well as on aws. How ever I am not getting a decent salary I am getting 11LPA. What am I doing wrong? I am not even getting any calls if I am trying to switch!]]></content:encoded></item><item><title>CDC: Unpublished manuscripts mentioning certain topics must be pulled or revised</title><link>https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction</link><author>KittenInABox</author><category>dev</category><category>hn</category><pubDate>Sun, 2 Feb 2025 04:52:28 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Slashdot Asks: Do You Remember Your High School&apos;s &apos;Computer Room&apos;?</title><link>https://developers.slashdot.org/story/25/02/02/0233216/slashdot-asks-do-you-remember-your-high-schools-computer-room?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 2 Feb 2025 02:36:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[Bill Gates' blog has been updated with short videos about his upcoming book, including one about how his school ended up with an ASR-33 teletype that could connect their Seattle classroom to a computer in California. "The teachers faded away pretty quickly," Gates adds, "But about six of us stayed hardcore. One was Paul Allen..." ‚Äî the future co-founder of Microsoft. And the experience clearly meant a lot to Gates. "Microsoft just never would've happened without Paul ‚Äî and this teletype room." 

In a longer post thanking his "brilliant" teachers, Gates calls his teletype experience "an encounter that would shape my entire future" and "opened up a whole new world for me." Gates also thanks World War II Navy pilot and Boeing engineer Bill Dougall, who "was instrumental in bringing computer access to our school, something he and other faculty members pushed for after taking a summer computer class... The fascinating thing about Mr. Dougall was that he didn't actually know much about programming; he exhausted his knowledge within a week. But he had the vision to know it was important and the trust to let us students figure it out." 

Gates shared a similar memory about the computer-room's 20-something overseer Fred Wright, who "intuitively understood that the best way to get students to learn was to let us explore on our own terms. There was no sign-up sheet, no locked door, no formal instruction."

Instead, Mr. Wright let us figure things out ourselves and trusted that, without his guidance, we'd have to get creative... Some of the other teachers argued for tighter regulations, worried about what we might be doing in there unsupervised. But even though Mr. Wright occasionally popped in to break up a squabble or listen as someone explained their latest program, for the most part he defended our autonomy... 
Mr. Wright gave us something invaluable: the space to discover our own potential.
 

Any Slashdot readers have a similarly impactful experience? Share your own thoughts and memories in the comments. 

Do you remember your high school's computer room?]]></content:encoded></item><item><title>Macrodata Refinement</title><link>https://lumon-industries.com/</link><author>gaws</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Feb 2025 21:46:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: ESP32 RC Cars</title><link>https://github.com/mattsroufe/esp32_rc_cars</link><author>mattsroufe</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Feb 2025 18:51:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This is a projected I started that blends both the fun of playing a split screen multiplayer driving game and controlling real rc cars.The cars can also be controlled via bluetooth gamepads and is meant to be easily hackable.]]></content:encoded></item><item><title>Grafana is the goat... Let&apos;s deploy the LGTM stack</title><link>https://www.youtube.com/watch?v=1X3dV3D5EJg</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/1X3dV3D5EJg?version=3" length="" type=""/><pubDate>Sat, 1 Feb 2025 15:24:39 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Get up to 67% off VPS at Hostinger. Use code FIRESHIP for an extra discount at https://hostinger.com/fireship

#linux #programming #softwareengineer 

üí¨ Chat with Me on Discord

https://discord.gg/fireship

üîó Resources

Docker 101 https://youtu.be/rIrNIzy6U_g
Full Deno Course https://fireship.io/courses/deno
Grafana on GitHub https://github.com/grafana/grafana

üî• Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

üé® My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

üîñ Topics Covered

What is Grafana? 
How to get started with software observability?
What is Open Telemetry?
How to deploy LGTM stack?
How monitor Linux server performance?]]></content:encoded></item><item><title>CDC data are disappearing</title><link>https://www.theatlantic.com/health/archive/2025/01/cdc-dei-scientific-data/681531/</link><author>doener</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Feb 2025 11:52:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Updated at 5:53 p.m. on January 31, 2025 Last night, scientists began to hear cryptic and foreboding warnings from colleagues: Go to the CDC website, and download your data now. They were all telling one another the same thing: Data on the website were about to disappear, or be altered, to comply with the Trump administration‚Äôs ongoing attempt to scrub federal agencies of any mention of gender, DEI, and accessibility. ‚ÄúI was up until 2 a.m.,‚Äù Angela Rasmussen, a virologist at the Vaccine and Infectious Disease Organization at the University of Saskatchewan who relies on the CDC‚Äôs data to track viral outbreaks, told me. She archived whatever she could.The full scope of the purge isn‚Äôt yet clear. One document obtained by  indicated that the government was, as of yesterday evening, intending to target and replace, at a minimum, several ‚Äúsuggested keywords‚Äù‚Äîincluding ‚Äúpregnant people, transgender, binary, non-binary, gender, assigned at birth, binary [], non-binary [], cisgender, queer, gender identity, gender minority, anything with pronouns‚Äù‚Äîin CDC content. While these terms are often politicized, some represent demographic variables that researchers collect when tracking the ebb and flow of diseases and health conditions across populations. Should they be reworded, or even removed entirely, from data sets to comply with the executive order, researchers and health-care providers might have a much harder time figuring out how diseases affect specific communities‚Äîmaking it more challenging to serve Americans on the whole.CDC data‚Äôs ‚Äúexplicit purpose‚Äù is to guide researchers toward the places and people who most need attention, Patrick Sullivan, an epidemiologist at Emory University and a former CDC Epidemic Intelligence Service officer, told me. As the changes unfold before him, he said, ‚Äúit‚Äôs hard to understand how this benefits health.‚ÄùWhen I contacted the CDC, a spokesperson redirected my requests for comment to the Department of Health and Human Services. After this story was published, an HHS spokesperson said that ‚Äúall changes to the HHS website and HHS division websites are in accordance with President Trump‚Äôs January 20 Executive Orders‚Äù on gender and DEI.The government appears to understand that these changes could have scientific implications: The document directing a review of CDC content suggests that some work could be altered without ‚Äúchanging the meaning or scientific integrity of the content,‚Äù and that any such changes should be considered ‚Äúroutine.‚Äù Changing other content, according to the document, would require review by an expert precisely because any alterations would risk scientific integrity. But the document does not specify how data would be sorted into those categories, or at whose discretion.‚ÄúMy fear is that in the short term, entire data sets would be taken down,‚Äù then reappear with demographic variables removed or altered to conform with DEI restrictions, Katie Biello, an epidemiologist at Brown, told me. Excising mention of gender and sexual orientation, for instance, from public-health data sets could require stripping entire columns of data out. If the government chooses to define sex as binary, transgender people and nonbinary people, among others, could be effectively erased. In response to the ongoing changes, some groups of researchers are now rushing to archive the CDC website in full.Acknowledging and addressing health differences among demographic groups is a basic epidemiological tenet, Biello told me, ‚Äúso we know where to target our health interventions.‚Äù She pointed to examples in her own field: Gay men have higher rates of STIs, but lower rates of obesity; transgender women have higher rates of HIV, but lower rates of prostate cancer. More broadly, demographic changes to data sets could limit the country‚Äôs ability to identify which Americans are most at risk from an expansive list of conditions including adolescent depression, STIs, even sex-specific cancers. Changing data sets in this way would be tantamount to ‚Äúerasing our ability to use data and evidence‚Äù to care for people, Rachel Hardeman, a health-equity expert at the University of Minnesota, told me.Jennifer Nuzzo, an epidemiologist at Brown, pointed to mpox as a recent example of how replacing ‚Äúgender‚Äù with ‚Äúsex,‚Äù or ignoring sexual orientation, could limit effective public-health responses. At the beginning of the United States‚Äô 2022 outbreak, neither researchers nor the public had much clarity on who was most affected, leading to widespread panic. ‚ÄúOfficials were talking about the situation as if it was a risk we equally faced,‚Äù Nuzzo said. By collecting detailed demographic information, researchers were able to show that the disease was primarily affecting men who have sex with men, allowing officials to more efficiently allocate resources, including vaccines, and bring the epidemic under control before it affected Americans more widely.A scrub such as this could also change how the government allocates funds for long-standing threats to public health, which could widen health-equity gaps, or reverse progress in combatting them. Rates of STIs more generally have recently begun to plateau in the U.S., after decades of steady increase‚Äîbut altering data that focus interventions on, say, transgender populations, or men who have sex with men, could undo those gains. If no data exist to prove that a health issue concentrates within a particular community, that ‚Äúprovides a justification to cut funding,‚Äù one researcher told me. (Several scientists who spoke with me for this article requested anonymity, for fear of retaliation for speaking out about the loss of federal data.) Sullivan, whose work focuses on HIV surveillance, compared the government‚Äôs actions to, effectively, destroying the road map to determining who in America most needs screening, pre-exposure prophylaxis, and treatment.Much of the data on the CDC website have been aggregated from states, so it would be possible for researchers to reassemble those data sets, Nuzzo pointed out. But that‚Äôs an onerous task, and several scientists told me they never thought they‚Äôd be in a position where they‚Äôd have to scramble to squirrel away publicly available federal data. Nuzzo also worried that states might be reluctant in the future to share data with the federal government, or might decide not to bother collecting certain data at all. On the most basic scientific level, changing federal-government data means those data become unreliable. Public-health data are collected with the intention of sussing out which populations most need health interventions; altering those data leaves behind a skewed portrait of reality.]]></content:encoded></item><item><title>Show HN: TalkNotes ‚Äì A site that turns your ideas into tasks</title><link>https://www.talknotes.tech/</link><author>ajlkshdaksjd</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Feb 2025 11:43:33 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[Turn any audio into smart notesTurn hours of note taking into seconds. Record voice notes or upload recorded audio, and let the AI transcribe & structure them into actionable text. Create todo lists, notes, flashcards, transcripts, and more! Works in 100+ languages.]]></content:encoded></item></channel></rss>