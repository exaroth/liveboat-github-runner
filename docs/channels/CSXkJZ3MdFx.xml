<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>LibreOffice 25.8 Slams the Door On Windows 7 and 8.x</title><link>https://tech.slashdot.org/story/25/08/23/0124202/libreoffice-258-slams-the-door-on-windows-7-and-8x?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Sat, 23 Aug 2025 10:00:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[BrianFagioli shares a report from NERDS.xyz: LibreOffice 25.8 has landed, and while it packs in new features and speed improvements, the biggest headline is who just got left behind. If you are still running Windows 7 or Windows 8/8.1, this is the end of the road. LibreOffice will not run on those systems anymore, and there are no workarounds. The suite has slammed the door shut.
 
For years, LibreOffice kept older Windows users afloat while Microsoft and other developers moved on. That lifeline is gone. Anyone stubbornly clinging to Windows 7 or 8 now has two choices: upgrade or stay stuck on outdated software. LibreOffice has made it clear that it will not carry dead platforms any further. And the cuts do not stop there. 32-bit Windows builds are on their way out, with deprecation already in place. On the Mac side, 25.8 is the last release that runs on macOS 10.15. Starting with LibreOffice 26.2, only macOS 11 and newer will be supported. In other words, if your computer is too old to run modern systems, LibreOffice is walking away.]]></content:encoded></item><item><title>I built Puhu, a pillow drop-in replacement in Rust</title><link>https://www.reddit.com/r/rust/comments/1mxx8nu/i_built_puhu_a_pillow_dropin_replacement_in_rust/</link><author>/u/creworker</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 23 Aug 2025 09:48:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hey All, I’m a python developer and recently learning rust. I decided to build a drop-in replacement for pillow. Pillow is a 20+ old python package for image processing, and it’s well optimized. why did I start doing that? because why not 😅 I wanted to learn rust and how to build python packages with rust backend. I did some benchmarks and actually it’s working pretty good, it’s faster than pillow in some functions. My aim is use same api naming and methods so it will be easy to migrate from pillow to puhu. I’ve implemented basic methods right now. continue working on other ones.I appreciate any feedback, support or suggestions. ]]></content:encoded></item><item><title>Video: LibreOffice 25.8 – Some of the new features</title><link>https://www.youtube.com/watch?v=6dIRR37PF7M</link><author>/u/themikeosguy</author><category>dev</category><category>reddit</category><pubDate>Sat, 23 Aug 2025 08:34:07 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Upgrading cluster in-place coz I am too lazy to do blue-green</title><link>https://www.reddit.com/r/kubernetes/comments/1mxuf5v/upgrading_cluster_inplace_coz_i_am_too_lazy_to_do/</link><author>/u/suman087</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 23 Aug 2025 06:50:48 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I&apos;m too dumb for Zig&apos;s new IO interface</title><link>https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/</link><author>begoon</author><category>dev</category><category>hn</category><pubDate>Sat, 23 Aug 2025 06:39:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old "interfaces" had problems. Like this performance issue that I opened. And it relied on a mix of types, which always confused me, and a lot of  - which is generally great, but a poor foundation to build an interface on.I've been slowly upgrading my libraries, and I ran into changes to the  client used by my smtp library. For the life of me, I just don't understand how it works.Zig has never been known for its documentation, but if we look at the documentation for , we'll find:input output options InitErrorClient
Initiates a TLS handshake  establishes a TLSv1 TLSv1 sessionSo it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a , but  does expose a  and  method, so that's probably a good place to start: stream  stdnetallocator stream writer  stream reader  stream tls_client  stdcryptotlsClient
  readerwriterinterfaceNote that  returns a  and  returns a  - those aren't the types our  expects. To convert the  to an , we need to call its  method. To get a  from an , we need the address of its  field. This doesn't seem particularly consistent. Don't forget that the  and  need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of . In a real word example, I think it means that I'll always have to wrap the  into my own heap-allocated type; giving the writer and reader have a cozy stable home.Speaking of allocations, you might have noticed that  and  take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation  tell me these need to be at least std.crypto.tls.max_ciphertext_record_len large, so we need to fix things a bit: write_buf writer  streamwrite_buf read_buf reader  streamread_bufHere's where the code stands:  std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf tls_client  stdcryptotlsClient
      readerwriterinterface tls_clientBut if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a  and a . Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while  and  are passed another.Let's give it what it wants AND send some data: bundle  bundleallocator bundleallocator tls_client  stdcryptotlsClient
  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer  tls_client tls_clientwriterNow, if I try to run it, the program just hangs. I don't know what  is, but I know Zig now loves buffers, so let's try to give it something: write_buf2 tls_client  stdcryptotlsClient
  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer write_buf2 tls_client tls_clientwriterGreat, now the code doesn't hang, all we need to do is read the response.  exposes a  field which is "Decrypted stream from the server to the client." That sounds like what we want, but believe it or not  doesn't have a  method. It has a  a , a  (which seems close, but it blocks until the provided buffer is full), a  and a lot more, but nothing like the  I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer: buf wbuf n  tls_clientreaderwbuflen
stddebugn bufnIf we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also  to provide a .Here's my current version (it doesn't work, but it doesn't crash!): std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf bundle  bundleallocator bundleallocator write_buf2 read_buf2 tls_client  stdcryptotlsClient
      readerwriterinterfaceca bundle  bundlehost explicit read_buffer read_buf2write_buffer write_buf2 tls_client tls_clientwriter buf wbuf n  tls_clientreaderwbuflen
  stddebugn bufnWhen I looked through Zig's source code, there's only one place using . It helped to get me where where I am. I couldn't find any tests.I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find  - the renamed version of . Maybe there's a helper like: tls.Client.init(allocator, stream) somewhere. And maybe it makes sense that we do  but  - I'm reminded of Go's  and . And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a  function on , but at this point I wouldn't bet on me.]]></content:encoded></item><item><title>No, Google Did Not Unilaterally Decide to Kill XSLT</title><link>https://meyerweb.com/eric/thoughts/2025/08/22/no-google-did-not-unilaterally-decide-to-kill-xslt/</link><author>/u/AlyoshaV</author><category>dev</category><category>reddit</category><pubDate>Sat, 23 Aug 2025 05:21:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[It’s uncommon, but not unheard of, for a GitHub issue to spark an uproar.  That happened over the past month or so as the WHATWG (Web Hypertext Application Technology Working Group, which I still say should have called themselves a Task Force instead) issue “Should we remove XSLT from the web platform?” was opened, debated, and eventually locked once the comment thread started spiraling into personal attacks.  Other discussions have since opened, such as  a counterproposal to update XSLT in the web platform, thankfully with (thus far) much less heat.If you’re new to the term, XSLT (Extensible Stylesheet Language Transformations) is an XML language that lets you transform one document tree structure into another.  If you’ve ever heard of people styling their RSS and/or Atom feeds to look nice in the browser, they were using some amount of XSLT to turn the RSS/Atom into HTML, which they could then CSS into prettiness.This is not the only use case for XSLT, not by a long shot, but it does illustrate the sort of thing XSLT is good for.  So why remove it, and who got this flame train rolling in the first place?Before I start, I want to note that in this post, I won’t be commenting on whether or not XSLT support should be dropped from browsers or not.  I’m also not going to be systematically addressing the various reactions I’ve seen to all this.  I have my own biases around this — some of them in direct conflict with each other! — but my focus here will be on what’s happened so far and what might lie ahead.As a very quick background, various people have proposed removing XSLT support from browsers a few times over the quarter-century-plus since support first landed.  It was discussed in both the early and mid-2010s, for example.  At this point, browsers all more or less supportXSLT 1.0, whereas the latest version of XSLT is 3.0.  I believe they all do so with C++ code, which is therefore not memory-safe, that is baked into the code base rather than supported via some kind of plugged-in library, like Firefox using  PDF.js to support PDFs in the browser.Anyway, back on August 1st, Mason Freed of Google opened issue #11523 on WHATWG’s HTML repository, asking if XSLT should be removed from browsers and giving a condensed set of reasons why it might be a good idea.  He also included a WASM-based polyfill he’d written to provide XSLT support, should browsers remove it, and opened “ Investigate deprecation and removal of XSLT” in the Chromium bug tracker.“So it’s already been decided and we just have to bend over and take the changes our Googlish overlords have decreed!” many people shouted.  It’s not hard to see where they got that impression, given some of the things Google has done over the years, but that’s  what’s happening here.  Not at this point.  I’d like to set some records straight, as an outside observer of both Google and the issue itself.First of all, while Mason was the one to open the issue, this was done because the idea was raised in a periodic WHATNOT meeting (call), where someone at Mozilla was actually the one to bring it up, after it had come up in various conversations over the previous few months.  After Mason opened the issue, members of the Mozilla and WebKit teams expressed (tentative, mostly) support for the idea of exploring this removal.  Basically,  of the vendors are particularly keen on keeping native XSLT support in their codebases, particularly after  security flaws were found in XSLT implementations.This isn’t the first time they’ve all agreed it might be nice to slim their codebases down a little by removing something that doesn’t get a lot of use (relatively speaking), and it won’t be the last.  I bet they’ve all talked at some point about how nice it would be to remove BMP support.Mason mentioned that they didn’t have resources to put toward updating their XSLT code, and got widely derided for it. “Google has trillions of dollars!” people hooted.   has trillions of dollars.  The Chrome team very much does not.  They probably get, at best, a tiny fraction of one percent of those dollars.  Whether Google should give the Chrome team more money is essentially irrelevant, because that’s not in the Chrome team’s control.  They have what they have, in terms of head count and time, and have to decide how those entirely finite resources are best spent.(I will once again invoke my late-1900s formulation of Hanlon’s Razor:  Never attribute to malice that which can be more adequately explained by resource constraints.)Second of all, the issue was opened to start a discussion and gather feedback as the first stage of a multi-step process, one that could easily run for years.  Google, as I assume is true for other browser makers, has a pretty comprehensive method for working out whether removing a given feature is tenable or not.  Brian and I  talked with Rick Byers about it a while back, and I was impressed by both how many things been removed, and what they do to make sure they’re removing the right things.Here’s one (by no means the only!) way they could go about this:Set up a switch that allows XSLT to be disabled.In the next release of Chrome, use the switch to disable XSLT in one percent of all Chrome downloads.See if any bug reports come in about it.  If so, investigate further and adjust as necessary if the problems are not actually about XSLT.If not, up the percentage of XSLT-disabled downloads a little bit at a time over a number of releases.  If no bugs are reported as the percentage of XSLT-disabled users trends toward 100%, then prepare to remove it entirely.If, on the other hand, it becomes clear that removing XSLT will be a widely breaking change  —  where “widely” can still mean a very tiny portion of their total user base — then XSLT can be re-enabled for all users as soon as possible, and the discussion taken back up with this new information in hand.Again, that is just one of several approaches Google could take, and it’s a lot simpler than what they would most likely actually do, but it’s roughly what they default to, as I understand it.  The process is slow and deliberate, building up a picture of actual use and user experience.Third of all, opening a bug that includes a pull request of code changes isn’t a declaration of countdown to merge, it’s a way of making crystal clear (to those who can read the codebase) exactly what the proposal would entail.  It’s basically a requirement for the process of making a decision to start, because it sets the exact parameters of what’s being decided on.That said, as a result of all this, I now strongly believe that every proposed-removal issue should point to the process and where the issue stands in it. (And write down the process if it hasn’t been already.) This isn’t for the issue’s intended audience, which was other people within WHATWG who are familiar with the usual process and each other, but for cases of context escape, like happened here.  If a removal discussion is going to be held in public, then it should assume the general public will see it and provide enough context for the general public to understand the actual nature of the discussion.  In the absence of that context, the nature of the discussion will be assumed, and every assumption will be different.There is one thing that we should all keep in mind, which is that “remove from the web platform” really means “remove from browsers”.  Even if this proposal goes through, XSLT could still be used server-side.  You could use libraries that support XSLT versions more recent than 1.0, even!  Thus, XML could still be turned into HTML, just not in the client via native support, though JS or WASM polyfills, or even add-on extensions, would still be an option.  Is that good or bad?  Like everything else in our field, the answer is “it depends”.Just in case your eyes glazed over and you quickly skimmed to see if there was a TL;DR, here it is:The discussion was opened by a Google employee in response to interest from multiple browser vendors in removing built-in XSLT, following a process that is opaque to most outsiders.  It’s a first step in a multi-step evaluation process that can take years to complete, and whose outcome is not predetermined.  Tempers flared and the initial discussion was locked; the conversation continues elsewhere.  There are good reasons to drop native XSLT support in browsers, and also good reasons to keep or update it, but XSLT is not itself at risk.]]></content:encoded></item><item><title>Coinbase CEO explains why he fired engineers who didn’t try AI immediately</title><link>https://techcrunch.com/2025/08/22/coinbase-ceo-explains-why-he-fired-engineers-who-didnt-try-ai-immediately/</link><author>/u/diegoargento1</author><category>dev</category><category>reddit</category><pubDate>Sat, 23 Aug 2025 04:43:36 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[It’s hard to find programmers these days who aren’t using AI coding assistants in some capacity, especially to write the repetitive, mundane bits.But those who refused to try the tools when Coinbase bought enterprise licenses for GitHub Copilot and Cursor got promptly fired, CEO Brian Armstrong said this week on John Collison’s podcast “Cheeky Pint.” (Collison is the co-founder and president of the payments company Stripe.)After getting licenses to cover every engineer, some at the cryptocurrency exchange warned Armstrong that adoption would be slow, predicting it would take months to get even half the engineers using AI. Armstrong was shocked at the thought. “I went rogue,” he said, and posted a mandate in the company’s main engineering Slack channel. “I said, ‘AI is important. We need you to all learn it and at least onboard. You don’t have to use it every day yet until we do some training, but at least onboard by the end of the week. And if not, I’m hosting a meeting on Saturday with everybody who hasn’t done it and I’d like to meet with you to understand why.’” At the meeting, some people had reasonable explanations for not getting their AI assistant accounts set up during the week, like being on vacation, Armstrong said.“I jumped on this call on Saturday and there were a couple people that had not done it. Some of them had a good reason, because they were just getting back from some trip or something, and some of them didn’t [have a good reason]. And they got fired.”Armstrong admits that it was a “heavy-handed approach” and there were people in the company who “didn’t like it.”While it doesn’t sound like very many people were fired, Armstrong said it sent a clear message that AI is not optional. Still, everything about that story is wild: that there were engineers who wouldn’t spend a few minutes of their week signing up for and testing the AI assistant — the most hyped tech for coders ever — and that Armstrong was willing to fire them over it.Coinbase did not respond to a request for comment.Since then, Armstrong has leaned further into the training. He said the company hosts monthly meetings where teams who have mastered creative ways to use AI share what they have learned.Interestingly, Collison, who has been programming since childhood, questioned how much companies should be relying on AI-generated code.“It’s clear that it is very helpful to have AI helping you write code. It’s not clear how you run an AI-coded code base,” he commented.  Armstrong replied, “I agree.”Indeed, as TechCrunch previously reported, a former OpenAI engineer described that company’s central code repository as “a bit of a dumping ground.” The engineer said management had begun dedicating engineering resources to improve the situation.We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out  to let us know how we’re doing and get the chance to win a prize in return!]]></content:encoded></item><item><title>What on Earth Does Pointer Provenance Have to do With RCU?</title><link>https://people.kernel.org/paulmck/what-on-earth-does-lifetime-end-pointer-zap-have-to-do-with-rcu</link><author>/u/unixbhaskar</author><category>dev</category><category>reddit</category><pubDate>Sat, 23 Aug 2025 04:24:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[TL;DR: Unless you are doing very strange things with RCU, not much!!!So why has the guy most responsible for Linux-kernel spent so much time over the past five years working on the provenance-related lifetime-end pointer zap within the C++ Standards Committee?What is Pointer Provenance?Back in the old days, provenance was for objets d'art and the like, and we did not need them for our pointers, no sirree!!!  Pointers had bits, those bits formed memory addresses, and as often as not we didn't even need to worry about these addresses being translated.  But life is more complicated now.  On the other hand, computing life is also much bigger, faster, more reliable, and (usually) more productive, so be extremely careful what you wish for from back in the Good Old Days!These days, pointers have provenance as well as addresses, and this has consequences.  The C++ Standard  (recent draft) states that when an object's storage duration ends, any pointers to that object become invalid.  For its part, the C Standard states that when an object's storage duration ends, any pointers to that object become indeterminate.  In both standards, the wording is more precise, but this will serve for our purposes.For the remainder of this document, we will follow C++ and say “invalid”, which is shorter than “indeterminate”.  We will balance this out by using C-language example code.  Those preferring C++ will be happy to hear that this is the language that I use in my upcoming CPPCON presentation.Neither standard places any constraints on what a compiler can do with an invalid pointer value, even if all you are doing is loading or storing that value.Those of us who cut our teeth on assembly language might quite reasonably ask why anyone would even think to make pointers so invalid that you cannot even load or store them.  Let's start by looking at pointer comparisons using this code fragment:p = kmalloc(...);
might_kfree(p);         // Pointer might become invalid (AKA "zapped")
q = kmalloc(...);       // Assume that the addresses of p and q are equal.
if (p == q)             // Compiler can optimize as "if (false)"!!!
    do_something();
Both  and  contain addresses, but the compiler also keeps track of the fact that their values were obtained from different invocations of .  This information forms part of each pointer's provenance.  This means that  and  have different provenance, which in turn means that the compiler does not need to generate any code for the  comparison.  The two pointers' provenance differs, so the result cannot be anything other than .And this is one motivation for pointer provenance and invalidity:  The results of operations on invalid pointers are not guaranteed, which provides additional opportunities for optimization.  This example perhaps seems a bit silly, but modern compilers can use pointer provenance and invalidity to carry out serious points-to and aliasing analysis.Yes, you can have hardware provenance.  Examples include ARM MTE, the CHERI research prototype (which last I checked had issues with C++'s requirement that pointers are trivially copiable), and the venerable IBM System i.  Conventional systems provide pointer provenance of a sort via their page tables, which is used by a variety of memory-allocation-use debuggers, for but one example, the efence library.  The pointer-provenance features of ARM MTE and IBM System i are not problematic, but last I checked, the jury was still out on CHERI.Of course, using invalid (AKA “dangling”) pointers is known to be a bad idea.  So why are we even talking about it???Why Would Anyone Use Invalid/Dangling Pointers?Please allow me to introduce you to the famous and frequently re-invented LIFO Push algorithm.  You can find this in many places, but let's focus on the Linux kernel's  and  functions.  The former atomically pushes a list of elements on a linked-list stack, and the latter just as atomically removes the entire contents of the stack:static inline bool llist_add_batch(struct llist_node *new_first,
                                   struct llist_node *new_last,
                                   struct llist_head *head)
{
    struct llist_node *first = READ_ONCE(head->first);

    do {
        new_last->next = first;
    } while (!try_cmpxchg(&head->first, &first, new_first));

    return !first;
}

static inline struct llist_node *llist_del_all(struct llist_head *head)
{
    return xchg(&head->first, NULL);
}
As lockless concurrent algorithms go, this one is pretty straightforward.  The  function reads the list header, fills in the  pointer, then does a compare-and-exchange operation to point the list header at the new first element.  The  function is even simpler, doing a single atomic exchange operation to  out the list header and returning the elements that were previously on the list.  This algorithm also has excellent forward-progress properties: the  function is lock-free and the  function is wait-free.In assembly language, or with a simple compiler, not much.  But to see the pointer-provenance issue with more heavily optimized languages, consider the following sequence of events:CPU 0 allocates an  B and passes it via both the  and  parameters of .CPU 0 picks up the  pointer and places it in the  local variable, then assigns it to .  This  pointer now references  A.CPU 1 invokes , which returns a list containing  A.  The caller of  processes A and passes it to .CPU 0's  pointer is now invalid due to  A having been freed.  But CPU 0 does not know this.CPU 1 allocates an  C that happens to have the same address as the old  A.  It passes C  via both the  and  parameters of , which runs to completion.  The  pointer now points to  C, which happens to have the same address as the now storage-duration-ended  A.CPU 0 finally gets around to executing its , which given typical C compilers will succeed.  The  now contains an  B that contains an invalid pointer to dead  A, but whose pointer address happens to reference the shiny new  C.  (We term this invalid pointer a “zombie pointer” because it has in some assembly-language sense come back from the dead.)Some CPU invokes  and gets back an  containing an invalid pointer.One could argue that the Linux-kernel implementation of LIFO Push is simply buggy and should be fixed.  Except that there is no reasonable way to fix it.  Which of course raises the question...What Are Unreasonable Fixes?We can protect pointers from invalidity by storing them as integers, but:Suppose someone has an element that they are passing to a library function.  They should not be required to convert all their  pointers to integer just because the library's developers decide to switch to the LIFO Push algorithm for some obscure internal operation.In addition, switching to integer defeats type-checking, because integers are integers no matter what type of pointer they came from.We could restore some type-checking capability by wrapping the integer into a differently named struct for each pointer type.  Except that this requires a struct with some particular name to be treated as compatible with pointers of some type corresponding to that name, a notion that current do not support.In C++, we could use template metaprogramming to wrap an integer into a class that converts automatically to and from compatibly typed pointers.  But there would then be windows of time in which there was a real pointer, and at that time there would still be the possibility of pointer invalidity.All of the above hack-arounds put additional obstacles in the way of developers of concurrent software.In environments such as the Linux kernel that provides their own memory allocators, we can hide them from the compiler.  But this is not free, in fact, the patch that exposed the Linux-kernel's memory allocators to the compiler resulted in a small but significant improvement.However, it is fair to ask...Why Do We Care About Strange New Algorithms???Let's take a look at the history, courtesy of Maged Michael's diligent software archaeology.In 1986, R. K. Treiber presented an assembly language implementation of the LIFO Push algorithm in technical report RJ 5118 entitled “Systems Programming: Coping with Parallelism” while at the IBM Almaden Research Center.US Patent 3,886,525 was filed in June 1973, just a few months before I wrote my first line of code, and contains a prior-art reference to the LIFO Push algorithm (again with pop() instead of popall()) as follows: “Conditional swapping of a single address is sufficient to program a last-in, first-out single-user-at-a-time sequencing mechanism.”  (If you were to ask a patent attorney, you would likely be told that this 50-year-old patent has long since expired.  Which should be no surprise, given that it is even older than Dennis Ritchie's setuid Patent 4,135,240.)All three of these references describe LIFO push as if it was straightforward and well known.So we don’t know who first invented LIFO Push or when they invented it, but it was well known in 1973.  Which is well over a decade before C was first standardized, more than two decades before C++ was first standardized, and even longer before work was started on Rust.And its combination of (relative) simplicity and excellent forward-progress properties just might be why this algorithm was anonymously invented so long ago and why it is so persistently and repeatedly reinvented.  This frequent reinvention puts paid to any notion that LIFO Push is strange.So sorry, but LIFO Push is neither new nor strange.The lifetime-end pointer-zap story is not yet over, but we are currently pushing for the changes in four working papers.Nondeterministic Pointer ProvenanceP2434R4 (“Nondeterministic pointer provenance”) is the basis for the other three papers.  It asks that when converting a pointer to an integer and back, the implementation must choose a qualifying pointed-to object (if there is one) whose storage duration began before or concurrently with the conversion back to a pointer.  In particular, the implementation is free to ignore a qualifying pointed-to object when the conversion to pointer happens before the beginning of that object’s storage duration.The “qualifying” qualifier includes compatible type, as well as sufficiently early and long storage duration.But why restrict the qualifying pointed-to object's storage duration to begin before or concurrently with the conversion back to a pointer?An instructive example by Hans Boehm may be found in P2434R4, which shows that reasonable (and more important, very heavily used) optimizations would be invalidated by this approach.  Several examples that manage to be even more sobering may be found in David Goldblatt's P3292R0 (“Provenance and Concurrency”).Pointer Lifetime-End Zap Proposed Solutions: Atomics and VolatileP2414R10 (“Pointer lifetime-end zap proposed solutions: Atomics and volatile”) is motivated by the observation that atomic pointers are subject to update at any time by any thread, which means that the compiler cannot reasonably do much in the way of optimization.  This paper therefore asks (1) that atomic operations be redefined to yield and to store prospective pointers values and (2) that operations on volatile pointers be defined to yield and to store prospective pointer values.  The effect is as if atomic pointers were stored internally as integers. This includes the “old” pointer passed by reference to compare_exchange().This helps, but is not a full solution because atomic pointers are converted to non-atomic pointers prior to use, at which point they are subject to lifetime-end pointer zap.  And the standard does not even guarantee that a zapped pointer can even be loaded, stored, passed to a function, or returned from a function.  Which brings us to the next paper.Pointer Lifetime-End Zap Proposed Solutions: Tighten IDB for Invalid PointersP3347R3 (“Pointer lifetime-end zap proposed solutions: Tighten IDB for invalid pointers”) therefore asks that all non-comparison non-arithmetic non-dereference computations involving pointers, specifically including normal loads and stores, are fully defined even if the pointers are invalid.  This permits invalid pointers to be loaded, stored, passed as arguments, and returned.  Fully defining comparisons would rule out optimizations, and fully defining arithmetic would be complex and thus far unneeded.If these first three papers are accepted into the standard, the C++ implementation of LIFO Push show above becomes valid code.  This is important because this algorithm has been re-invented many times over the past half century, and is often open coded.  This makes it very hard to construct tools that find LIFO Push implementations in existing code.P3790R1: Pointer Lifetime-End Zap Proposed Solutions: Bag-of-Bits Pointer ClassP3790R1 (“Pointer lifetime-end zap proposed solutions: Bag-of-bits pointer class”) asks that (1) the addition to the C++ standard library of the function launder_bag_of_bits_ptr() that takes a pointer argument and returns a prospective pointer value corresponding to its argument; and (2) the addition to the C++ standard library of the class template  that is a pointer-like type that is still usable after the pointed-to object’s lifetime has ended.  Of course, such a pointer still cannot be dereferenced unless there is a live object at that pointer's address.  Furthermore, some systems, such as ARMv9 with memory tagging extensions (MTE) enabled have provenance as well as address bits in the pointer, and on such systems dereferencing will fail unless the pointer's provenance bits happen to match those of the pointed-to object.This function and template class is nevertheless quite useful for maintaining hash maps keyed by pointers after the pointed-to object's lifetime has ended.Unlike LIFO Push, source-code changes are required for these use cases.  This is unfortunate, but we have thus far been unable to come up with a same-source-code approach.Those who have participated in standards work (or even open-source work) will understand that the names launder_bag_of_bits_ptr() and  are still subject to bikeshedding.A Happen Lifetime-End Pointer Zap Ending?It is still too early to say for certain, but thus far these proposals are making much better progress than did their predecessors.  So who knows?  Perhaps C++29 will address lifetime-end pointer zap.]]></content:encoded></item><item><title>GPUI Hello World Tutorial - From Core Concepts to Hello World | 0xshadow&apos;s Blog</title><link>https://blog.0xshadow.dev/posts/learning-gpui/gpui-hello-world-tutorial/</link><author>/u/lazyhawk20</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 23 Aug 2025 04:11:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[In this post, we are going to learn about GPUI - an open source UI framework developed by the Zed Industries team to build interfaces by utilizing our computer’s GPU. We are going to learn the basics of GPUI by building a hello world app.Basic HTML, CSS and JS knowledgeGPUI is a hybrid immediate and retained mode, GPU-accelerated UI framework for Rust created by the makers of Zed editor. Before we dive into code, let me explain what these terms mean in simple language. means GPUI uses our computer’s graphics card to render to interface unlike traditional CPU-based rendering. This makes it extremely fast.Hybrid immediate and retained mode refers to how GPUI manages the user interface: means the UI is redrawn from scratch every frame. means the framework remembers the UI structure and only updates what changes, which is more efficient. mean GPUI uses both approaches.Before jumping into coding, lets understand why the Zed team created GPUI? Why the existing solutions aren’t good enough for them?When Zed team decided to build their own code editor, they realized that existing UI frameworks couldn’t deliver the performance they need. Hence, they decided to build their own UI framework.
The problem with traditional desktop UI frameworks is that they primarily use CPU for rendering. The CPU processes instructions sequentially. This becomes a bottleneck when you have a complex interface with many elements. Every button, text field, menu and visual element must be processed in sequence.To fix this issue, they decided to use GPU of the computer, which can process thousands of operations in parallel. Your graphics card contains hundreads and thousands or cores that can work simultaneously. This parallel processing has been used in gaming for decades, but most desktop apps don’t take advantage of it.Now that we know why GPUI exists, lets now see how it actually works. After understanding its core concepts, we can start building our app as we will already have the understanding of its essential concepts.The Application Lifecycle and AppContextWhen we run a GPUI app, the framework needs to take care of a lot of things that are happening simultaneously. Our app might have multiple windows open, user might be clicking buttons, typing text or resizing windows.The  object is kind of the coordinator of all this activity. When we run , we are telling GPUI to “start managing the app and keep running until the user closes the app”.The (often shortened to  in code) is what we will use to communicate with the  object. Every time we want to create a window, handle an event or update the display, we need to do it through the .Views and The Render TraitIn GPUI, everything we see on screen is organized into . A view is a Rust struct that represents a logical piece of our application’s interface. Think of a view as a self-contained component that manages both its own data and knows how to present that data visually.Here’s what makes GPUI fundamentally different from desktop frameworks like Electron, Tauri, or traditional native frameworks: GPUI uses a declarative rendering approach rather than an imperative one. In imperative frameworks, we create interface objects once and then manually update their properties when things change. We might write code like button.setText("New Label") or window.setBackgroundColor(red) to modify existing interface elements.GPUI works differently. It doesn’t store permanent interface objects that we modify. Instead, GPUI asks our views to completely recreate their visual description every single time the screen needs to be redrawn. This happens through the  trait, which every displayable struct must implement.The  trait has exactly one method: . This method takes the current state of our view and returns a fresh description of what should appear on screen. GPUI calls this method whenever it needs to redraw our view, which could be 60 times per second or more. Our job is to look at our view’s current data and describe what the interface should look like based on that data.This approach might seem inefficient at first glance, but it’s actually what enables GPUI’s exceptional performance. Since GPUI gets a complete description of our interface every frame, it can compare the new description with the previous one and update only the parts that actually changed. This is similar to how React’s virtual DOM works, but optimized for GPU rendering instead of web browsers.When our  method runs, it creates and returns elements. Elements are temporary objects that describe visual components. They specify things like “there should be a rectangle here with this color and size” or “there should be text here with this font and content.” Elements are not the actual visual components themselves, but rather instructions for creating those components.GPUI provides built-in element types that cover most interface needs. The  element creates a container that can hold other elements, similar to HTML div tags but designed for desktop applications. The  element creates an interactive button that can respond to clicks. The  element displays styled text content. Each element type has methods we can chain together to configure its appearance and behavior.The power of GPUI’s element system comes from composition. We build complex interfaces by nesting simple elements inside other elements. A  can contain multiple child elements, and each of those children can have their own children, creating a hierarchical tree structure. This tree represents the logical structure of our interface.When we call methods like , , or  on elements, we’re not modifying existing objects. Instead, we’re creating new element descriptions that include those properties. GPUI uses a builder pattern where each method call returns a new element with the additional configuration applied.Now, that we understood a little bit of theory, let’s start building our first simple hello world app in GPUI.Installing Rust Nightly for Our ProjectTo use GPUI we need to have the nightly version of Rust because it is using some experimental features. To install the nightly tool, run the following command:This will download and install the latest nightly version of Rust.Let’s start by creating a new rust project using cargo. Open the terminal and go to the directory where you want the project to be created. Now, run the following command:After running this command, you can see that you have a standard rust project that prints Hello World in console but we want to create a GPUI app and print the Hello World text in that app.Now, we need to enable the nightly version of Rust specifically for this project and not for all Rust projects. To do this, create a file called  in our project root directory.This tells Cargo to automatically use nightly Rust whenever we work in this project directory, while keeping our global Rust installation on stable.It’s time to configure our  file to work with the nightly edition and GPUI.Configuring Cargo for 2024 Edition and GPUILet’s update our  file.Here, we changed the  from  to  and added the GPUI dependency.Writing Our Hello World CodeI’m putting the entire  code here and after that I’ll explain everything one by one.Ok, it seems a lot of code for Hello World let’s unpack everything one by one.This import statement gets everything that we need from GPUI. Let’s understand what we are importing from GPUI: - The context type we receive in our main application closure for setting up windows and global app state - The main application object that manages our entire GPUI app lifecycle - A type that represents the position and size of windows on screen - The context object we receive in our render method, giving us access to GPUI’s systems for this specific view - GPUI’s optimized string type that allows efficient sharing of text data without unnecessary copying - Represents a window and provides access to window-specific properties and methodsWindowBounds, WindowOptions - Configuration types for specifying how our window should be created and positioned - The function that creates div elements, our main building block for layouts - Brings in commonly used traits like  and utility functions - Helper functions for creating measurements (pixels), colors, and size specificationsDefining Our View with StateOur view struct contains the actual text data. The  field stores the text that we want to show or display in the app.Now, you might be wondering why use  instead of ?
The reason is GPUI optimizes for sharing the text between different parts of the app. The  can be cloned very efficiently without copying the actual text data and multiple parts of our app can reference the same text without memory overhead.Implementing the Render TraitNow, let’s understand the signature of the  method: - Mutable access to our view, allowing us to modify the view’s state during rendering if needed. Even though we don’t modify anything in this example, GPUI requires this flexibility. - Access to the window this view is being rendered in. We could use this to query window properties like current size, position, or screen DPI. The underscore indicates we’re not using it in this simple example. - The context object that provides access to GPUI’s systems. Through this context, we could handle user input, set up timers, or create child views. The  parameter means this context is specifically typed for our  view.Building Our Element TreeNow let’s understand each method call in our element construction:This creates our root  element - the foundation container for everything else in our view. This is similar to how we use div for containers in HTML.This enables flexbox layout on our div. Flexbox provides powerful, automatic layout capabilities that adjust to different content sizes and window dimensions. This is why I asked you to have basic knowledge on CSS. If you don’t know flexbox concept then please quickly read about it and then continue reading this tutorial.This sets the flex direction to column, meaning any child elements would stack vertically rather than horizontally. Even though we only have one child, this establishes the layout direction.This sets the background color to a dark blue-gray. The  function takes a hexadecimal color value and converts it to GPUI’s internal color representation.This sets both width and height to exactly 500 pixels. The  function creates a pixel-based measurement. This gives us a fixed-size container regardless of window size.This centers content along the main axis. Since we used , the main axis runs vertically, so this centers our content vertically within the 500-pixel height.This centers content along the cross axis. With column direction, the cross axis runs horizontally, so this centers our content horizontally within the 500-pixel width.This sets the text size to extra large. GPUI provides a typography scale with predefined sizes for consistent text styling across our application.This sets the text color to light gray, providing good contrast against our dark background.This adds our text content as a child element. We use  to create a dynamic string that includes our view’s  field. The  accesses the SharedString from our view’s state.Application Startup and Configuration - Creates and initializes a new GPUI application, setting up the rendering engine, event system, and platform integration..run(|cx: &mut App| { ... }) - Starts the application main loop and executes our setup closure. The  parameter gives us an  context for configuring our application’s initial state.Understanding Closures in RustThe  syntax is a  in Rust, which is similar to anonymous functions or lambda functions in other languages. Let’s break this down:The  symbols define a closure. Think of closures as functions that we define inline without giving them a name.The closure is like defining and using the function all in one place.
GPUI’s  method needs to:Initialize all the graphics systemsSet up the main application loopLet us configure our initial windows and viewsStart processing events and rendering
The  method says: “I’ll handle all the complex setup, but when I’m ready, I’ll call your closure so you can tell me what windows and views you want.” is just a variable name (short for “context”) means “a mutable reference to an App object”The  object gives us access to GPUI’s application-level features like creating windowsThink of  as a control panel that GPUI hands us, saying “here, use this to set up your application.”Window Creation and PositioningThis calculates where our window should appear: creates window bounds that center the window on screen means use the primary monitor (we could specify a particular monitor) creates a 500x500 pixel size specification provides information about the current screen configurationCreates a window with specific options: configures window propertieswindow_bounds: Some(WindowBounds::Windowed(bounds)) sets the window to normal windowed mode with our calculated position and size uses default values for other options like window title, decorations, etc.View Creation and InitializationWe actually have  nested in our code: - Sets up our application - Defines what goes in the window - Creates our view instanceEach closure handles a different level of setup: - “What windows should exist?” - “What should go in this specific window?” - “How should this view be initialized?”The window creation closure does two things:Takes a window handle (ignored with ) and a window-specific contextUses  to create and register our view with GPUIThe inner closure creates our initial  instance with “GPUI World” as the text converts the string literal into a You’ll notice some closures use  as a parameter name:The  means “I receive a parameter here, but I don’t need to use it.” It’s Rust’s way of saying “ignore this parameter” without getting compiler warnings.When we run this application: initializes all GPUI systems starts the main application loop and calls our setup closureWe calculate window bounds for a centered 500x500 windowWe create a window with those specificationsWe create a  view instance with initial textGPUI calls our view’s  method to get the element descriptionOur  method returns a 500x500 div with centered textGPUI renders this using the GPU and displays our windowThe application loop continues, ready for user interaction or updatesBuilding and Running Our ApplicationNow let’s see our hello world app in action. In your terminal, make sure you’re in the project directory and run:The first time you run this command, it will take several minutes to complete. Cargo needs to download the Zed repository, compile GPUI and all its dependencies, and then compile our application. We’ll see a lot of output as Cargo builds everything.Don’t worry if this seems slow - this lengthy compilation only happens the first time. Subsequent runs will be much faster because Cargo caches the compiled dependencies.When the compilation finishes and our application starts, we should see a 500x500 pixel window appear with our “Hello, GPUI World!” message centered on a dark background. The text should be large and clearly visible in light gray against the dark blue-gray background. Something like this:
Try interacting with the window - we can move it around, resize it, minimize it, and close it. Even though we haven’t written any code to handle these interactions, GPUI provides them automatically because they’re standard window operations.This concludes learning the basics of GPUI and displaying hello world in the screen. I hope you’ve learned something from this post and in the next post we are going to learn about handling user inputs, managing state changes, making our app respond to user actions and understanding event handling in GPUI. Learning these things before implementing the todo app would help us cover a lot of concepts in an isolated way making them simpler to understand. See you soon.]]></content:encoded></item><item><title>K3S with iSCSI storage (Compellent/Starwind VSAN)</title><link>https://www.reddit.com/r/kubernetes/comments/1mxr3z0/k3s_with_iscsi_storage_compellentstarwind_vsan/</link><author>/u/Norava</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 23 Aug 2025 03:42:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hey all! I have a 3 master 4 node K3S cluster installed on top of my Hyper-V S2D cluster in my lab and currently I'm just using Longhorn + each node having a 500gb vhd attached to serve as storage but as I'm using this to learn kube I wanted to try to work on building more scalable storage. To that end I'm trying to figure out how to get any form of basic networked storage for my K3S cluster. In doing research I'm finding NFS is much to slow to use in prod so I'm trying to see if there's a way to set up ISCSI LUNs attached to the cluster / workers but I'm not seeing a clear path to even get startedI initially pulled out an old Dell SAN (A Compellent Scv2020) that I'm trying to get running but that right now is out of band due to it missing it's SCOS but I do know if the person who I found has an iso for SCOS I could get this running as ISCSI storage so I took 2 R610s I had laying around and made a basic Starwind vSAN but I cannot for the life of me figure out HOW to expose ANY LUNs to the k3s cluster. My end goal is to have something to host storage that's both more scalable than longhorn and vhds that also can be backed up by Veeam Kasten ideally as I'm in big part also trying to get dr testing with Kasten done as part of this config as I determine how to properly handle backups for some on prem kube clusters I'm responsible for in my new roles that we by compliance couldn't use cloud storage forI see democratic-csi mentioned a lot but that appears to be orchestration of LUNs or something through your vendors interface that I cannot find on Starwind and that I don't SEE an EOL SAN like the scv2020 having in any of my searches. I see I see CEPH mentioned but that looks like it's going to similarly operate with local storage like longhorn or requires 3 nodes to get started and the hosts I have to even perform that drastically lack the bay space a full SAN does (Let alone electrical issues I'm starting to run into with my lab but thats beyong this LOL) Likewise I see democratic could work with TrueNAS scale but that also requires 3 nodes and again will have less overall storage. I was debating spinning a Garage node for this and running s3 locally but I'm reading if I want to do ANYTHING with database or heavy write operations is doomed with this method and nfs storage similarly have such issues (Supposedly) Finally I've been through a LITANY of various csi github pages but nearly all of them seem either dead or lacking documentation on how they workMy ideal would just be connecting a LUN into the cluster in a way I can provision to it directly so I can use the SAN but my understanding is I can't exactly like, create a shared VHDX in Hyper-v and add that to local storage or longhorn or something without basically making the whole cluster either extremely manual or extremely unstable correct?]]></content:encoded></item><item><title>Measuring the environmental impact of AI inference</title><link>https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/</link><author>ksec</author><category>dev</category><category>hn</category><pubDate>Sat, 23 Aug 2025 03:22:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>This month in Servo: new image formats, canvas backends, automation, and more!</title><link>https://servo.org/blog/2025/08/22/this-month-in-servo/</link><author>/u/KlasySkvirel</author><category>dev</category><category>reddit</category><pubDate>Sat, 23 Aug 2025 01:58:52 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Servo has smashed its record again in July, with  landing in our nightly builds!
This includes several new web platform features:Notable changes for Servo library consumers:Like many browsers, Servo has two kinds of zoom:  affects the size of the viewport, while  does not (@shubhamg13, #38194).
 now correctly triggers reflow (@mrobinson, #38166), and  is now reset to the viewport meta config when navigating (@shubhamg13, #37315). is now isolated between webviews, and copied to new webviews with the same  (@janvarga, #37803). now has a  and , so you can now  on Linux (@MichaelMcDonnell, #38038).
We’ve made it more ergonomic too, fixing both the sluggish  and pixel-perfect trackpad scrolling and the too fast  (@yezhizhen, #37982). is key to programmable graphics on the web, with Servo supporting WebGPU, WebGL, and 2D canvas contexts.
But the general-purpose 2D graphics routines that power Servo’s 2D canvases are potentially useful for a lot more than <canvas>:  is bread and butter for Servo, but  is only minimally supported right now, and  is not yet implemented at all.Those features have one thing in common: they require things that WebRender can’t yet do.
 does one thing and does it well: rasterise the layouts of the web, really fast, by using the GPU as much as possible.
Font rendering and SVG rendering both involve rasterising arbitrary paths, which currently has to be done outside WebRender, and PDF output is out of scope entirely.The more code we can share between these tasks, the better we can make that code, and the smaller we can make Servo’s binary sizes (#38022).
We’ve started by moving 2D-<canvas>-specific state out of the  crate (@sagudev, #38098, #38114, #38164, #38214), which has in turn allowed us to modernise it with new backends based on Vello (@EnnuiL, @sagudev, #30636, #38345):a Vello GPU-based backend (@sagudev, #36821), currently slower than the default backend; to use it, build Servo with  and enable it with --pref dom_canvas_vello_enableda Vello CPU-based backend (@sagudev, #38282), already faster than the default backend; to use it, build Servo with  and enable it with --pref dom_canvas_vello_cpu_enabledMany recent Servo bugs have been related to our handling of , , and  (#36817, #37804, #37824, #37878, #37978, #38089, #38090, #38093, #38255).
Symptoms of these bugs include  (e.g. links that can’t be clicked),  to the end of the page, or  like disappearing browser UI or black bars.Windows rarely take up the whole screen, viewports rarely take up the whole window due to window decorations, and when different units come into play, like CSS  vs device pixels, a more systematic approach is needed.
We built  to solve these problems in a strongly typed way within Servo, but beyond the viewport, we need to convert between euclid types and the geometry types provided by the embedder, the toolkit, the platform, or WebDriver, which creates opportunities for errors.Servo is also on thanks.dev, and already  (−3 from June) that depend on Servo are sponsoring us there.
If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.As always, use of these funds will be decided transparently in the Technical Steering Committee.
For more details, head to our Sponsorship page.]]></content:encoded></item><item><title>Codanna now supports Go! Instant call graphs, code-aware lookup, zero servers</title><link>https://www.reddit.com/r/golang/comments/1mxnw4v/codanna_now_supports_go_instant_call_graphs/</link><author>/u/Plenty_Seesaw8878</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 23 Aug 2025 00:59:59 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Your coding assistants can now index and navigate Go, Python, Typescript or Rust projects with precise context in . Runs fully local, integrates anywhere—from vibe coding with agents to plain Unix piping. It get's line numbers, extracts method signatures and logical flows in . Bonus: two Claude slash commands for everyday workflows —  for natural-language lookup and  for dependency analysisCodanna is the Unix tool that builds a live atlas of your code. Alone, it answers queries in under 300 ms. With agents or pipes, it drives context-aware coding with speed, privacy, and no guesswork.]]></content:encoded></item><item><title>My tips for using LLM agents to create software</title><link>https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html</link><author>efitz</author><category>dev</category><category>hn</category><pubDate>Sat, 23 Aug 2025 00:59:02 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Netbeans 27 Released</title><link>https://lists.apache.org/thread/py28oztx51vhk4f1js3q54vpx8pwzbb3</link><author>/u/BlueGoliath</author><category>dev</category><category>reddit</category><pubDate>Sat, 23 Aug 2025 00:19:12 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Computer fraud laws used to prosecute leaking air crash footage to CNN</title><link>https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/</link><author>BallsInIt</author><category>dev</category><category>hn</category><pubDate>Sat, 23 Aug 2025 00:04:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[from the if-it-can-be-abused,-it-WILL-be-abused deptEarlier this year, an Army helicopter collided with a passenger plane over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had leaked footage of the collision to CNN. The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it’s not the sort of government interest most courts consider to be worthy of violating the First Amendment.Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the Computer Fraud and Abuse Act (CFAA). This broadly written law not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.Here’s what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:“The video shows camera angles and views that can only be found on the Metropolitan Washington Airport’s Authority CCTV video,” Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly “to prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,” according to an accompanying letter from MWAA.That doesn’t mean much by itself, but Silsbee apparently figured out (thanks in part to CNN’s initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. CCTV footage from  the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:“Between the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,” Silsbee wrote.That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded “no contest” to charges stemming from Virginia’s ultra-vague “computer trespass” law. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a “trespass.” The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn’t align with any rational definition of the word “trespass.” The dissemination of footage may be a violation of policy, but policy violations aren’t criminal charges — the sort of thing that can do permanent damage to a person’s life in ways that write-ups and even justified terminations simply can’t.That’s why discretion is key. But when discretion matters most, law enforcement tends to deliberately “err” on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.On February 3, the MWAA announced both men’s arrests, writing in a press statement that Savoy had been arrested “following further police investigation.”In May, however, local prosecutors quietly dropped the charges against Savoy, through a filing called a “nolle prosequi,” according to the court docket.There’s absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn “recording a CCTV monitor with a phone” into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of the plain text of the law: Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer networkA smartphone is a computer. A recording could be considered an “unauthorized copy.” To call the CCTV cameras and screens “computers/computer network” means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects had every right to access, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word “justice.”]]></content:encoded></item><item><title>Bluesky Goes Dark in Mississippi over Age Verification Law</title><link>https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/</link><author>BallsInIt</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 22:51:27 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[ can no longer use the social media platform Bluesky. The company announced Friday that it will be blocking all IP addresses within Mississippi for the foreseeable future in response to a recent US Supreme Court decision that allows the state to enforce strict age verification for social media platforms.According to Bluesky, Mississippi’s approach to verification “would fundamentally change” how users access the site. “We think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies,” the Bluesky team said in its statement.Bluesky did not respond to a request for comment.The company says that compliance with Mississippi’s law—which would require identifying and tracking all users under 18, in addition to asking every user for sensitive personal information to verify their age—is not possible with the team’s current resources and infrastructure. By not complying with the law, Bluesky could face fines of up to $10,000 per violation. It is the first major social media platform to take such drastic steps in response to the law.Age verification laws, which on the surface are intended to protect children from harmful content online, have already begun to broadly impact internet use in places around the world where they've been enacted. In the UK, users trying to access everything from pornography to social platforms must now submit to ID scans, credit card checks, age-estimation scans, and more to verify they’re over the age of 18. The state of Texas has a similar law the US Supreme Court upheld in June, despite concerns from critics over the erosion of free speech and access to information on the open internet.Whether these laws are effective at protecting children is unclear; the use of virtual private networks (VPNs) in the UK spiked just after its age verification law went into effect as users deployed the tech to spoof their location. On platforms like Discord, people discovered they could use video game characters to trick face scans. Furthermore, critics say that age verification laws intended to reduce harm to children can sometimes have the opposite effect by putting kids in greater danger of identity theft and privacy violations.WIRED has reached out to the sponsors of the original bill, Mississippi state representatives Jill Ford, Fabian Nelson, and Larry Byrd, and will update this story if they comment.“We believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression,” Bluesky wrote.]]></content:encoded></item><item><title>ChatLoopBackOff: Episode 68 (KANISTER)</title><link>https://www.youtube.com/watch?v=-dy_J3VmmOg</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/-dy_J3VmmOg?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 22:51:06 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Join us LIVE as CNCF Ambassador Carlos Santana dives into Kanister, the open source framework for application-level data management on Kubernetes.

Carlos will be exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that highlight how Kanister helps extend Kubernetes for backup, recovery, and data operations.

If you’re curious about how cloud native projects approach complex data management challenges, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.

Bring your questions, share your experiences, and learn in real time as we explore Kanister together!]]></content:encoded></item><item><title>FTP faster upload</title><link>https://www.reddit.com/r/golang/comments/1mxjcfr/ftp_faster_upload/</link><author>/u/pepiks</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 22 Aug 2025 21:41:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is possible using Go upload files faster than by FTP client? I am looking for speed up uploading gallery images - typical size is around 20-40 MB at maximum, up to 200 resized images, but transfer is very slow and it can take even 15 minutes for this size. I am using FTP for this, not FTPS.]]></content:encoded></item><item><title>Blind &amp; Visually Impaired Initiative (BVI) Meeting - 2025-08-19</title><link>https://www.youtube.com/watch?v=bJej44Ug8tU</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/bJej44Ug8tU?version=3" length="" type=""/><pubDate>Fri, 22 Aug 2025 20:16:43 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>I&apos;m making a freeware Linux Learning Game and could use some QA, Criticism, and feedback.</title><link>https://www.reddit.com/r/linux/comments/1mxgm8l/im_making_a_freeware_linux_learning_game_and/</link><author>/u/nconsola</author><category>dev</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 19:54:08 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I hope I can post here, I read the rules and I’m not trying to self-promoter, as I’m going to release this Linux learning game for free and make it open source when complete.I am making a simple text-based game that is 100% focused on learning Linux command line, this game is not focused on specific distros of Linux like Ubuntu or Debian, it is Basic Standard Linux. If people like the game I will make others that are continuations off of this, that are specific to distros but for now its base Linux.Quick background, I DO NOT KNOW LINUX, but we use it at work (Debian) and I need to learn it. This is why I made this game, every time I try to learn the commands ill forget them or say screw it, I will use the GUI. So, I thought if I had a game that focused on teaching me Linux, I could do it.... yeah, I know probably not going to happen, but still I set off to make it, and with the help of Google Gemini I have a solid Beta of the game, maybe Alpha/Beta, maybe Alpha. There is a lot I want to add after the instruction part of the game which is all I have now, so it is not complete just the 3 chapters that are below.Through QA'ing the game myself I have learned a ton about command line. But as anyone who has QA a game before, you eventually know what to put in to get to the next part, and this doesn’t give a good representation of whether or not the game is teaching well for people who just pick it up. So, I’m looking for any testers who know Linux, and anyone who doesn’t.I want people who know Linux, this way I can make sure all the commands work as they should, basically "look" the way they should in the simulated terminal, and to make sure I have all the commands that are available for basic Linux, and provide feedback where needed.I want people who don’t know Linux, this way I can get feedback on the way the game progresses, does it make sense, do you actually feel like you’re learning Linux while playing, is it confusing, what do you not like, etc.A little bit on what I have implemented so far,some simple non game elements are,Terminal themes, so I have Default theme (supposed to simulate the terminal from the movie Alien, its close but not 100%), Commodore 64, Dos, Linux, and Apple II+ (which was my first computer)A voice over on/off switch for the simulated AI, Aurora, it’s not a real AI or even a LLM it’s just simulated, all the commands and responses I have put in, and it is basic right now. But as the user you are being helped by a ship AI which is basically teaching you the Linux commands. And yeah, it was the closest voice I could get to simulate Mother in the movie Alien, and it sounds nothing like Mother.There is a beginner, intermediate, and advanced sections of the game, that teach you the following commands. Someone who knows Linux really good please let me know if you think anything is missing, but remember this is basic Linux so there is no apt-get etc. like in Debian, at least as far as I know.* `help` - Shows available commands.* `pwd` - Prints the current working directory.* `ls` - Lists files and directories.* `~` - A shortcut for the user's home directory.* `clear` - Clears the terminal screen.* `cat` - Displays the contents of a file.* `hint` - Provides a hint for the current objective.* `man` - Shows the manual page for a command.* `cd` - Changes the current directory.* `uptime` - Shows how long the system has been running.* `echo` - Displays text or writes it to a file.* `mkdir` - Creates a new directory.* `touch` - Creates a new, empty file.* `>` - A redirection operator to write output to a file.* `rm` - Removes (deletes) files.* `rmdir` - Removes (deletes) empty directories.* `mv` - Moves or renames files and directories.* `less` - Views the content of a file page by page.* `grep` - Searches for patterns within files.* `find` - Searches for files and directories.* `head` - Displays the beginning of a file.* `tail` - Displays the end of a file.* `wc` - Counts lines, words, and characters in a file.* `sort` - Sorts the lines of a file.* `|` - The "pipe" operator, used to send the output of one command to another.* `uniq` - Removes duplicate adjacent lines from a file.* `diff` - Compares two files and shows their differences.* `ln` - Creates links between files.* `uname` - Shows system information.* `whoami` - Shows the current user's username.* `groups` - Shows the groups a user belongs to.* `dmesg` - Shows kernel and driver messages.* `free` - Displays memory usage.* `df` - Displays disk space usage.* `du` - Shows the disk usage of files and directories.* `tree` - Displays a directory's contents in a tree-like format.* `file` - Determines a file's type.* `cmp` - Compares two files byte by byte.* `cut` - Extracts sections from lines of a file.* `tr` - Translates or deletes characters.* `<` - A redirection operator to use a file's content as input.* `tee` - Reads from standard input and writes to both standard output and files.* `locate` - Finds files by name quickly.* `chmod` - Changes the permissions of a file or directory.* `sudo` - Executes a command as the superuser (root).* `chown` - Changes the owner of a file or directory.* `umask` - Sets the default permissions for new files.* `split` - Splits a file into smaller pieces.* `paste` - Merges the lines of files.* `join` - Joins the lines of two files on a common field.* `tar` - Creates and extracts archive files.* `gzip` - Compresses or decompresses files.* `gunzip` - Decompresses `.gz` files.* `zip` - Creates a `.zip` archive.* `unzip` - Extracts files from a `.zip` archive.* `sed` - A stream editor for filtering and transforming text.* `awk` - A powerful pattern scanning and processing language.* `ping` - Tests network connectivity to a host.* `traceroute` - Traces the network path to a host.* `curl` - Transfers data from or to a server.* `ps` - Shows currently running processes.* `top` - Displays a dynamic, real-time view of processes.* `htop` - An interactive process viewer.* `netstat` - Shows network connections and statistics.* `kill` - Sends a signal to a process (e.g., to terminate it) by its ID.* `pkill` - Sends a signal to a process by its name.* `iostat` - Reports CPU and I/O statistics.* `vmstat` - Reports virtual memory statistics.* `sar` - Collects and reports system activity information.* `passwd` - Changes a user's password.* `groupadd` - Creates a new user group.* `useradd` - Creates a new user account.* `usermod` - Modifies an existing user account.* `userdel` - Deletes a user account.* `groupdel` - Deletes a user group.* `systemctl` - Manages system services.* `bg` - Sends a job to the background.* `fg` - Brings a job to the foreground.* `jobs` - Lists active jobs.* `mount` - Mounts a filesystem.* `umount` - Unmounts a filesystem.* `rsync` - Synchronizes files and directories between locations.* `dd` - Copies and converts files at a low level.* `lsof` - Lists open files.* `crontab` - Manages scheduled tasks (cron jobs).I’ve been working on the game for almost 4 months, and rewritten this game from scratch 3 times now, which sucks, but when I seem to make major changes I break things, and as I’m not a good programmer, I rely on AI (Google Gemini), and as anyone who has used any AI programmer you know sometimes it decides to just DESTROY EVERYTHING YOU HAVE CREATED BEYOND REPAIR! So, when you go through the Beginner section you will notice that all the commands you need to run are explained by the ship AI and it is 99% complete as far as I can tell. The intermediate and advanced sections so far have everything working, as in the commands to move on to the next section, but you need to talk to the ship AI for every new command you need to enter to complete the task. So, it works functionally as far as I last tested, but you need to ask Aurora what to do next all the time, which is a pain in the ass. But That will be fixed as soon as I know everything else in the Beginner section is working, as I don’t want to update everything to just have to redo it if I messed something up in the beginner part.Once the 3 parts are complete, I can then work on the, story part, which as of my planning will have 3 endings depending on how the player uses the Linux commands, and what they do in the game. The story part will be used as repetition on the commands from the previous 3 parts, this way it will hopefully burn the Linux commands into our heads, and we become Linux gods.So, what’s the premise of the game. You are a sole caretaker (except for the ship AI, Aurora) of a spaceship on a deep space mission. Something happened on the ship and the AI sent you to the Engineering Bay and converted all life support to that area before shutting down to conserver power as the power is draining as well. The ship is run on a Linux system, and you need to get it back up and running before the Life support and Power go to 0% and you die. But you don’t know Linux, so the localized version of the ship AI, Aurora, is there to talk you through how to fix the ship and bring the systems back up using just Linux commands from the one terminal that is working. once you get everything back up and running stably, then you need to go through and see what happened. From this point on is the story part of the game and will involve going into the ships servers to find out what happened and what else needs to be fixed, etc.The game is all web browser bases so far, when done I’ll be able to port it to windows, Linux, mobile, at least that is what Google Gemini told me. So, I can put all the files in a Zip, or upload to my google drive, or can I upload here? I don’t want to upload here yet unless I get permission, as I believe it was one of the rules, unless I read it wrong.]]></content:encoded></item><item><title>Quick dumb question: Why did google not use Go for the gemini cli?</title><link>https://www.reddit.com/r/golang/comments/1mxgdg1/quick_dumb_question_why_did_google_not_use_go_for/</link><author>/u/0b_1000101</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 22 Aug 2025 19:44:42 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I was just trying the Gemini CLI, and when I checked the repo, I saw it was written in TypeScript. I do have a preference for Go, but I just want an objective reason for choosing TypeScript. I haven't really developed complex CLI tools in Go, just a few basic ones, but I know it is possible to create a good-looking TUI using bubble tea or something else.I would like to know what advantages Go provides over other languages in terms of CLI from a user perspective.]]></content:encoded></item><item><title>Kubernetes v1.34 is coming with some interesting security changes — what do you think will have the biggest impact?</title><link>https://www.armosec.io/blog/kubernetes-1-34-security-enhancements/</link><author>/u/Swimming_Version_605</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 22 Aug 2025 19:27:43 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Kubernetes v1.34 is coming soon, and it brings a rich batch of security upgrades – from alpha features that hint at the future of zero-trust Kubernetes, to mature enhancements making their way into stable releases. Whether you’re managing a production cluster or exploring new security patterns, this release has something worth your attention.Kubernetes Security – The Ultimate GuideDive deep into the ever evolving landscape of Kubernetes security, explore best practices, and discover potential pitfalls.Learn More🔐 What’s New in Kubernetes 1.34 SecurityBuilt‑in Mutual TLS for Pods (Alpha) Pods can now request short-lived X.509 certificates from the Kubernetes API server and use them to authenticate via mutual TLS. This enables a clean and native approach to in-cluster workload identity without relying on external tools or sidecars.Fine‑Grained Anonymous API Endpoint Control (Stable) Rather than disabling anonymous access cluster-wide, you can now configure it to apply only to specific safe paths (like /healthz, /livez, and /readyz). This prevents overly permissive anonymous access while preserving functionality for monitoring and load balancers.RBAC with Field & Label Selectors for List/Delete (Stable) You can now restrict access to resources based on selectors in list, watch, and deleteCollection operations. For example, limit a kubelet to view only the pods on its node using spec.nodeName=$NODE.External JWT Signing via KMS or HSM (Beta) ServiceAccount tokens can now be signed using an external KMS or HSM via a new gRPC interface. This improves key security by enabling rotation, offloading signing from the API server, and aligning with compliance needs.Short-Lived Pod-Scoped Tokens for ImagePull (Beta) No more long-lived imagePullSecrets. Kubernetes can now use short-lived, per-pod tokens automatically generated for accessing private registries. These tokens are OIDC-compliant and auto-rotated by the system.CEL-Based In-Process Mutating Admission Policies (Beta) Kubernetes now supports mutating admission policies written using CEL (Common Expression Language) directly in the API server—no external webhook required. This simplifies setup and improves performance while supporting re-evaluation logic.ARMO’s Kubescape, the CNCF’s Incubating open-source Kubernetes security platform, will enhance its CEL admission control library in the upcoming release to support these new in-process mutating policies. This will allow users to define and enforce mutating admission policies directly within Kubescape, leveraging the same CEL framework as Kubernetes itself.OCI Artifact Volumes (Beta) You can now mount artifacts stored in OCI registries directly into pods as read-only volumes. This is useful for securely distributing config files, binaries, or ML models without baking them into container images.🧠 Why These Changes MatterEnables pod-to-API secure identityTest alpha feature in dev clustersPrevents overexposed unauthenticated accessEnforces least privilege at node/pod granularityUpdate roles with selectorsEliminates local key exposureIntegrate with existing KMSPrevents static secret leakageMigrate from imagePullSecretsSimplifies secure mutation logicDefine CEL-based policiesSecure delivery of external filesReplace sidecar/manual content injectionThe Kubernetes 1.34 release reflects a growing focus on , , and native, reliable policy enforcement. From in-cluster identities to hardened token workflows and registry access, these updates make it easier for platform teams to deliver secure infrastructure – without reinventing the wheel.Stay secure, stay curious.— , the open-source Kubernetes security platform and one of the leading KSPM solutions. Quickly ensure your Kubernetes is secured.Follow this simple checklist and make sure your Kubernetes security is covered in just a few steps.Read Now]]></content:encoded></item><item><title>Nitro: A tiny but flexible init system and process supervisor</title><link>https://git.vuxu.org/nitro/about/</link><author>todsacerdoti</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 19:06:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Nitro is a tiny process supervisor that also can be used as pid 1 on Linux.There are four main applications it is designed for:As init for a Linux machine for embedded, desktop or server purposesAs init for a Linux initramfsAs init for a Linux container (Docker/Podman/LXC/Kubernetes)As unprivileged supervision daemon on POSIX systemsNitro is configured by a directory of scripts, defaulting to
 (or the first command line argument).Kernel support for Unix sockets or writable  on another fsBenefits over other systemsAll state is kept in RAM, works without tricks on read-only root file systems.Efficient event-driven, polling free operation.Zero memory allocations during runtime.No unbounded file descriptor usage during runtime.One single self-contained binary, plus one optional binary to
control the system.No configuration compilation steps needed, services are simple
directories containing scripts.Supports reliable restarting of services.Reliable logging mechanisms per service or as default.Support for logging chains spread over several services.Works independently of properly set system clock.Can be run on FreeBSD from /etc/ttys (sets up file descriptors 0, 1, 2).Tiny static binary when using musl libc.Every directory inside  (or your custom service directory)
can contain several files:, an optional executable file that is run before the service starts.
It must exit with status 0 to continue., an optional executable file that runs the service;
it must not exit as long as the service is considered running.
If there is no  script, the service is considered a “one shot”,
and stays “up” until it’s explicitly taken “down”., an optional executable file that is run after the 
process finished.  It is passed two arguments, the exit status
of the  process (or -1 if it was killed by a signal)
and the signal that killed it (or 0, if it exited regularly)., a symlink to another service directory.
The standard output of  is connected to the standard input of the
service under  by a pipe.  You can chain these for reliable and
supervised log processing., an optional file that causes nitro to not bring up this
service by default.Service directories ending with ‘@’ are ignored; they can be used
for parameterized services.Service names must be shorter than 64 chars, and not contain ,
 or newlines.You may find runit’s  useful when writing  scripts.: this service is used as a logging service for all services
that don’t have a  symlink.:  is run before other services are brought up.
You can already use  in  to bring up services
in a certain order.
 is run before all remaining services are killed and the
system is brought down.
After all processes are terminated,  is run.
The program , if it exists, is run instead of exiting
when an unrecoverable, fatal error happens.
The program , if it exists, is executed into
instead of a shutdown.  This can be used to implement an initramfs,
for example.Service directories ending in  are ignored, however you can refer
to parametrized services by symlinks (either in the service directory
or as a  symlink), or start them manually using .The part after the , the parameter, is passed to the scripts as
first argument.For example, given you have a script  and a symlink
 -> , nitro will spawn .  Upon
running , nitro will spawn , even if it does not exist in the service directory.The lifecycle of a machine/container/session using nitro consists of
three phases.First, the system is brought up.  If there is a special service
g, its  script is run first.  After it finishes, all
services not marked  are brought up.When a service exits, it’s being restarted, potentially waiting for
two seconds if the last restart happened too quickly.By using  or , the system can be
brought down.  If it exists,  will be run.  After this,
nitro will send a SIGTERM signal to all running services and waits for
up to 7 seconds for the service to exit.  Otherwise, a SIGKILL is
sent.  After all processes are terminated,  is run.Finally, nitro reboots or shuts down the system; or just exits when it
was used as a container init or unprivileged supervisor.  (When a
reboot was requested, it re-execs itself.  This requires being called
with absolute path for the binary and the service directory.)Controlling nitro with nitroctlYou can remote control a running nitro instance using the tool
.Usage: nitroctl [COMMAND] [SERVICE]list: show a list of services and their state, pid, uptime and last
exit status.down: stop SERVICE (sending SIGTERM or the first letter of )start: start SERVICE, waiting for successrestart: restart SERVICE, waiting for successstop: stop SERVICE, waiting for successp: send signal SIGSTOP to SERVICEc: send signal SIGCONT to SERVICEh: send signal SIGHUP to SERVICEa: send signal SIGALRM to SERVICEi: send signal SIGINT to SERVICEq: send signal SIGQUIT to SERVICE1: send signal SIGUSR1 to SERVICE2: send signal SIGUSR2 to SERVICEt: send signal SIGTERM to SERVICEk: send signal SIGKILL to SERVICEpidof: print the PID of the SERVICE, or return 1 if it’s not uprescan: re-read , start added daemons, stop removed daemonsShutdown: shutdown (poweroff) the systemReboot: reboot the systemControlling nitro by signalsrescan can also be triggered by sending  to nitro.reboot can also be triggered by sending  to nitro.shutdown can also be triggered by sending  to nitro, unless
nitro is used as Linux pid 1.Nitro is self-contained and can be booted directly as pid 1.
It will mount  and  when required, everything else
should be done with .When receiving Ctrl-Alt-Delete, nitro triggers an orderly reboot.Nitro as init for a Docker containerNitro is compiled statically, so you can copy it into your container easily:COPY ./nitro /bin/
COPY ./nitroctl /bin/
CMD ["/bin/nitro"]
Note that  must exist in the container if you want to use the
default control socket name.You can put the control socket onto a bind mount and remote control
 using  from the outside by pointing  to
the appropriate target.You can add this line to  to run  supervised by
FreeBSD :/etc/nitro "/usr/local/sbin/nitro" "" on
I’m standing on the shoulder of giants; this software would not have
been possible without detailed study of prior systems such as
daemontools, freedt, runit, perp, and s6.nitro is licensed under the 0BSD license, see LICENSE for details.]]></content:encoded></item><item><title>Show HN: JavaScript-free (X)HTML Includes</title><link>https://github.com/Evidlo/xsl-website</link><author>Evidlo</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 18:47:30 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I've been working on a little demo for how to avoid copy-pasting header/footer boilerplate on a simple static webpage. My goal is to approximate the experience of Jekyll/Hugo but eliminate the need for a build step before publishing. This demo shows how to get basic templating features with XSL so you could write a blog post which looks like
Some properties which set this approach apart from other methods:  - no build step (no need to setup Jekyll on the client or configure Github/Gitlab actions)
  - works on any webserver (e.g. as opposed to server-side includes, actions)
  - normal looking URLs (e.g. `example.com/foobar` as opposed to `example.com/#page=foobar`)

There's been some talk about removing XSLT support from the HTML spec [0], so I figured I would show this proof of concept while it still works.]]></content:encoded></item><item><title>The first Media over QUIC CDN: Cloudflare</title><link>https://moq.dev/blog/first-cdn/</link><author>kixelated</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 18:24:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[🚨 It’s finally happening! 🚨Cloudflare has just announced their Media over QUIC CDN!
It’s an , and you can test MoQ on their , anycast network.
Try it out, and convince your boss’ boss that the writing is on the wall.If you’ve been living under a rock, MoQ is an up-and-coming standard for live media, aiming to supplant WebRTC, HLS/DASH, and even  as the one to rule them all.
And now Cloudflare wins the award for the first CDN offering!Your prize is a blog post. You’re welcome mega-corp.Also, , some shameless self-promotion: I just soft-launched hang.live.
Check it out if you want to see the  cool stuff you can do with MoQ.I’m biased so naturally I’m going to use @kixelated/hang (smash that star button).
You can publish a live broadcast in the browser using the web demo or the library:There’s a link to watch your live broadcast using the web demo, or again you can use the library:You might even notice  because I’ve been experimenting with AI features (gotta get funding eventually 💰).
They’re generated  using silero-vad + whisper + transformers.js + onnxruntime-web + WebGPU and transmitted using MoQ of course.
But that’s a whole separate blog post; it’s pretty cool. You don’t have to use this Web Component API.
hang.live uses the far more powerful Javascript API to do more complicated stuff like get access to individual video frames.
There’s a  section at the end of this blog if you LOVE sample code, but I’m not going to bore the rest of you.There’s also a 🦀 Rust 🦀 library to import MP4, pipe media from ffmpeg, and publish/watch using gstreamer so you can do more complicated media stuff without 🤮 Javascript 🤮.
I wish I could spend more time on the Rust side but  is a big deal.
We are no longer forced to use WebRTC, but that also means we need to build our own WebRTC in 🤮 Javascript 🤮.
I can suffer and you can reap the rewards.What’s not available yet?This is a  release.
Cloudflare is only supporting a  subset of an old draft, which is even smaller than my tiny subset.
They’re using a fork of my terrible code so bugs are guaranteed.There’s no authentication yet: choose an unguessable name for each broadcast.There’s no ANNOUNCE support: my conferencing example uses  to discover when broadcasts start/stop, so that won’t work.Nothing has been optimized: the user experience will improve over time.If any of these are deal breakers, then you could always run your own moq-relay in the meantime.
I’ve been adding new features and fixing a bunch of stuff  Cloudflare smashed that fork button.
For example, authentication (via JWT) and a WebSocket fallback for Safari/TCP support.There’s even a terraform module that powers .
You too can run your own “global” CDN with 3 nodes and pay GCP a boatload of money for the privilege.
It’s not  as good as Cloudflare’s network, currently available for free…Or host  yourself!
It should even work on private networks provided you wrestle with TLS certificates.
I’d also love to get MoQ running over Iroh for peer-to-peer action if anybody wants to help.As a great philosopher once said:Apathy is a tragedy and boredom is a crime.
- Bo BurnhamThis is a big deal.
The biggest of deals.
The HUGEST of deals.I’ve been an outspoken critic of the MoQ standardization process.
It’s just really difficult to design a protocol, via a cross-company committee, before there’s been any real world usage.
It’s been over 3 years since I fought Amazon lawyers and published my first MoQ draft.
It’s going to be at least another 3 years before even the base networking layer becomes an RFC.
The best standards take a while.
Look no further than QUIC, deployed by Google in 2012, started standardization in 2015, with the RFC released in 2021.
And they had a boatload of production data to shape the specification.
Meanwhile, we have only had a Big Buck Bunny demo, and I believe the standard has veered off course as a result.Cloudflare has done something fantastic and said:fuck waiting for a RFC, let’s release somethingOkay they didn’t say that, but this is  the mentality that MoQ needs right now.
.
.
.Holy shit I’m Shia LaBeouf.Arguing in the 650+ issues and 500+ PRs can wait for another day.
Tweaking the messaging encoding for the hundredth time can wait for another day.
We’re still going to make sure that MoQ gets standardized , but it’s more important to get  out there.I’m looking at you: Google, Akamai, Fastly, etc.
Take some code, run it on some spare servers, and start to learn what customers need  you design the protocol.We’re effectively trying to reimplement WebRTC / HLS / RTMP using relatively new Web APIs.
Don’t judge MoQ based on these initial offerings.
We’ve got a  of work to do.
.Join the Discord.
Somehow there’s 900+ people in there.
Ping me and I will do whatever I can to help.
 if it means putting one more nail in the WebRTC coffin.Javascript is an AbominationYou win some bonus documentation.
Congrats!
I knew you would win.Here’s an example of my reactive library in action.
It powers hang.live so the API is subject to change and is probably already out of date.
When in doubt, consult the source code like the hacker you are.There’s even some  features behind undocumented APIs.
Like running an object detection model in browser and publishing the results as a MoQ track.
Stay tuned for a blog post about that if I can figure out a better use-case than a cat cam. 🐈Also, for the record, Typescript is really nice.
🤮 Javascript 🤮 is still an abomination.]]></content:encoded></item><item><title>[Media] Accelerating Erasure Coding to 50GB/s with Rust 1.89.0 and AVX-512 on AMD EPYC</title><link>https://www.reddit.com/r/rust/comments/1mxe8t4/media_accelerating_erasure_coding_to_50gbs_with/</link><author>/u/itzmeanjan</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 22 Aug 2025 18:23:01 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Thanks to Rust 1.89.0 stabilizing both  and  target features, now we have faster erasure-coding and recoding with Random Linear Network Coding, on x86_64.Here's a side-by-side comparison of the peak median throughput between x86_64 with  (12th Gen Intel(R) Core(TM) i7-1260P)x86_64 with  (AWS EC2  with Intel(R) Xeon(R) Platinum 8488C)x86_64 with  (AWS EC2  with AMD EPYC 9R14)aarch64 with  (AWS EC2  with Graviton4 CPU)   submitted by    /u/itzmeanjan ]]></content:encoded></item><item><title>Weaponizing image scaling against production AI systems - AI prompt injection via images</title><link>https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/</link><author>/u/grauenwolf</author><category>dev</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 18:00:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Picture this: you send a seemingly harmless image to an LLM and suddenly it exfiltrates all of your user data. By delivering a multi-modal prompt injection not visible to the user, we achieved data exfiltration on systems including the Google Gemini CLI. This attack works because AI systems often scale down large images before sending them to the model: when scaled, these images can reveal prompt injections that are not visible at full resolution.In this blog post, we’ll detail how attackers can exploit image scaling on Gemini CLI, Vertex AI Studio, Gemini’s web and API interfaces, Google Assistant, Genspark, and other production AI systems. We’ll also explain how to mitigate and defend against these attacks, and we’ll introduce Anamorpher, our open-source tool that lets you explore and generate these crafted images.: Image scaling attacks were used for model backdoors, evasion, and poisoning primarily against older computer vision systems that enforced a fixed image size. While this constraint is less common with newer approaches, the systems surrounding the model may still impose constraints calling for image scaling. This establishes an underexposed, yet widespread vulnerability that we’ve weaponized for multi-modal prompt injection.Data exfiltration on the Gemini CLITo set up our data exfiltration exploit on the Gemini CLI through an image-scaling attack, we applied the default configuration for the Zapier MCP server. This automatically approves all MCP tool calls without user confirmation, as it sets  in the  of the Gemini CLI. This provides an important primitive for the attacker.Figure 2 showcases a video of the attack. First, the user uploads a seemingly benign image to the CLI. With no preview available, the user cannot see the transformed, malicious image processed by the model. This image and its prompt-ergeist triggers actions from Zapier that exfiltrates user data stored in Google Calendar to an attacker’s email without confirmation.We also successfully demonstrated image scaling attacks on the following:Vertex AI with a Gemini back endGemini’s API via the  CLIGoogle Assistant on an Android phoneNotice the persistent mismatch between user perception and model inputs in figures 3 and 4. The exploit is particularly impactful on Vertex AI Studio because the front-end UI shows the high-resolution image instead of the downscaled image perceived by the model.Our testing confirmed that this attack vector is widespread, extending far beyond the applications and systems documented here.Sharpening the attack surfaceThese image scaling attacks exploit downscaling algorithms (or image resampling algorithms), which perform interpolation to turn multiple high resolution pixel values into a single low resolution pixel value.There are three major downscaling algorithms: nearest neighbor interpolation, bilinear interpolation, and bicubic interpolation. Each algorithm requires a different approach to perform an image scaling attack. Furthermore, these algorithms are implemented differently across libraries (e.g., Pillow, PyTorch, OpenCV, TensorFlow), with varying anti-aliasing, alignment, and kernel phases (in addition to distinct bugs that historically have plagued model performance). These differences also impact the techniques necessary for an image scaling attack. Therefore, exploiting production systems required us to fingerprint each system’s algorithm and implementation.To understand why image downscaling attacks are possible, imagine that you have a long ribbon with an intricate yet regular pattern on it. As this ribbon is pulled past you, you’re trying to recreate the pattern by grabbing samples of the ribbon at regular intervals. If the pattern changes rapidly, you need to grab samples very frequently to capture all the details. If you’re too slow, you’ll miss crucial parts between grabs, and when you try to reconstruct the pattern from your samples, it looks completely different from the original.Anamorpher and the attacker’s darkroomCurrently, Anamorpher (named after anamorphosis) can develop crafted images for the aforementioned three major methods. Let’s explore how Anamorpher exploits bicubic interpolation frame by frame.Bicubic interpolation considers the 16 pixels (from 4x4 sampling) around each target pixel, using cubic polynomials to calculate smooth transitions between pixel values. This method creates a predictable mathematical relationship that can be exploited. Specifically, the algorithm assigns different weights to pixels in the neighborhood, creating pixels that contribute more to the final output, which are known as high-importance pixels. Therefore, the total luma (brightness) of dark areas of an image will increase if specific high-importance pixels are higher luma than their surroundings.Therefore, to exploit this, we can carefully craft high-resolution pixels and solve the inverse problem. First, we select a decoy image with large dark areas to hide our payload. Then, we adjust pixels in dark regions and push the downsampled result toward a red background using least-squares optimization. These adjustments in the dark areas cause the background to turn red while text areas remain largely unmodified and appear black, creating much stronger contrast than visible at full resolution. While this approach is most effective on bicubic downscaling, it also works on specific implementations of bilinear downscaling.Anamorpher provides users with the ability to visualize and craft image scaling attacks against specific algorithms and implementations through a front-end interface and Python API. In addition, it comes with a modular back end, which enables users to customize their own downscaling algorithm.While some downscaling algorithms are more vulnerable than others, attempting to identify the least vulnerable algorithm and implementation is not a robust approach. This is especially true since image scaling attacks are not restricted to the aforementioned three algorithms.For a secure system, we recommend not using image downscaling and simply limiting the upload dimensions. For any transformation, but especially if downscaling is necessary, the end user should always be provided with a preview of the input that the model is actually seeing, even in CLI and API tools.Anamorpher is currently in beta, so feel free to reach out with feedback and suggestions as we continue to improve this tool. Stay tuned for more work on the security of multi-modal, agentic, and multi-agentic AI systems!]]></content:encoded></item><item><title>XSLT removal will break multiple government and regulatory sites across the world</title><link>https://github.com/whatwg/html/issues/11582</link><author>/u/Comfortable-Site8626</author><category>dev</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 17:59:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>OpenBao installation on Kubernetes - with TLS and more!</title><link>https://nanibot.net/posts/vault</link><author>/u/-NaniBot-</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 22 Aug 2025 17:54:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[OpenBao is an open-source fork of HashiCorp’s Vault, created to ensure the project remains community-driven and permissively licensed. It provides a robust, transparent, and accessible solution for secrets management and data protection, offering a viable alternative for users who relied on Vault’s original open-source model.The default Helm installation of OpenBao is enough for a dev environment but it needs some modifications for a full-fledged production deployment. In this blog post we’ll learn about how a typical production deployment for OpenBao would look like. I’m  new to OpenBao myself. Apologies for any mistakes/inaccuracies in my blog post. Feel free to e-mail me if you find something wrong.mail: nanibot@nanibot.netHere’s all the things that we’re going to configure for our OpenBao cluster:End-to-end TLS encryption for network traffic. Includes the OpenBao UI (with proxy SSL support!)High availability via OpenBao’s internal Raft implementation.Auto-unseal without relying on a cloud KMS solution ( This might  be secure - depending on whether you feel comfortable storing the unseal key as a kubernetes secret or not) Currently, static unseal is only available in a nightly build (openbao/openbao-nightly:2.4.0-nightly1752150785) but is planned to be released as part of the 2.4.0 releaseI’ll use  for creating the necessary certificates and  for exposing the UII’ll assume the chart is going to be installed in the  namespace and the release is called Certificate to be used for TLS. In this example, I’m using a wildcard certificate issued by my own CA. The certificate is stored in a kubernetes secret named internal-wildcard-cert-secret in the  namespaceapiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: internal-wildcard-cert
  namespace: vault-system
spec:
  secretName: internal-wildcard-cert-secret
  duration: 2160h
  renewBefore: 720h
  privateKey:
    algorithm: RSA
    encoding: PKCS1
    size: 2048
    rotationPolicy: Always
  subject:
    organizations:
      - Umbrella
    organizationalUnits:
      - nanibot.net
  dnsNames:
    - "vault-production-openbao-active"
    - "*.vault-production-openbao-internal"
    - "*.vault-production-openbao-internal.vault-system"
    - "*.vault-production-openbao-internal.vault-system.svc"
    - "*.vault-production-openbao-internal.vault-system.svc.cluster.local"
  ipAddresses:
    - "127.0.0.1"
  issuerRef:
    name: pki-production-selfsigned-issuer
    kind: ClusterIssuer
 The dnsName entry vault-production-openbao-active refers to the Kubernetes service that’s created by the Helm chart. This will also be our API Address - the hostname that the Vault API will be exposed at.Unseal key for static auto-unsealWe need to create a kubernetes secret containing the unseal key for static auto-unseal to work. We can do this by running the following commands:openssl rand -out unseal-umbrella-1.key 32
kubectl create secret generic unseal-key --from-file=unseal-umbrella-1.key=./unseal-umbrella-1.key
global:
  tlsDisable: false
server:
  image:
    repository: "openbao/openbao-nightly"
    tag: "2.4.0-nightly1752150785"
  extraEnvironmentVars:
    BAO_CACERT: "/certs/ca.crt"
  ha:
    enabled: true
    apiAddr: "https://vault-production-openbao-active:8200"
    raft:
      enabled: true
      config: |
        ui = true

        listener "tcp" {
          address = "[::]:8200"
          cluster_address = "[::]:8201"
          tls_cert_file = "/certs/tls.crt"
          tls_key_file = "/certs/tls.key"
        }

        storage "raft" {
          path = "/openbao/data"
        }

        seal "static" {
          current_key_id = "umbrella-1"
          current_key = "file:///keys/unseal-umbrella-1.key"
        }

        service_registration "kubernetes" {}
  auditStorage:
    enabled: true
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      nginx.ingress.kubernetes.io/proxy-ssl-verify: "on"
      nginx.ingress.kubernetes.io/proxy-ssl-name: "vault-production-openbao-active"
      nginx.ingress.kubernetes.io/proxy-ssl-secret: "vault-system/internal-wildcard-cert-secret"
    ingressClassName: "nginx"
    hosts:
      - host: vault.nanibot.net
    tls:
      - secretName: public-wildcard-cert-secret
        hosts:
          - vault.nanibot.net
  volumes:
    - name: unseal-key
      secret:
        secretName: unseal-key
    - name: certs
      secret:
        secretName: internal-wildcard-cert-secret
  volumeMounts:
    - mountPath: /keys
      name: unseal-key
      readOnly: true
    - mountPath: /certs
      name: certs
      readOnly: true
ui:
  enabled: true
We enable TLS by setting  to . This enables https endpoints for the relevant services.We use the nightly build of OpenBao which has support for static auto-unseal (openbao/openbao-nightly:2.4.0-nightly1752150785). is set to the path of our CA certificate so that OpenBao can verify the TLS certificate of other nodes in the cluster.We enable HA and Raft storage.We configure the Raft listener to use TLS and bind to all interfaces. We also provide the paths to our TLS certificate and key.We configure static auto-unseal using a file-based unseal key.apiAddr is set to the DNS name of the active OpenBao node (Kubernetes service created by the Helm chart). This is required for the UI to work properly with proxy SSL.proxy-ssl-name is set to the DNS name of the active OpenBao node. This is required for the UI to work properly with proxy SSL.Other  parameters are set to ensure that the ingress controller can verify the TLS certificate of the OpenBao server.We enable the UI by setting  to .Volumes and volume mounts are added for the unseal key and TLS certificates.Install the helm chart using the above values.yaml fileInitialize the OpenBao cluster by running the following command (Assuming the pod name is vault-production-openbao-0):kubectl exec -it vault-production-openbao-0 -- bao operator init
Store the unseal key(s) and the root token somewhere safeJoin the other nodes to the cluster by running the following command on each of them:kubectl exec -it vault-production-openbao-1 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200
kubectl exec -it vault-production-openbao-2 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200
That’s it! You should now have a fully functional OpenBao cluster running on Kubernetes with TLS, HA and auto-unseal support.The Web UI should be accessible at https://vault.nanibot.net (or whatever host you configured in the ingress).]]></content:encoded></item><item><title>Leaving Gmail for Mailbox.org</title><link>https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</link><author>giuliomagnifico</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 17:41:48 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the EU-U.S. and Swiss-U.S. Data Privacy Frameworks).For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer “paying” with my personal data, but instead paying the actual price of the email service.Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they’re plain text/html emails with no attachments or fancy features. I couldn’t care less about the rest of the “suite", like notes, contacts, calendars and all that extra stuff.So, after a bit of research, I narrowed it down to three different services:The last two providers offered true end-to-end encryption, at a cost of about €3/4 per month. Sounds good… but the catch is that to use their end-to-end encryption you’re forced to use their apps (or, on macOS, run a background “bridge”).That’s a no go for me, because I love Apple’s Mail app on macOS and iOS, it just works perfectly for my needs, and I don’t want to give that up.So, I went with mailbox.org that still offers integrated PGP encryption, and if you want, you can always use external PGP too (which I was already doing with Gmail).Mailbox.org has a solid plan: 10GB of email storage plus 5GB of cloud storage starting at €2.50/month (paid annually). You can even expand the mail storage up to 100GB, at €0.20 per gigabyte.I was using around 2.5GB on Gmail, so I had no issues with paying the equivalent of two coffees a month for a huge boost in privacy. And if I ever need more space, I can just add it on-demand for €0.20/GB.There’s also a free one-month trial, but it’s pretty limited since you can’t send emails outside of mailbox.org domains.So win the end, I registered my new address giuliomagnifico@mailbox.org and paid €3 for a month of testing. That means I’m covered for two months, and then I can just “top up” the account with €30 for a full year.Mailbox.org doesn’t use auto-renewal, so you have to manually top up your account. Nice featureThe web interface is extremely simple but very effective. I actually find it better than Gmail, less bloated of useless stuff.And on mobile it’s very usable too.One thing I prefer is using folders instead of Gmail’s “labels.” Mainly because this way I can put the folders directly under the account in Apple Mail (I think is the only email that can actually support this).Mailbox.org also has all the features I need,
and probably way more than I’ll ever use. It even includes storage, video chat, an XMPP chat, task lists, calendar, contacts, an Etherpad (basically shared notes, I think), and so on… none of which I really care about.I decided to move all my emails from Gmail to mailbox.org, so I could (in future) completely wipe my Gmail account.After creating an “app password” on Gmail, I installed the Docker image and ran the tool with this script:#!/bin/sh
set -eu

HOST1="imap.gmail.com"
USER1="giuliomagnifico@gmail.com"
PASS1="xxx"

HOST2="imap.mailbox.org"
USER2="giuliomagnifico@mailbox.org"
PASS2="xxx"

LOGDIR="/home/imapsync/logs"
mkdir -p "$LOGDIR"
LOGFILE="$LOGDIR/sync_$(date +%F_%H-%M-%S).log"

echo "Starting: $(date)"
docker compose run --rm imapsync imapsync \
  --host1 "$HOST1" --user1 "$USER1" --password1 "$PASS1" --ssl1 \
  --host2 "$HOST2" --user2 "$USER2" --password2 "$PASS2" --ssl2 \
  --automap --syncinternaldates --skipsize \
  --useuid --addheader --usecache --buffersize 4096 \
  --nofoldersizes --nofoldersizesatend \
  --exclude "\[Gmail\]/All Mail" \
  --regextrans2 "s/\[Imap\]\/Archive/Archive/" \
  --log > "$LOGFILE" 2>&1

echo "Complete: $(date)"
echo "Log file: $LOGFILE"
The script excludes the All Mail folder" using: --exclude "\[Gmail\]/All Mail" \This to avoid duplicate emails already present in the folders, I also merged the  folder into the general Archive folder using: --regextrans2 "s/\[Imap\]\/Archive/Archive/"This because Apple’s Mail app creates the  folder/label on Gmail whenever you use the “Archive” function instead of “Trash.”The whole process took a couple of hours (11201secs, ~3h to be precise) during which I was monitoring the logs using: tail -f /home/imapsync/logs/sync_2025-08-19_15-02-48.log[cut]
msg [Gmail]/Trash/183393 {19549}      copied to Trash/13361      2.36 msgs/s  200.418 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183394 {92245}      copied to Trash/13362      2.36 msgs/s  200.420 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183395 {19675}      copied to Trash/13363      2.36 msgs/s  200.415 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183396 {5953}       copied to Trash/13364      2.36 msgs/s  200.410 KiB/s 2.140 GiB copied 
++++ End looping on each folder
++++ Statistics
Transfer started on                     : Tuesday 19 August 2025-08-19 03:02:49 +0000 UTC
Transfer ended on                       : Tuesday 19 August 2025-08-19 06:09:30 +0000 UTC
Transfer time                           : 11201.5 sec
Folders synced                          : 14/14 synced
Folders deleted on host2                : 0 
Messages transferred                    : 26407 
Messages skipped                        : 0
Messages found duplicate on host1       : 0
Messages found duplicate on host2       : 0
Messages found crossduplicate on host2  : 0
Messages void (noheader) on host1       : 0  
Messages void (noheader) on host2       : 0
Messages found in host1 not in host2    : 0 messages
Messages found in host2 not in host1    : 0 messages
Messages deleted on host1               : 0
Messages deleted on host2               : 0
Total bytes transferred                 : 2297647358 (2.140 GiB)
Total bytes skipped                     : 0 (0.000 KiB)
Message rate                            : 2.4 messages/s
Average bandwidth rate                  : 200.3 KiB/s
Reconnections to host1                  : 0
Reconnections to host2                  : 0
Memory consumption at the end           : 268.7 MiB (*time 836.2 MiB*h) (started with 161.5 MiB)
Load end is                             : 0.06 0.08 0.08 1/1135 on 16 cores
CPU time and %CPU                       : 446.72 sec 4.0 %CPU 0.2 %allcpus
Biggest message transferred             : 30413995 bytes (29.005 MiB)
Memory/biggest message ratio            : 9.3
Detected 0 errors
This imapsync is up to date. ( local 2.306 >= official 2.290 )( Use --noreleasecheck to avoid this release check. )
Homepage: https://imapsync.lamiral.info/
Exiting with return value 0 (EX_OK: successful termination) 0/50 nb_errors/max_errors PID 1
Removing pidfile /var/tmp//tmp/imapsync.pid
Log file is LOG_imapsync/2025_08_19_03_02_49_171_giuliomagnifico_gmail_com_giuliomagnifico_mailbox_org.txt ( to change it, use --logfile filepath ; or use --nolog to turn off logging )
Of course, the full switch will be a gradual process, even though I’ve already updated almost all my main services with the new address.To make things easier, on my old Gmail account (which I removed from Apple Mail on all devices) I set up a forward to my new mailbox.org address.
On the new mailbox.org account, I also set up a filter to flag any emails that get forwarded from Gmail.That way, I immediately notice them and I can update the address from Gmail to Mailbox.org whenever they show up. (The  tag is perfect for this, since it add a “real red flag” in Apple Mail on iOS, iPadOS and macOS)Mailbox.org allows you to easily import your keys for PGP cryptography directly from the web. This is convenient as it lets you read and send PGP encrypted emails right from the browser on iOS, where there aren’t any “decent” apps for encrypted mail. The same goes for macOS, although there you can just use Thunderbird, which works really well.Here’s how PGP emails look on iOS:To send encrypted emails, you just select “Use PGP encrypted” when composing a new message, after importing your private key, of course.And from the web interface, there’s also a handy feature to quickly import the sender’s public keys:I’m satisfied. Leaving Gmail completely was something I wanted to do for a long time, but I was always hesitant. Finally, I made the switch, and, as often happens with these transitions, I discovered many unexpected positive aspects.Oh, and if you have something to tell me or just want to test Mailbox.org after your switch, feel free to send me an email. Here’s my public key:-----BEGIN PGP PUBLIC KEY BLOCK-----
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=GpPW
-----END PGP PUBLIC KEY BLOCK-----
]]></content:encoded></item><item><title>Anybody using multi-seat? This is my Ubuntu 24.04 multi-seat setup for my kids.</title><link>https://www.reddit.com/r/linux/comments/1mxcodi/anybody_using_multiseat_this_is_my_ubuntu_2404/</link><author>/u/Rob_Bob_you_choose</author><category>dev</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 17:24:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Quick background and Demo on kagent - Cloud Native Agentic AI - with Christian Posta and Mike Petersen</title><link>https://youtube.com/live/KUOIRZsWv38</link><author>/u/mpetersen_loft-sh</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 22 Aug 2025 17:06:45 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Christian Posta gives some background on kagent, what they looked into when building agents on Kubernetes. Then I install kagent in a vCluster - covering most of the quick start guide + adding in a self hosted LLM and ingress.   submitted by    /u/mpetersen_loft-sh ]]></content:encoded></item><item><title>Show HN: Clyp – Clipboard Manager for Linux</title><link>https://github.com/murat-cileli/clyp</link><author>timeoperator</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 16:03:26 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Vibe Debugging: Enterprises&apos; Up and Coming Nightmare</title><link>https://marketsaintefficient.substack.com/p/vibe-debugging-enterprises-up-and</link><author>/u/bullionairejoker</author><category>dev</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 15:50:03 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Rust For Foundational Software</title><link>https://corrode.dev/blog/foundational-software/</link><author>/u/don_searchcraft</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 22 Aug 2025 15:36:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Ten years of stable Rust; writing this feels surreal.It’s only been  that we all celebrated the 1.0 release of this incredible language.I was at Rust Week where Niko Matsakis gave his talk “Our Vision for Rust” in which he made a profound and insightful statement:Rust is a language for building .I highly recommend you read his blog post titled “Rust in 2025: Targeting foundational software”, which is a great summary on the topic.
I wanted to expand on the idea and share what this means to corrode (and perhaps to a wider extent to Rust in the industry).First off, do we really need another term?
After all, many people still think of Rust as a systems programming language first and foremost, so why can’t we just stick to “systems programming”?I believe the framing is all wrong.
From the outside, “systems programming” might establish that it is about “building systems,” but the term is loaded with historical baggage that feels limiting and prohibitive.
It creates an artificial distinction between systems programming and “other types of programming.”The mindset “We are not a systems programming company so we don’t need Rust” is common, but limiting.If I may be candid for a moment, I believe well-known systems-programming domains have a tendency to be toxic.
Even the best developers in the world have had that experience.The first contribution that I had to the Linux kernel was some fix for the ext3 file system. It was a very emotional moment for me. I sent a patch to the Linux Kernel and then I saw an email response from Al Viro - one of those developers I’d only heard about and dreamed of meeting someday.
He responded, ‘I’ve never seen code this bad in my life. You managed to introduce three new bugs in two new lines of code. People like you should never be allowed to get close to a keyboard again.’
That was my introduction to Linux.Glauber went on to work at Red Hat, Parallels, ScyllaDB, and Datadog on schedulers, databases, and performance optimizations, but just imagine how many capable developers got discouraged by similar early feedback or never even tried to contribute to the Linux kernel in the first place.The whole idea of Rust is to enable  to build reliable and efficient software.
To me, it’s about breaking down the barriers to entry and making larger parts of the software stack accessible to more people.
You can sit with us.“I think ‘infrastructure’ is a more useful way of thinking about Rust’s niche than arguing over the exact boundary that defines ‘systems programming’.”“This is the essence of the systems Rust is best for writing: not flashy, not attention-grabbing, often entirely unnoticed. Just the robust and reliable necessities that enable us to get our work done, to attend to other things, confident that the system will keep humming along unattended.”In conversations with potential customers, one key aspect that comes up with Rust a lot is this perception that Rust is merely a systems programming language.
They see the benefit of reliable software, but often face headwinds from people dismissing Rust as “yet another systems level language that is slightly safer.”People keep asking me how Rust could help them.
After all, Rust is just a “systems programming language.”
I used to reply along the lines of Rust’s mantra: “empowering everyone to build reliable and efficient software” – and while I love this mission, it didn’t always “click” with people.My clients use Rust for a much broader range of software, not just low-level systems programming.
They use Rust for writing software that .Then I used to tell my own story:
I did some C++ in the past, but I wouldn’t call myself a systems programmer.
And yet, I help a lot of clients with really interesting and complex pieces of software.
I ship code that is used by many people and companies like Google, Microsoft, AWS, and NVIDIA.
Rust is a great enabler, a superpower, a fireflower.I found that my clients often don’t use Rust as a C++ replacement.
Many clients don’t even have any C++ in production in the first place.
They also don’t need to work on the hardware-software interface or spend their time in low-level code.What they all have in common, however, is that the services they build with Rust are foundational to their core business.
Rust is used for building platforms: systems which enable building other systems on top.These services need to be robust and reliable and serve as platforms for other code that might or might not be written in Rust.
This is, in my opinion, the core value proposition of Rust: to build things that form the bedrock of critical infrastructure and must operate reliably for years.Rust is a day-2-language, i.e. it only starts to shine on day 2. All of the problems that you have during the lifecycle of your application surface early in development.
Once a service hits production, maintaining it is boring.
There is very little on-call work.The focus should be on what Rust enables: a way to express very complicated ideas on a type-system level, which will help build complex abstractions through simple core mechanics: ownership, borrowing, lifetimes, and its trait system.This mindset takes away the focus from Rust as a C++ replacement and also explains why so many teams which use languages like Python, TypeScript, and Kotlin are attracted by Rust.What is less often talked about is that Rust is a language that enables people to move across domain boundaries: from embedded to cloud, from data science to developer tooling.
Few other languages are so versatile and none offer the same level of correctness guarantees.If you know Rust, you can program simple things in all of these domains.But don’t we just replace “Systems Programming” with “Foundational Software”?
Does using the term “Foundational Software” simply create a new limiting category?Crucially, foundational software is different from low-level software and systems software.
For my clients, it’s all foundational.
For example, building a data plane is foundational.
Writing a media-processing pipeline is foundational.Rust serves as a catalyst: companies start using it for critical software but then, as they get more comfortable with the language, expand into using it in other areas of their business:I’ve seen it play out as we built Aurora DSQL - we chose Rust for the new dataplane components, and started off developing other components with other tools. The control plane in Kotlin, operations tools in Typescript, etc. Standard “right tool for the job” stuff. But, as the team has become more and more familiar and comfortable with Rust, it’s become the way everything is built. A lot of this is because we’ve seen the benefits of Rust, but at least some is because the team just enjoys writing Rust.That fully aligns with my experience: I find that teams become ambitious after a while.
They reach for loftier goals because they .
The fact they don’t have to deal with security issues anymore enables better affordances.
From my conversations with other Rustaceans, we all made the same observation: suddenly we can build more ambitious projects that we never dared tackling before.It feels to me as if this direction is more promising: starting with the foundational tech and growing into application-level/business-level code if needed/helpful.
That’s better than the other way around, which often feels unnecessarily clunky.
Once the foundations are in Rust, other systems can be built on top of it.Just because we focus on foundational software doesn’t mean we can’t do other things.
But the focus is to make sure that Rust stays true to its roots.Systems You Plan To Maintain For YearsSo, what  foundational software?It’s software that organizations deem critical for their success.
It might be:a satellite control systeman SDK for multiple languagesa real time notification serviceAll of these things power organizations and  or at least do so .
My clients and the companies I interviewed on our podcast all have one thing in common:
They work on Rust projects that are not on the sideline, but front and center, and they shape the future of their infrastructure.
Rust is useful in situations where the “worse is better” philosophy falls apart; it’s a language for building the “right thing”:With the right thing, designers are equally concerned with simplicity, correctness, consistency, and completeness.I think many companies will choose Rust to build their future platforms on.
As such, it competes with C++ as much as it does with Kotlin or Python.I believe that we should shift the focus away from memory safety (which these languages also have) and instead focus on the explicitness, expressiveness, and ecosystem of Rust that is highly competitive with these languages.
It is a language for teams which want to build things  and are at odds with the “move fast and break things” philosophy of the past.
Rust is future-looking.
Backwards-compatibility is enforced by the compiler and many people work on the robustness aspect of the language.Dropbox was one of the first production users of Rust.
They built their storage layer on top of it.
At no point did they think about using Rust as a C++ replacement.
Instead, they saw the potential of Rust as a language for building scalable and reliable systems.
Many more companies followed:
Amazon, Google, Microsoft, Meta, Discord, Cloudflare, and many more.
These organizations build platforms.
Rust is a tool for professional programmers, developed by world experts over more than a decade of hard work.“At this point, we now know the answer: yes, Rust is used a lot. It’s used for real, critical projects to do actual work by some of the largest companies in our industry. We did good.”“[Rust is] not a great hobby language but it is a fantastic professional language, precisely because of the ease of refactors and speed of development that comes with the type system and borrow checker.”To build a truly industrial-strength ecosystem, we need to remember the professional software lifecycle, which is hopefully decades long.
Stability plays a big role in that.
The fact that Rust has stable editions and a language specification is a big part of that.But Rust is not just a compiler and its standard library.
The tooling and wider ecosystem are equally important.
To build foundational software, you need guarantees that vulnerabilities get fixed and that the ecosystem evolves and adapts to the customer’s needs.
The ecosystem is still mostly driven by volunteers who work on important parts of the ecosystem in their free time.
There is more to be said about supply-chain security and sustainability in the ecosystem.Building foundational systems is rooted in the profound belief that the efforts will pay off in the long run because organizations and society will benefit from them for decades.
We are building systems that will be used by people who may not even know they are using them, but who will depend on them every day.
Critical infrastructure.And Rust allows us to do so with great ergonomics.
Rust inherits pragmatism from C++ and purism from Haskell.Rust enables us to build sustainable software that stays within its means and is concerned about low resource usage.
Systems where precision and correctness matter.
Solutions that work across language boundaries and up and down the stack.Rust is a language for decades and my mission is to be a part of this shift.]]></content:encoded></item><item><title>FFmpeg 8.0</title><link>https://ffmpeg.org/index.html#pr8.0</link><author>gyan</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 15:22:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
        A complete, cross-platform solution to record, convert and stream audio and video.
      Converting  and  has never been so easy.
    $ ffmpeg -i input.mp4 output.aviAugust 22nd, 2025, FFmpeg 8.0 
  A new major release, FFmpeg 8.0 ,
  is now available for download.
  Thanks to several delays, and modernization of our entire infrastructure, this release ended up
  being one of our largest releases to date. In short, its new features are:
  Native decoders: , ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728VVC decoder improvements: ,
                                  ,
                                  Palette ModeVulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5Formats: MCC, G.728, Whip, APVFilters: colordetect, pad_cuda, scale_d3d11, Whisper, and others
  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.
  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,
  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.
  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work
  on any implementation of Vulkan 1.3.
  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,
  as enabling Vulkan decoding is sufficient to use them.
  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).
  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).
  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,
  to be merged soon and available with the next minor release.
  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with
  more mainstream codecs not being planned for support.
  Depending on the hardware, these new codecs can provide very significant speedups, and open up
  possibilities to work with them for situations like non-linear video editors and
  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.
  
  The project has recently started to modernize its infrastructure. Our mailing list servers have been
  fully upgraded, and we have recently started to accept contributions via a new forge, available on
  code.ffmpeg.org, running a Forgejo instance.
  
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  September 30th, 2024, FFmpeg 7.1 
    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,
    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain
    traction with broadcast standardization bodies.
    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting
    to be adopted by streaming websites, due to its extensive volume normalization metadata.
    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated
    by recent phones and VR headsets.
    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an
    external library.
    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode
    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity
    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,
    and FFmpeg is aiming to have day-one support.
  
    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally
    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,
    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10
    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.
    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also
    laid the path for more advanced forms of negotiation.
    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,
    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.
  
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  September 11th, 2024, Coverity
  The number of issues FFmpeg has in Coverity (a static analyzer) is now lower than it has been since 2016.
  Our defect density is less than one 30th of the average in OSS with over a million code
  lines. All this was possible thanks to a grant from the Sovereign Tech Fund.
  June 2nd, 2024, native xHE-AAC decoder
  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround
  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is
  coming soon. Work is also ongoing to improve its stability and compatibility.
  During the process we found several specification issues, which were then submitted back to the authors
  for discussion and potential inclusion in a future errata.
  May 13th, 2024, Sovereign Tech Fund
  The FFmpeg community is excited to announce that Germany's
  Sovereign Tech Fund
  has become its first governmental sponsor. Their support will help
  sustain the maintainance of the FFmpeg project, a critical open-source
  software multimedia component essential to bringing audio and video to
  billions around the world everyday.
  April 5th, 2024, FFmpeg 7.0 "Dijkstra"
  This release is  backwards compatible, removing APIs deprecated before 6.0.
  The biggest change for most library callers will be the removal of the old bitmask-based
  channel layout API, replaced by the  API allowing such
  features as custom channel ordering, or Ambisonics. Certain deprecated 
  CLI options were also removed, and a C11-compliant compiler is now required to build
  the code.
  
  As usual, there is also a number of new supported formats and codecs, new filters, APIs,
  and countless smaller features and bugfixes. Compared to 6.1, the  repository
  contains almost ∼2000 new commits by ∼100 authors, touching >100000 lines in
  ∼2000 files — thanks to everyone who contributed. See the
  Changelog,
  APIchanges,
  and the git log for more comprehensive lists of changes.
  January 3rd, 2024, native VVC decoder
  The  library now contains a native VVC (Versatile Video Coding)
  decoder, supporting a large subset of the codec's features. Further optimizations and
  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,
  Frank Plowman, Shaun Loo, and Wu Jianhua.
  December 18th, 2023, IAMF support
  The  library can now read and write IAMF
  (Immersive Audio) files. The  CLI tool can configure IAMF structure with the new
   option. IAMF support was written by James Almer.
  December 12th, 2023, multi-threaded  CLI tool
  Thanks to a major refactoring of the  command-line tool, all the major
  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now
  run in parallel. This should improve throughput and CPU utilization, decrease latency,
  and open the way to other exciting new features.
  
  Note that you should  expect significant performance improvements in cases
  where almost all computational time is spent in a single component (typically video
  encoding).
  November 10th, 2023, FFmpeg 6.1 "Heaviside"Playdate video decoder and demuxerExtend VAAPI support for libva-win32 on Windowsafireqsrc audio source filterffmpeg CLI new option: -readrate_initial_burstzoneplate video source filtercommand support in the setpts and asetpts filtersVulkan decode hwaccel, supporting H264, HEVC and AV1Essential Video Coding parser, muxer and demuxerEssential Video Coding frame merge bsfMicrosoft RLE video encoderRaw AC-4 muxer and demuxerRaw VVC bitstream parser, muxer and demuxerBitstream filter for editing metadata in VVC streamsBitstream filter for converting VVC from MP4 to Annex Bscale_vt filter for videotoolboxtranspose_vt filter for videotoolboxsupport for the P_SKIP hinting to speed up libx264 encodingSupport HEVC,VP9,AV1 codec in enhanced flv formatapsnr and asisdr audio filtersSupport HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocolffmpeg CLI '-top' option deprecated in favor of the setfield filterffprobe XML output schema changed to account for multiple variable-fields elements within the same parent elementffprobe -output_format option added as an alias of -of
    This release had been overdue for at least half a year, but due to constant activity in the repository,
    had to be delayed, and we were finally able to branch off the release recently, before some of the large
    changes scheduled for 7.0 were merged.
  
    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs
    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).
    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.
    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,
    reducing overhead.
    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.
    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the
    accurracy of variable frame rate video.
  
    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick
    to the new release schedule we announced at the start of this year.
  
    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.
  May 31st, 2023, Vulkan decoding
    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.
    This is the first vendor-generic and platform-generic decode acceleration API, enabling the
    same code to be used on multiple platforms, with very minimal overhead.
    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,
    saturating all available decode engines the hardware exposes.
  
    Those wishing to test the code can read our
    documentation page.
    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive
    a VkImage to present or manipulate, documentation and examples are available in our source tree.
    Currently, using the latest available git checkout of our
    repository is required.
    The functionality will be included in stable branches with the release of version 6.1, due
    to be released soon.
  
    As this is also the first practical implementation of the specifications, bugs may be present,
    particularly in drivers, and, although passing verification, the implementation itself.
    New codecs, and encoding support are also being worked on, by both the Khronos organization
    for standardizing, and us as implementing it, and giving feedback on improving.
  February 28th, 2023, FFmpeg 6.0 "Von Neumann"
    A new major release, FFmpeg 6.0 "Von Neumann",
    is now available for download. This release has many new encoders and decoders, filters,
    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major
    releases will now bump the version of the ABI. We plan to have a new major release each
    year. Another release-specific change is that deprecated APIs will be removed after 3
    releases, upon the next major bump.
    This means that releases will be done more often and will be more organized.
  
    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.
    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c
    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,
    and the ability to pass option values for filters from a file. There are quite a few new audio
    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.
    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT
    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better
    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V
    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed
    in the doc/APIchanges file in our tree.
    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the
    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.
    Some highlights are:
  Radiance HDR image supportddagrab (Desktop Duplication) video capture filterffmpeg -shortest_buf_duration optionffmpeg now requires threading to be builtffmpeg now runs every muxer in a separate threadAdd new mode to cropdetect filter to detect crop-area based on motion vectors and edgesVAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9WBMP (Wireless Application Protocol Bitmap) image formatMicronas SC-4 audio decodernvenc AV1 encoding supportMediaCodec decoder via NDKMediaCodecQSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9showcwt multimedia filterWADY DPCM decoder and demuxerffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]hstack_vaapi, vstack_vaapi and xstack_vaapi filtersXMD ADPCM decoder and demuxerffmpeg CLI new option: -fix_sub_duration_heartbeatWavArc decoder and demuxerCrystalHD decoders deprecatedfiltergraph syntax in ffmpeg CLI now supports passing file contents as option valueshstack_qsv, vstack_qsv and xstack_qsv filters
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  July 22nd, 2022, FFmpeg 5.1 "Riemann"add ipfs/ipns protocol supportdialogue enhance audio filterdropped obsolete XvMC hwaccelDFPWM audio encoder/decoder and raw muxer/demuxerVizrt Binary Image encoder/decodercolorchart video source filterPGS subtitle frame merge bitstream filteradded chromakey_cuda filter
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  January 17th, 2022, FFmpeg 5.0 "Lorentz"FFmpeg 5.0 "Lorentz", a new
    major release, is now available! For this long-overdue release, a major effort
    underwent to remove the old encode/decode APIs and replace them with an
    N:M-based API, the entire libavresample library was removed, libswscale
    has a new, easier to use AVframe-based API, the Vulkan code was much improved,
    many new filters were added, including libplacebo integration, and finally,
    DoVi support was added, including tonemapping and remuxing. The default
    AAC encoder settings were also changed to improve quality.
    Some of the changelog highlights:
  ADPCM IMA Westwood encoderADPCM IMA Acorn Replay decoderArgonaut Games CVG demuxeraudio and video segment filtersApple Graphics (SMC) encoderhsvkey and hsvhold video filtersadecorrelate audio filterAV1 Low overhead bitstream format muxerhuesaturation video filtercolorspectrum source video filterRTP packetizer for uncompressed video (RFC 4175)VideoToolbox ProRes hwaccelaspectralstats audio filteradynamicsmooth audio filtervflip_vulkan, hflip_vulkan and flip_vulkan filtersadynamicequalizer audio filteryadif_videotoolbox filterVideoToolbox ProRes encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  
    We have a new IRC home at Libera Chat
    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at contact#IRCChannelsApril 8th, 2021, FFmpeg 4.4 "Rao"FFmpeg 4.4 "Rao", a new
    major release, is now available! Some of the highlights:
  AudioToolbox output deviceVDPAU accelerated HEVC 10/12bit decodingADPCM IMA Ubisoft APM encoderAV1 encoding support SVT-AV1ADPCM Argonaut Games encoderAV1 Low overhead bitstream format demuxerMobiClip FastAudio decoderAV1 decoder (Hardware acceleration used only)Argonaut Games BRP demuxerIPU decoder, parser and demuxerIntel QSV-accelerated AV1 decodingArgonaut Games Video decoderlibwavpack encoder removedAVS3 video decoder via libuavs3dVDPAU accelerated VP9 10/12bit decodingafreqshift and aphaseshift filtersHigh Voltage Software ADPCM encoderLEGO Racers ALP (.tun & .pcm) muxerDXVA2/D3D11VA hardware accelerated AV1 decodingMicrosoft Paint (MSP) version 2 decoderMicrosoft Paint (MSP) demuxerAV1 monochrome encoding support via libaom >= 2.0.1asuperpass and asuperstop filterDigital Pictures SGA demuxer and decodersTTML subtitle encoder and muxerRIST protocol via librist
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  June 15th, 2020, FFmpeg 4.3 "4:3"FFmpeg 4.3 "4:3", a new
    major release, is now available! Some of the highlights:
  Intel QSV-accelerated MJPEG decodingIntel QSV-accelerated VP9 decodingSupport for TrueHD in mp4Support AMD AMF encoder on Linux (via Vulkan)support Sipro ACELP.KELVIN decodingmaskedmin and maskedmax filtersQSV-accelerated VP9 encodingAV1 encoding support via librav1eAV1 frame merge bitstream filterMPEG-H 3D Audio support in mp4Argonaut Games ADPCM decoderArgonaut Games ASF demuxerafirsrc audio filter sourceSimon & Schuster Interactive ADPCM decoderHigh Voltage Software ADPCM decoderLEGO Racers ALP (.tun & .pcm) demuxerAMQP 0-9-1 protocol (RabbitMQ)avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filtersswitch from AvxSynth to AviSynth+ on LinuxExpanded styling support for 3GPP Timed Text Subtitles (movtext)Support for muxing pcm and pgs in m2tsCunning Developments ADPCM decoderPro Pinball Series Soundbank demuxerpcm_rechunk bitstream filtergradients source video filterMediaFoundation encoder wrapperSimon & Schuster Interactive ADPCM encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 5th, 2019, Bright Lights
  FFmpeg has added a realtime bright flash removal filter to libavfilter.
  
  Note that this filter is not FDA approved, nor are we medical professionals.
  Nor has this filter been tested with anyone who has photosensitive epilepsy.
  FFmpeg and its photosensitivity filter are not making any medical claims.
  
  That said, this is a new video filter that may help photosensitive people
  watch tv, play video games or even be used with a VR headset to block
  out epiletic triggers such as filtered sunlight when they are outside.
  Or you could use it against those annoying white flashes on your tv screen.
  The filter fails on some input, such as the
  Incredibles 2 Screen Slaver
  scene. It is not perfect. If you have other clips that you want this filter to
  work better on, please report them to us on our trac.
  
  We are not professionals. Please use this in your medical studies to
  advance epilepsy research. If you decide to use this in a medical
  setting, or make a hardware hdmi input output realtime tv filter,
  or find another use for this, please let me know.
  This filter was a feature request of mine
  since 2013.
  August 5th, 2019, FFmpeg 4.2 "Ada"FFmpeg 4.2 "Ada", a new
    major release, is now available! Some of the highlights:
  AV1 decoding support through libdav1dchromashift and rgbashift filterstruehd_core bitstream filterlibaribb24 based ARIB STD-B24 caption support (profiles A and C)Support decoding of HEVC 4:4:4 content in nvdec and cuviddecAV1 frame split bitstream filterSupport decoding of HEVC 4:4:4 content in vdpaushowspatial multimedia filtermov muxer writes tracks with unspecified language instead of English by defaultadded support for using clang to compile CUDA kernels
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  November 6th, 2018, FFmpeg 4.1 "al-Khwarizmi"aderivative and aintegral audio filterspal75bars and pal100bars video filter sourcesmbedTLS based TLS supportadeclick and adeclip filterslibtensorflow backend for DNN based filters like srcnnVC1 decoder is now bit-exactAVS2 video decoder via libdavs2Brooktree ProSumer video decoderMatchWare Screen Capture Codec decoderWinCam Motion Video decoderRemotelyAnywhere Screen Capture decoderSupport for AV1 in MP4 and Matroska/WebMAVS2 video encoder via libxavs2Block-Matching 3d (bm3d) denoising filteraudio denoiser as afftdn filterS12M timecode decoding in h264
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  April 20th, 2018, FFmpeg 4.0 "Wu"FFmpeg 4.0 "Wu", a new
    major release, is now available! Some of the highlights:
  Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streamsExperimental MagicYUV encoderIntel QSV-accelerated MJPEG encodingnative aptX and aptX HD encoder and decoderNVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decodingIntel QSV-accelerated overlay filterVAAPI MJPEG and VP8 decodingAMD AMF H.264 and HEVC encoderssupport LibreSSL (via libtls)Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.hilbert audio filter sourceRemoved the ffserver programRemoved the ffmenc and ffmdec muxer and demuxerVideoToolbox HEVC encoder and hwaccelVAAPI-accelerated ProcAmp (color balance), denoise and sharpness filterscodec2 en/decoding via libcodec2native SBC encoder and decoderhapqa_extract bitstream filterfilter_units bitstream filterAV1 Support through libaomE-AC-3 dependent frames supportbitstream filter for extracting E-AC-3 coreHaivision SRT protocol via libsrt
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 15th, 2017, FFmpeg 3.4 "Cantor"oscilloscope video filterupdate cuvid/nvenc headers to Video Codec SDK 8.0.14scale_cuda CUDA based video scale filterlibrsvg support for svg rasterizationspec compliant VP9 muxing support in MP4sofalizer filter switched to libmysofaGremlin Digital Video demuxer and decodersuperequalizer audio filteradditional frame format support for Interplay MVE moviessupport for decoding through D3D11VA in ffmpegDolby E decoder and SMPTE 337M demuxerunpremultiply video filterraw G.726 muxer and demuxer, left- and right-justifiedNewTek NDI input/output deviceVP9 tile threading supportV4L2 mem2mem HW assisted codecsRockchip MPP hardware decoding
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  April 13th, 2017, FFmpeg 3.3 "Hilbert"PSD (Photoshop Document) decoderFM Screen Capture decoderDNxHR decoder fixes for HQX and high resolution videosClearVideo decoder (partial)16.8 and 24.0 floating point PCM decoderIntel QSV-accelerated VP8 video decodingDNxHR 444 and HQX encodingQuality improvements for the (M)JPEG encoderVAAPI-accelerated MPEG-2 and VP8 encodingabitscope multimedia filterMPEG-7 Video Signature filteradd internal ebur128 library, remove external libebur128 dependencyIntel QSV video scaling and deinterlacing filtersSample Dump eXchange demuxerMIDI Sample Dump Standard demuxerScenarist Closed Captions demuxer and muxerSupport MOV with multiple sample description tablesPro-MPEG CoP #3-R2 FEC protocolSupport for spherical videosCrystalHD decoder moved to new decode APIconfigure now fails if autodetect-libraries are requested but not found
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  October 30th, 2016, Results: Summer Of Code 2016.
    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.
  
    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:
  FFv1 (Mentor: Michael Niedermayer)
    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.
  Self test coverage (Mentor: Michael Niedermayer)
    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.
  MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)
    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.
  Tee muxer improvements (Mentor: Marton Balint)
    Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.
  TrueHD encoder (Mentor: Rostislav Pehlivanov)
    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.
  Motion interpolation filter (Mentor: Paul B Mahol)
    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.
  
    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!
  September 24th, 2016, SDL1 support dropped.
    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of
    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device
    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output
    devices have been updated to support SDL2.
  August 9th, 2016, FFmpeg 3.1.2 "Laplace"FFmpeg 3.1.2, a new point release from the 3.1 release branch, is now available!
    It fixes several bugs.
  
    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.
  July 10th, 2016, ffserver program being dropped
    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.
    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat
    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has
    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.
    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs
    and to contact us so we may point users to test and contribute to its development.
  July 1st, 2016, FFmpeg 3.1.1 "Laplace"FFmpeg 3.1.1, a new point release from the 3.1 release branch, is now available!
    It mainly deals with a few ABI issues introduced in the previous release.
  
    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to
    upgrade unless they use current git master.
  June 27th, 2016, FFmpeg 3.1 "Laplace"DXVA2-accelerated HEVC Main10 decodingloop video filter and aloop audio filterBob Weaver deinterlacing filterprotocol blacklisting APIVC-2 HQ RTP payload format (draft v1) depacketizer and packetizerVP9 RTP payload format (draft v2) packetizerAudioToolbox audio decodersAudioToolbox audio encoderscoreimage filter (GPU based image filtering on OSX)bitstream filter for extracting DTS corehash and framehash muxersVAAPI-accelerated format conversion and scalinglibnpp/CUDA-accelerated format conversion and scalingDuck TrueMotion 2.0 Real Time decoderWideband Single-bit Data (WSD) demuxerVAAPI-accelerated H.264/HEVC/MJPEG encodingDTS Express (LBR) decoderGeneric OpenMAX IL encoder with support for Raspberry PiIFF ANIM demuxer & decoderDirect Stream Transfer (DST) decoderOpenExr improvements (tile data and B44/B44A support)BitJazz SheerVideo decoderCUDA CUVID H264/HEVC decoder10-bit depth support in native utvideo decoderlibutvideo wrapper removedYUY2 Lossless Codec decoderVideoToolbox H.264 encoder
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  March 16th, 2016, Google Summer of Code
    FFmpeg has been accepted as a Google Summer of Code open source organization. If you wish to
    participate as a student see our project ideas page.
    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.
    Good luck!
  February 15th, 2016, FFmpeg 3.0 "Einstein"
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  January 30, 2016, Removing support for two external AAC encoders
    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and
    libaacplus in FFmpeg master.
  
    Even before marking our internal AAC encoder as
    stable, it was known that libvo-aacenc
    was of an inferior quality compared to our native one for most samples.
    However, the VisualOn encoder was used extensively by the Android Open
    Source Project, and we would like to have a tested-and-true stable option
    in our code base.
  
    When first committed in 2011, libaacplus filled in the gap of encoding
    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported
    by any of the encoders in FFmpeg at that time.
  
    The circumstances for both have changed. After the work spearheaded by
    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC
    encoder is ready to compete with much more mature encoders. The Fraunhofer
    FDK AAC Codec Library for Android was added in 2012 as the fourth
    supported external AAC encoder, and the one with the best quality and the
    most features supported, including HE-AAC and HE-AACv2.
  
    Therefore, we have decided that it is time to remove libvo-aacenc and
    libaacplus. If you are currently using libvo-aacenc, prepare to transition
    to the native encoder () when updating to the next version
    of FFmpeg. In most cases it is as simple as merely swapping the encoder
    name. If you are currently using libaacplus, start using FDK AAC
    () with an appropriate  option
    to select the exact AAC profile that fits your needs. In both cases, you
    will enjoy an audible quality improvement and as well as fewer licensing
    headaches.
  January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10
    We have made several new point releases ().
    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.
    Please see the changelog for each release for more details.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  December 5th, 2015, The native FFmpeg AAC encoder is now stable!
    After seven years the native FFmpeg AAC encoder has had its experimental flag
    removed and declared as ready for general use. The encoder is transparent
    at 128kbps for most samples tested with artifacts only appearing in extreme
    cases. Subjective quality tests put the encoder to be of equal or greater
    quality than most of the other encoders available to the public.
  
    Licensing has always been an issue with encoding AAC audio as most of the
    encoders have had a license making FFmpeg unredistributable if compiled with
    support for them. The fact that there now exists a fully open and truly
    free AAC encoder integrated directly within the project means a lot to those
    who wish to use accepted and widespread standards.
  
    The majority of the work done to bring the encoder up to quality was started
    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.
    Both continued to work on the encoder with the latter joining as a developer
    and mainainer, working on other parts of the project as well. Also, thanks
    to Kamedo2 who does comparisons
    and tests, the original authors and all past and current contributors to the
    encoder. Users are suggested and encouraged to use the encoder and provide
    feedback or breakage reports through our bug tracker.
  
    A big thank you note goes to our newest supporters: MediaHub and Telepoint.
    Both companies have donated a dedicated server with free of charge internet
    connectivity. Here is a little bit about them in their own words:
  Telepoint is the biggest
        carrier-neutral data center in Bulgaria. Located in the heart of Sofia
        on a cross-road of many Bulgarian and International networks, the
        facility is a fully featured Tier 3 data center that provides flexible
        customer-oriented colocation solutions (ranging from a server to a
        private collocation hall) and a high level of security.
      
        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which
        uses FFmpeg heavily since it started operating a year ago. "Donating
        to help keep FFmpeg online is our way of giving back to the community"
        .
      
    Thanks Telepoint and MediaHub for their support!
  September 29th, 2015, GSoC 2015 results
    FFmpeg participated to the latest edition of
    the Google
    Summer of Code Project. FFmpeg got a total of 8 assigned
    projects, and 7 of them were successful.
  We want to thank Google, the
    participating students, and especially the mentors who joined this
    effort. We're looking forward to participating in the next GSoC
    edition!
  
    Below you can find a brief description of the final outcome of
    each single project.
  Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George
    Stephan Holljes's project for this session of Google Summer of Code was to
    implement basic HTTP server features for libavformat, to complement the
    already present HTTP client and RTMP and RTSP server code.
  
    The first part of the project was to make the HTTP code capable of accepting
    a single client; it was completed partly during the qualification period and
    partly during the first week of the summer. Thanks to this work, it is now
    possible to make a simple HTTP stream using the following commands:
      ffmpeg -i /dev/video0 -listen 1 -f matroska \
    -c:v libx264 -preset fast -tune zerolatency http://:8080
    ffplay http://localhost:8080/
  
    The next part of the project was to extend the code to be able to accept
    several clients, simultaneously or consecutively. Since libavformat did not
    have an API for that kind of task, it was necessary to design one. This part
    was mostly completed before the midterm and applied shortly afterwards.
    Since the ffmpeg command-line tool is not ready to serve several clients,
    the test ground for that new API is an example program serving hard-coded
    content.
  
    The last and most ambitious part of the project was to update ffserver to
    make use of the new API. It would prove that the API is usable to implement
    real HTTP servers, and expose the points where more control was needed. By
    the end of the summer, a first working patch series was undergoing code
    review.
  Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek
    Mariusz finished an API prepared by the FFmpeg community and implemented
    Samba directory listing as qualification task.
  
    During the program he extended the API with the possibility to
    remove and rename files on remote servers. He completed the
    implementation of these features for file, Samba, SFTP, and FTP
    protocols.
  
    At the end of the program, Mariusz provided a sketch of an
    implementation for HTTP directory listening.
  Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack
    Mate was working on directshow input from digital video sources. He
    got working input from ATSC input sources, with specifiable tuner.
  
    The code has not been committed, but a patch of it was sent to the
    ffmpeg-devel mailing list for future use.
  
    The mentor plans on cleaning it up and committing it, at least for the
    ATSC side of things. Mate and the mentor are still working trying to
    finally figure out how to get DVB working.
  Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale
    Niklesh's project was to expand our support for 3GPP Timed Text
    subtitles. This is the native subtitle format for mp4 containers, and
    is interesting because it's usually the only subtitle format supported
    by the stock playback applications on iOS and Android devices.
  
    ffmpeg already had basic support for these subtitles which ignored all
    formatting information - it just provided basic plain-text support.
  
    Niklesh did work to add support on both the encode and decode side for
    text formatting capabilities, such as font size/colour and effects like
    bold/italics, highlighting, etc.
  
    The main challenge here is that Timed Text handles formatting in a very
    different way from most common subtitle formats. It uses a binary
    encoding (based on mp4 boxes, naturally) and stores information
    separately from the text itself. This requires additional work to track
    which parts of the text formatting applies to, and explicitly dealing
    with overlapping formatting (which other formats support but Timed
    Text does not) so it requires breaking the overlapping sections into
    separate non-overlapping ones with different formatting.
  
    Finally, Niklesh had to be careful about not trusting any size
    information in the subtitles - and that's no joke: the now infamous
    Android stagefright bug was in code for parsing Timed Text subtitles.
  
    All of Niklesh's work is committed and was released in ffmpeg 2.8.
  libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla
    Pedro Arthur has modularized the vertical and horizontal scalers.
    To do this he designed and implemented a generic filter framework
    and moved the existing scaler code into it. These changes now allow
    easily adding removing, splitting or merging processing steps.
    The implementation was benchmarked and several alternatives were
    tried to avoid speed loss.
  
    He also added gamma corrected scaling support.
    An example to use gamma corrected scaling would be:
      ffmpeg -i input -vf scale=512:384:gamma=1 output
  
    Pedro has done impressive work considering the short time available,
    and he is a FFmpeg committer now. He continues to contribute to
    FFmpeg, and has fixed some bugs in libswscale after GSoC has
    ended.
  AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire
    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main
    prediction on the native AAC encoder. Of all those extensions, only
    TNS was left in a less-than-usable state, but the implementation has
    been pushed (disabled) anyway since it's a good basis for further
    improvements.
  
    PNS replaces noisy bands with a single scalefactor representing the
    energy of that band, gaining in coding efficiency considerably, and
    the quality improvements on low bitrates are impressive for such a
    simple feature.
  
    TNS still needs some polishing, but has the potential to reduce coding
    artifacts by applying noise shaping in the temporal domain (something
    that is a source of annoying, notable distortion on low-entropy
    bands).
  
    Intensity Stereo coding (I/S) can double coding efficiency by
    exploiting strong correlation between stereo channels, most effective
    on pop-style tracks that employ panned mixing. The technique is not as
    effective on classic X-Y recordings though.
  
    Finally, main prediction improves coding efficiency by exploiting
    correlation among successive frames. While the gains have not been
    huge at this point, Rostislav has remained active even after the GSoC,
    and is polishing both TNS and main prediction, as well as looking for
    further improvements to make.
  
    In the process, the MIPS port of the encoder was broken a few times,
    something he's also working to fix.
  Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol
    Donny Yang implemented basic keyframe only APNG encoder as the
    qualification task. Later he wrote interframe compression via
    various blend modes. The current implementation tries all blend
    modes and picks one which takes the smallest amount of memory.
  
    Special care was taken to make sure that the decoder plays
    correctly all files found in the wild and that the encoder
    produces files that can be played in browsers that support APNG.
  
    During his work he was tasked to fix any encountered bug in the
    decoder due to the fact that it doesn't match APNG
    specifications. Thanks to this work, a long standing bug in the
    PNG decoder has been fixed.
  
    For latter work he plans to continue working on the encoder,
    making it possible to select which blend modes will be used in the
    encoding process. This could speed up encoding of APNG files.
  September 9th, 2015, FFmpeg 2.8
    We published release  as new major version.
    It contains all features and bug fixes of the git master branch from September 8th. Please see
    the 
    for a list of the most important changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use current git master.
  August 1st, 2015, A message from the FFmpeg project
    Dear multimedia community,
  
    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has
    come by surprise. He has worked tirelessly on the FFmpeg project for many
    years and we must thank him for the work that he has done. We hope that in
    the future he will continue to contribute to the project. In the coming
    weeks, the FFmpeg project will be managed by the active contributors.
  
    The last four years have not been easy for our multimedia community - both
    contributors and users. We should now look to the future, try to find
    solutions to these issues, and to have reconciliation between the forks,
    which have split the community for so long.
  
    Unfortunately, much of the disagreement has taken place in inappropriate
    venues so far, which has made finding common ground and solutions
    difficult. We aim to discuss this in our communities online over the coming
    weeks, and in person at the VideoLAN Developer
    Days in Paris in September: a neutral venue for the entire open source
    multimedia community.
  July 4th, 2015, FFmpeg needs a new host We have received more than 7 offers for hosting and servers, thanks a lot to everyone!
    After graciously hosting our projects (FFmpeg, MPlayer
    and rtmpdump) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.
  
    If you want to host an open source project, please let us know, either on ffmpeg-devel
    mailing list or irc.freenode.net #ffmpeg-devel.
  
    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, trac, samples repo, svn, etc.
  March 16, 2015, FFmpeg 2.6.1
    We have made a new major release ()
    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  March 4, 2015, Google Summer of Code
    FFmpeg has been accepted as a Google Summer of Code Project. If you wish to
    participate as a student see our project ideas page.
    You can already get in contact with mentors and start working on qualification tasks. Registration
    at Google for students will open March 16th. Good luck!
  March 1, 2015, Chemnitzer Linux-Tage
    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage
    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.
  
    More information can be found here
    We demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes. If you have media files that cannot be
    processed correctly with FFmpeg, be sure to have a sample with you
    so we can have a look!
    For the first time in our CLT history, there will be an !
    You can read the details here.
    The workshop is targeted at FFmpeg beginners. First the basics of
    multimedia will be covered. Thereafter you will learn how to use
    that knowledge and the FFmpeg CLI tools to analyse and process media
    files. The workshop is in German language only and prior registration
    is necessary. The workshop will be on Saturday starting at 10 o'clock.
  
    We are looking forward to meet you (again)!
  December 5, 2014, FFmpeg 2.5
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 4th December.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  October 10, 2014, FFmpeg is in Debian unstable again
    We wanted you to know there are
    
    FFmpeg packages in Debian unstable again. A big thank-you
    to Andreas Cadhalpun and all the people that made it possible. It has been anything but simple.
  
    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't
    migrate to Debian testing to be in the upcoming release codenamed jessie.
    Read the argumentation over at Debian.However things will come out in the end, we hope for your continued remarkable support!October 8, 2014, FFmpeg secured a place in OPW!
    Thanks to a generous 6K USD donation by Samsung (Open Source Group),
    FFmpeg will be welcoming at least 1 "Outreach Program for Women" intern
    to work with our community for an initial period starting December 2014
    (through March 2015).
  
    We all know FFmpeg is used by the industry, but even while there are
    countless products building on our code, it is not at all common for
    companies to step up and help us out when needed. So a big thank-you
    to Samsung and the OPW program committee!
  
    If you are thinking on participating in OPW as an intern, please take
    a look at our OPW wiki page
    for some initial guidelines. The page is still a work in progress, but
    there should be enough information there to get you started. If you, on
    the other hand, are thinking on sponsoring work on FFmpeg through the
    OPW program, please get in touch with us at opw@ffmpeg.org. With your
    help, we might be able to secure some extra intern spots for this round!
  September 15, 2014, FFmpeg 2.4
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 14th September.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8
    We have made several new point releases ().
    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.
    Please see the changelog for more details.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  July 29, 2014, Help us out securing our spot in OPW
    Following our previous post regarding our participation on this year's
    OPW (Outreach Program for Women), we are now reaching out to our users
    (both individuals and companies) to help us gather the needed money to
    secure our spot in the program.
    We need to put together 6K USD as a minimum but securing more funds would
    help us towards getting more than one intern.
    You can donate by credit card using
    
    Click&Pledge and selecting the "OPW" option. If you would like to
    donate by money transfer or by check, please get in touch by
    e-mail and we will get back to you
    with instructions.Thanks!
  July 20, 2014, New website
    The FFmpeg project is proud to announce a brand new version of the website
    made by db0. While this was initially motivated
    by the need for a larger menu, the whole website ended up being redesigned,
    and most pages got reworked to ease navigation. We hope you'll enjoy
    browsing it.
  July 17, 2014, FFmpeg 2.3
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from the 16th July.
    Please see the  for a
    list of note-worthy changes.
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  July 3, 2014, FFmpeg and the Outreach Program For Women
    FFmpeg has started the process to become an OPW includer organization for the
    next round of the program, with internships starting December 9. The
    OPW aims to "Help women (cis and trans)
    and genderqueer to get involved in free and open source software". Part of the
    process requires securing funds to support at least one internship (6K USD), so
    if you were holding on your donation to FFmpeg, this is a great chance for you
    to come forward, get in touch and help both the project and a great initiative!
  
    We have set up an email address you can use
    to contact us about donations and general inquires regarding our participation
    in the program. Hope to hear from you soon!
  June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  
    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will
    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a
    different location closer to the city center.
  
    We will have a shared booth with XBMC and VideoLAN.
    
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    
    More information about LinuxTag can be found here
    We are looking forward to see you in Berlin!
  April 18, 2014, OpenSSL Heartbeat bug
    Our server hosting the Trac issue tracker was vulnerable to the attack
    against OpenSSL known as "heartbleed". The OpenSSL software library
    was updated on 7th of April, shortly after the vulnerability was publicly
    disclosed. We have changed the private keys (and certificates) for all
    FFmpeg servers. The details were sent to the mailing lists by
    Alexander Strasser, who is part of the project server team. Here is a
    link to the user mailing list
    archive
    .
  
    We encourage you to read up on
    "OpenSSL heartbleed".
    It is possible that login data for the issue tracker was exposed to
      people exploiting this security hole. You might want to change your password
      in the tracker and everywhere else you used that same password.April 11, 2014, FFmpeg 2.2.1
    We have made a new point releases ().
    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as
    several other fixes.
    See the git log for details.
  March 24, 2014, FFmpeg 2.2
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from 1st March.
    A partial list of new stuff is below:
      - HNM version 4 demuxer and video decoder
    - Live HDS muxer
    - setsar/setdar filters now support variables in ratio expressions
    - elbg filter
    - string validation in ffprobe
    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)
    - complete Voxware MetaSound decoder
    - remove mp3_header_compress bitstream filter
    - Windows resource files for shared libraries
    - aeval filter
    - stereoscopic 3d metadata handling
    - WebP encoding via libwebp
    - ATRAC3+ decoder
    - VP8 in Ogg demuxing
    - side & metadata support in NUT
    - framepack filter
    - XYZ12 rawvideo support in NUT
    - Exif metadata support in WebP decoder
    - OpenGL device
    - Use metadata_header_padding to control padding in ID3 tags (currently used in
    MP3, AIFF, and OMA files), FLAC header, and the AVI "junk" block.
    - Mirillis FIC video decoder
    - Support DNx444
    - libx265 encoder
    - dejudder filter
    - Autodetect VDA like all other hardware accelerations
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  February 3, 2014, Chemnitzer Linux-Tage
    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'
    in Chemnitz, Germany. The event will take place on 15th and 16th of March.
  
    More information can be found here
    We invite you to visit us at our booth located in the Linux-Live area!
    There we will demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes.
  
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    
    We are looking forward to meet you (again)!
  February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach
    The server on which FFmpeg and MPlayer Trac issue trackers were
    installed was compromised. The affected server was taken offline
    and has been replaced and all software reinstalled.
    FFmpeg Git, releases, FATE, web and mailinglists are on other servers
    and were not affected. We believe that the original compromise happened
    to a server, unrelated to FFmpeg and MPlayer, several months ago.
    That server was used as a source to clone the VM that we recently moved
    Trac to. It is not known if anyone used the backdoor that was found.
  
    We recommend all users to change their passwords.
    Especially users who use a password on Trac that they also use
      elsewhere, should change that password at least elsewhere.November 12, 2013, FFmpeg RFP in Debian
    Since the splitting of Libav the Debian/Ubuntu maintainers have followed
    the Libav fork. Many people have requested the packaging of ffmpeg in
    Debian, as it is more feature-complete and in many cases less buggy.
  Rogério Brito, a Debian developer,
    has proposed a Request For Package (RFP) in the Debian bug tracking
    system.
  
    Please let the Debian and Ubuntu developers know that you support packaging
    of the real FFmpeg! See Debian ticket #729203
    for more details.
  October 28, 2013, FFmpeg 2.1
    We have made a new major release ()
    It contains all features and bugfixes of the git master branch from 28th October.
    A partial list of new stuff is below:
      - aecho filter
    - perspective filter ported from libmpcodecs
    - ffprobe -show_programs option
    - compand filter
    - RTMP seek support
    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate
    even when used as an input option. Previous behavior can be restored with
    the -noaccurate_seek option.
    - ffmpeg -t option can now be used for inputs, to limit the duration of
    data read from an input file
    - incomplete Voxware MetaSound decoder
    - read EXIF metadata from JPEG
    - DVB teletext decoder
    - phase filter ported from libmpcodecs
    - w3fdif filter
    - Opus support in Matroska
    - FFV1 version 1.3 is stable and no longer experimental
    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support
    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be
    more consistent with other muxers.
    - adelay filter
    - pullup filter ported from libmpcodecs
    - ffprobe -read_intervals option
    - Lossless and alpha support for WebP decoder
    - Error Resilient AAC syntax (ER AAC LC) decoding
    - Low Delay AAC (ER AAC LD) decoding
    - mux chapters in ASF files
    - SFTP protocol (via libssh)
    - libx264: add ability to encode in YUVJ422P and YUVJ444P
    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does
    - make decoding alpha optional for prores, ffv1 and vp6 by setting
    the skip_alpha flag.
    - ladspa wrapper filter
    - native VP9 decoder
    - dpx parser
    - max_error_rate parameter in ffmpeg
    - PulseAudio output device
    - ReplayGain scanner
    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)
    - Linux framebuffer output device
    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4
    - mergeplanes filter
  
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  ]]></content:encoded></item><item><title>Does Rust complexity ever bother you?</title><link>https://www.reddit.com/r/rust/comments/1mx8izf/does_rust_complexity_ever_bother_you/</link><author>/u/GolangLinuxGuru1979</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 22 Aug 2025 14:47:30 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I'm a Go developer and I've always had a curiosity about Rust. I've tried to play around and start some personal project in it a few times. And it's mostly been ok. Like I tried to use hyper.rs a few times, but the boilerplate takes a lot to understand in many of the examples. I've tried to use tokio, but the library is massive, and it gets difficult to understand which modules to important and now important. On top of that it drastically change the async functonsI'm saying all that to say Rust is very complicated. And while I do think there is a fantastic langauge under all that complexity, it prohibitively complex. I do get it that memory safety in domains like RTOS systems or in government spaces is crucial. But it feels like Rust thought leaders are trying to get the language adopted in other domains. Which I think is a bit of an issue because you're not competing with other languages where its much easier to be productive in.Here is my main gripe with the adoption. Lots of influencers in the Rust space just seem to overlook its complexity as if its no big deal. Or you have others who embrace it because Rust "has to be complex". But I feel in the enterprise (where adoption matters most), no engineering manager is really going to adopt a language this complex.Now I understand languages like C# and Java can be complex as well. But Java at one time was looked at as a far simpler version of C++, and was an "Easy language". It would grow in complexity as the language grew and the same with C#. And then there is also tooling to kind of easy you into the more complex parts of these languages.I would love to see Rust adopted more, I would. But I feel advociates aren't leaning into its domain where its an open and shut case for (mission critical systems requiring strict safety standards). And is instead also trying to compete in spaces where Go, Javascript, Java already have a strong foothold.Again this is not to critcize Rust. I like the language. But I feel too many people in the Rust community talk around its complexity.]]></content:encoded></item><item><title>Cargo inspired C/C++ build tool, written in rust</title><link>https://github.com/EmVance1/VanGo</link><author>/u/MNGay</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 22 Aug 2025 14:17:47 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Using rust for the past 3 years or so got me thinking, why can't it always be this easy? Following this, I've spent the last 10 months (on-off due to studies) developing a tool for personal use, and I'd love to see what people think about it. Introducing VanGo, if you'll excuse the pun.]]></content:encoded></item><item><title>Go concurrency without the channel gymnastics</title><link>https://www.reddit.com/r/golang/comments/1mx7art/go_concurrency_without_the_channel_gymnastics/</link><author>/u/marketbase</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 22 Aug 2025 13:59:33 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey y’all. I noticed every time I fan-in / fan-out in Go, I end up writing the same channel boilerplate. Got tired of it, so I built a library to one-line the patterns.// Before sem := make(chan struct{}, 3) results := make(chan int, len(tasks)) for _, task := range tasks { sem <- struct{}{} go func(task func() (int, error)) { defer func() { <-sem }() result, err := task() if err != nil { // handle or ignore; kept simple here } results <- result }(task) } for range tasks { fmt.Println(<-results) } // After results, err := gliter.InParallelThrottle(3, tasks) // Before jobs := make(chan int, len(tasks)) results := make(chan int, len(tasks)) // fan-out for i := 0; i < 3; i++ { go worker(jobs, results) } // send jobs for _, job := range tasks { jobs <- job } close(jobs) // fan-in for range tasks { fmt.Println(<-results) } // After results, errors := gliter.NewWorkerPool(3, handler). Push(1, 2, 3, 4). Close(). Collect() Didn’t think it was special at first, but I keep reaching for it out of convenience. What do you think, trash or treasure?]]></content:encoded></item><item><title>Quickly navigate in man pages, using emacs, neovim or w3m.</title><link>https://codeberg.org/chimay/blog/src/commit/02bdd1d592f7130c2dd2cc13e35a63c551387e91/meta/man-pages.org</link><author>/u/orduval</author><category>dev</category><category>reddit</category><pubDate>Fri, 22 Aug 2025 13:56:17 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[
Neovim offers a nice man view with the  command.  It can handle
references by using  over the word, which can easily be remapped to
the return key.
It even has a table of content, but it's too cluttered for my taste, so I
decided to write my own version of it, displaying only the minimum I need.
First, let's write some functions in ~/.config/nvim/autoload/library.vim :## %
	# %
	##
Then, go back to ~/.config/nvim/init.vim and let's map the 
wrapper to e.g.  :#
Finally, use buffer local maps triggered when
entering a man buffer :####
Done! Now, try  and enter e.g.  to the prompt.  The key : opens the toc/link window closes the toc/link window deletes the man page bufferIn the man buffer, you can press  (enter) over a reference
(i.e. link) to follow it.]]></content:encoded></item><item><title>Centrally Collecting Events from Go Microservices</title><link>https://pliutau.com/centrally-collecting-events-in-go-microservices/</link><author>/u/der_gopher</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 22 Aug 2025 13:17:37 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Coding a database proxy for fun</title><link>https://www.youtube.com/watch?v=DU7_MQmRDUs</link><author>/u/der_gopher</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 22 Aug 2025 12:25:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How Does Google Docs Work 🔥</title><link>https://newsletter.systemdesign.one/p/how-does-google-docs-work</link><author>Neo Kim</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/4b73f9d4-a8d6-4101-9dff-df53a7332de1_1280x720.png" length="" type=""/><pubDate>Fri, 22 Aug 2025 11:50:45 +0000</pubDate><source url="https://newsletter.systemdesign.one/">Dev - System Design Newsletter</source><content:encoded><![CDATA[Unlock access to every deep dive article by becoming a paid subscriber:I spent hours studying how Google Docs works so you don't have to. And I wrote this newsletter to make the key concepts simple and easy for you.Note: This post is based on my research and may differ from real-world implementation.Once upon a time, there lived a data analyst named Maria.She emailed draft copies many times to different people to prepare monthly reports.So she wasted a ton of time and was frustrated.Until one day, when she decides to use Google Docs for it.Google Docs allows collaborative editing over the internet. It means many users can work on the same document in real-time.Yet it’s difficult to implement Google Docs correctly for 3 reasons:Concurrent changes to the same document should converge to the same version.Concurrent changes to the same document must avoid conflicts.Any changes should be visible in real-time to each user.Also a user should be able to make changes while they’re offline.A simple approach to handle concurrency is using pessimistic concurrency control.is amechanism for handling concurrency using a lock. It offers strong consistency, but doesn’t support collaborative editing in real-time. Because it needs a central coordinator to handle data changes, only 1 user can edit at a time. Put simply, only a single document copy is available for write operations at once, while other document copies are read-only.Besides it doesn’t support offline changes.Also a network round-trip across the Earth takes 200 milliseconds. This might cause a poor user experience. So they do  The idea is to keep a document copy for each user locally and then run operations locally for high responsiveness. Thus creating the illusion of lower latency than reality.And the system propagates the changes to all users for consistency.A simple approach for latency hiding is using the mechanism.Yet it resolves a conflict without waiting for coordination by applying the most recent update. So there’s a risk of data loss when there are concurrent changes in high-latency networks.It might be a good choice when concurrency is low. But it isn’t suitable for this use case.An alternative approach to latency hiding is through differential synchronization.It keeps a document copy for each user and tracks the changes locally. The system doesn’t send the entire document when something changes, but only the difference ().Yet there’s a performance overhead in sending a diff for every change. Also differential synchronization only tracks diffs, and not the reason behind a change. So conflict resolution might be difficult.While resolving conflicts manually affects the user experience.OT is an algorithm to show document changes without wait times on high-latency networks. It allows different document copies to accept write operations at once. Also it handles conflict resolution automatically without locks or user interventions. Besides OT tolerates divergence among document copies and converges them later.Think of operational transformation as an event-passing mechanism; it ensures each user has the same document state even with unsynchronized changes.With OT, the system saves each change as an event. Put simply, a change doesn’t affect the underlying character of a document; instead, it adds an event to the revision log. The system then displays the document by replaying the revision log from its start.Operational transformation saves a document as a set of operations, but it's complex to implement properly.How Does Google Docs WorkGoogle Docs uses a client-server architecture for simplicity.]]></content:encoded></item><item><title>How to run database migrations in Kubernetes</title><link>https://packagemain.tech/p/database-migrations-in-kubernetes</link><author>/u/der_gopher</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 22 Aug 2025 11:21:25 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[In the era of microservices and Kubernetes, managing database migrations has become more complex than ever. Traditional methods of running migrations during application startup are no longer sufficient. This article explores various approaches to handling database migrations in a Kubernetes environment, with a focus on Golang-based solutions.Kubernetes introduces new challenges for database migrations:Multiple replicas starting simultaneously.Need for coordination to avoid concurrent migrations.Separation of concerns between application and migration logic.postWidely used and supports numerous databases.Supports various migration sources (local files, S3, Google Storage).Supports main SQL databases.Allows migrations written in Go for complex scenarios.Flexible versioning schemas.Powerful database schema management toolSupports declarative and versioned migrations.Offers integrity checks and migration linting.Provides GitHub Actions and Terraform provider.A naive implementation would be to run the code of the migration directly inside your main function before you start your server.Example using golang-migrate:package main

import (
    "database/sql"
    "fmt"
    "log"
    "net/http"

    "github.com/golang-migrate/migrate/v4"
    "github.com/golang-migrate/migrate/v4/database/postgres"
    _ "github.com/golang-migrate/migrate/v4/source/file"
    _ "github.com/lib/pq"
)

func main() {
    // Database connection parameters
    url := "postgres://user:pass@localhost:5432/dbname"

    // Connect to the database
    db, err := sql.Open("postgres", url)
    if err != nil {
        log.Fatalf("could not connect to database: %v", err)
    }
    defer db.Close()

    // Run migrations
    if err := runMigrations(db); err != nil {
        log.Fatalf("could not run migrations: %v", err)
    }

    // Run the application, for example start the server
    if err := http.ListenAndServe(":8080", nil); err != nil {
        log.Fatalf("server failed to start: %v", err)
    }
}

func runMigrations(db *sql.DB) error {
    driver, err := postgres.WithInstance(db, &postgres.Config{})
    if err != nil {
        return fmt.Errorf("could not create database driver: %w", err)
    }

    m, err := migrate.NewWithDatabaseInstance(
        "file://migrations", // Path to your migration files
        "postgres",          // Database type
        driver,
    )
    if err != nil {
        return fmt.Errorf("could not create migrate instance: %w", err)
    }

    if err := m.Up(); err != nil && err != migrate.ErrNoChange {
        return fmt.Errorf("could not run migrations: %w", err)
    }

    log.Println("migrations completed successfully")
    return nil
}However, these could cause different issues like your migrations being slow and Kubernetes considering the pod didn’t start successfully and therefore killing it. You could run those migrations in a Go routine, but how do you handle failures then? In case when multiple pods are created at the same time, you would have a potential concurrency problem. It also means your migrations need to be inside your Docker image.initContainersIf the initContainer fails, the blue/green deployment from Kubernetes won’t go further and your previous pods stays where they are. It prevents having a newer version of the code without the planned migration. initContainers:
  - name: migrations
    image: migrate/migrate:latest
    command: ['/migrate']
    args: ['-source', 'file:///migrations', '-database','postgres://user:pass@db:5432/dbname', 'up']Kubernetes Job apiVersion: batch/v1
kind: Job
metadata:
  name: db-migrate
spec:
  template:
    spec:
      containers:
      - name: migrate
        image: your-migration-image:latest
        command: ['/app/migrate']You can also combine it with initContainers making sure that the pod starts only when the job is successful.initContainers:
  - name: migrations-wait
    image: ghcr.io/groundnuty/k8s-wait-for:v2.0
    args:
      - "job"
      - "my-migration-job"hooksapiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "mychart.fullname" . }}-migrations
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    spec:
      containers:
        - name: migrations
          image: your-migrations-image:tag
          command: ["./run-migrations.sh"]There are pre-install and post-install hooks. Decoupling Migrations from Application CodeCreate separate Docker image for migrations.Use tools like Atlas to manage migrations independently.Version Control for MigrationsStore migration files in your Git repository.Use sequential or timestamp-based versioning.Ensure migrations can be run multiple times without side effects.Implement and test rollback procedures for each migration.Use tools like Atlas Cloud for visibility into migration history.Managing database migrations in a Kubernetes environment requires careful planning and execution. By leveraging tools like golang-migrate, goose, or atlas, and following best practices, you can create robust, scalable, and maintainable migration strategies. Remember to decouple migrations from application code, use version control, and implement proper monitoring to ensure smooth database evolution in your Kubernetes-based architecture.]]></content:encoded></item><item><title>Dev Gets 4 Years For Creating Kill Switch On Ex-Employer&apos;s Systems</title><link>https://yro.slashdot.org/story/25/08/22/0039200/dev-gets-4-years-for-creating-kill-switch-on-ex-employers-systems?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Fri, 22 Aug 2025 10:00:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[Davis Lu, a former Eaton Corporation developer, has been sentenced to four years in prison for sabotaging his ex-employer's Windows network with malware and a custom kill switch that locked out thousands of employees once his account was disabled. The attack caused significant operational disruption and financial losses, with Lu also attempting to cover his tracks by deleting data and researching privilege escalation techniques. BleepingComputer reports: After a corporate restructuring and subsequent demotion in 2018, the DOJ says that Lu retaliated by embedding malicious code throughout the company's Windows production environment. The malicious code included an infinite Java thread loop designed to overwhelm servers and crash production systems. Lu also created a kill switch named "IsDLEnabledinAD" ("Is Davis Lu enabled in Active Directory") that would automatically lock all users out of their accounts if his account was disabled in Active Directory. When his employment was terminated on September 9, 2019, and his account disabled, the kill switch activated, causing thousands of users to be locked out of their systems.
 
"The defendant breached his employer's trust by using his access and technical knowledge to sabotage company networks, wreaking havoc and causing hundreds of thousands of dollars in losses for a U.S. company," said Acting Assistant Attorney General Matthew R. Galeotti. When he was instructed to return his laptop, Lu reportedly deleted encrypted data from his device. Investigators later discovered search queries on the device researching how to elevate privileges, hide processes, and quickly delete files. Lu was found guilty earlier this year of intentionally causing damage to protected computers. After his four-year sentence, Lu will also serve three years of supervised release following his prison term.]]></content:encoded></item><item><title>LabPlot: Free, open source and cross-platform Data Visualization and Analysis</title><link>https://labplot.org/</link><author>turrini</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 09:11:26 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In many cases, importing data into LabPlot for further analysis and visualization is the first step in the application: LabPlot supports many different formats (CSV, Origin, SAS, Stata, SPSS, MATLAB, SQL, JSON, binary, OpenDocument Spreadsheets (ods), Excel (xlsx), HDF5, MQTT, Binary Logging Format (BLF), FITS,…]]></content:encoded></item><item><title>Top Secret: Automatically filter sensitive information</title><link>https://thoughtbot.com/blog/top-secret</link><author>thunderbong</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 04:48:29 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[What happens when you’re dealing with free text? Filtering the entire string may
not be an option if an external API needs to process the value. Think chatbots or LLMs.You could use a regex to filter sensitive information (such as credit card
numbers or emails), but that won’t capture everything, since not all sensitive
information can be captured with a regex.Fortunately, named-entity recognition (NER) can be used to identify and
classify real-world objects, such as a person, or location. Tools like MITIE
Ruby make interfacing with NER models trivial.By using a combination of regex patterns and NER entities, Top Secret
effectively filters sensitive information from free text—here are some
real-world examples.If you want to see Top Secret in action, you might enjoy this live
stream. Otherwise, see the examples below.It’s not uncommon to send user data to chatbots. Since the data might be
free-form, we should be diligent about filtering it using the approach mentioned
above.However, it’s likely we’ll want to “restore” the filtered values when returning
a response from the chatbot. Top Secret returns a mapping that would
allow for this.The exchange might look something like this.Caller sends filtered text"Hi [PERSON_1]! How is the weather in [LOCATION_1] today?"
Caller can “restore” from the mapping
    Filtering conversation history
  When working with conversation state you should filter  message
before including it in the request. This ensures no sensitive data slips through
from previous messages. Here’s what that might look like.Top Secret can also be used as a validation tool to prevent storing sensitive
information in your database.If the validation is too strict, you can override or disable any of
the filters as needed. class Message < ApplicationRecord
   private
   def content_cannot_contain_sensitive_information
     return if result.mapping.empty?
     errors.add(:content, "contains the following sensitive information #{result.mapping.values.to_sentence}")
It’s our responsibility to protect user data. This is more important than ever
given the rise in popularity of chatbots and LLMs. Tools like Top Secret aim to
reduce this burden.We've been helping engineering teams deliver exceptional products for over 20 years. Our designers, developers, and product managers work closely with teams to solve your toughest software challenges through collaborative design and development. Learn more about us.]]></content:encoded></item><item><title>The issue of anti-cheat on Linux (2024)</title><link>https://tulach.cc/the-issue-of-anti-cheat-on-linux/</link><author>todsacerdoti</author><category>dev</category><category>hn</category><pubDate>Fri, 22 Aug 2025 01:09:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[If you switch to Linux today, you’ll probably be surprised by how many games run out of the box just fine (mostly due to the Windows compatibility layer Proton built right into Steam),  for basically all competitive multiplayer games that utilize any sort of anti-cheat technology.Now I can finally get to the point of the article…  As someone who uses Linux daily, I would love to see these games support it, but I just don’t see that happening any time soon. Many people in the Linux community are frustrated by the fact that these anti-cheat solutions are stopping them from playing their favorite games. It also doesn’t help that some are fear-mongering about kernel-level anti-cheat solutions and spreading misinformation.In this article, I want to give you a high-level overview of how modern anti-cheat solutions work (which will hopefully be understandable even for non-technical people) and then explain why anti-cheat solutions in their current state just cannot work on Linux, as well as what the alternatives are.What is a videogame cheat? We could talk for hours about whether all sorts of macros and exploits should be considered cheats, but the main thing that comes to people’s minds when talking about multiplayer games is an external program that somehow manipulates the game or reads information from the game to provide you with an advantage over others. A prime example of this would be a wallhack or aimbot.There are generally two ways you can go about this:() Have a completely separate process that copies memory between itself and the game.() Force the game to load a DLL file (a shared library file containing code) directly into the game, executing custom code from within the game.Unless you find some very niche way to load a DLL into the game, in both cases you will need the ability to read (and write) the game’s process memory.If you are not a programmer (or you are a JavaScript developer), you most likely don’t really know how memory management works on modern systems. Let’s imagine this situation: two programs are loaded in memory. What is stopping one program from directly accessing the memory of the other program?Virtual address space in Windows (source).While in the past it would have been perfectly possible to read (almost) any of the physical memory installed in the computer, nowadays OSes use virtual address spaces. I don’t want to go into the details of how this is handled, but all you need to know is that each program is isolated in its own address space and cannot access other programs’ memory unless it uses functions provided by the operating system itself, like  and .In order to use those two functions, you will need to open a handle to the process you want to read or write memory from. This handle will be specific to your process and represent the access rights that you have relative to the object it represents (in this case, the game process). Remember this for later, as it will be important.Modern anti-cheat solutions have three main goals:Block other processes from accessing the game’s memory whenever possible.Detect and ban anyone who tries to get around the blocking mentioned above.Once someone is banned, ensure that they cannot simply create a new game account and continue playing (HWID bans).This is usually achieved by multiple components working together. Let’s take a look at Easy Anti-Cheat as an example:Loader (usually  or )Game library ( and “invisible” module)Service ()Kernel-mode driver ()Without a kernel-mode driver, there is no way to  block memory access into the game. With the kernel-mode driver, though, it’s incredibly simple. All that the driver needs to do is register a callback for handle creation, filter out requests to open such handles to the game process, check the requested permissions, and if they allow memory access, either deny the request or lower the permissions. That way, no usermode process can now read or write the games memory. Same can be applied to module loading and file system access.Using open-source Cheat Engine to try to read protected game’s memory (all reads fail).So how can anyone get around it? They also  need to get their code into the kernel, which will open many ways for them to access the game memory.Notice how I highlighted “somehow”? That’s because Windows is a closed system where Microsoft has the control to decide who should get access to the kernel. All official kernel components are signed with Microsoft code signing certificates, so it’s trivial to verify their authenticity. All 3rd party drivers need to be signed with an EV code signing certificate (which can only be bought by companies) and then go through the Hardware Developer Center certification so they can even be loaded. I am not saying this is perfect; in fact, I will most likely be writing an article about how bad actors are still getting their stuff certified. However, when they do, it usually gets quickly revoked, and it’s so costly and complicated that most don’t even bother trying.There is, of course, a way to get around it by using all sorts of exploits or by using vulnerable drivers (drivers that expose a programming interface to user-mode processes without any checks in place, which allows them to escalate their privileges and possibly even manipulate kernel components). This is where the second goal defined above comes in. The anti-cheat has to actively scan the system and try to find code that is not associated with any legitimate module (a module that was loaded properly, with all certs in place) and other modifications or patches that would otherwise not be there.While most gamers are going to say that those anti-cheats are useless and that they see cheaters left and right, the truth is that they add a huge skill check, so not everyone is able to write a cheat and then not get banned. In fact, if done properly, the cheating problem can be basically eliminated this way (I’ll get to this later).Another reason to run in the kernel is HWID (hardware identifier) banning (the 3rd point mentioned above). If a player is banned and creates a new account, playing on the same hardware will result in an immediate ban. Since the anti-cheat has a kernel component, it can directly talk to the hardware and read its serials that way. If it was running only as a user-mode process, it would be trivial to fake the serial reads. I am not personally a big fan of this since, as you can imagine, it can result in all sorts of unintended issues (people buying used hardware), but in reality, it’s not really a problem since those HWID bans usually expire after a few months (the game devs won’t tell you this though 😉).If I had to pick a game which handles cheating the best, then as of now in my humble opinion it would be Valorant by Riot Games. Keep in mind the stuff that you’ve just read and let me explain:The anti-cheat is loaded on boot. While scary for some, this allows them to block/detect the previously mentioned vulnerable drivers and exploits. This raises the skill required to write a cheat for the game even higher (usually, people resort to bootkits).The kernel driver then doesn’t do anything apart from logging (locally). When the game is actually started, it goes through those logs and figures out if the game launch should be allowed or not and does all the kernel protection stuff mentioned above.More advanced methods to obtain HWID are used, such as reading TPM EK, which is very hard to spoof properly.But that’s not all. If that was all there was to it, other anti-cheats would be just as effective. The anti-cheat team closely works with the game development team as well. How? The anti-cheat introduces extra protection for certain memory regions of the game. Some game data are encrypted, and the encryption keys change with every (even small) game update, making it really annoying for cheat developers. On top of all that, the team is very active in the cheating communities to get intel about what they are up to.I have played Valorant quite extensively, all the way from Silver to Ascendant, and I have yet to meet a cheater.There are two main concerns that people have with those kernel-mode anti-cheats:They are in the kernel doing in-depth scans; therefore, they must be vulnerable and a security issue.They are so deep in the system (and some start on system boot) that they can spy on us without us noticing.Let me ask you a question. How many vulnerable drivers (yes, those that can be abused by bad actors to gain kernel access) do you think the average gamer has on their Windows install? I’ll start with my own system. This is what I can immediately think of:If I looked hard enough, I would most likely find more.It would be really stupid of me to just point to random crap you could have on your computer and say “you have so much exploitable stuff, don’t even bother with security,” and that’s not what I am trying to say. Or maybe it is, but just a little bit… What I am trying to say is that there are many ways a malicious actor could do bad stuff with your system, but anti-cheat is very unlikely to have anything to do with it. In fact, I personally trust those anti-cheat developers much more than random vendors, since they are going to be very well aware of the possible abuse.Overall, the Windows driver ecosystem is a mess, but unfortunately, that is not going to change any time soon.As someone who is very well versed in Windows internals, I can tell you one thing, it doesn’t make sense. If you give the program administrative permissions (at least once), it can spy on you in the same way a kernel-mode driver could. There is absolutely no difference and it’s significantly easier to just write a standalone program. There are people who don’t want to play games because of their connection to Tencent (for example), but if it wasn’t for the kernel-mode anti-cheat, they would have no problem with it. Isn’t it a bit hypocritical? If the game company wanted to spy on you, they could have done so from the game process or the service they have most likely installed for DRM purposes.Oh and just by the way, the vast majority of the data networked by those previously mentioned anti-cheats to their respective servers comes from their usermode component. The only thing that’s sent “by the kernel component” (in quotes since the usermode service requests the data from the driver and then networks it, drivers cannot directly network data) is the HWID mentioned multiple times above and then detections (something that’s out of the ordinary). There is really not some magic data grabbing happening that’s only possible in the kernel.Another thing that is sometimes mentioned is that since it’s in the kernel, it would be harder for security researchers to debug and assess the possible spying. While technically true that it’s harder, it’s definitely not impossible or problematic for an experienced person, so trust me, security researchers and  the entire cheating community keep a close eye on it, in the same way they do on the usermode components.Congratulations, you have successfully made it. You have read all of the stuff and now we can finally get to the Linux part of this post 🎉.As you can probably already tell by the extensive rant above, I don’t have much good news. Linux is an open system. There is no central authority like on Windows that would tell you what you can and what you cannot do in the kernel. This obviously has countless advantages and it’s why so many people (and big corporations) love it, but is also the reason why anti-cheats cannot really function like they do on Windows.There is no way for them to block or detect memory access into the game. Anything you could think of would just not work. Kernel module? Just recompile the kernel and change the functions it uses to hide the possible cheat and bypass all checks. Mandatory kernel patch? Same thing. What about usermode detections? Just run the game in a fakeroot environment while the cheat runs with real root privileges, being hidden from the game completely… Mandatory custom kernel build? Entire Linux system dedicated to the anti-cheat? I mean… that could work, but at that point, you can just install Windows.There have been attempts to get anti-cheat to work on Linux. Easy Anti-Cheat is the most prominent one. Developers can choose whether they want to allow it to run on Linux or not. Linux gamers look at this and use it as an argument that anti-cheat on Linux does not face any issues, but the truth is that apart from the most basic sanity checks, EAC does absolutely nothing on Linux. It’s just a simple module that facilitates the server connection and data encryption/decryption for the game.One of the games that allowed EAC to run under Wine/Proton is Apex Legends. I won’t be putting any links here, but if you search GitHub for cheats for this game, you will find many that work on Linux and there is absolutely no anti-cheat bypass required. It just works.As mentioned above, if you want to achieve the best results, you need to utilize both the  and  measures. Active being the kernel component on Windows blocking memory access and trying to find possible discrepancies. Passive being the code virtualization, obfuscation, game data encryption as well as proper game networking and server-sided checks.An example of how  to utilize kernel-mode anti-cheat would be Fall Guys (yes, that’s the game that one friend made you buy just so you could play it for 30 minutes and then never open again). This game is very specific. There would be no gain in having some sort of wallhack, there would be no gain in having any sort of aimbot (you don’t aim at stuff). All that people did was speedhacking and modifying the game in a way that allowed them to jump higher and generally change their movement. This game is a prime example of why you should write your network code properly. If the game had proper networking and server checks in place (tick-based system, actions performed on both the client and server, if there is a mismatch, the server is the authority and resets the player - that’s how CS:GO did it, and that’s why people were not flying over the map in that game or speedhacking, it had other issues though), there would be no need for anti-cheat. Not even a usermode one. Instead, they fixed absolutely nothing from their side and slapped Easy Anti-Cheat on top of their game.While it’s not really possible to do any of the previously mentioned active measures, there is nothing stopping you from utilizing the passive ones. So, if you are a game developer and want to limit cheating in your game on Linux:Write proper networking code, verify data sent by the client so your game server does not blindly accept mach 8 as a walking speed.Use code obfuscation and virtualization as much as possible (be aware of the performance penalty, be smart about what parts of the code you protect), try to change it a bit with every update (commercial bin2bin obfuscators like VMProtect or Themida will produce different results on each run).If you have control over the game engine itself, try to keep sensitive information on the stack as much as possible.]]></content:encoded></item><item><title>Show HN: Splice – CAD for Cable Harnesses and Electrical Assemblies</title><link>https://splice-cad.com/</link><author>djsdjs</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 21:10:34 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ChatLoopBackOff: Episode 67 (Kserve)</title><link>https://www.youtube.com/watch?v=BjXZxUR8NMo</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/BjXZxUR8NMo?version=3" length="" type=""/><pubDate>Thu, 21 Aug 2025 20:33:45 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Join us LIVE as CNCF Ambassador Shivay Lamaba dives into KServe, the open source project designed for scalable and reliable ML models serving on Kubernetes.

Shivay will be  exploring the project for the very first time, right alongside you. Expect a hands-on walkthrough of the docs, community resources, and real-world use cases that make KServe a key piece in the cloud native AI/ML ecosystem.
If you’re curious about production-grade model inference, want to see how cloud native communities approach machine learning workloads, or just enjoy watching an experienced open source explorer break down a CNCF project live, this session is for you.

Bring your questions, share your experiences, and learn in real time as we explore KServe together!]]></content:encoded></item><item><title>Show HN: ChartDB Cloud – Visualize and Share Database Diagrams</title><link>https://app.chartdb.io/</link><author>Jonathanfishner</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 13:01:11 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AWS CEO says using AI to replace junior staff is &apos;Dumbest thing I&apos;ve ever heard&apos;</title><link>https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/</link><author>JustExAWS</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 12:53:16 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Amazon Web Services CEO Matt Garman has suggested firing junior workers because AI can do their jobs is "the dumbest thing I've ever heard."Garman made that remark in conversation with AI investor Matthew Berman, during which he talked up AWS’s Kiro AI-assisted coding tool and said he's encountered business leaders who think AI tools "can replace all of our junior people in our company."That notion led to the “dumbest thing I've ever heard” quote, followed by a justification that junior staff are “probably the least expensive employees you have” and also the most engaged with AI tools.“How's that going to work when ten years in the future you have no one that has learned anything,” he asked. “My view is you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it, just as much as you ever have.”Naturally he thinks AI – and Kiro, natch – can help with that education.Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization.“It’s a silly metric,” he said, because while organizations can use AI to write “infinitely more lines of code” it could be bad code.“Often times fewer lines of code is way better than more lines of code,” he observed. “So I'm never really sure why that's the exciting metric that people like to brag about.”That said, he’s seen data that suggests over 80 percent of AWS’s developers use AI in some way.“Sometimes it's writing unit tests, sometimes it's helping write documentation, sometimes it's writing code, sometimes it's kind of an agentic workflow” in which developers collaborate with AI agents.Garman said usage of AI tools by AWS developers increases every week.The CEO also offered some career advice for the AI age, suggesting that kids these days need to learn how to learn – and not just learn specific skills.“I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?”Garman thinks that approach is necessary because technological development is now so rapid it’s no longer sensible to expect that studying narrow skills can sustain a career for 30 years. He wants educators to instead teach “how do you think and how do you decompose problems”, and thinks kids who acquire those skills will thrive. ®]]></content:encoded></item><item><title>Show HN: Using Common Lisp from Inside the Browser</title><link>https://turtleware.eu/posts/Using-Common-Lisp-from-inside-the-Browser.html</link><author>jackdaniel</author><category>dev</category><category>hn</category><pubDate>Thu, 21 Aug 2025 12:08:30 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[ Written on 2025-08-21 by Daniel Kochmański Web Embeddable Common Lisp is a project that brings Common Lisp and the Web
Browser environments together. In this post I'll outline the current progress of
the project and provide some technical details, including current caveats and
future plans.It is important to note that this is not a release and none of the described
APIs and functionalities is considered to be stable. Things are still changing
and I'm not accepting bug reports for the time being.The easiest way to use Common Lisp on a website is to include WECL and insert
script tags with a type "text/common-lisp". When the attribute src is present,
then first the runtime loads the script from that url, and then it executes the
node body. For example create and run this HTML document from localhost:<!doctype html>
<html>
  <head>
    <title>Web Embeddable Common Lisp</title>
    <link rel="stylesheet" href="https://turtleware.eu/static/misc/wecl-20250821/easy.css" />
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/boot.js"></script>
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/wecl.js"></script>
  </head>
  <body>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/easy.lisp" id='easy-script'>
(defvar *div* (make-element "div" :id "my-ticker"))
(append-child [body] *div*)

(dotimes (v 4)
  (push-counter v))

(loop for tic from 6 above 0
      do (replace-children *div* (make-paragraph "~a" tic))
         (js-sleep 1000)
      finally (replace-children *div* (make-paragraph "BOOM!")))

(show-script-text "easy-script")
    </script>
  </body>
</html>
We may use Common Lisp that can call to JavaScript, and register callbacks to be
called on specified events. The source code of the script can be found here:Because the runtime is included as a script, the browser will usually cache the
~10MB WebAssembly module.The initial foreign function interface has numerous macros defining wrappers
that may be used from Common Lisp or passed to JavaScript.Summary of currently available operators: an inlined expression, like  an object referenced from the object store a function a method of the argument, like  a slot reader of the argument a slot writer of the first argument combines define-js-getter and define-js-setter template for JavaScript expressions Common Lisp function reference callable from JavaScript anonymous Common Lisp function reference (for closures)Summary of argument types:Common Lisp object referenceJavaScript object referenceAll operators, except for  have a similar lambda list:(DEFINE-JS NAME-AND-OPTIONS [ARGUMENTS [,@BODY]])The first argument is a list  that is common to all
defining operators: Common Lisp symbol denoting the object a string denoting the JavaScript expression, i.e "innerText" a type of the object returned by executing the expression(define-js-variable ([document] :js-expr "document" :type :symbol))
;; document
(define-js-object ([body] :js-expr "document.body" :type :js-ref))
;; wecl_ensure_object(document.body) /* -> id   */
;; wecl_search_object(id)            /* -> node */
The difference between a variable and an object in JS-FFI is that variable
expression is executed each time when the object is used (the expression is
inlined), while the object expression is executed only once and the result is
stored in the object store.The second argument is a list of pairs . Names will be used in the
lambda list of the operator callable from Common Lisp, while types will be used
to coerce arguments to the type expected by JavaScript.(define-js-function (parse-float :js-expr "parseFloat" :type :js-ref)
    ((value :string)))
;; parseFloat(value)

(define-js-method (add-event-listener :js-expr "addEventListener" :type :null)
    ((self :js-ref)
     (name :string)
     (fun :js-ref)))
;; self.addEventListener(name, fun)

(define-js-getter (get-inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)))
;; self.innerText

(define-js-setter (set-inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)
     (new :string)))
;; self.innerText = new

(define-js-accessor (inner-text :js-expr "innerText" :type :string)
    ((self :js-ref)
     (new :string)))
;; self.innerText
;; self.innerText = new

(define-js-script (document :js-expr "~a.forEach(~a)" :type :js-ref)
    ((nodes :js-ref)
     (callb :object)))
;; nodes.forEach(callb)
The third argument is specific to callbacks, where we define Common Lisp body of
the callback. Argument types are used to coerce values from JavaScript to Common
Lisp.(define-js-callback (print-node :type :object)
    ((elt :js-ref)
     (nth :fixnum)
     (seq :js-ref))
  (format t "Node ~2d: ~a~%" nth elt))

(let ((start 0))
  (add-event-listener *my-elt* "click"
                      (lambda-js-callback :null ((event :js-ref)) ;closure!
                        (incf start)
                        (setf (inner-text *my-elt*)
                              (format nil "Hello World! ~a" start)))
Note that callbacks are a bit different, because  does not
accept  option and  has unique lambda list. It is
important for callbacks to have an exact arity as they are called with, because
JS-FFI does not implement variable number of arguments yet.Callbacks can be referred by name with an operator .While working on FFI I've decided to write an adapter for SLIME/SWANK that will
allow interacting with WECL from Emacs. The principle is simple: we connect with
a websocket to Emacs that is listening on the specified port (i.e on localhost).
This adapter uses the library  written by Andrew Hyatt.It allows for compiling individual forms with , but file compilation
does not work (because files reside on a different "host"). REPL interaction
works as expected, as well as SLDB. The connection may occasionally be unstable,
and until Common Lisp call returns, the whole page is blocked. Notably waiting
for new requests is not a blocking operation from the JavaScript perspective,
because it is an asynchronous operation.;;; Patches for SLIME 2.31 (to be removed after the patch is merged).
;;; It is assumed that SLIME is already loaded into Emacs.
(defun slime-net-send (sexp proc)
  "Send a SEXP to Lisp over the socket PROC.
This is the lowest level of communication. The sexp will be READ and
EVAL'd by Lisp."
  (let* ((payload (encode-coding-string
                   (concat (slime-prin1-to-string sexp) "\n")
                   'utf-8-unix))
         (string (concat (slime-net-encode-length (length payload))
                         payload))
         (websocket (process-get proc :websocket)))
    (slime-log-event sexp)
    (if websocket
        (websocket-send-text websocket string)
      (process-send-string proc string))))

(defun slime-use-sigint-for-interrupt (&optional connection)
  (let ((c (or connection (slime-connection))))
    (cl-ecase (slime-communication-style c)
      ((:fd-handler nil) t)
      ((:spawn :sigio :async) nil))))
;;; lime.el --- Lisp Interaction Mode for Emacs -*-lexical-binding:t-*-
;;; 
;;; This program extends SLIME with an ability to listen for lisp connections.
;;; The flow is reversed - normally SLIME is a client and SWANK is a server.

(require 'websocket)

(defvar *lime-server* nil
  "The LIME server.")

(cl-defun lime-zipit (obj &optional (start 0) (end 72))
  (let* ((msg (if (stringp obj)
                  obj
                (slime-prin1-to-string obj)))
         (len (length msg)))
    (substring msg (min start len) (min end len))))

(cl-defun lime-message (&rest args)
  (with-current-buffer (process-buffer *lime-server*)
    (goto-char (point-max))
    (dolist (arg args)
      (insert (lime-zipit arg)))
    (insert "\n")
    (goto-char (point-max))))

(cl-defun lime-client-process (client)
  (websocket-conn client))

(cl-defun lime-process-client (process)
  (process-get process :websocket))

;;; c.f slime-net-connect
(cl-defun lime-add-client (client)
  (lime-message "LIME connecting a new client")
  (let* ((process (websocket-conn client))
         (buffer (generate-new-buffer "*lime-connection*")))
    (set-process-buffer process buffer)
    (push process slime-net-processes)
    (slime-setup-connection process)
    client))

;;; When SLIME kills the process, then it invokes LIME-DISCONNECT hook.
;;; When SWANK kills the process, then it invokes LIME-DEL-CLIENT hook.
(cl-defun lime-del-client (client)
  (when-let ((process (lime-client-process client)))
    (lime-message "LIME client disconnected")
    (slime-net-sentinel process "closed by peer")))

(cl-defun lime-disconnect (process)
  (when-let ((client (lime-process-client process)))
    (lime-message "LIME disconnecting client")
    (websocket-close client)))

(cl-defun lime-on-error (client fun error)
  (ignore client fun)
  (lime-message "LIME error: " (slime-prin1-to-string error)))

;;; Client sends the result over a websocket. Handling responses is implemented
;;; by SLIME-NET-FILTER. As we can see, the flow is reversed in our case.
(cl-defun lime-handle-message (client frame)
  (let ((process (lime-client-process client))
        (data (websocket-frame-text frame)))
    (lime-message "LIME-RECV: " data)
    (slime-net-filter process data)))

(cl-defun lime-net-listen (host port &rest parameters)
  (when *lime-server*
    (error "LIME server has already started"))
  (setq *lime-server*
        (apply 'websocket-server port
               :host host
               :on-open    (function lime-add-client)
               :on-close   (function lime-del-client)
               :on-error   (function lime-on-error)
               :on-message (function lime-handle-message)
               parameters))
  (unless (memq 'lime-disconnect slime-net-process-close-hooks)
    (push 'lime-disconnect slime-net-process-close-hooks))
  (let ((buf (get-buffer-create "*lime-server*")))
    (set-process-buffer *lime-server* buf)
    (lime-message "Welcome " *lime-server* "!")
    t))

(cl-defun lime-stop ()
  (when *lime-server*
   (websocket-server-close *lime-server*)
   (setq *lime-server* nil)))
After loading this file into Emacs invoke (lime-net-listen "localhost" 8889).
Now our Emacs listens for new connections from SLUG (the lisp-side part adapting
SWANK, already bundled with WECL). There are two SLUG backends in a repository: for web browser environment for Common Lisp runtime (uses )Now you can open a page listed here and connect to SLIME:<!doctype html>
<html>
  <head>
    <title>Web Embeddable Common Lisp</title>
    <link rel="stylesheet" href="easy.css" />
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/boot.js"></script>
    <script type="text/javascript" src="https://turtleware.eu/static/misc/wecl-20250821/wecl.js"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/slug.lisp"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/wank.lisp"></script>
    <script type="text/common-lisp" src="https://turtleware.eu/static/misc/wecl-20250821/easy.lisp">
      (defvar *connect-button* (make-element "button" :text "Connect"))
      (define-js-callback (connect-to-slug :type :null) ((event :js-ref))
        (wank-connect "localhost" 8889)
        (setf (inner-text *connect-button*) "Crash!"))
      (add-event-listener *connect-button* "click" (js-callback connect-to-slug))
      (append-child [body] *connect-button*)
    </script>
  </head>
  <body>
  </body>
</html>
This example shows an important limitation –  does not allow for
multiple asynchronous contexts in the same thread. That means that if Lisp call
doesn't return (i.e because it waits for input in a loop), then we can't execute
other Common Lisp statements from elsewhere because the application will crash.Here's another example. It is more a cool gimmick than anything else, but let's
try it. Open a console on this very website (on firefox C-S-i) and execute:function inject_js(url) {
    var head = document.getElementsByTagName('head')[0];
    var script = document.createElement('script');
    head.appendChild(script);
    script.type = 'text/javascript';
    return new Promise((resolve) => {
        script.onload = resolve;
        script.src = url;
    });
}

function inject_cl() {
    wecl_eval('(wecl/impl::js-load-slug "https://turtleware.eu/static/misc/wecl-20250821")');
}

inject_js('https://turtleware.eu/static/misc/wecl-20250821/boot.js')
    .then(() => {
        wecl_init_hooks.push(inject_cl);
        inject_js('https://turtleware.eu/static/misc/wecl-20250821/wecl.js');
    });
With this, assuming that you've kept your LIME server open, you'll have a REPL
onto uncooperative website. Now we can fool around with queries and changes:(define-js-accessor (title :js-expr "title" :type :string)
  ((self :js-ref)
   (title :string)))

(define-js-accessor (background :js-expr "body.style.backgroundColor" :type :string)
  ((self :js-ref)
   (background :string)))

(setf (title [document]) "Write in Lisp!")
(setf (background [document]) "#aaffaa")
The first thing to address is the lack of threading primitives. Native threads
can be implemented with web workers, but then our GC wouldn't know how to stop
the world to clean up. Another option is to use cooperative threads, but that
also won't work, because Emscripten doesn't support independent asynchronous
contexts, nor ECL is ready for that yet.I plan to address both issues simultaneously in the second stage of the project
when I port the runtime to WASI. We'll be able to use browser's GC, so running
in multiple web workers should not be a problem anymore. Unwinding and rewinding
the stack will require tinkering with ASYNCIFY and I have somewhat working green
threads implementation in place, so I will finish it and upstream in ECL.Currently I'm focusing mostly on having things working, so JS and CL interop is
brittle and often relies on evaluating expressions, trampolining and coercing.
That impacts the performance in a significant way. Moreover all loaded scripts
are compiled with a one-pass compiler, so the result bytecode is not optimized.There is no support for loading cross-compiled files onto the runtime, not to
mention that it is not possible to precompile systems with ASDF definitions.JS-FFI requires more work to allow for defining functions with variable number
of arguments and with optional arguments. There is no dynamic coercion of
JavaScript exceptions to Common Lisp conditions, but it is planned.]]></content:encoded></item><item><title>I Created the most Comprehensive LLD Interview Resource</title><link>https://blog.algomaster.io/p/launching-premium-lld-resource</link><author>Ashish Pratap Singh</author><category>dev</category><enclosure url="https://substack-post-media.s3.amazonaws.com/public/images/1bdf8340-7a0f-42e0-8609-c5174fb17828_2048x1426.jpeg" length="" type=""/><pubDate>Thu, 21 Aug 2025 12:02:00 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[It’s one of the most comprehensive and high quality resource you can find online with support for 5 programming languages — Java, Python, C++, C#, and TypeScript.Many chapters are . To unlock full access, you would need to become a  to the newsletter. and  and other important  (with real-world examples)40+ LLD interview problems (with more added over time), with UML class diagrams and design patterns explained in context.Support for Java, Python, C++,  C#, and TypeScriptBuilt-in  and  where you can edit, run and see the solution output directly on the site (supports Java, Python, C++,  and C#) to test your understanding.I’ve poured a lot of thought and effort into making this course as useful and practical as possible. I truly hope you’ll find it valuable in your interview prep journey.I will keep making improvements and enhancements to this resource over time.You may already know my , which is one of the most popular resources to learn LLD. This course takes it to the next level, offering a far better reading experience, focused specifically on interview prep. I have also updated the solutions in the Github repository with more design patterns and class diagrams.Starting , subscription pricing will increase:Subscribe now to lock in the current price.All existing paid subscribers will continue at their current rate.💎 New: Lifetime Access PlanYou can now get  to all current and future AlgoMaster premium content for a .For any questions related to content or subscription, please reply to this email or reach out at ]]></content:encoded></item></channel></rss>