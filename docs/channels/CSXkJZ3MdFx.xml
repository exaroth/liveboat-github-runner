<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Single Responsibility Principle (SRP) In React: Write Focused Components</title><link>https://thetshaped.dev/p/single-responsibility-principle-srp-in-react-write-focused-components</link><author>/u/pepincho</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 16:57:22 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[SOLID PrinciplesSOLIDSOLIDBy definition, the principle states:”A class should only have one reason to change.”In other words, a class, function, or module should have a single responsibility.Let’s first see how things look like when we violate the SRP in React and write not so good enough components.The problem with this component is that it:handles data fetching for the productsmanages the loading and error states handles the form for adding a new producttakes care of the display and layout of the productsProductsDashboard Componentharder to understand, maintain, debug, test, and reuseBe the Senior who delivers the standard for writing robust React Components.separate focused componentsSeparate data handling from User InterfaceWrite small and focused componentsPrefer composing a set of smaller componentsOrganize your components in terms of layersThat's all for today. I hope this was helpful. ✌️How I can help you further?Become the Senior React Engineer at your company! 🚀I share daily practical tips to level up your skills and become a better engineer.Thank you for being a great supporter, reader, and for your help in growing to 18.3K+ subscribers this week 🙏You can also hit the like ❤️ button at the bottom to help support me or share this with a friend. It helps me a lot! 🙏]]></content:encoded></item><item><title>Installing operators and CRs in automated way?</title><link>https://www.reddit.com/r/kubernetes/comments/1ilitg3/installing_operators_and_crs_in_automated_way/</link><author>/u/Such_Relative_9097</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 9 Feb 2025 16:40:13 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi, maybe I’m wrong but I see some technologies officially provide their k8s installation with operators and CRs (being installed after) instead of official helm chart. We all know the cons/pros using helm… and the advantages of operators.. but how the operator installation will work in automation? I mean, seem to be the CR yaml must be deployed after the operator yaml to function properly. In my case I do not mind using operators but I need an automated way to deploy them.. Maybe I grasp the concept all wrong… how you guys tackle this? Which tools? (Ansible for instance) … my case is very specific one because I must provide to the customer a bundle of charts (umbrella) .. so I can’t even use ansible and etc.. ok I can create helm chart that will deploy the operator and the CR but it feels weird and definitely I need your opinion and guidance about the matter. Thank you ..]]></content:encoded></item><item><title>From Engineer to Principal Solutions Architect at AWS with Prasad Rao</title><link>https://newsletter.eng-leadership.com/p/from-engineer-to-principal-solutions</link><author>/u/gregorojstersek</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 16:29:18 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Finding skilled developers is hard → even for seasoned engineering leaders. Traditional hiring cycles can take months while critical features sit in the backlog.Need to scale your team quickly or find specialized talent that's hard to source locally? They match you with developers from Europe and Latin America who integrate seamlessly into your workflow → without the long hiring cycles or commitment of long-term contracts.They don't just check résumés, but put developers through a rigorous multi-step vetting process that assesses technical skills, problem-solving abilities, and communication.Let’s get back to this week’s thought!When we talk about career paths, all of our journeys are unique. There is no one single best path that works for all of us. It’s important that you find out what path you wish to pursue and work towards it.Prasad RaoPrasad has chosen the Architecture path and I can see why very clearly. He grew as an engineer early on but later moved to the role of a Pre-sales Engineer, which helped him with speaking in both business and technical terms, creating presentations, understanding clients and managing multiple stakeholders.That particular set of skills is often hard to get from engineering roles alone, but it plays a crucial part in an Architect positions.Now, let’s get into his journey, Prasad, over to you!When engineers think about career growth, they often focus on two traditional paths: either advancing as an Individual Contributor (climbing from Senior to Staff Engineer and beyond) or transitioning into Engineering Management. However, there are other powerful career paths that are sometimes overlooked.In this article, I am sharing my journey of transitioning from an engineer to a Solutions Architect at AWS, offering insights into this alternative career trajectory.My career began as a .NET Developer, where I gradually progressed to become a Senior Developer and then a Tech Lead. The year I graduated, I interviewed for a couple of Big Tech companies but never got beyond the initial coding rounds. I thought I wasn't made for Big Tech and made peace with it. After a few initial job switches, I settled into a consulting company and started climbing the ladder. Unlike many others who choose the management track, I wanted to grow as an Individual Contributor (IC). However, many companies do not have a clearly defined IC growth path. The natural progression is to become a manager. I was looking for alternative opportunities.Even though I was working for a consulting company, I was in a unique position as I was working on building a product for a financial services company. When the product was ready, they needed someone in the field to demo it to potential clients. I took the role, and that opened a whole new world of possibilities for me.That's when I was first introduced to pre-sales activities. Instead of being an engineer on the product team, I would be part of the field team as a pre-sales engineer.Pro Tip: Don’t shy away from exploring lateral roles when an opportunity arises. Career growth is never linear.Frankly, before I started, I didn't know if such a role existed. As with any new role, there were pros and cons of being a pre-sales engineer. The pros were that I would be working more closely with customers onsite and traveling the world. The cons were that I would work on activities sometimes frowned upon by engineers - stakeholder management, customer interactions, gathering requirements, creating sales presentations, etc.Frankly, it was stepping out of my comfort zone, but it helped me develop skills that I never knew would be so important in my career growth. After a couple of years in this role working with multiple customers, I understood the importance of understanding customers' requirements and working backward to achieve them.Being an engineer, I had mostly focused on completing the tasks assigned to me. I never focused on understanding the real business reasons behind the tasks. Other skills I learned in this role:Speaking both business and technical languagesCreating stellar technical presentations and product demonstrationsUnderstanding prospective clients' business needs and designing customized solutionsCollaborating with multiple stakeholders like sales representatives, client executives, business analysts, and product teamsPro Tip: Spend time with your business stakeholders to understand the WHY behind the tasks you do on a daily basis.After a few years in the role of Pre-sales engineer, I got an opportunity to consult as a Senior Engineer/Tech Lead for a financial services customer on a digital transformation project. With improved business acumen and communication skills, I quickly became the go-to person for the customer. It was a development project, but my focus shifted to understanding the bigger picture.As developers, we design and architect components, so we're already doing architecture work without realizing it. A key to transitioning to an architect role is viewing these individual components and systems through a wider lens.To grow as an architect, you need to learn system design, distributed systems, and integration patterns. You also need to develop the mindset of considering scalability, performance, security, cost, and other factors. But those are technical skills that are relatively easy to pick up. The important thing to develop as an architect is perspective - while developers often focus deeply on specific components, architects maintain a holistic view of the entire system. For that, understanding the business requirements is essential.Thinking Like an ArchitectIn architecture, there is nothing defined as right or wrong - it's always a trade-off. There is a reason architects start their answers with "It depends." It depends on the requirements. It depends on what you would like to achieve. It depends on what constraints you're working with. Every architectural decision involves balancing multiple factors: scalability versus simplicity, performance versus maintainability, time-to-market versus perfect technical design, or cost versus capability.Lastly, to grow as an architect, it is also important to develop a broad understanding of multiple technologies. This enables you to make informed decisions about choosing the right tools for specific problems.Pro Tip: Shift your focus to look at the big picture and have a holistic view. Architects learn to navigate the ambiguity in requirements and constraints.With 10+ years in the industry, I had gained enough experience in different roles - developer, tech lead, pre-sales engineer, and architect. I was looking for a new job when one of my seniors suggested I try for AWS.I was reluctant, thinking that I would have to go through coding exercises. I was pretty hands-on, but solving LeetCode was not my cup of tea. To my surprise, I learned that coding rounds happen only for SDE roles. So if you are a naive person like me thinking that jobs at Big Tech are out of reach because of coding rounds, then please explore other roles.I browsed through multiple roles to find one that matched my profile - 'Solutions Architect, Microsoft Developer Tools on AWS'. The job role mentioned experience required with .NET and Microsoft workload stack. In the role, I had to help customers migrate and modernize their Microsoft workloads on AWS.My decade-long career was all in Microsoft technologies, but I had zero experience with AWS. I was pretty candid about that in my call with the recruiter, and still my profile got shortlisted. The entire interview process (8 rounds) was completely based on my experience and my learning ability with new technologies. No coding round whatsoever. I'm not saying the interviews were easy or that I did not have to prepare. It took a lot of hard work to prepare for the interviews, but the diverse experience I had gained helped me position myself as a good fit for a Solutions Architect role, which requires both technical and consulting skills.Pro Tip: Don't let coding interviews discourage you from applying at MAANG+ companies. Explore jobs other than SDE roles and you'll be surprised at how different the interview processes are.Having spent 5 years in the Solutions Architect (SA) role at AWS and being promoted to Principal SA has given me enough insights into what is required to become a successful SA. Ask any experienced SA about what their day-to-day role looks like, and they would answer, "Every day is different." And that's true.Let me give you a glimpse of the different hats an SA wears in their role. It will not only help you understand what a typical day of an SA looks like but also the skills you should be developing if you aspire to become an SA.cross-functional collaboratorsSo, one day I might spend a full day in whiteboarding sessions with customers, understanding their requirements and coming up with solutions using different AWS services. Another day might find me glued to my computer, creating a proof of concept to showcase the capabilities of AWS services. Then there might be days when I speak at conferences, evangelizing AWS services. Or I might spend a full day conducting workshops and training sessions to upskill customers on AWS.Pro Tip: As every customer is different and every customer problem is unique, so is each day of a Solutions Architect. And that's what I like about the SA role. BeSA (Become a Solutions Architect) BeSA YouTube channelTechnical Track: AWS GenAI immersive hands-on workshopsBehavioural Track: Cracking the Solutions Architect Interview at AWSLinkedInWe are not over yet! 2 more things.WriteEdgeMahdiIf you are someone looking to have discussions with like-minded people → More than 468 people are already there!Liked this article? Make sure to 💙 click the like button.Feedback or addition? Make sure to 💬 comment.Know someone that would find this helpful? Make sure to 🔁 share this post.herehereherehereIf you wish to make a request on particular topic you would like to read, you can send me an email to info@gregorojstersek.com.This newsletter is funded by paid subscriptions from readers like yourself.If you aren’t already, consider becoming a paid subscriber to receive the full experience!You are more than welcome to find whatever interests you here and try it out in your particular case. Let me know how it went! Topics are normally about all things engineering related, leadership, management, developing scalable products, building teams etc.]]></content:encoded></item><item><title>SysV init 3.14 released</title><link>https://github.com/slicer69/sysvinit/releases/tag/3.14</link><author>/u/gabriel_3</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 16:11:14 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>DOKS vs GKE</title><link>https://www.reddit.com/r/kubernetes/comments/1ilhsz1/doks_vs_gke/</link><author>/u/Impossible-Night4276</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 9 Feb 2025 15:56:53 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I used GKE at my job but I'm starting a personal project now so I'm shopping around for a managed clusterI can get a basic cluster on DOKS for $12/month while GKE charges about $100/month?I understand the sentiment "DigitalOcean is for hobbyists" and "GCP is for enterprises" but why is that? What does GKE provide that DOKS doesn't?]]></content:encoded></item><item><title>What do you use for deployments?</title><link>https://www.reddit.com/r/golang/comments/1ilh1a5/what_do_you_use_for_deployments/</link><author>/u/stas_spiridonov</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 9 Feb 2025 15:22:49 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I have been working in companies with in-house built systems for builds and deployments, where all pf that stuff is maintained by separate infra teams. So I am honestly out of the loop of what normal people use to deploy their apps.I am deploying to a bunch of hosts/VMs. I have several services, all in Go, so it is mostly a single binary file, sometimes a binary and a text config or a folder with js/css/images. I don’t have a problem of managing dependencies. My apps are stateful, they store data locally in files. Some apps Re web or grpc apps, some are async workers. I have a simple capistrano-like script which copies new artifacts to each host over ssh, updates a symlink and restarts the service. It works. But I am curious what tools do you use for that without reinventing a wheel?I am trying to avoid any new dependency unless it is absolutely necessary. So if you mention a system, please also write what exactly problem you were trying to solve with it.]]></content:encoded></item><item><title>Brainfly: A high-performance Brainf**k JIT and AOT compiler built on top of C# type system</title><link>https://github.com/hez2010/Brainfly/blob/main/Intro.md</link><author>/u/hez2010</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 13:46:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Don&apos;t &quot;optimize&quot; conditional moves in shaders with mix()+step()</title><link>https://iquilezles.org/articles/gpuconditionals/</link><author>romes</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 12:42:54 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
In this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with branching in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up:
So, say I have this code, which I actually published the other day: snap45(  v )
{
     s = (v);
     x = (v.x);
     x>?(s.x,):
           x>?s*():
                      (,s.y);
}
The exact details of what it does don't matter for this discussion. All we care about is the two ternary operations, which as you know, implement conditional execution. Indeed, depending on the value of the variable , the function will return different results. This could be implemented also with regular  statements, and all that I'm going to say stays the same.
But here's the problem - when seeing code like this, somebody somewhere will invariably propose the following "optimization", which replaces what they believe (erroneously) are "conditional branches" by arithmetical operations. They will suggest something like this: snap45(  v )
{
     s = (v);
     x = (v.x);

     w0 = (,x);
     w1 = (,x)*(-w0);
     w2 = -w0-w1;

     res0 = (s.x,);
     res1 = (s.x,s.y)*();
     res2 = (,s.y);

     w0*res0 + w1*res1 + w2*res2;
}
There are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. "select"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. Again, there is no branching - the instruction pointer isn't manipulated, there's no branch prediction involved, no instruction cache to invalidation, no nothing.
For the record, of course real branches do happen in GPU code, but those are not what's used by the GPU for small moves between registers like we have here. This is true for any GPU made in the last 20+ years. While I'm not an expert in CPUs, I am pretty sure this is true for them as well.
The second wrong thing with the supposedly optimizer version is that it actually runs much slower than the original version. The reason is that the  function is actually implemented like this: step(  x,  y )
{
     x < y ?  : ;
}
So people using the step() "optimization" are using the ternary operation anyways, which produces the  or  which they will use to select the output, only wasting two multiplications and one or two additions. The values could have been conditionally moved directly, which is what the original shader code did.
But don't take my word for it, let's look at the generated machine code for the relevant part of the shader I published:
GLSL x>?(s.x,):
       x>?s*():
                  (,s.y);
AMD Compiler     s0,      v3, , v1
     v4, , v0
     s1,   vcc, (v2), s0
 v3, 0, v3, vcc
 v0, v0, v4, vcc
 vcc, (v2), s1
 v1, v1, v3, vcc
 v0, 0, v0, vcc
Microsoft Compiler   r0.xy, l(, ), v0.xy
   r0.zw, v0.xy, l(, )
 r0.xy, -r0.xyxx, r0.zwzz
 r0.xy, r0.xyxx
  r1.xyzw, r0.xyxy, l4()
   r2.xy, l(,), v0.xx  r0.z, l()
 r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy
 o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw
Here we can see that the GPU is not branching. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).
The Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison ( - "lt"=less than) and the masking or conditional move ( - mov=move, c=conditionally).
Not related to the discussion, but also note that the  call does not become a GPU instruction and instead becomes an instruction modifier, which is free.
So, if you ever see somebody proposing this a = ( b, c, ( y, x ) );
as an optimization to
then please correct them for me. The misinformation has been around for 20 years / 10 GPU generation, and that's more than too long.]]></content:encoded></item><item><title>AI Code Generators Are Creating a Generation of “Copy-Paste Coders” — Here’s How We Fix It</title><link>https://medium.com/mr-plan-publication/ai-code-generators-are-creating-a-generation-of-copy-paste-coders-heres-how-we-fix-it-d49a3aef8dc2?sk=4f546231cd24ca0e23389a337724d45c</link><author>/u/TerryC_IndieGameDev</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 12:29:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[You’re mentoring a junior developer. They’re breezing through tasks, their screen a blur of AI-generated code snippets. They meet every deadline, their GitHub commits glowing green. But then you ask them to explain  their code works. Silence. They fumble through jargon, their confidence crumbling like a house of cards.This isn’t a hypothetical scenario. It’s happening in startups, corporate IT departments, and bootcamps worldwide. AI-powered tools like GitHub Copilot are reshaping coding — but beneath the hype lies a crisis we’re too afraid to name: We’re raising a generation of developers who can’t think for themselves.Let’s be clear: AI code completion isn’t evil. For seasoned developers, it’s like having a tireless intern handle boilerplate code. But for juniors? It’s become the programming equivalent of GPS addiction.I recently reviewed code from a junior who’d “solved” a complex sorting problem using Copilot. When I…]]></content:encoded></item><item><title>C++ on Steroids: Bjarne Stroustrup Presents Guideline-Enforcing &apos;Profiles&apos; For Resource and Type Safety</title><link>https://developers.slashdot.org/story/25/02/09/0636247/c-on-steroids-bjarne-stroustrup-presents-guideline-enforcing-profiles-for-resource-and-type-safety?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 9 Feb 2025 12:04:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA["It is now 45+ years since C++ was first conceived," writes 74-year-old C++ creator Bjarne Stroustrup in an article this week for Communications of the ACM. But he complains that many developers "use C++ as if it was still the previous millennium," in an article titled 21st Century C++ that promises "the key concepts on which performant, type safe, and flexible C++ software can be built: resource management, life-time management, error-handling, modularity, and generic programming... 

"At the end, I present ways to ensure that code is contemporary, rather than relying on outdated, unsafe, and hard-to-maintain techniques: guidelines and profiles."

To help developers focus on effective use of contemporary C++ and avoid outdated "dark corners" of the language, sets of guidelines have been developed. Here I focus on the C++ Core guidelines that I consider the most ambitious... My principal aim is a type-safe and resource-safe use of ISO standard C++. That is: 

- Every object is exclusively used according to its definition
- No resource is leaked 
This encompasses what people refer to as memory safety and much more. It is not a new goal for C++. Obviously, it cannot be achieved for every use of C++, but by now we have years of experience showing that it can be done for modern code, though so far enforcement has been incomplete... When thinking about C++, it is important to remember that C++ is not just a language but part of an ecosystem consisting of implementations, libraries, tools, teaching, and more. 
WG21 (and others) are working on "profiles" to enforce guidelines (though they're "not yet available, except for experimental and partial versions"). But Stroustrup writes that the C++ Core Guidelines "use a strategy known as subset-of-superset."

 First: extend the language with a few library abstractions: use parts of the standard library and add a tiny library to make use of the guidelines convenient and efficient (the Guidelines Support Library, GSL).
 Next: subset: ban the use of low-level, inefficient, and error-prone features. 
What we get is "C++ on steroids": Something simple, safe, flexible, and fast; rather than an impoverished subset or something relying on massive run-time checking. Nor do we create a language with novel and/or incompatible features. The result is 100% ISO standard C++. Messy, dangerous, low-level features can still be enabled and used when needed. 
Stroustrup writes that the C++ Core Guidelines focus on rules "we hope that everyone eventually could benefit from."

No uninitialized variables
No range or nullptr violations
No resource leaks
No dangling pointers
No type violations
No invalidation


Bjarne Stroustrup answered questions from Slashdot readers in 2014...
]]></content:encoded></item><item><title>This package helped us cut cloud costs in half while greatly improving our services response times</title><link>https://github.com/viccon/sturdyc</link><author>/u/Mysterious-Ad516</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 9 Feb 2025 11:55:39 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubeconfig Operator: Create restricted kubeconfigs as custom resources</title><link>https://www.reddit.com/r/kubernetes/comments/1ild0hg/kubeconfig_operator_create_restricted_kubeconfigs/</link><author>/u/ASBroadcast</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 9 Feb 2025 11:45:47 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[There recently was a post by the Reddit engineer u/keepingdatareal about their new SDK to build operators: Achilles SDK. It allows you to specify Kubernetes operators as finite state machines. Pretty neat!I used it to build a Kubeconfig Operator. It is useful for anybody who quickly wants to hand out limited access to a cluster without having OIDC in place. I also like to create a "daily-ops" kubeconfig to protect myself from accidental destructive operations. It usually has readonly permissions + deleting pods + creating/deleting portforwards.Unfortunately, I can just add a single image but check out the repo's README.md to see a graphic of the operator's behavior specified as a FSM. Here is a sample Kubeconfig manifest: apiVersion: kind: Kubeconfig metadata: name: restricted-access spec: clusterName: local-kind-cluster # specify external endpoint to your kubernetes API. # You can copy this from your other kubeconfig. server: https://127.0.0.1:52856 expirationTTL: 365d clusterPermissions: rules: - apiGroups: - "" resources: - namespaces verbs: - get - list - watch namespacedPermissions: - namespace: default rules: - apiGroups: - "" resources: - configmaps verbs: - '*' - namespace: kube-system rules: - apiGroups: - "" resources: - configmaps verbs: - get - list - watchklaud.works/v1alpha1 If you like the operator I'd be happy about a Github star ⭐️. The core logic is already fully covered by tests. So feel free to use it in production. Should any issue arise, just open a Github issue or text me here and I'll fix it.]]></content:encoded></item><item><title>Classic Data science pipelines built with LLMs</title><link>https://github.com/Pravko-Solutions/FlashLearn/tree/main/examples</link><author>galgia</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 11:39:38 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Watchexec v2.3.0 · with systemfd integration: `--socket`</title><link>https://github.com/watchexec/watchexec/releases/tag/v2.3.0</link><author>/u/passcod</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 9 Feb 2025 11:29:25 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Testing errors</title><link>https://bitfieldconsulting.com/posts/testing-errors</link><author>/u/EightLines_03</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 9 Feb 2025 09:53:04 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>RetroFab: Playable 3D simulations of vintage electronic games</title><link>https://itizso.itch.io/retrofab</link><author>todsacerdoti</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 09:50:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[( April 1980 - October 1980 )In 1980 Nintendo released it's first series of handheld electronic games each featuring a dedicated LCD game with a bonus watch function. The silver metallic front plate gave the series it's name. ( January 1981 - April 1981 )After the huge success of the Silver Series Nintendo released the next generation of it's Game & Watch™ handhelds the following year, featuring a new alarm function, color background-foils and a distinctive gold metallic front plate.  ( June 1981 - April 1982 )Featuring a wide LCD screen and impressive line-up of timeless classics (including some of the first licenced titles) Nintendo's Wide Screen Series ranks among some of the most popular Game & Watch™ titles.  ( May 1982 - August 1989 )In 1982 Nintendo introduced the innovative Multi Screen series whose most prominent feature was the use of two LCD screens in one foldable handheld with beautiful background-foil artwork and fine LCD elements.( October 1982 - October 1991 )Nintendo made further subtle improvements to the earlier Wide Screen series with a new box design featuring vivid artwork and a more colorful metallic front plate making the single screen LCD handhelds even classier.( April 1983 - August 1983 )Nintendo fulfilled the dream of many gamers to have their own mini arcade machine with the introduction of the Table Top Series which featured full color LCD graphics for the first time.( August 1983 - September 1984 )A few months after the introduction of the Table Top Series, Nintendo released a refinement of that technology in the form of the more portable Panorama Screen Series\.( July 1984 - November 1984 )The Micro Vs. System introduced two-player gaming to the Game & Watch™ series by attaching two wired controllers to a foldable unit that featured a wide LCD screen.( June 1986 - November 1986 )The Crystal Screen series introduced a new wide format translucent LCD handheld featuring a transparent see-thru screen and innovative side-scrolling gameplay.Featuring a colored LCD screen that no longer required an external light source (required by the Table Top and Panorama Screen series) and a new large portrait format LCD screen, the Super Color Series was perfectly suited for the two games released in this format.Nintendo released a limited edition of it's Super Mario Bros handheld LCD game as a prize for winners of a competition it held in 1987. Only 10,000 copies were given away, making it the most rare of the 3 Game & Watch versions of the game.]]></content:encoded></item><item><title>Minikube versus Kind: GPU Support</title><link>https://www.reddit.com/r/kubernetes/comments/1ilb8v2/minikube_versus_kind_gpu_support/</link><author>/u/MaximumNo4105</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 9 Feb 2025 09:39:33 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I come from a machine learning background with some, little, DevOps experience. I am trying to deploy a local Kubernetes cluster with NVIDIA GPU support.I have so far been using Kind to do so, deploying three services and exposing them via an ingress controller locally, but I stumbled upon what seems to be an ongoing issue with providing GPU support to the containers when using kind. I have already set the container runtime to use NVIDIA's runtime. I have followed guides on installing NVIDIA plugin into the cluster, mounting the correct GPU devices paths, providing tolerations as to where a deployment which requires GPU access can be deployed to, I have tried everything, but still I am unable to access the GPUs fromIs this a known issue within the DevOps community?If so, would switching to minikube make gaining access to the GPUs any easier? Has anyone got any experience deploying a minikube cluster locally and successfully gaining access to the GPUs?I appreciate your help and time to read this.Any help whatsoever is welcomed.]]></content:encoded></item><item><title>Clippy appreciation post</title><link>https://www.reddit.com/r/rust/comments/1ilav8q/clippy_appreciation_post/</link><author>/u/pnuts93</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 9 Feb 2025 09:11:50 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[As a Rust amateur I just wanted to share my positive experience with Clippy. I am generally fond of code lints, but especially in a complex language with a lot of built-in functionalities as Rust, I found Clippy to be very helpful in writing clean and idiomatic code and I would highly recommend it to other beginners. Also, extra points for the naming]]></content:encoded></item><item><title>Mathematics in the 20th century, by Michael Atiyah [pdf] (2002)</title><link>https://marktomforde.com/academic/miscellaneous/images/atiyah20thcentury.pdf</link><author>practal</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 08:54:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>IO_uring Zero-Copy Receive Support Ready For Linux 6.15 Networking</title><link>https://www.phoronix.com/news/IO_uring-Zero-Copy-Receive-Net</link><author>/u/unixbhaskar</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 08:33:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via Twitter, LinkedIn, or contacted via MichaelLarabel.com.]]></content:encoded></item><item><title>Modern-Day Oracles or Bullshit Machines</title><link>https://thebullshitmachines.com/</link><author>ctbergstrom</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 08:24:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[In a series of five- to ten-minute lessons, we will explain what these machines are, how they work, and how to thrive in a world where they are everywhere.You will learn when these systems can save you a lot of time and effort. You will learn when they are likely to steer you wrong. And you will discover how to see through the hype to tell the difference. ]]></content:encoded></item><item><title>Have we ever considered allowing orphans in bin crates?</title><link>https://www.reddit.com/r/rust/comments/1il9vnl/have_we_ever_considered_allowing_orphans_in_bin/</link><author>/u/MengerianMango</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 9 Feb 2025 07:59:52 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[The main reason for not allowing them is that a dep anywhere in your tree could add a trait and break everything, to put it simply. That's a good reason to disallow them in libs, but for bin crates that's not really a concern, and arguably the risk is something we should leave up to the application developer, the end user.I'd also consider going a step further, since there will be commonly popular orphans. Perhaps they should also be allowed in first order dependencies of bin crates, to prevent people from vendoring/copy-pasting common orphans. The same logic of allowing user discretion applies to first order dependencies to a bin crate. It would be sorta weird to create a sort of bifurcation in the lib crates, but not really a huge issue since most don't need orphans anw.This is an existing concept, allowing/semi-encouraging dirtiness and laziness in bin/app crates for the sake of productivity. Like how anyhow is bad in libs but ok if you're writing an app.]]></content:encoded></item><item><title>I think linux is actually easier to use than windows now</title><link>https://www.reddit.com/r/linux/comments/1il9tes/i_think_linux_is_actually_easier_to_use_than/</link><author>/u/petitlita</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 07:55:09 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I had to reinstall windows on the one PC that I was (previously) running windows on, basically just for debugging windows programs and the 2 games that don't play well with linux. One is a ported browser game that still works in browser and the other is kinitopet where windows being required is kinda understandable. Found a disk for windows that came with a laptop and put it in, oops, I don't have TPM 2. Tried downloading windows 10. Mysterious driver issues that it refused to elaborate on, apparently I needed to find these drivers and put them on a USB without it giving me any information on what I was looking for. I got sick of dealing with it at this point since it really gave no information and I just wanted to play witcher, though I know if I had worked out the driver issues I would still need to work through getting a local account, debloating the OS, modifying the registry, etc, just to get it to run in a way any reasonable person would expect a normal computer to behave.So I decide to just put endeavour OS on it instead (I have a recent nvidia GPU and I am lazy) and like, yeah it works well basically immediately, but what surprised me was how well it played with... everything. On windows, I spent 2 hours just fixing weird audio bugs with the steelseries wireless headset I have but it just works and connects immediately after I turn it on now. I didn't need to use their bloatware to turn off sidetone. The controller I use would require a bit of fiddling to connect when I turned it on on windows but on linux I just pick it up and it works. I install my games and they all (minux the aforementioned two) just work perfectly immediately. I don't get random video stuttering that I had on windows. WHEN did the linux experience become so seamless?Edit: In case anyone is curious, in witcher I am getting 60fps (cap) when previously I was getting like 45 lol]]></content:encoded></item><item><title>Fluxcd useful features</title><link>https://www.reddit.com/r/kubernetes/comments/1il9d9q/fluxcd_useful_features/</link><author>/u/Upper-Aardvark-6684</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 9 Feb 2025 07:22:39 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I have been using fluxcd as gitops tool since 6 months at my job. The most useful features I found was the dependson and wait parameters that help me better manage dependencies. I want to know if there are more such features that I might have missed or not used and have been useful to you. Let me know how flux has helped you in your k8s deployments.]]></content:encoded></item><item><title>Ohkami web framework v0.22 is out; runs on native, Cloudflare Workers, and AWS Lambda!</title><link>https://github.com/ohkami-rs/ohkami</link><author>/u/kanarus</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 9 Feb 2025 04:37:59 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How do I convince someone that Linux is, in fact, a &quot;normal operating system?&quot;</title><link>https://www.reddit.com/r/linux/comments/1il6p1k/how_do_i_convince_someone_that_linux_is_in_fact_a/</link><author>/u/FutureSuccess2796</author><category>dev</category><category>reddit</category><pubDate>Sun, 9 Feb 2025 04:31:51 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[EDIT: Want to clarify that the people who were telling me this were my parents and not a friend or just someone random. Just making note because I was vague about it prior. My bad!I'll try to make this as quick as I possibly can: I finally decide to dive into the world of Linux and I LOVE it! Been using that more than Windows now on my laptop and I've also been experimenting with the terminal and installing some different applications to try out too. Now I'm extremely passionate about technology and learning new things, and I'm also someone who can get hyperfixated on a topic. And this time it was about the cool things I was working on using Ubuntu, which included installing and adding different applications' dependencies as well as trying out some other things to make my experience more decentralized. Well, that backfired because I was went off on and told that I need to just go back to "a normal device and stop playing around with that USB all the time."I tried to explain that Ubuntu is a normal operating system but was just a little different than Windows or Apple and that I installed it myself on my own USB. But that did nothing. And discussing decentralization made them start presuming that I was "up to no good and that I would get into trouble with that command stuff," which is referencing the terminal and what it looks like when I install packages. Like, really, people? I get that not everybody's versed in computers, but you're literally assuming that something I'm passionate about must be troublesome because you're not familiar with it. Any way to explain that I'm just being tech-savvy and using a regular operating system that just happens to look a little different? ]]></content:encoded></item><item><title>Brain Hyperconnectivity in Children with Autism and Its Links to Social Deficits (2013)</title><link>https://www.cell.com/cell-reports/fulltext/S2211-1247(13)00570-6</link><author>stmw</author><category>dev</category><category>hn</category><pubDate>Sun, 9 Feb 2025 03:54:04 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Carbon is not a programming language (sort of)</title><link>https://herecomesthemoon.net/2025/02/carbon-is-not-a-language/</link><author>/u/cramert</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 9 Feb 2025 00:38:57 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[
              In case you’ve not heard of it,  is Google’s experimental
              open-source “C++-successor
              language”. As a very rough first approximation, think Objective-C/Swift, Java/Kotlin, C/C++, C++/Carbon.
              It is also frequently mentioned in the same breath as Herb Sutter’s Cppfront and Sean Baxter’s Circle (and
              Rust, surprise surprise).
            
              Like with any ‘successor language’, the overall goal includes (at the bare minimum) near-seamless
              interoperability, as well as  improvements over the original language.
              (Otherwise it can hardly be called a successor, duh.)
            
              If you’ve clicked on the article, you’re probably waiting for me to admit that I’m lying, and to tell you
              that (in fact) Carbon  a programming language.
            
              And yes, it’s true! Carbon is a programming language. (Or rather, it’s  to be a
              programming language. Carbon is an experimental project and hasn’t hit its 0.1 release milestone yet. The
              Carbon developers are very transparent about this.)
            
              But in my humble opinion, thinking of Carbon as a ‘programming language’ is kind of missing the point. Let
              me tell you how I think about Carbon, and why I think that it’s more interesting than most people give it
              credit for:
            
                Carbon is a concentrated experimental effort to develop tooling that will facilitate automated
                large-scale long-term migrations of existing C++ code to a modern, well-annotated programming language
                with a modern, transparent process of evolution and governance model.
              
              The entirety of Carbon (the language, as well as the project) is built around making this goal possible.
              (Disclaimer, I don’t speak for Carbon, take my words with a grain of salt.)
            In this post, I want to convince you of the following points:
                Carbon is a project to investigate the possibility of a large-scale reduction of C++ technical debt via
                automated code migration.
                Many so-called ‘successor languages’ are . They don’t make
                 an explicit goal, and generally build a layer of abstraction on top of
                or rely on their host language.
              
                All of this is downstream of Google’s disagreements with the C++ Standard Committee. In fact, while all
                of this is about reducing technical debt, it’s also about reducing the organizational costs involved in
                having to coordinate migrations and language evolution with the committee.
              Developing a new programming language is probably necessary to achieve the goals of the project.
              I’d like to bring special attention to the point about governance: This isn’t just a technical issue. It’s
              a governance issue. It’s a “We just straight-up disagree on the future direction of the C++ programming
              language.” sort of issue. I already went over these cultural disagreements in
              a previous post.
            
              (The astute reader will note that you can evolve and govern your own programming language however you
              want, without needing to deal with WG21 (aka the C++ Standard Committee, aka the authority that decides
              what C++ .))
            
              At this point I’d  to reach for the Herb Sutter “We must minimize the need to change existing
              code.” quote again,
              but I’ll instead just state the obvious:
            
              A large-scale migration to a different programming language is  paradigm. As far as
              changes to existing code go, it’s uncompromising. It’s an approach that’s only going to work for a subset
              of people, and in fact,
              Carbon’s goal document
              lists “We consider it a non-goal to support legacy code for which the source code is no longer available”.
            
              In other words, the language is not for everyone. That’s fine! I am still very interested in it. I care
              about Carbon since I believe that it’s trying to solve the hardest problem C++ is currently facing.
            
              This isn’t any  technical issue (there are many, many of those), no, and it’s not even a
              broad concern such as memory safety.
            
              It’s the problem of C++ slowly calcifying and struggling to modernize. It’s about ABI, about dozens of
              tools but no agreed upon standards, and it’s about backwards compatibility. It’s about allowing existing
              C++ code to , modernize and change, in spite of decades of technical debt, multiple
              implementations, and many different users with different expectations and requirements.
            This is, in other words, an , and a .
              If you believe that certain multi-million line C++ codebases are still going to exist in twenty years,
              then you should understand the business case for Carbon.
            A short lesson in history
              Let’s briefly summarize the backstory for those who haven’t kept track. You could (very roughly) say that
              Google is developing Carbon due to conflicts with WG21, and disagreements about the future of the C++
              language.
            
              What matters is that Google contributed to WG21 for many years, and that it has a vested interest in the
              future of the language, due to owning many,  million lines of C++ code. It’s hard to
              overstate how critical C++ is for Google’s infrastructure, and for modern technology in general.
            
              The short summary is that Google’s developers (not just Google’s, mind you) disagreed with other parts of
              the committee about the
              future direction of the C++ language. There are a lot of reasons for this, and
              a lot of ink has been spilled on the
              topic. Eventually, after trying to work with WG21 for many years, Google basically threw in the towel.
              (You cannot blame them. They tried hard, and the WG21 process is notoriously slow and frustrating.)
            
              At this point, a lot of people might think that the core disagreement between Google and WG21 was about
              ‘memory safety’, or something like that.
            
              The current memory safety hype is a pretty big deal for C++, but the ball was already rolling several
              years ago. All of this started with concerns about C++’s complexity
              and .
              It turns out that fixing certain issues would require backwards incompatible changes (bad!). Coordinating
              this across the entire C++ ecosystem would be more or less impossible.
            
              I’ll not get into the details and instead point at Chandler Carruth’s
              ‘There are no zero-cost abstractions’
              for an example: It pins down how first of all,  has a runtime overhead, and
              second of all, how fixing this would require an ABI-break and a language change.
            
              (That doesn’t mean Google doesn’t care about memory safety, of course. They do. But memory safety isn’t
              what started the whole conflict, even though it’s currently carrying the torch. That’s why memory safety
              is still relevant to all of this, especially since making C++ memory safe without compromising the vision
              of the standard committee looks more or less impossible.)
            Migration & Language Evolution
              First of all, that Carbon has  as one of its goals should be clear. The
              Carbon people are
              very explicit about this. It’s also a common theme in
              their talks.
            
              This is, first and foremost, about moving  from the “We mustn’t break existing code, so we
              had to squeeze in this new feature/syntax in some awkward way .” approach to language
              evolution. ( and the (proposed) reflection operator () are sending
              their regards.)
            
              This approach to language evolution kind of sucks. It’s not like the committee doesn’t
               the problem, or doesn’t  to evolve the language. The committee is not
              evil, and it’s not your enemy. C++ is just incredibly hard to evolve for all sorts of reasons, which would
              honestly justify an article on their own (ABI, multiple implementations and the committee process, no
              unified ecosystem, no editions or epochs system, no unified migration tooling, widespread dynamic linking,
              etc.)
            Carbon’s goal is to move away from that.
              How? Via automated tooling, a well-defined process of language evolution with clear guarantees, etc.
              Carbon is still highly experimental, so the details are still WIP. If I had to guess, I’d say they’re
              planning to follow in the footsteps of other modern languages. As an example, consider how Rust manages:
            
                Have a new ’edition’ every three years or so. Each edition is allowed to make certain breaking changes,
                but modules from separate editions can be compiled and linked together.
              
                Ship automated migration tooling with the language, which allows an automatic migration of code to the
                new edition whenever possible.
              
                If you want to eg. introduce a new keyword, you  it one edition ahead of time.
                Raw Identifier syntax
                allows migration and use of old code that still uses this keyword as an identifier.
              In contrast to this, I don’t think there’s any feasible roadmap to get C++ to this state.It’s only possible with an ecosystem split, and that’s exactly the point.
              Let me reiterate this. If there’s one thing you take from this article, it’s this here: The
               is to make it possible to take existing C++ code and to put it onto a path
              towards a modern, well-defined process for future evolution and changes.
            
              This is the point. If you want to be cynical, it’s about cutting the dependency on the standard committee,
              and it’s about allowing any forwards-looking, backwards-incompatible changes , without
              having to worry about someone else’s ancient binaries from the 80s.
            
              I just want to make clear that I believe that Carbon is radically different from
              Cpp2 (ie. Herb Sutter’s experimental
              project to evolve C++).
            
              The major difference is that Cpp2 tries to leverage the existing C++ language to its full extent, while
              Carbon tries to minimize its dependency on C++ wherever possible.
            
              Cpp2 takes the same approach C++ originally did: It transpiles its own code to the host language, and is
              thus deeply and intrinsically linked to it. It reuses the C++ standard library with all of its problems,
              it aims to maintain the C++ ecosystem instead of splitting it.
            
              Perhaps most importantly, Cpp2 also cannot go  than C++: It cannot directly interface with
              the compiler, since it’s written to be used by “any standard-compliant C++20 compiler”.
            
              It should be obvious that it’s basically impossible for Cpp2 to make meaningful reductions to C++’s
              technical debt. Yes, it can be “C++, except with better defaults and syntax.”, but that’s all it can
              feasibly be as long as full backwards compatibility is an explicit goal. Reducing C++’s technical debt on
              a deeper level is  for Cpp2.
            
              It  reduce the number of ways there are to initialize objects by simplifying syntax. It
               make any changes that would require an ABI-break, it cannot add null-safety to the
              language (eg.  can still be null,  can still be valueless), and
              it can’t prevent your code from blowing up in exciting ways due to lifetime issues.
              Carbon has the advantage that it  make these changes. (eg. Carbon is planning to move away
              from exceptions, in favor of treating errors as values.)
            
              Just to be clear, this is fine! I am not saying Cpp2 is bad, and I’m curious to see how the project
              develops. I am just highlighting that Carbon and Cpp2 are completely different projects with
              completely different scopes and goals.
            
              It is written by Herb Sutter, someone who very clearly  C++ as it is, and who wants to make
              it easier to use. It’s about having a new syntax, and making it  to apply best practices
              to your C++ code.
            
              This is a great idea, and a much less invasive proposal than Carbon. Carbon isn’t that. Carbon is about
              reworking the language from the ground up. It’s about building a  language that can
              support almost all of the same semantics, but is still critically different. It’s about reworking the
              fundamentals, and building stronger abstractions.
            
              So in short, Cpp2 works  C++, and Carbon is trying to  a better C++ from
              scratch, while cutting its dependency on C++ almost completely.
            
              Is Carbon feasible? I’ll be honest, I have no clue. C++ code is  and this project is
              (more or less) unprecedented. (Which is, again, why I am interested in it.)
            The reasons why I believe it  be technically feasible at all are simple:
                Carbon doesn’t attempt to do the impossible: The goal is a tool-assisted migration of idiomatic code,
                not a fully automated migration of  code. (What does ‘idiomatic’ mean? Who knows. Probably
                something like ‘well-annotated and easy to handle for static analyzers’. Figuring out how to draw the
                boundary of which code can be migrated is part of the project.)
              
                Carbon is capable of leveraging its underlying tooling to do a  of the hard work. For
                example, resolving C++ templates and function calls is handled by Clang and LLVM. This should not be
                much of a surprise. Clang can be used as a library, and this is exactly what you’d expect it to excel
                at. (Swift is
                already doing this for its C++ interop.)
              
                Carbon already demonstrated that its chosen abstractions are capable of supporting some pretty “fun” C++
                features.
              Let me quickly substantiate some of that.
              So, here’s the thing. Carbon can
              convert your C++ to Carbon and then run it against the old test suite. (Or that’s the plan, at
              least.)
            (You do have a test suite, right?)
              If the code compiles and all tests pass, this should give you confidence in the resulting code
              proportional to your confidence in your own test suite. (This is especially helpful for changes
               the initial automated migration, even if it’s just clean-up work.)
            This approach is  for all sorts of reasons.
              First of all, it means that Carbon can leverage
              existing C++ test suites to test its own migration and interop capabilities. This is great.
            
              Second of all, it puts  burden on the user and sets a minimal bar for what Carbon means with
              ‘migration of idiomatic C++’: You should  have some tests in your code. If you critically
              depend on something, then you should have a test for it.
            Generalization and unification of C++ features
              If you have no idea what you’re looking at: This is legal C++. Calling  a ‘pointer’ is a
              stretch, in practice it is just an  relative to the location of an object of this class in
              memory.
            
              Two funfacts: First, this can also be used to refer to methods. Second, this value can be null, and it’s
              null-value is , since  would point to an actual field.
            
              When I see a feature like this, my first question would be whether Carbon is even capable of
               this specific type of behavior, and it turns out that, yes, they have thought about
              this.
            
              Carbon is building  on top of  (which can broadly be
              understood as C++0x Concepts or Rust traits).
            
              There’s a simple reason for that: Carbon wants to support  checked generics
              (roughly, you’ll know that a generic function can be instantiated without having to look into the body of
              the function. This is not the case for templates.) As a consequence,  which
              you can “do” with a value needs to be implemented as an interface, so that you can specify that an
              incoming value fulfills this constraint.
            
              Consequently,  are implemented via a so-called
               interface, which (as far as I can tell) generalizes expressions of the form
              , whether  is a field, a static member function, a method, a member access
              pointer, or who knows what else. Any  which implements  (where
               is the class of ) can be used as .
            
              The pattern of unifying abstractions as interfaces gets used a lot: It turns out that deep within Carbon,
              function calls are implemented as a synthesized type which implements some  interface.
              This is used to unify functions, methods, lambdas, etc. Every single thing in Carbon which you can “call”
              is just some value implementing the  interface.
            
              Sorry, I’m basically just rehashing parts of Chandler Carruth’s (highly technical) talk here. For the full
              picture, please just go and watch it. He’s a great speaker, and I don’t trust myself to get every
              technical detail right.
            
              The point is, if you’re wondering what the Carbon people are working on, then it’s this kind of stuff.
              They’re building  which are general enough to to make all sorts of gnarly C++
              semantics (eg. member access pointers) work, but have a  simpler underlying model. (eg. it
              unifies everything that can be called,  gives you the ‘concept’/interface for free).
            
              Is this going to work for the rest of the language? Who knows! C++ is complicated, probably too
              complicated to manage. That’s the whole reason why Carbon even exists.
            Digression: Why not Rust™? Why not C++?
              Rust is really just too different for an automated conversion of C++ code to Rust code to be feasible,
              it’s as simple as that. I even
              wrote an article
              getting into the differences in type inference alone.
            
              You have no class inheritance, no templates, no specialization, no ad-hoc function overloads, no implicit
              conversions, and there’s still the whole deal with the borrow checker. Any conversion of modern
              general-purpose C++ code to Rust basically amounts to a rewrite, which is just not something you can do
              with classic automation tooling.
            
              Carbon has the luxury of being able to support both templates  checked generics (ie. something
              like Rust traits or C++0x concepts), and a way to migrate between them.
            
              As for a C++-to-more-modern-C++-migration, it just doesn’t solve the question of language evolution.
              You’re still heavily limited by what you can do, unless you also commit to a proper fork of C++ and
              possibly Clang.
            
              Which…might be viable, but makes it much harder to implement clean abstractions from the get-go. It also
              doesn’t help that a fork runs a pretty severe risk of being ‘usable’ right from the get-go (meaning that
              people will want to use it, and the boundary between C++ and Carbon will be muddier).
            
              As I said, Carbon is a moonshot project to allow modern C++ codebases to evolve. (They might stop being
              called “C++” in the process, but that’s probably fine. The only constant in life is change, or something
              like that.)
            
              The north star goal is, of course, that of a gradual but mostly automated migration of existing C++ code
              to Carbon code, followed by  migrations to fix and improve this code using Carbon’s
              modern, more powerful semantics (eg. null safety).
            
              From this angle, and with the historical background in mind, let’s address the elephant in the room
              and take a stab at describing how some people feel about Carbon, by rephrasing my interpretation of its
              goals in the most cynical way possible. I’m deeply sorry to anyone who’s working on Carbon, since this is
              going to feel like I’m twisting a proverbial knife:
            
                Carbon’s primary goal is a large-scale migration of Google’s enormous pile of (highly specific,
                exception-less, Abseil and Protobuf-using, Clang-based, Bazel-built) C++ technical debt into a modern
                language capable of supporting Google’s needs, and
                over whose governance Google is capable of exerting a significant amount of control.
              There we go. Do
              you see the elephant yet?
            
              It’s pretty hard to miss since I highlighted it. (Sorry, I know that it’s the second time I made that
              joke.)
            
              This is about the least charitable way to phrase it, of course. I’m bringing this up for the obvious
              reasons: Carbon is spearheaded by a big tech company, and people have various concerns.
            
              These include the concern that Google trying to ’take control’ of C++ via a divide-and-conquer approach,
              that Carbon will favor Google’s style of C++ at the expense of others, and the classic sentiment that
              Carbon will eventually be abandoned and dropped (potentially hanging early adopters out in the dry).
            
              As I already gestured at before, all of this is about , and by extension about governance.
            
              As long as we’re willing to say that Carbon is about reducing the reliance on the C++ Standard Committee,
              it’s pretty clear that that governance-shaped hole has to be filled , and that someone (or
              some group of people) has to decide the future direction of the language.
            
              I’ll be honest, I can make no guarantees here. I am not working on Carbon, and the dynamics here are far beyond my scope.
            
              I can point out that Carbon is an Apache-licensed open source project,
              open for contributors right now, and that it has an explicit “The intent is that […] Carbon remains a community-driven project, avoiding
              situations where any single organization controls Carbon’s direction.”
              disclaimer
              in its FAQ, but that’s not going to convince you if you’re worried about bad intentions.
            
              So. What I  tell you is that I believe that putting governance of the language into the hands
              of the open-source community is critical for Carbon’s long-term success, and that Carbon’s developers
              understand this.
            
              Whether Carbon will find widespread adoption depends on whether  trust
              Carbon’s stewards to handle the language with enough responsibility that migrating their own C++ code to
              Carbon seems like a safe offer.
            
              This sort of trust is hard to establish as long as there’s a single owner,  if that
              owner is Google.
            
              Second: That Carbon finds any public adoption at all is also pretty important  the primary
              goal was just to use it purely within Google. This might come as a surprise, but it’s pretty simple:
              People who’re expected to use Carbon first need to learn Carbon. This is  easier
              when Google can rely on a broad ecosystem of tutorials, libraries and discussion boards outside of its
              intranet.
            
              So in other words, for Carbon to become successful, it’s critical that there’s a public community, and
              that enterprise users of C++  Carbon.
            These are huge incentives to push the language towards independent community ownership.
              Both of these points (trust by enterprise users and need for a public community) were
               for Go (which was also
              spearheaded by Google), primarily due to Go’s simplicity, the fact that there was far less competition in
              the programming language space when Go released, and the fact that it was a language for greenfield
              projects. (That is, it didn’t require convincing ancient C++ coders to perform a massive migration and
              rework their tool chain.)
            
              My understanding is that
              Carbon’s leads understand all of this, and want the project to be community driven. For now, that’s more than good enough for me. For a
              project this early in its life-cycle, it’s nice to see that they’re thinking about this at all, and have
              made an explicit commitment to community ownership.
            Conclusion: There is no free lunch.
              The prospect of building a  to C++—arguably single most important programming language
              currently in existence—sounds like it should be doomed to fail.
            
              I’ll repeat what I said before, and what should be common knowledge: C++ is an incredibly complex
              programming language. It’s under-annotated, has multiple implementations (governed by a 2000+ page ISO
              Standard document), carries four decades of technical baggage, is full of undefined behavior, and has a
              frequently abused Turing-complete quasi-code-generation meta-programming language built into it.
            
              All of that should make it near impossible to succeed C++. Complexity is in fact a form of job security.
              So why am I still relatively confident in Carbon’s potential?
            Simple, it’s mainly since the priorities look correct to me. Carbon understands that
                C++’s inability to evolve, modernize, deprecate, migrate and standardize is  critical issue
                which the language is facing today.
                You cannot improve on this without making concessions. This goes both ways: There is old C++ code which
                you will not be able to support. At the same time, there are C++ features which you  to
                support, whether you want to or not.
              
                This is a herculean task that requires a massive initial investment (a whole new programming language),
                and a complete rethinking of tooling, communication, software engineering and language development
                practices.
              
              The inability to evolve is an issue for people who’re just starting to learn C++, and who stumble into
              every single footgun that hasn’t been taken care of over the past thirty years.
            
              It’s an issue for people who care for high-quality code, readability or memory safety, and see no viable
              path towards getting their C++ codebase into that state.
            
              It’s an issue for committee and compiler contributors, who need to carefully consider how a new
              feature will interact with literally everything else the language already supports.
            
              You might disagree with that assessment. It might not be an issue . That’s
              fine. C++ (for a given version, anyway) will stay exactly as it is. It’s not going to go away anytime
              soon, and that’s a good thing. People depend on that. Critical infrastructure depends on that.
            
              As for myself, I am incredibly glad to see that  is trying to take this bull by the horns,
              and willing to face this charging billion lines-of-code mountain of complexity and technical debt head-on.
            
              At last but not at least just since it would be  if we (humanity, as a whole)
              could actually pull it off, and don’t need to pass tales warning people about the dangers of using
               across the generations.
            
              It might take a while, but that’s fine. This is a long-term project. It  to be a long-term
              project to make this work. Once you start thinking about it from that perspective, everything makes a lot
              more sense.
            
              Remember, those millions of lines of C++ code are not going to go away anytime soon. They’ll still be
              there in a few decades. It’s either a large-scale migration (in some form or another), or nothing.
            
              In the meantime, if you’re remotely interested, I’ll reiterate that I highly recommend Carbon’s talks, eg.
              this one.
            
              Writing this took significantly longer than expected. The total number of footnotes written and deleted is
              about forty.
            Let me know if you got something out of it—It means a lot to me.
              Questions, suggestions, comments, writing advice, reading recommendations, music suggestions, pictures of
              pets and basically anything else you can think of are all welcome via my contact e-mail at the bottom of
              the page.
            ]]></content:encoded></item><item><title>I release Beta of my code editor Gladius</title><link>https://www.reddit.com/r/rust/comments/1il108b/i_release_beta_of_my_code_editor_gladius/</link><author>/u/njs5i</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 8 Feb 2025 23:35:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[After several years of coding, I think I have "good enough" Beta release of my CLI, keyboard-only code editor Gladius. I would like to especially thanks all contributors of the project so far.]]></content:encoded></item><item><title>Is there a way to shutdown a gouroutine</title><link>https://www.reddit.com/r/golang/comments/1ikyda1/is_there_a_way_to_shutdown_a_gouroutine/</link><author>/u/Alone-Employ-1985</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 8 Feb 2025 21:37:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[So i have a long function that runs as a goroutine for every http request i recieve for every idI want to stop the already started goroutine if another request with same id is made   submitted by    /u/Alone-Employ-1985 ]]></content:encoded></item><item><title>Carbon is not a programming language (sort of)</title><link>https://herecomesthemoon.net/2025/02/carbon-is-not-a-language/</link><author>/u/SophisticatedAdults</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 21:30:37 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
              In case you’ve not heard of it,  is Google’s experimental
              open-source “C++-successor
              language”. As a very rough first approximation, think Objective-C/Swift, Java/Kotlin, C/C++, C++/Carbon.
              It is also frequently mentioned in the same breath as Herb Sutter’s Cppfront and Sean Baxter’s Circle (and
              Rust, surprise surprise).
            
              Like with any ‘successor language’, the overall goal includes (at the bare minimum) near-seamless
              interoperability, as well as  improvements over the original language.
              (Otherwise it can hardly be called a successor, duh.)
            
              If you’ve clicked on the article, you’re probably waiting for me to admit that I’m lying, and to tell you
              that (in fact) Carbon  a programming language.
            
              And yes, it’s true! Carbon is a programming language. (Or rather, it’s  to be a
              programming language. Carbon is an experimental project and hasn’t hit its 0.1 release milestone yet. The
              Carbon developers are very transparent about this.)
            
              But in my humble opinion, thinking of Carbon as a ‘programming language’ is kind of missing the point. Let
              me tell you how I think about Carbon, and why I think that it’s more interesting than most people give it
              credit for:
            
                Carbon is a concentrated experimental effort to develop tooling that will facilitate automated
                large-scale long-term migrations of existing C++ code to a modern, well-annotated programming language
                with a modern, transparent process of evolution and governance model.
              
              The entirety of Carbon (the language, as well as the project) is built around making this goal possible.
              (Disclaimer, I don’t speak for Carbon, take my words with a grain of salt.)
            In this post, I want to convince you of the following points:
                Carbon is a project to investigate the possibility of a large-scale reduction of C++ technical debt via
                automated code migration.
                Many so-called ‘successor languages’ are . They don’t make
                 an explicit goal, and generally build a layer of abstraction on top of
                or rely on their host language.
              
                All of this is downstream of Google’s disagreements with the C++ Standard Committee. In fact, while all
                of this is about reducing technical debt, it’s also about reducing the organizational costs involved in
                having to coordinate migrations and language evolution with the committee.
              Developing a new programming language is probably necessary to achieve the goals of the project.
              I’d like to bring special attention to the point about governance: This isn’t just a technical issue. It’s
              a governance issue. It’s a “We just straight-up disagree on the future direction of the C++ programming
              language.” sort of issue. I already went over these cultural disagreements in
              a previous post.
            
              (The astute reader will note that you can evolve and govern your own programming language however you
              want, without needing to deal with WG21 (aka the C++ Standard Committee, aka the authority that decides
              what C++ .))
            
              At this point I’d  to reach for the Herb Sutter “We must minimize the need to change existing
              code.” quote again,
              but I’ll instead just state the obvious:
            
              A large-scale migration to a different programming language is  paradigm. As far as
              changes to existing code go, it’s uncompromising. It’s an approach that’s only going to work for a subset
              of people, and in fact,
              Carbon’s goal document
              lists “We consider it a non-goal to support legacy code for which the source code is no longer available”.
            
              In other words, the language is not for everyone. That’s fine! I am still very interested in it. I care
              about Carbon since I believe that it’s trying to solve the hardest problem C++ is currently facing.
            
              This isn’t any  technical issue (there are many, many of those), no, and it’s not even a
              broad concern such as memory safety.
            
              It’s the problem of C++ slowly calcifying and struggling to modernize. It’s about ABI, about dozens of
              tools but no agreed upon standards, and it’s about backwards compatibility. It’s about allowing existing
              C++ code to , modernize and change, in spite of decades of technical debt, multiple
              implementations, and many different users with different expectations and requirements.
            This is, in other words, an , and a .
              If you believe that certain multi-million line C++ codebases are still going to exist in twenty years,
              then you should understand the business case for Carbon.
            A short lesson in history
              Let’s briefly summarize the backstory for those who haven’t kept track. You could (very roughly) say that
              Google is developing Carbon due to conflicts with WG21, and disagreements about the future of the C++
              language.
            
              What matters is that Google contributed to WG21 for many years, and that it has a vested interest in the
              future of the language, due to owning many,  million lines of C++ code. It’s hard to
              overstate how critical C++ is for Google’s infrastructure, and for modern technology in general.
            
              The short summary is that Google’s developers (not just Google’s, mind you) disagreed with other parts of
              the committee about the
              future direction of the C++ language. There are a lot of reasons for this, and
              a lot of ink has been spilled on the
              topic. Eventually, after trying to work with WG21 for many years, Google basically threw in the towel.
              (You cannot blame them. They tried hard, and the WG21 process is notoriously slow and frustrating.)
            
              At this point, a lot of people might think that the core disagreement between Google and WG21 was about
              ‘memory safety’, or something like that.
            
              The current memory safety hype is a pretty big deal for C++, but the ball was already rolling several
              years ago. All of this started with concerns about C++’s complexity
              and .
              It turns out that fixing certain issues would require backwards incompatible changes (bad!). Coordinating
              this across the entire C++ ecosystem would be more or less impossible.
            
              I’ll not get into the details and instead point at Chandler Carruth’s
              ‘There are no zero-cost abstractions’
              for an example: It pins down how first of all,  has a runtime overhead, and
              second of all, how fixing this would require an ABI-break and a language change.
            
              (That doesn’t mean Google doesn’t care about memory safety, of course. They do. But memory safety isn’t
              what started the whole conflict, even though it’s currently carrying the torch. That’s why memory safety
              is still relevant to all of this, especially since making C++ memory safe without compromising the vision
              of the standard committee looks more or less impossible.)
            Migration & Language Evolution
              First of all, that Carbon has  as one of its goals should be clear. The
              Carbon people are
              very explicit about this. It’s also a common theme in
              their talks.
            
              This is, first and foremost, about moving  from the “We mustn’t break existing code, so we
              had to squeeze in this new feature/syntax in some awkward way .” approach to language
              evolution. ( and the (proposed) reflection operator () are sending
              their regards.)
            
              This approach to language evolution kind of sucks. It’s not like the committee doesn’t
               the problem, or doesn’t  to evolve the language. The committee is not
              evil, and it’s not your enemy. C++ is just incredibly hard to evolve for all sorts of reasons, which would
              honestly justify an article on their own (ABI, multiple implementations and the committee process, no
              unified ecosystem, no editions or epochs system, no unified migration tooling, widespread dynamic linking,
              etc.)
            Carbon’s goal is to move away from that.
              How? Via automated tooling, a well-defined process of language evolution with clear guarantees, etc.
              Carbon is still highly experimental, so the details are still WIP. If I had to guess, I’d say they’re
              planning to follow in the footsteps of other modern languages. As an example, consider how Rust manages:
            
                Have a new ’edition’ every three years or so. Each edition is allowed to make certain breaking changes,
                but modules from separate editions can be compiled and linked together.
              
                Ship automated migration tooling with the language, which allows an automatic migration of code to the
                new edition whenever possible.
              
                If you want to eg. introduce a new keyword, you  it one edition ahead of time.
                Raw Identifier syntax
                allows migration and use of old code that still uses this keyword as an identifier.
              In contrast to this, I don’t think there’s any feasible roadmap to get C++ to this state.It’s only possible with an ecosystem split, and that’s exactly the point.
              Let me reiterate this. If there’s one thing you take from this article, it’s this here: The
               is to make it possible to take existing C++ code and to put it onto a path
              towards a modern, well-defined process for future evolution and changes.
            
              This is the point. If you want to be cynical, it’s about cutting the dependency on the standard committee,
              and it’s about allowing any forwards-looking, backwards-incompatible changes , without
              having to worry about someone else’s ancient binaries from the 80s.
            
              I just want to make clear that I believe that Carbon is radically different from
              Cpp2 (ie. Herb Sutter’s experimental
              project to evolve C++).
            
              The major difference is that Cpp2 tries to leverage the existing C++ language to its full extent, while
              Carbon tries to minimize its dependency on C++ wherever possible.
            
              Cpp2 takes the same approach C++ originally did: It transpiles its own code to the host language, and is
              thus deeply and intrinsically linked to it. It reuses the C++ standard library with all of its problems,
              it aims to maintain the C++ ecosystem instead of splitting it.
            
              Perhaps most importantly, Cpp2 also cannot go  than C++: It cannot directly interface with
              the compiler, since it’s written to be used by “any standard-compliant C++20 compiler”.
            
              It should be obvious that it’s basically impossible for Cpp2 to make meaningful reductions to C++’s
              technical debt. Yes, it can be “C++, except with better defaults and syntax.”, but that’s all it can
              feasibly be as long as full backwards compatibility is an explicit goal. Reducing C++’s technical debt on
              a deeper level is  for Cpp2.
            
              It  reduce the number of ways there are to initialize objects by simplifying syntax. It
               make any changes that would require an ABI-break, it cannot add null-safety to the
              language (eg.  can still be null,  can still be valueless), and
              it can’t prevent your code from blowing up in exciting ways due to lifetime issues.
              Carbon has the advantage that it  make these changes. (eg. Carbon is planning to move away
              from exceptions, in favor of treating errors as values.)
            
              Just to be clear, this is fine! I am not saying Cpp2 is bad, and I’m curious to see how the project
              develops. I am just highlighting that Carbon and Cpp2 are completely different projects with
              completely different scopes and goals.
            
              It is written by Herb Sutter, someone who very clearly  C++ as it is, and who wants to make
              it easier to use. It’s about having a new syntax, and making it  to apply best practices
              to your C++ code.
            
              This is a great idea, and a much less invasive proposal than Carbon. Carbon isn’t that. Carbon is about
              reworking the language from the ground up. It’s about building a  language that can
              support almost all of the same semantics, but is still critically different. It’s about reworking the
              fundamentals, and building stronger abstractions.
            
              So in short, Cpp2 works  C++, and Carbon is trying to  a better C++ from
              scratch, while cutting its dependency on C++ almost completely.
            
              Is Carbon feasible? I’ll be honest, I have no clue. C++ code is  and this project is
              (more or less) unprecedented. (Which is, again, why I am interested in it.)
            The reasons why I believe it  be technically feasible at all are simple:
                Carbon doesn’t attempt to do the impossible: The goal is a tool-assisted migration of idiomatic code,
                not a fully automated migration of  code. (What does ‘idiomatic’ mean? Who knows. Probably
                something like ‘well-annotated and easy to handle for static analyzers’. Figuring out how to draw the
                boundary of which code can be migrated is part of the project.)
              
                Carbon is capable of leveraging its underlying tooling to do a  of the hard work. For
                example, resolving C++ templates and function calls is handled by Clang and LLVM. This should not be
                much of a surprise. Clang can be used as a library, and this is exactly what you’d expect it to excel
                at. (Swift is
                already doing this for its C++ interop.)
              
                Carbon already demonstrated that its chosen abstractions are capable of supporting some pretty “fun” C++
                features.
              Let me quickly substantiate some of that.
              So, here’s the thing. Carbon can
              convert your C++ to Carbon and then run it against the old test suite. (Or that’s the plan, at
              least.)
            (You do have a test suite, right?)
              If the code compiles and all tests pass, this should give you confidence in the resulting code
              proportional to your confidence in your own test suite. (This is especially helpful for changes
               the initial automated migration, even if it’s just clean-up work.)
            This approach is  for all sorts of reasons.
              First of all, it means that Carbon can leverage
              existing C++ test suites to test its own migration and interop capabilities. This is great.
            
              Second of all, it puts  burden on the user and sets a minimal bar for what Carbon means with
              ‘migration of idiomatic C++’: You should  have some tests in your code. If you critically
              depend on something, then you should have a test for it.
            Generalization and unification of C++ features
              If you have no idea what you’re looking at: This is legal C++. Calling  a ‘pointer’ is a
              stretch, in practice it is just an  relative to the location of an object of this class in
              memory.
            
              Two funfacts: First, this can also be used to refer to methods. Second, this value can be null, and it’s
              null-value is , since  would point to an actual field.
            
              When I see a feature like this, my first question would be whether Carbon is even capable of
               this specific type of behavior, and it turns out that, yes, they have thought about
              this.
            
              Carbon is building  on top of  (which can broadly be
              understood as C++0x Concepts or Rust traits).
            
              There’s a simple reason for that: Carbon wants to support  checked generics
              (roughly, you’ll know that a generic function can be instantiated without having to look into the body of
              the function. This is not the case for templates.) As a consequence,  which
              you can “do” with a value needs to be implemented as an interface, so that you can specify that an
              incoming value fulfills this constraint.
            
              Consequently,  are implemented via a so-called
               interface, which (as far as I can tell) generalizes expressions of the form
              , whether  is a field, a static member function, a method, a member access
              pointer, or who knows what else. Any  which implements  (where
               is the class of ) can be used as .
            
              The pattern of unifying abstractions as interfaces gets used a lot: It turns out that deep within Carbon,
              function calls are implemented as a synthesized type which implements some  interface.
              This is used to unify functions, methods, lambdas, etc. Every single thing in Carbon which you can “call”
              is just some value implementing the  interface.
            
              Sorry, I’m basically just rehashing parts of Chandler Carruth’s (highly technical) talk here. For the full
              picture, please just go and watch it. He’s a great speaker, and I don’t trust myself to get every
              technical detail right.
            
              The point is, if you’re wondering what the Carbon people are working on, then it’s this kind of stuff.
              They’re building  which are general enough to to make all sorts of gnarly C++
              semantics (eg. member access pointers) work, but have a  simpler underlying model. (eg. it
              unifies everything that can be called,  gives you the ‘concept’/interface for free).
            
              Is this going to work for the rest of the language? Who knows! C++ is complicated, probably too
              complicated to manage. That’s the whole reason why Carbon even exists.
            Digression: Why not Rust™? Why not C++?
              Rust is really just too different for an automated conversion of C++ code to Rust code to be feasible,
              it’s as simple as that. I even
              wrote an article
              getting into the differences in type inference alone.
            
              You have no class inheritance, no templates, no specialization, no ad-hoc function overloads, no implicit
              conversions, and there’s still the whole deal with the borrow checker. Any conversion of modern
              general-purpose C++ code to Rust basically amounts to a rewrite, which is just not something you can do
              with classic automation tooling.
            
              Carbon has the luxury of being able to support both templates  checked generics (ie. something
              like Rust traits or C++0x concepts), and a way to migrate between them.
            
              As for a C++-to-more-modern-C++-migration, it just doesn’t solve the question of language evolution.
              You’re still heavily limited by what you can do, unless you also commit to a proper fork of C++ and
              possibly Clang.
            
              Which…might be viable, but makes it much harder to implement clean abstractions from the get-go. It also
              doesn’t help that a fork runs a pretty severe risk of being ‘usable’ right from the get-go (meaning that
              people will want to use it, and the boundary between C++ and Carbon will be muddier).
            
              As I said, Carbon is a moonshot project to allow modern C++ codebases to evolve. (They might stop being
              called “C++” in the process, but that’s probably fine. The only constant in life is change, or something
              like that.)
            
              The north star goal is, of course, that of a gradual but mostly automated migration of existing C++ code
              to Carbon code, followed by  migrations to fix and improve this code using Carbon’s
              modern, more powerful semantics (eg. null safety).
            
              From this angle, and with the historical background in mind, let’s address the elephant in the room
              and take a stab at describing how some people feel about Carbon, by rephrasing my interpretation of its
              goals in the most cynical way possible. I’m deeply sorry to anyone who’s working on Carbon, since this is
              going to feel like I’m twisting a proverbial knife:
            
                Carbon’s primary goal is a large-scale migration of Google’s enormous pile of (highly specific,
                exception-less, Abseil and Protobuf-using, Clang-based, Bazel-built) C++ technical debt into a modern
                language capable of supporting Google’s needs, and
                over whose governance Google is capable of exerting a significant amount of control.
              There we go. Do
              you see the elephant yet?
            
              It’s pretty hard to miss since I highlighted it. (Sorry, I know that it’s the second time I made that
              joke.)
            
              This is about the least charitable way to phrase it, of course. I’m bringing this up for the obvious
              reasons: Carbon is spearheaded by a big tech company, and people have various concerns.
            
              These include the concern that Google trying to ’take control’ of C++ via a divide-and-conquer approach,
              that Carbon will favor Google’s style of C++ at the expense of others, and the classic sentiment that
              Carbon will eventually be abandoned and dropped (potentially hanging early adopters out in the dry).
            
              As I already gestured at before, all of this is about , and by extension about governance.
            
              As long as we’re willing to say that Carbon is about reducing the reliance on the C++ Standard Committee,
              it’s pretty clear that that governance-shaped hole has to be filled , and that someone (or
              some group of people) has to decide the future direction of the language.
            
              I’ll be honest, I can make no guarantees here. I am not working on Carbon, and the dynamics here are far beyond my scope.
            
              I can point out that Carbon is an Apache-licensed open source project,
              open for contributors right now, and that it has an explicit “The intent is that […] Carbon remains a community-driven project, avoiding
              situations where any single organization controls Carbon’s direction.”
              disclaimer
              in its FAQ, but that’s not going to convince you if you’re worried about bad intentions.
            
              So. What I  tell you is that I believe that putting governance of the language into the hands
              of the open-source community is critical for Carbon’s long-term success, and that Carbon’s developers
              understand this.
            
              Whether Carbon will find widespread adoption depends on whether  trust
              Carbon’s stewards to handle the language with enough responsibility that migrating their own C++ code to
              Carbon seems like a safe offer.
            
              This sort of trust is hard to establish as long as there’s a single owner,  if that
              owner is Google.
            
              Second: That Carbon finds any public adoption at all is also pretty important  the primary
              goal was just to use it purely within Google. This might come as a surprise, but it’s pretty simple:
              People who’re expected to use Carbon first need to learn Carbon. This is  easier
              when Google can rely on a broad ecosystem of tutorials, libraries and discussion boards outside of its
              intranet.
            
              So in other words, for Carbon to become successful, it’s critical that there’s a public community, and
              that enterprise users of C++  Carbon.
            These are huge incentives to push the language towards independent community ownership.
              Both of these points (trust by enterprise users and need for a public community) were
               for Go (which was also
              spearheaded by Google), primarily due to Go’s simplicity, the fact that there was far less competition in
              the programming language space when Go released, and the fact that it was a language for greenfield
              projects. (That is, it didn’t require convincing ancient C++ coders to perform a massive migration and
              rework their tool chain.)
            
              My understanding is that
              Carbon’s leads understand all of this, and want the project to be community driven. For now, that’s more than good enough for me. For a
              project this early in its life-cycle, it’s nice to see that they’re thinking about this at all, and have
              made an explicit commitment to community ownership.
            Conclusion: There is no free lunch.
              The prospect of building a  to C++—arguably single most important programming language
              currently in existence—sounds like it should be doomed to fail.
            
              I’ll repeat what I said before, and what should be common knowledge: C++ is an incredibly complex
              programming language. It’s under-annotated, has multiple implementations (governed by a 2000+ page ISO
              Standard document), carries four decades of technical baggage, is full of undefined behavior, and has a
              frequently abused Turing-complete quasi-code-generation meta-programming language built into it.
            
              All of that should make it near impossible to succeed C++. Complexity is in fact a form of job security.
              So why am I still relatively confident in Carbon’s potential?
            Simple, it’s mainly since the priorities look correct to me. Carbon understands that
                C++’s inability to evolve, modernize, deprecate, migrate and standardize is  critical issue
                which the language is facing today.
                You cannot improve on this without making concessions. This goes both ways: There is old C++ code which
                you will not be able to support. At the same time, there are C++ features which you  to
                support, whether you want to or not.
              
                This is a herculean task that requires a massive initial investment (a whole new programming language),
                and a complete rethinking of tooling, communication, software engineering and language development
                practices.
              
              The inability to evolve is an issue for people who’re just starting to learn C++, and who stumble into
              every single footgun that hasn’t been taken care of over the past thirty years.
            
              It’s an issue for people who care for high-quality code, readability or memory safety, and see no viable
              path towards getting their C++ codebase into that state.
            
              It’s an issue for committee and compiler contributors, who need to carefully consider how a new
              feature will interact with literally everything else the language already supports.
            
              You might disagree with that assessment. It might not be an issue . That’s
              fine. C++ (for a given version, anyway) will stay exactly as it is. It’s not going to go away anytime
              soon, and that’s a good thing. People depend on that. Critical infrastructure depends on that.
            
              As for myself, I am incredibly glad to see that  is trying to take this bull by the horns,
              and willing to face this charging billion lines-of-code mountain of complexity and technical debt head-on.
            
              At last but not at least just since it would be  if we (humanity, as a whole)
              could actually pull it off, and don’t need to pass tales warning people about the dangers of using
               across the generations.
            
              It might take a while, but that’s fine. This is a long-term project. It  to be a long-term
              project to make this work. Once you start thinking about it from that perspective, everything makes a lot
              more sense.
            
              Remember, those millions of lines of C++ code are not going to go away anytime soon. They’ll still be
              there in a few decades. It’s either a large-scale migration (in some form or another), or nothing.
            
              In the meantime, if you’re remotely interested, I’ll reiterate that I highly recommend Carbon’s talks, eg.
              this one.
            
              Writing this took significantly longer than expected. The total number of footnotes written and deleted is
              about forty.
            Let me know if you got something out of it—It means a lot to me.
              Questions, suggestions, comments, writing advice, reading recommendations, music suggestions, pictures of
              pets and basically anything else you can think of are all welcome via my contact e-mail at the bottom of
              the page.
            ]]></content:encoded></item><item><title>Jacksonpollock.org (2003)</title><link>https://jacksonpollock.org/</link><author>memalign</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 21:22:49 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Tips for mathematical handwriting (2007)</title><link>https://johnkerl.org/doc/ortho/ortho.html</link><author>susam</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 19:20:10 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Now that you’re majoring in one of the technical disciplines
(engineering, science, or math), you’re going to be spending a
significant amount of time communicating in writing with others.  You may find
that previously unimportant details, such as crossing your ’s,
now become essential — not only so that others can understand you, but
also so that you can avoid mistaking your own 2 for
 and so on.  This is especially important if your
handwriting (like mine!) is less than perfect.

Before I continue, take a fresh look at our Roman alphabet, the digits, and
the Greek alphabet:

Notice that these mechanically typeset symbols are all clear and distinct
(except that lowercase omicron and most of the uppercase Greek letters look
like Roman letters — we don’t use these “duplicates”).

 When we write by hand, though, symbols can become ambiguous —
we’re not machines, and things get a little loopy when we hurry.  In
prose, surrounding letters can disambiguate a questionable letter — e.g.
you can guess that the fourth letter of  has to be an .
But in mathematical expressions we mix symbols from different alphabets, in
different orders, so context can’t assist us — and when we guess,
we often guess wrong.  So it now becomes very important that each letter be
clearly recognizable on its own merits.

Here are samples, followed by the points I consider most important.

]]></content:encoded></item><item><title>Need Your Suggestions -- Building a Syncthing-like System for Our Final Project</title><link>https://www.reddit.com/r/golang/comments/1ikuyvg/need_your_suggestions_building_a_syncthinglike/</link><author>/u/HunTTeR-47</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 8 Feb 2025 19:10:02 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[My team and I are in our final semester of Computer Science, and we’re working on a file synchronization system inspired by Syncthing. We’re planning to include features like , , and  (Windows, Linux, and maybe Android).We’d love to hear your thoughts on:: Are there any cool or efficient methods we should look into?: What would make this system stand out or be more useful?: Any tutorials, tools, or reading materials you’d recommend?If you’ve worked on something similar or have ideas, we’d really appreciate your input! Thanks in advance for your help – you’re awesome!]]></content:encoded></item><item><title>Golang used to connect a PS1 to a PS4 by converting serial to websocket data</title><link>https://www.youtube.com/watch?v=ODRqp4eJ-F0</link><author>/u/Western-Hotel8723</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 8 Feb 2025 18:27:01 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Chez Scheme txtar port from Go</title><link>https://git.sr.ht/~egtann/txtar/</link><author>hellcow</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 18:24:39 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[txtar enables you to work with a simple text archive format compatible with
https://golang.org/x/tools/txtar. It concatenates files together and allows for
a top-level comment.Math implementation
-- math.h --
#ifndef MATH_H
#define MATH_H

int add(int a, int b);

#endif
-- math.c --
#include "math.h"

int add(int a, int b) {
    return a + b;
}
This format is easy for humans to read and write by hand and is perfect for
test data.
$makeinstall


$If you want to remove the library from your system, simply run .You can obtain these libraries and many more via Thunderchez:$gitclonehttps://github.com/ovenpasta/thunderchez


$/path/to/thunderchez
All the exports of this library are documented with type expectations. I
encourage you to scan the implementation.A few common example usecases are presented below for convenience:To use this library, simply import :#Construct an archive from a list of filenames#Write text to an archive file#Retrieve a file from an archiveCopyright (C) 2025 Evan Tann, ParaVoce LLCThis program is free software: you can redistribute it and/or modify it under
the terms of the GNU Affero General Public License as published by the Free
Software Foundation, either version 3 of the License, or (at your option) any
later version.This program is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.]]></content:encoded></item><item><title>Most Linux users dont allow the browser to collect data about their system. So, we won?</title><link>https://www.reddit.com/r/linux/comments/1iktbzl/most_linux_users_dont_allow_the_browser_to/</link><author>/u/ParamedicDirect5832</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 18:01:37 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Can anyone ELI5 the general rust in linux kernel drama?</title><link>https://www.reddit.com/r/linux/comments/1ikt1fq/can_anyone_eli5_the_general_rust_in_linux_kernel/</link><author>/u/Nothos927</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 17:49:43 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I only vaguely follow kernel dev but I've seen there's been another instance of drama over incorporating rust into the kernel that only seems to make complete sense if you already know what's going on.As far as I can tell, roughly what's happened so far is:Linus (and other maintainers?) have traditionally been iffy on adding new languages like C++ to the kernelHowever with rust becoming more popular and younger coders who learnt rust first it was decided to allow some small bits of rust in the mainline kernel codebaseA certain subset of maintainers were/are extremely opposed to rust codeThere isn't actually much rust code there yet, what is there is mostly just the plumbing needed to get the rust code able to call existing functions safely. We are seeing more out of tree rust drivers being written that rely on these interfaces.So really I'm wondering how off the mark that assessment is and why some maintainers still have so much opposition? Is it ideological? Technical? It also seems like this entire thing is touching on broader issues with the kernel development process itself and stuff like tooling?]]></content:encoded></item><item><title>Writing a simple windows driver in Rust</title><link>https://scorpiosoftware.net/2025/02/08/writing-a-simple-driver-in-rust/</link><author>ingve</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 17:25:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The Rust language ecosystem is growing each day, its popularity increasing, and with good reason. It’s the only mainstream language that provides memory and concurrency safety at compile time, with a powerful and rich build system (cargo), and a growing number of packages (crates).My daily driver is still C++, as most of my work is about low-level system and kernel programming, where the Windows C and COM APIs are easy to consume. Rust is a system programming language, however, which means it plays, or at least can play, in the same playground as C/C++. The main snag is the verbosity required when converting C types to Rust. This “verbosity” can be alleviated with appropriate wrappers and macros. I decided to try writing a simple WDM driver that is not useless – it’s a Rust version of the “Booster” driver I demonstrate in my book (Windows Kernel Programming), that allows changing the priority of any thread to any value.To prepare for building drivers, consult Windows Drivers-rs, but basically you should have a WDK installation (either normal or the EWDK). Also, the docs require installing LLVM, to gain access to the Clang compiler. I am going to assume you have these installed if you’d like to try the following yourself.We can start by creating a new Rust library project (as a driver is a technically a DLL loaded into kernel space):We can open the booster folder in VS Code, and begin are coding. First, there are some preparations to do in order for actual code to compile and link successfully. We need a  file to tell cargo to link statically to the CRT. Add a  file to the root booster folder, with the following code:fn main() -> Result<(), wdk_build::ConfigError> {
    std::env::set_var("CARGO_CFG_TARGET_FEATURE", "crt-static");
    wdk_build::configure_wdk_binary_build()
}
(Syntax highlighting is imperfect because the WordPress editor I use does not support syntax highlighting for Rust)Next, we need to edit  and add all kinds of dependencies. The following is the minimum I could get away with:[package]
name = "booster"
version = "0.1.0"
edition = "2021"

[package.metadata.wdk.driver-model]
driver-type = "WDM"

[lib]
crate-type = ["cdylib"]
test = false

[build-dependencies]
wdk-build = "0.3.0"

[dependencies]
wdk = "0.3.0"       
wdk-macros = "0.3.0"
wdk-alloc = "0.3.0" 
wdk-panic = "0.3.0" 
wdk-sys = "0.3.0"   

[features]
default = []
nightly = ["wdk/nightly", "wdk-sys/nightly"]

[profile.dev]
panic = "abort"
lto = true

[profile.release]
panic = "abort"
lto = true
The important parts are the WDK crates dependencies. It’s time to get to the actual code in . We start by removing the standard library, as it does not exist in the kernel:Next, we’ll add a few  statements to make the code less verbose:use core::ffi::c_void;
use core::ptr::null_mut;
use alloc::vec::Vec;
use alloc::{slice, string::String};
use wdk::*;
use wdk_alloc::WdkAllocator;
use wdk_sys::ntddk::*;
use wdk_sys::*;
The  crate provides the low level interop kernel functions. the  crate provides higher-level wrappers.  is an interesting one. Since we can’t use the standard library, you would think the types like  are not available, and technically that’s correct. However,  is actually defined in a lower level module named , that can be used outside the standard library. This works because the only requirement for  is to have a way to allocate and deallocate memory. Rust exposes this aspect through a global allocator object, that anyone can provide. Since we have no standard library, there is no global allocator, so one must be provided. Then,  (and ) can work normally:#[global_allocator]
static GLOBAL_ALLOCATOR: WdkAllocator = WdkAllocator;
This is the global allocator provided by the WDK crates, that use and to manage allocations, just like would do manually.Next, we add two  crates to get the support for the allocator and a panic handler – another thing that must be provided since the standard library is not included.  has a setting to abort the driver (crash the system) if any code panics:extern crate wdk_panic;
extern crate alloc;
Now it’s time to write the actual code. We start with , the entry point to any Windows kernel driver:#[export_name = "DriverEntry"]
pub unsafe extern "system" fn driver_entry(
    driver: &mut DRIVER_OBJECT,
    registry_path: PUNICODE_STRING,
) -> NTSTATUS {
Those familiar with kernel drivers will recognize the function signature (kind of). The function name is  to conform to the snake_case Rust naming convention for functions, but since the linker looks for , we decorate the function with the  attribute. You could use  and just ignore or disable the compiler’s warning, if you prefer.We can use the familiar  macro, that was reimplemented by calling , as you would if you were using C/C++. You can still call , mind you, but  is just easier:println!("DriverEntry from Rust! {:p}", &driver);
let registry_path = unicode_to_string(registry_path);
println!("Registry Path: {}", registry_path);
Unfortunately, it seems  does not yet support a , so we can write a function named  to convert a  to a normal Rust string:fn unicode_to_string(str: PCUNICODE_STRING) -> String {
    String::from_utf16_lossy(unsafe {
        slice::from_raw_parts((*str).Buffer, (*str).Length as usize / 2)
    })
}
Back in , our next order of business is to create a device object with the name “\Device\Booster”:let mut dev = null_mut();
let mut dev_name = UNICODE_STRING::default();
string_to_ustring("\\Device\\Booster", &mut dev_name);

let status = IoCreateDevice(
    driver,
    0,
    &mut dev_name,
    FILE_DEVICE_UNKNOWN,
    0,
    0u8,
    &mut dev,
);
The  function converts a Rust string to a :fn string_to_ustring<'a>(s: &str, uc: &'a mut UNICODE_STRING) -> Vec<u16> {
    let mut wstring: Vec<_> = s.encode_utf16().collect();
    uc.Length = wstring.len() as u16 * 2;
    uc.MaximumLength = wstring.len() as u16 * 2;
    uc.Buffer = wstring.as_mut_ptr();
    wstring
}
This may look more complex than we would like, but think of this as a function that is written once, and then just used all over the place. In fact, maybe there is such a function already, and just didn’t look hard enough. But it will do for this driver.If device creation fails, we return a failure status:if !nt_success(status) {
    println!("Error creating device 0x{:X}", status);
    return status;
}
 is similar to the  macro provided by the WDK headers.Next, we’ll create a symbolic link so that a standard  call could open a handle to our device:let mut sym_name = UNICODE_STRING::default();
let _ = string_to_ustring("\\??\\Booster", &mut sym_name);
let status = IoCreateSymbolicLink(&mut sym_name, &mut dev_name);
if !nt_success(status) {
    println!("Error creating symbolic link 0x{:X}", status);
    IoDeleteDevice(dev);
    return status;
}
All that’s left to do is initialize the device object with support for Buffered I/O (we’ll use  for simplicity), set the driver unload routine, and the major functions we intend to support:    (*dev).Flags |= DO_BUFFERED_IO;

    driver.DriverUnload = Some(boost_unload);
    driver.MajorFunction[IRP_MJ_CREATE as usize] = Some(boost_create_close);
    driver.MajorFunction[IRP_MJ_CLOSE as usize] = Some(boost_create_close);
    driver.MajorFunction[IRP_MJ_WRITE as usize] = Some(boost_write);

    STATUS_SUCCESS
}
Note the use of the Rust  type to indicate the presence of a callback.The unload routine looks like this:unsafe extern "C" fn boost_unload(driver: *mut DRIVER_OBJECT) {
    let mut sym_name = UNICODE_STRING::default();
    string_to_ustring("\\??\\Booster", &mut sym_name);
    let _ = IoDeleteSymbolicLink(&mut sym_name);
    IoDeleteDevice((*driver).DeviceObject);
}
We just call  and , just like a normal kernel driver would. We have three request types to handle – , , and . Create and close are trivial – just complete the IRP successfully:unsafe extern "C" fn boost_create_close(_device: *mut DEVICE_OBJECT, irp: *mut IRP) -> NTSTATUS {
    (*irp).IoStatus.__bindgen_anon_1.Status = STATUS_SUCCESS;
    (*irp).IoStatus.Information = 0;
    IofCompleteRequest(irp, 0);
    STATUS_SUCCESS
}
The  is an  but it’s defined with a  containing  and . This seems to be incorrect, as  should be in a  with  (not ). Anyway, the code accesses the  member through the “auto generated” union, and it looks ugly. Definitely something to look into further. But it works.The real interesting function is the  handler, that does the actual thread priority change. First, we’ll declare a structure to represent the request to the driver:#[repr(C)]
struct ThreadData {
    pub thread_id: u32,
    pub priority: i32,
}
The use of  is important, to make sure the fields are laid out in memory just as they would with C/C++. This allows non-Rust clients to talk to the driver. In fact, I’ll test the driver with a C++ client I have that used the C++ version of the driver. The driver accepts the thread ID to change and the priority to use. Now we can start with :unsafe extern "C" fn boost_write(_device: *mut DEVICE_OBJECT, irp: *mut IRP) -> NTSTATUS {
    let data = (*irp).AssociatedIrp.SystemBuffer as *const ThreadData;
First, we grab the data pointer from the  in the IRP, as we asked for Buffered I/O support. This is a kernel copy of the client’s buffer. Next, we’ll do some checks for errors:let status;
loop {
    if data == null_mut() {
        status = STATUS_INVALID_PARAMETER;
        break;
    }
    if (*data).priority < 1 || (*data).priority > 31 {
        status = STATUS_INVALID_PARAMETER;
        break;
    }
The  statement creates an infinite block that can be exited with a . Once we verified the priority is in range, it’s time to locate the thread object:let mut thread = null_mut();
status = PsLookupThreadByThreadId(((*data).thread_id) as *mut c_void, &mut thread);
if !nt_success(status) {
    break;
}
 is the one to use. If it fails, it means the thread ID probably does not exist, and we break. All that’s left to do is set the priority and complete the request with whatever status we have:        KeSetPriorityThread(thread, (*data).priority);
        ObfDereferenceObject(thread as *mut c_void);
        break;
    }
    (*irp).IoStatus.__bindgen_anon_1.Status = status;
    (*irp).IoStatus.Information = 0;
    IofCompleteRequest(irp, 0);
    status
}
The only remaining thing is to sign the driver. It seems that the crates support signing the driver if an INF or INX files are present, but this driver is not using an INF. So we need to sign it manually before deployment. The following can be used from the root folder of the project:signtool sign /n wdk /fd sha256 target\debug\booster.dll
The  uses a WDK test certificate typically created automatically by Visual Studio when building drivers. I just grab the first one in the store that starts with “wdk” and use it.The silly part is the file extension – it’s a DLL and there currently is no way to change it automatically as part of cargo build. If using an INF/INX, the file extension does change to SYS. In any case, file extensions don’t really mean that much – we can rename it manually, or just leave it as DLL. The resulting file can be installed in the “normal” way for a software driver, such as using the  tool (from an elevated command window), on a machine with test signing on. Then  can be used to load the driver into the system:sc.exe sc create booster type= kernel binPath= c:\path_to_driver_file
sc.exe start booster
I used an existing C++ application that talks to the driver and expects to pass the correct structure. It looks like this:#include <Windows.h>
#include <stdio.h>

struct ThreadData {
	int ThreadId;
	int Priority;
};

int main(int argc, const char* argv[]) {
	if (argc < 3) {
		printf("Usage: boost <tid> <priority>\n");
		return 0;
	}

	int tid = atoi(argv[1]);
	int priority = atoi(argv[2]);

	HANDLE hDevice = CreateFile(L"\\\\.\\Booster",
		GENERIC_WRITE, 0, nullptr, OPEN_EXISTING, 0,
		nullptr);

	if (hDevice == INVALID_HANDLE_VALUE) {
		printf("Failed in CreateFile: %u\n", GetLastError());
		return 1;
	}

	ThreadData data;
	data.ThreadId = tid;
	data.Priority = priority;
	DWORD ret;
	if (WriteFile(hDevice, &data, sizeof(data),
		&ret, nullptr))
		printf("Success!!\n");
	else
		printf("Error (%u)\n", GetLastError());

	CloseHandle(hDevice);

	return 0;
}
Here is the result when changing a thread’s priority to 26 (ID 9408):Writing kernel drivers in Rust is possible, and I’m sure the support for this will improve quickly. The WDK crates are at version 0.3, which means there is still a way to go. To get the most out of Rust in this space, safe wrappers should be created so that the code is less verbose, does not have  blocks, and enjoys the benefits Rust can provide. Note, that I may have missed some wrappers in this simple implementation.You can find a couple of more samples for KMDF Rust drivers here.]]></content:encoded></item><item><title>Show HN: FlashSpace – fast, open-source, macOS Spaces replacement</title><link>https://github.com/wojciech-kulik/FlashSpace</link><author>wojciech-kulik</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 17:19:23 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[I've recently launched a new open-source project aimed at enhancing the experience of switching between Spaces/workspaces on macOS. The built-in Spaces feature is often slow and unfriendly. This project is designed to boost your productivity :). Enjoy!]]></content:encoded></item><item><title>Mastering cross-database operations with PostgreSQL FDW</title><link>https://packagemain.tech/p/mastering-cross-database-operations-with-postgres-fdw</link><author>/u/der_gopher</author><category>dev</category><category>reddit</category><pubDate>Sat, 8 Feb 2025 17:17:47 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[PostgreSQL’s In this post we will talk about:Setting up FDW to connect to external databases (ex: another PostgreSQL instance).Importing foreign schemas.Creating archival policies with stored proceduresAutomating archival tasks with pg_cronMultiplayerFDW is an extension that implements the SQL/MED standard, allowing PostgreSQL to interact with external data sources.There is a lot of different FDW extensions that will let you connect to a variety of databases (relational, no-sql, files etc..)Connects to other PostgreSQL databases (local or remote)Supports read/write operationsReads data from flat files (CSV, TSV, etc..)Connects to MySQL/MariaDB databasesSupports basic queries and joinshereAs you can imagine, there are many use cases possible. From reading files from your database, to getting cache keys from Redis and augment them from content stored in your PostgreSQL.The use case we will focus on, is an automatic archival from PostgreSQL to PostgreSQL.-- Enable FDW   
CREATE EXTENSION IF NOT EXISTS postgres_fdw;-- Create a foreign server
CREATE SERVER postgres_server FOREIGN DATA WRAPPER postgres_fdw
   OPTIONS (host '{FOREIGN_HOST}', dbname '{FOREIGN_DB_NAME}');

-- Map local PostgreSQL user to foreign server credentials
CREATE USER MAPPING FOR postgres SERVER postgres_server
   OPTIONS (user '{FOREIGN_USERNAME}', password '{FOREIGN_PASSWORD}');You might be tempted to import foreign schema from the other postgres into the “public” schema of your current instance, however you can’t have 2 tables with the same name. That’s why, I would suggest you create a separate schema, you can call it foreign_schema or whatever you wish:-- Create a new schema to import the tables into
CREATE SCHEMA foreign_schema;

-- Import all tables from the foreign postgreSQL database
IMPORT FOREIGN SCHEMA public FROM SERVER postgres_server INTO foreign_scehma;

-- You can also import only specific tables
IMPORT FOREIGN SCHEMA public LIMIT TO ({TABLE1}, {TABLE2}) FROM SERVER postgres_server INTO foreign_schema;Now you can query your foreign database:SELECT * FROM foreign_schema.{TABLE1};The distant server is used as archive and the current db (the one you are connected to) is your live DB. CREATE TABLE IF NOT EXISTS transactions (
    id uuid DEFAULT uuid_generate_v4(),
    amount INT DEFAULT 0,
    created_at   TIMESTAMPTZ DEFAULT NOW(),

    PRIMARY KEY (id)
); CREATE OR REPLACE PROCEDURE archive_old_transactions()  
LANGUAGE plpgsql  
AS $$  
BEGIN  
  -- Move data older than 1 year to archive  
  INSERT INTO foreign_schema.transactions  
  SELECT * FROM public.transactions  
  WHERE created_at < NOW() - INTERVAL '1 year';  

  -- Delete archived data from main table  
  DELETE FROM public.transactions  
  WHERE created_at < NOW() - INTERVAL '1 year';  
END;  
$$
;  -- Enable pg cron   
CREATE EXTENSION IF NOT EXISTS pg_cron;-- Run at 2 AM daily
SELECT cron.schedule(
  'archive_transactions',
  '0 2 * * *',
  'CALL archive_old_transactions()'
); CREATE VIEW combined_transactions AS (
    WITH remote_data AS (
      SELECT * FROM foreign_schema.transactions
    ),
    local_data AS (
      SELECT * FROM public.transactions
    )
    SELECT * FROM remote_data
    UNION ALL
    SELECT * FROM local_data
);You will notice that I didn’t simply do a UNION of two tables, I used CTE (Common Table Expressions) because it is crucial for optimizing queries with foreign tables. Essentially, it containerizes the FDW query, because the query planner will have to ask the foreign database to execute its part, and the clearer this query is, the faster it will be.Containerize the FDW query to reduce data transferFilter data at the source Minimize the returned row count Nothing forces you to have the exact same copy of foreign and local table, neither you have to force foreign keys or equivalent indexes. It is probably recommended to create specific indexes in your foreign table that will match the query patterns they will be submitted to. Because you don’t query archive data the same way you might query live data. The concept of archival is often to reclaim space, but if you were to need faster access to data from both the archive and the live DB then you could use Materialized views to ensure fast queries. So if you query transactions from 2 years ago from now, until now. You necessarily have to query both tables.SELECT * FROM combined_transactions WHERE created_at BETWEEN(NOW() - INTERVAL '2 year', NOW()); PostgreSQL FDW transforms your database into a unified gateway for cross-database operations. By combining it with pg_cron and stored procedures, you can automate complex workflows like archival, reporting, and data synchronization without external tools. Multiplayer]]></content:encoded></item><item><title>Securing Kubernetes Secrets &amp; Disaster Recovery with SOPS and FluxCD — My Journey</title><link>https://www.reddit.com/r/kubernetes/comments/1ikrydu/securing_kubernetes_secrets_disaster_recovery/</link><author>/u/mustybatz</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 8 Feb 2025 17:04:02 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I recently explored securing Kubernetes secrets and disaster recovery using  in a GitOps setup, and I thought this could be helpful for others working with Kubernetes (home labs or production).Encrypt and store secrets directly in Git with .Automatically decrypt and deploy them using .Disaster recovery using GitOps workflows + backup strategies with NAS and Velero.Do you prefer  or ?What’s your go-to strategy for persistent data backups?Let me know your thoughts or feedback!    submitted by    /u/mustybatz ]]></content:encoded></item><item><title>The Deck: An open-source cross-platform multiplayer card game engine in Flutter</title><link>https://github.com/xajik/thedeck</link><author>igor_st</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 15:47:36 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>We are destroying software</title><link>https://antirez.com/news/145</link><author>antirez</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 14:48:25 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>VSCode’s SSH agent is bananas</title><link>https://fly.io/blog/vscode-ssh-wtf/</link><author>zdyxry</author><category>dev</category><category>hn</category><pubDate>Sat, 8 Feb 2025 01:25:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We’re interested in getting integrated into the flow VSCode uses to do remote editing over SSH, because everybody is using VSCode now, and, in particular, they’re using forks of VSCode that generate code with LLMs. ”hallucination” is what we call it when LLMs get code wrong; “engineering” is what we call it when people do.LLM-generated code is useful in the general case if you know what you’re doing. But it’s ultra-useful if you can close the loop between the LLM and the execution environment (with an “Agent” setup). There’s lots to say about this, but for the moment: it’s a semi-effective antidote to hallucination: the LLM generates the code, the agent scaffolding runs the code, the code generates errors, the agent feeds it back to the LLM, the process iterates. So, obviously, the issue here is you don’t want this iterative development process happening on your development laptop, because LLMs have boundary issues, and they’ll iterate on your system configuration just as happily on the Git project you happen to be working in. A thing you’d really like to be able to do: run a closed-loop agent-y (“agentic”? is that what we say now) configuration for an LLM, on a clean-slate Linux instance that spins up instantly and that can’t screw you over in any way. You get where we’re going with this.Anyways! I would like to register a concern.Emacs hosts the spiritual forebearer of remote editing systems, a blob of hyper-useful Elisp called “Tramp”. If you can hook Tramp up to any kind of interactive environment — usually, an SSH session — where it can run Bourne shell commands, it can extend Emacs to that environment.So, VSCode has a feature like Tramp. Which, neat, right? You’d think, take Tramp, maybe simplify it a bit, switch out Elisp for Typescript.Unlike Tramp, which lives off the land on the remote connection, VSCode mounts a full-scale invasion: it runs a Bash snippet stager that downloads an agent, including a binary installation of Node. The agent runs over port-forwarded SSH. It establishes a WebSockets connection back to your running VSCode front-end. The underlying protocol on that connection can:Wander around the filesystem
Launch its own shell PTY processes
In security-world, there’s a name for tools that work this way. I won’t say it out loud, because that’s not fair to VSCode, but let’s just say the name is murid in nature.I would be a little nervous about letting people VSCode-remote-edit stuff on dev servers, and apoplectic if that happened during an incident on something in production. It turns out we don’t have to care about any of this to get a custom connection to a Fly Machine working in VSCode, so none of this matters in any kind of deep way, but: we’ve decided to just be a blog again, so: we had to learn this, and now you do too.]]></content:encoded></item><item><title>Google&apos;s 7-Year Slog To Improve Chrome Extensions Still Hasn&apos;t Satisfied Developers</title><link>https://developers.slashdot.org/story/25/02/07/2246202/googles-7-year-slog-to-improve-chrome-extensions-still-hasnt-satisfied-developers?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Sat, 8 Feb 2025 01:25:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[The Register's Thomas Claburn reports: Google's overhaul of Chrome's extension architecture continues to pose problems for developers of ad blockers, content filters, and privacy tools. [...] While Google's desire to improve the security, privacy, and performance of the Chrome extension platform is reasonable, its approach -- which focuses on code and permissions more than human oversight -- remains a work-in-progress that has left extension developers frustrated.
 
Alexei Miagkov, senior staff technology at the Electronic Frontier Foundation, who oversees the organization's Privacy Badger extension, told The Register, "Making extensions under MV3 is much harder than making extensions under MV2. That's just a fact. They made things harder to build and more confusing." Miagkov said with Privacy Badger the problem has been the slowness with which Google addresses gaps in the MV3 platform. "It feels like MV3 is here and the web extensions team at Google is in no rush to fix the frayed ends, to fix what's missing or what's broken still." According to Google's documentation, "There are currently no open issues considered a critical platform gap," and various issues have been addressed through the addition of new API capabilities.
 
Miagkov described an unresolved problem that means Privacy Badger is unable to strip Google tracking redirects on Google sites. "We can't do it the correct way because when Google engineers design the [chrome.declarativeNetRequest API], they fail to think of this scenario," he said. "We can do a redirect to get rid of the tracking, but it ends up being a broken redirect for a lot of URLs. Basically, if the URL has any kind of query string parameters -- the question mark and anything beyond that -- we will break the link." Miagkov said a Chrome developer relations engineer had helped identify a workaround, but it's not great. Miagkov thinks these problems are of Google's own making -- the company changed the rules and has been slow to write the new ones. "It was completely predictable because they moved the ability to fix things from extensions to themselves," he said. "And now they need to fix things and they're not doing it."]]></content:encoded></item><item><title>Obscure islands I find interesting</title><link>https://amanvir.com/obscure-islands</link><author>venusgirdle</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 22:23:31 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: ExpenseOwl – Simple, self-hosted expense tracker</title><link>https://github.com/Tanq16/ExpenseOwl</link><author>import-base64</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 20:56:58 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>UK demands backdoor for encrypted Apple user data...</title><link>https://www.youtube.com/watch?v=ozkg_iW9mNU</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/ozkg_iW9mNU?version=3" length="" type=""/><pubDate>Fri, 7 Feb 2025 20:20:21 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Try Brilliant free for 30 days https://brilliant.org/fireship You’ll also get 20% off an annual premium subscription.

The United Kingdom is demanding Apple build a backdoor to access encrypted iCloud user data. Learn how end-to-end-encryption works and other tools that protect your digital privacy. 

#apple #tech #thecodereport 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources

Mainstream Source https://www.washingtonpost.com/technology/2025/02/07/apple-encryption-backdoor-uk/
Full Cryptography Tutorial https://youtu.be/NuyzuNBFWxQ
Apple Intelligence gone Wild https://youtu.be/7rXgVsIGvGQ
Tails OS in 100 Seconds https://youtu.be/mVKAyw0xqxw

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered

- Why does the UK want Apple iCloud data?
- What is a double rachet algorithm?
- How does signal e2ee protocol work?
- How do I keep browsing data private?
- Best Linux distro for privacy]]></content:encoded></item><item><title>Do-nothing scripting: the key to gradual automation (2019)</title><link>https://blog.danslimmon.com/2019/07/15/do-nothing-scripting-the-key-to-gradual-automation/</link><author>tehnub</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 19:48:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Every ops team has some manual procedures that they haven’t gotten around to automating yet. Toil can never be totally eliminated.Very often, the biggest toil center for a team at a growing company will be its procedure for modifying infrastructure or its procedure for provisioning user accounts. Partial instructions for the latter might look like this:Create an SSH key pair for the user.Commit the public key to Git and push to master.Wait for the build job to finish.Find the user’s email address in the employee directory.Send the user their private key via 1Password.This is a relatively short example. Sometimes there are 20 steps in the process. Sometimes there are branches and special cases to keep track of as you go. Over time, these procedures can become unmanageably large and complex.Procedures like this are frustrating because they’re focus-intensive yet require very little thought. They demand our full attention, but our attention isn’t rewarded with interesting problems or satisfying solutions – just another checkbox checked. I have a word for a procedure like this: a .We know that this procedure is ripe for automation. We can easily see how to automate any given step. And we know that a computer could carry out the instructions with far greater speed and accuracy than we can, and with less tendency toward practical drift.However, automating slogs sometimes feels like an all-or-nothing proposition. Sure, we could write a script to handle step 2, or step 5. But that wouldn’t  make the procedure any less cumbersome. It would lead to a proliferation of single-purpose scripts with different conventions and expectations, and you’d still have to follow a documented multi-step procedure for using those scripts.This perception of futility is the problem we really need to solve in order to escape from these manual slogs. I’ve found an approach that works pretty reliably: .Almost any slog can be turned into a . A do-nothing script is a script that encodes the instructions of a slog, encapsulating each step in a function. For the example procedure above, we could write the following do-nothing script:import sys

def wait_for_enter():
    raw_input("Press Enter to continue: ")

class CreateSSHKeypairStep(object):
    def run(self, context):
        print("Run:")
        print("   ssh-keygen -t rsa -f ~/{0}".format(context["username"]))
        wait_for_enter()

class GitCommitStep(object):
    def run(self, context):
        print("Copy ~/new_key.pub into the `user_keys` Git repository, then run:")
        print("    git commit {0}".format(context["username"]))
        print("    git push")
        wait_for_enter()

class WaitForBuildStep(object):
    build_url = "http://example.com/builds/user_keys"
    def run(self, context):
        print("Wait for the build job at {0} to finish".format(self.build_url))
        wait_for_enter()

class RetrieveUserEmailStep(object):
    dir_url = "http://example.com/directory"
    def run(self, context):
        print("Go to {0}".format(self.dir_url))
        print("Find the email address for user `{0}`".format(context["username"]))
        context["email"] = raw_input("Paste the email address and press enter: ")

class SendPrivateKeyStep(object):
    def run(self, context):
        print("Go to 1Password")
        print("Paste the contents of ~/new_key into a new document")
        print("Share the document with {0}".format(context["email"]))
        wait_for_enter()

if __name__ == "__main__":
    context = {"username": sys.argv[1]}
    procedure = [
        CreateSSHKeypairStep(),
        GitCommitStep(),
        WaitForBuildStep(),
        RetrieveUserEmailStep(),
        SendPrivateKeyStep(),
    ]
    for step in procedure:
        step.run(context)
    print("Done.")
This script doesn’t actually  any of the steps of the procedure. That’s why it’s called a do-nothing script. It feeds the user a step at a time and waits for them to complete each step manually.At first glance, it might not be obvious that this script provides value. Maybe it looks like all we’ve done is make the instructions harder to read. But the value of a do-nothing script is immense:It’s now much less likely that you’ll lose your place and skip a step. This makes it easier to maintain focus and power through the slog.Each step of the procedure is now encapsulated in a function, which makes it possible to replace the text in any given step with code that performs the action automatically.Over time, you’ll develop a library of useful steps, which will make future automation tasks more efficient.A do-nothing script doesn’t save your team any manual effort. It lowers the activation energy for automating tasks, which allows the team to eliminate toil over time.]]></content:encoded></item><item><title>Show HN: A website that heatmaps your city based on your housing preferences</title><link>https://theretowhere.com/</link><author>WiggleGuy</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 18:23:40 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cities can cost effectively start their own utilities</title><link>https://kevin.burke.dev/kevin/norcal-cities-new-utility/</link><author>kevinburke</author><category>dev</category><category>hn</category><pubDate>Fri, 7 Feb 2025 17:55:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[PG&E's rates are high enough that, even with the massive headache and expense
involved, it's feasible for cities to create their own utility and undercut
PG&E's rates. When the savings per household are around $800-$1200 per
year, though, they should take it seriously.Here are the basic components of how much it costs to get electricity to your
house. The cost to actually generate the electricity in a power plant
or utility-scale solar farm. This varies by time of day but typically costs
about 4 cents per kilowatt hour; you can see the current wholesale rate on the
CAISO website. How much it costs to move the power from the power source to
a local substation/transformer, over large transmission lines. PG&E breaks this
out in its detailed rate chart at about 4 cents per kilowatt hour. How much to get the power from your local substation to your
house over local power lines. In PG&E's rate chart, they charge 20 cents per
kilowatt hour for this. That just does not match up with how much it actually
costs them to transmit power over local lines and keep the lines maintained. Operations, maintenance, profit. This is where PG&E is
actually seeing large expenses, because their coverage area is massive, it
costs a lot of money to deliver power to rural customers, and they are also
undertaking a massive project to underground utility lines in fire-prone areas.The high price and design of the electricity system have a number of bad
effects:People really hate inflation. When utility bills spike, it makes people
unhappy and also fuels the (not incorrect) perception that California is poorly
governed.Lower income people spend a higher percentage of their income on electricity,
so higher utility bills disproportionately hurt them.The net effect of charging higher rates to everyone to pay for undergrounding
is that people who live in urban areas are paying more money to subsidize
energy transmission for people who live in $2 million houses in places like the
Berkeley and Orinda hills. This makes no sense.Higher rates for electricity make electricity less competitive vs. gasoline
when people are considering a car purchase. It makes electricity less
competitive vs. natural gas for heating a house, heating water, or choosing a
laundry machine. As gas is warming the planet and electricity is substantially
easier to generate in abundance from renewable sources, it's just bad policy to
have high electricity rates.Let's walk through what this might look like for a particular city to undercut
PG&E's rates. I will pick Walnut Creek because it's a reasonably big city with
a good mix of detached homes and multifamily. Walnut Creek also has experience
with public ownership of amenities - the City operates a golf course and a
downtown parking garage with ground floor retail.There are a number of particular problems with applying PG&E's rates to Walnut
Creek:Walnut Creek is an urban area with a compact footprint that has little acreage
in a high severity wildfire zone. It has two transmission lines as well as a
local transformer grid along Ygnacio Valley Road. It is very cheap to transmit
power from power plants to Walnut Creek, and from transmission lines to every
house in Walnut Creek.Walnut Creek has an above average number of apartments. Apartments do not
have as much space for rooftop solar, and landlords don't have an incentive to
provide rooftop solar because they typically pass through utility costs. This
means NEM1 and NEM2 subsidies — 12% of the average non-solar bill
— disproportionately hurt Walnut Creek renters.Local businesses have disproportionately high energy costs. Safeway and Whole
Foods need to keep a row of refrigerators and freezers running 24/7. When they
pay PG&E's rates to do that, those high energy costs are passed through as
higher food prices.Palo Alto's total electric consumption was 830 gigawatt hours in 2024 - 19% of
this usage was residential, and 81% was businesses and industry uses. Applying
some adjustments for Walnut Creek - our population is bigger, it's a bit hotter
here, and energy use has increased - let's say Walnut Creek uses about 1150
gigawatt hours per year.Palo Alto earned $172 million in revenue for 830 gigawatt hours, which is about
20 cents per kilowatt hour.Here's where Palo Alto's utility company spends money:Acquisition of the network and financing costThe first thing you need to do is buy out PG&E's distribution network - all of
the power poles and local equipment that sits between the transmission lines and
people's houses. San Francisco proposed buying this for $2.5 billion in 2019;
PG&E rejected this offer for being too low. Adjusted for inflation and Walnut
Creek's population, this is about $230 million, let's round up and say $350
million. Let's also assume it costs $50 million in startup costs and one time
expenses to hire utility staff, buy equipment, marketing expense.Cities with an AA credit rating can issue a 30 year loan at about 4% interest.
Borrowing $400 million would cost about $23 million per year in interest and
principal payments.$23 million per year of financing cost spread across 1150 gigawatt hours is
only about 2 cents per kilowatt hour.Generation and distributionPalo Alto spent $114 million buying energy in 2024, about 14 cents per kilowatt
hour. Let's assume Walnut Creek can get power for about 17 cents per kWh.This covers customer service, financial management, billing, engineering work
for maintenance (tree trimming etc), and resource management. Palo Alto spent
$65 million on these expenses in 2023. Let's assume Walnut Creek's costs are
much higher at $90 million per year. This is about 8 cents per kilowatt hour.Another $25 million per year is allocated for grid modernization,
undergrounding, and reliability work. Let's assume this is $35 million per year
for Walnut Creek, which would be about 3 cents per kilowatt hour.Adding this up, we get 30 cents per kilowatt hour, which is ten cents lower than
PG&E's base rate and about 15 cents lower than PG&E's blended rate. At 1150
gigawatt hours, this would save Walnut Creek residential ratepayers about $23
million per year in total, about $800 per ratepayer, and Walnut Creek businesses
about $92 million per year. That is a  amount of money that could go
toward much more productive uses - paying higher salaries, lowering prices for
goods, spending more at local businesses.Most elected officials would jump at the chance to mail every household a $800
check every year. The next best thing is to put $800 back in their pocket.Other Benefits for Walnut CreekThere are huge ancilliary benefits for Walnut Creek to running its own utility
network.Green infrastructure investments: Walnut Creek has made sustainability a
key priority. Palo Alto owns a share in a hydroelectric dam, and Santa Clara
owns a share in a geothermal plant. At a time when there are exciting new
technologies that have the potential to reduce greenhouse gas emissions and
deliver clean, cheap energy to residents - things like Fervo Energy that use
the tech behind fracking to deliver geothermal power - Walnut
Creek can use its very low cost of capital to finance these investments. This
is something PG&E cannot do as effectively, because as a public utility with
massive amounts of debt and wildfire liability, their borrowing cost is much
higher. Public ownership would enable transformative green energy investments
with a low borrowing cost.Encouraging the green transition: A 25% reduction in the cost of
electricity relative to natural gas would make electric upgrades like heat pump
water heaters or electric cars much more financially prudent investments. Like every city in California, Walnut Creek has boom
and bust cycles. Utilities have much more stable revenues than cities. Walnut
Creek could borrow from its utility in recessions, and loan money during booms. Walnut Creek has a number of unincorporated
pockets (San Miguel CDP, Shell Ridge CDP) that
administratively make little sense - they are served by different police,
they have different tax rules. If these homes could save $800 per year on
their utility bill by joining Walnut Creek, this may provide an incentive to
incorporate, which would ultimately lead to better governance.Even if Walnut Creek doesn't ultimately pursue its own utility, just
investigating the possibility may lead PG&E to offer concessions such as
undergrounding the transmission line over downtown. Because you can't
build under a transmission line, this makes a 100 foot wide strip of very
valuable land undevelopable. St. Paul's would love to redevelop its parking
lot under the transmission line for affordable housing, but can only develop
tiny corners of the lot with the transmission line overhead. Undergrounding
the line would deliver huge benefits to Walnut Creek.Lowering the cost of urban living in safe places: PG&E's current rate
structure has urban rate payers subsidize rural rate payers and people who live
in wildfire zones in e.g. the Orinda Hills, who need substantial investment in
order to receive power without sparking wildfires. This is bad policy - instead
of subsidizing fire zones, it should be cheap to live in safe places and more
expensive to live in dangerous places. Lower cost of electricity would reverse
these trends.California is kneecapping its own climate transition with high electricity
prices. The resulting inflation hurts our state's ability to retain a high
class, diverse workforce. Perversely, it also serves as a subsidy to wildfire
zones at the expense of infill areas. It's time to reverse those trends and
deliver lower energy prices in places we want more Californians to live.What should I do if I want this to happen? Cities around the region are
doing "priority setting" exercises for 2025. Contact your Mayor or City
Council and ask them to explore the possibility of creating their own utility,
potentially partnering with other cities. I would probably select cities that do
not have large fire zones (ie, not Orinda or Moraga).]]></content:encoded></item></channel></rss>