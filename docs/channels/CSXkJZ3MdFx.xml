<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Resilient Multi-Cloud Strategies: Harnessing Kubernetes, Cluster API, and... T. Rahman &amp; J. Mosquera</title><link>https://www.youtube.com/watch?v=4DjydLH21nM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/4DjydLH21nM?version=3" length="" type=""/><pubDate>Sun, 20 Apr 2025 22:00:06 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon events in Hong Kong, China (June 10-11); Tokyo, Japan (June 16-17); Hyderabad, India (August 6-7); Atlanta, US (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Resilient Multi-Cloud Strategies: Harnessing Kubernetes, Cluster API, and Cell-Based Architecture - Tasdik Rahman & Javi Mosquera, New Relic 

In today's multi-cloud world, resilience and high availability at scale are crucial. This session will cover how we utilized Kubernetes with Cluster API and other cloud native components, to deploy a cell-based architecture across multiple cloud providers, scaling to 270+ clusters and 18,000+ nodes, creating independent, isolated cells that limit failures and improve uptime, thus simplifying compliance, cost management, and disaster recovery planning.

We'll explore how Cluster API facilitates seamless automation of cluster creation and management across our multi-cloud setup, upgrades, enhancing autonomy and resilience. Moreover, we'll highlight real-world use cases sharing our learnings from automation built for efficient management of k8s clusters while limiting operational overhead.

End users will learn from this talk on how they can use ClusterAPI, to automate their multi cloud cluster lifecycle management and leverage cellular architecture to build a highly available setup.]]></content:encoded></item><item><title>Dart is not just for Flutter, it&apos;s time we start using it on the server. I built wailuku an open source web framework inspired by express.js to help those who want to transtition from js to dart.</title><link>https://github.com/aminedakhlii/wailuku</link><author>/u/PaleContribution6199</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 21:26:23 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[why use dart on the server ? 1- unified language for full stack as Flutter now supports almost all platforms + web 2- compiled language3- null safety and type safe4- a strong community with a variety of packages that server almost every scenarioI think it's time dart gets more recognition on the server, so I built wailuku, a lightweight backend framework that emulates express.js syntax. I'd be super helpful if I can get some feedback, suggestions and contributions. ]]></content:encoded></item><item><title>I&apos;ve Updated My Minecraft Rust Reverse proxy !</title><link>https://www.reddit.com/r/rust/comments/1k3whs6/ive_updated_my_minecraft_rust_reverse_proxy/</link><author>/u/Shadoxter</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 20 Apr 2025 21:04:31 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[A while back I shared my Minecraft reverse proxy Infrarust, which I built while learning Rust. What started as a simple domain-based Minecraft routing tool has grown significantly over the past few months, and I'd love to share what's new!Infrarust is a Minecraft proxy written in Rust that exposes a single Minecraft server port and handles routing to different backend servers. But now it's more!🚀 Server Manager (v1.3.0)On-demand server provisioning: Servers automatically start when players try to connect: Idle servers shut down after configurable periods: Support for Pterodactyl Panel API and local process management: Proxy protocol is supported for both receiving it and sending it to a server !Ban by IP, username, or UUID with custom durationsPersistent storage with automatic expirationDetailed management via CLI commandsReal-time server and player management with commands like , , Rich formatting with colors and tab completionAutomatic discovery of Minecraft servers in Docker containersDynamic reconfiguration when containers start/stopReorganized into specialized crates for better maintainabilityTrait-based API design for flexibilityStandardized logging with the  ecosystem📊 Telemetry support (v1.1.0)Custom Grafana dashboard to supervise the running proxyThis project has been an incredible learning journey. When I first posted, macros scared me! Now I'm implementing trait-based abstractions and async providers. The Rust community resources have been invaluable in helping me learn more about this incredible language !Check out the GitHub repo or visit the documentation to get started (Not updated as of the latest version 1.3.0 was release not long ago).I'd love to hear your feedback, especially on the code architecture and best practices. How does my approach to the provider system and async code look to a more experienced Rust developers (in crates/infrarust_server_manager)?I'm still on a big refactor for my 2.0 release that doesn't have a release date at all. Anyway, thanks for your time! 🦀]]></content:encoded></item><item><title>tiling window manager for the masses!</title><link>https://www.reddit.com/r/linux/comments/1k3vegn/tiling_window_manager_for_the_masses/</link><author>/u/Savings_Walk_1022</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 20:13:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[hey guys i made a window manager called sxwm!its a really, really, really easily configurable tiling windowmanager like dwm or i3wm.its also really fast and uses 0.2m of memory!i hope this can let people experience tiling wm's without any fear.i turned 16 meaning i can have an internship and for a job you need a portfolio. I have nothing so when i found this 2 year old scrap project i thought this was perfect!i also dont like how time consuming patching dwm is and how the quality of the patches vary a  so this project includes all the necessary features of a window manager and makes configuring it easy even though its from a C header.i hope you likemy project and if you make any good improvements please make sure to make a pull request so i can incorporate it to the main branch]]></content:encoded></item><item><title>No Arch hasnt gotten that much better, its Ubuntu that has gotten progressively worse.</title><link>https://www.reddit.com/r/linux/comments/1k3sh49/no_arch_hasnt_gotten_that_much_better_its_ubuntu/</link><author>/u/babuloseo</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 17:59:59 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[See snap breaking server functionality, desktop functionality and more, I stopped using Ubuntu in a server capacity when snaps started breaking packages and was the preffered or default way of installing key packages that I need on my servers. Whereas in Arch things are working pretty damn well, that I am using it in a server capacity and it hasnt dissapointed me yet, it has dissapointed me in late 2010s when I was using custom AURs or patches to support some things, but it feels like Arch has come very very far nowadays whereas Ubuntu seems to have gotten worse slowly.]]></content:encoded></item><item><title>First hormone-free male birth control pill enters human trials</title><link>https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/</link><author>Teever</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 17:54:59 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>JSON Schema to Go struct? or alternatives</title><link>https://www.reddit.com/r/golang/comments/1k3sb3j/json_schema_to_go_struct_or_alternatives/</link><author>/u/One_Poetry776</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 20 Apr 2025 17:52:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm  to Go, and I'm looking for the most idiomatic or recommended way to deal with a JSON Schema.Is there a recommended way to create/generate a model (Go struct or else) based on JSON Schema?{ "$schema": "http://json-schema.org/draft-04/schema#", "type": "object", "properties": { "spec": { "type": "object" }, "metadata": { "type": "object", "properties": { "labels": { "type": "object", "properties": { "abc": { "type": "boolean" } }, "required": [ "abc" ] } }, "required": [ "labels" ] } }, "required": [ "spec", "metadata" ] } obj.LoadFromSchema(schemaFile).Metadata.Labels // {"abc": true}Any insight will be helpful! Cheers   submitted by    /u/One_Poetry776 ]]></content:encoded></item><item><title>Ubuntu 6.06 (2006)</title><link>https://www.reddit.com/r/linux/comments/1k3rsih/ubuntu_606_2006/</link><author>/u/HeitorMD2</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 17:29:53 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The movie mistake mystery from &quot;Revenge of the Sith&quot;</title><link>https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html</link><author>CharlesW</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 17:29:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The finale of James Cameron's epic "Aliens" (1986) features the android Bishop (Lance Henriksen) getting severed in half, but still functioning enough to save Newt (Carrie Henn) from getting sucked into the vacuum of space. The action-packed scene features an absolutely wonderful accidental reveal of how the cut-in-half android was accomplished on the set:The amazing makeup effects applied to Henriksen's body covers the bottom half of his body which is hidden through a hole in the set. But in order to get that little bit of extra athletic stretch to grab Newt, Henriksen popped his body out of the hole a little too far, revealing the classic stage trick. However, I'd gather that 99% of the audience has never noticed this little reveal of stagecraft since our eyes are fixed on Newt on screen right, sliding toward the airlock, and not on the ground contact of Bishop's half-body, which had already been firmly established in the scene.Avoiding reflections of the crew appearing to camera is a constant struggle for filmmakers. In Steven Spielberg's first masterpiece "Duel" (1971), David (Dennis Weaver) gets into a phone booth to make a call, with the front glass face of the booth aimed directly at the camera, and if the audience's gaze drifted off of Weaver's face, they could catch a glimpse of the crew:In the reflection, we see a few crew members on screen left, the camera itself, and director Spielberg on the right (he's the one shuffling left and right, who lowers his head in the middle of the take). Again, like all the examples I'm providing in this article, hardly anyone would ever notice these moments. When a viewer catches these brief moments, the illusion of the movie is briefly broken, but for fans of the filmmaking process, it's a joyful reminder of the overall magic trick. The most intimate movie scene with only two characters in a desolate, isolated environment actually was created by dozens and dozens of crew members standing slightly out of frame.Look for another accidental 'crew caught on camera' moment in the reflection in a car window in the 'leave the gun, take the cannoli' scene from "The Godfather" (1972), one that very few people ever notice.Here's a super quick revealing mistake from "The Dark Knight" (2008) that is a true "you'll never see this in real time" moment:Although "The Dark Knight" example gives the audience a much clearer look at the camera operator, the focus puller(?) and the camera itself reflected in the interrogation room’s mirrors, the shot is a lot harder to see the crew members and equipment in real time due to the chaotic and energetic camera movement, as opposed to the locked off nature of the "Duel" example.Amazingly, many folks who watch that clip from the dramatic drowning sequence cannot consciously see the bit of filmmaking that literally blocks the actors in an intimate moment. This is my favorite example of a movie's incredible emotional power — the scene is so dramatic and intense that most viewers cannot consciously see a giant cloth wiping away water from the lens of the camera in the middle of a shot.Incidentally, some of these revealing mistakes are being erased from cinema history due to overzealous restoration projects — the process of “cleaning up” a film for newer formats like Blu-ray and 4K — which is deeply wrong. This is a much bigger topic on which I have very strong thoughts and the hottest of takes. Just look at what modern restorations have done to two of these revealing mistakes from "Goodfellas" and "Aliens":Painting out these movie mistakes as part of a restoration is wrong. What's in the movie is in the movie, and altering the movie to this extent is a form of revisionist history. Cinema is worse off when over-aggressive restorations alter the action within the frame. To me, this is equivalent to swapping out an actor's performance with a different take, or changing the music score during an action sequence, or replacing a puppet creature with a computer graphics version of the same creature decades after release.  Movies are  But I digress.Like I said at the start, movies are handmade, and that's true even in today's landscape where digital visual effects are a prominent part of filmmaking. In the same way that physical crews use physical tools to build sets, construct costumes and craft props, visual effects artists use digital tools to craft an image. And with the hand-made nature of any art form, the lack of clinical accuracy lends to its charm and sometimes offers an accidental peek behind the scenes of how the art was constructed.Every few years, a "Star Wars" revealing mistake bubbles up on the internet, one from the Mustafar sequence from Episode III, "Revenge of the Sith" (2005). But the bizarre moment in the single shot was not as easily explainable as the examples I've shown above.Being in the privileged position of currently working at Industrial Light & Magic, the visual effects company that made the visual effects for the movie (and having worked on that movie [and that sequence!]), I took it upon myself to try and solve the mystery.Please enjoy the story, written by Ian Kintzle, of how I investigated the mystery of the "Force Ghost" in "Revenge of the Sith", as it originally appeared in the Star Wars Celebration Program for Japan 2025.]]></content:encoded></item><item><title>generic Raw helm chart with rich features</title><link>https://www.reddit.com/r/kubernetes/comments/1k3qz0e/generic_raw_helm_chart_with_rich_features/</link><author>/u/Coding-Sheikh</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 20 Apr 2025 16:53:56 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[   submitted by    /u/Coding-Sheikh ]]></content:encoded></item><item><title>The subtle art of waiting</title><link>https://blog.frankel.ch/subtle-art-waiting/</link><author>/u/nfrankel</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 20 Apr 2025 16:37:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[It might sound strange to wait in Kubernetes.
The self-healing nature of the Kubernetes platform is one of its biggest benefits.
Let’s consider two pods: a Python application and a PostgreSQL database.The application starts very fast and eagerly tries to establish a connection to the database.
Meanwhile, the database is initializing itself with the provided data;
the connection fails.
The pod ends up in the  state.After a while, Kubernetes requests the application pod’s state.
Because it’s failed, it terminates it and starts a new pod.
At this point, two things can happen:
the database pod isn’t ready yet, and it’s back to square one or it’s ready, and the application finally connects.With the above probe, Kubernetes waits for an initial ten seconds before requesting the pod’s status.
If it fails, it waits for another ten seconds.
Rinse and repeat 30 times before it fails definitely.You may have noticed the HTTP  endpoint above.
Kubernetes offers two exclusive Probe configuration settings:
 or .
The former is suitable for web applications, while the latter is for other applications.
It implies we need to know which kind of container the pod contains and how to check its status, provided it can.
I’m no PostgreSQL expert, so I searched for a status check command.
The Bitnami Helm Chart looks like the following when applied:Note that the above is a simplification, as it gladly ignores the database name and an SSL certificate.The startup probe speeds things up compared to the default situation if you configure it properly.
You can set a long initial delay, and then shorter increments.
Yet, the more diverse the containers, the harder it gets to configure, as you need to be an expert in each of the underlying containers.It would be beneficial to look for alternatives.]]></content:encoded></item><item><title>I made a thing</title><link>https://www.reddit.com/r/rust/comments/1k3q6nh/i_made_a_thing/</link><author>/u/Same_Breakfast_695</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 20 Apr 2025 16:18:24 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[So the last couple of weeks I have been trying to reimplement Homebrew with rust, including some added concurrency and stuffs for better performance. Damn I might be in over my head. Brew is way more complex than I initially thought.Anyway, bottle installs and casks should work for the most part (still some fringe mach-o patching issues and to be honest, I can't test every single bottle and cask)Build from source is not yet implemented but I got most of the code ready.If anyone wants to try it out, I'd be grateful for every bug report. I'll never find them on my own.]]></content:encoded></item><item><title>Things Zig comptime won&apos;t do</title><link>https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html</link><author>JadedBlueEyes</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 15:57:37 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ArcoLinux Lead Steps Down After Eight Years</title><link>https://linux.slashdot.org/story/25/04/19/1954232/arcolinux-lead-steps-down-after-eight-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sun, 20 Apr 2025 15:34:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA["The time has come for me to step away," ArcoLinux lead Erik Dubois posted last week. ("After eight years of dedication to the ArcoLinux project and the broader Linux community...") 

'Learn, have fun, and enjoy' was our motto for the past eight years — and I really had fun doing all this," Dubois says in a video version of his farewell post. "And if we reflect back on this teaching and the building and promoting of Linux, it was fun. But the time has come for me to step away..." 

Over its eight years ArcoLinux "accomplished several important milestones," reports Linux magazine, "such as creating over 5,000 educational videos; the creation of ArcoInstall; the Carli education project; the Arch Linux Calamares Installer (ALCI); the ArcoPlasma, ArcoNet, ArcroPro, and Ariser variants; and much more."
According to Dubois, they weren't just creating a distribution but a mindset. 

Dubois says that the code will remain online so others can learn from, fork, or remix the distro. He also indicated that ArcoLinux will supply users with a transition package to help them convert their existing ArcoLinux systems to Arch Linux. That package will remove ArcoLinux branding, replace pacman.conf with an Arch and Chaotic-AUR focused config file, and change the arcolinux-mirrorlist to a single source.
 

 It's FOSS News describes ArcoLinux as one of those "user-friendly Arch-based distros that give you a bleeding-edge experience."
The reasoning behind this move, as shared by Erik, is his advancing age and him realizing that he doesn't have the same level of mental focus or stamina he used to have before. He has found himself making small mistakes, the kind that can negatively affect a major undertaking like this... Come July 1, 2025, the transition period will end, marking a stop to all development, including the deactivation of the ArcoLinux social media handles. The Telegram and Discord communities will stay a bit longer but will close up eventually. 
"I want to leave ArcoLinux while it's still strong, and while I can look back with pride at everything we've accomplished together," Dubois says in their post...]]></content:encoded></item><item><title>15 Reasons I Love Go</title><link>https://appliedgo.net/why-go/</link><author>/u/ChristophBerger</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 20 Apr 2025 15:26:56 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Go became known for its built-in concurrency and the absence of inheritance. But Go has many more advantages than these. I collected 15 of them here. Each of them might not seem world-shaking when looked at in isolation, but taken together, they build a unique composition of features that make Go the language we Gophers could love to pieces.Use these reasons as a decision aid. They are unbiased, guaranteed! ;)Go's simplicity and lightweight syntax make it an easy language to learn and adopt, especially for newer programmers.Unlike some languages, Go avoids complex features and minimizes bells and whistles. Developing in Go feels like a breath of fresh air, enabling you to focus on solving problems rather than wrestling with the language itself.Go's simplicity and “boringness” make it the language of choice for mixed teams. Quoting Jonathan Hall, “I can take a group of juniors and seniors and put them together, and they can be productive together, without having to spend months ramping up the juniors to understand all the arcane syntax in a language.”If you're looking for a modern language that gets out of your way, Go is your choice.One of Go's standout features is its unbelievably fast compiler. We're talking seconds for even large codebases.This super quick feedback loop improves developer productivity tremendously. You don't waste precious time waiting for bloated builds. Instead, you can iterate rapidly and use the time saved to add more value to your programs. It's almost like using an interpreted language.For this reason alone, Go is worth adopting if you value fast development speed.3. Go collects your garbageGo's built-in garbage collection eliminates the need for manual memory management, making Go far easier to program than languages like C and C++.As a developer, you don't have to worry about freeing memory or tracking allocations. The garbage collector handles all of that for you efficiently and without pause. Programming in Go feels like a relief, knowing you can focus on high-level app logic rather than nitty-gritty memory details.This simplicity is one reason Go is loved by so many developers today!4. Concurrency made easierConcurrency is baked into Go, unlike many other languages.Goroutines and channels make it substantially easier to write asynchronous code that fully utilize multicore processors. Go uses a model called CSP that makes concurrency much easier to reason about. Concurrency is the future as Moore's Law ends, and Go makes it more approachable.If you need to build high-performance backends and systems, Go's excellent concurrency support is a must-have feature.Go was designed from day one for scalability.Its hyper-lightweight threads (a.k.a. goroutines) and built-in concurrency features allow Go programs to take advantage of multicore CPUs and distributed computing clusters. Go handles all the scheduling of goroutines behind the scenes, enabling you to focus on your program's logic and architecture.Some of the world's most demanding and highest-traffic systems are built with Go, including Docker, Kubernetes, CloudFlare, and Dropbox. If you need scalability, check out Go!Go is a statically typed language like Java or C++.This means that variables have a fixed type known at compile time, rather than changing dynamically like in Python. Static typing catches errors earlier in the dev process and improves runtime performance since types don't need to be checked at runtime.Moreover, static typing means better self-documentation. And Go's type inference keeps the syntax simple. You get safety and speed without the verbosity.For any large program, static typing is a must-have feature.Go ships with a robust standard library, so you can be productive right away.The standard library provides high-quality implementations for things like HTTP servers, JSON parsing, cryptography, and more.No need for stitching together external libraries or reinventing the wheel. The standard library is cross-platform and has bindings for Windows and Linux systems programming.Whether building a web app, utility, or system tool, you can get quite far with Go's standard library.Even though Go is garbage collected, it provides low-level access to memory and pointers like C for high-performance use cases.You can pass pointers to avoid copying data and write superfast programs that run closer to the metal.Go gives you the control when you need it. Unlike many managed languages, Go recognizes that sometimes you require expressive low-level access to memory and internals for the hottest code paths.Go helps write your code with sympathy for the underlying platform to extract the highest performance.9. Interfaces over inheritanceInterfaces in Go provide powerful polymorphism and abstraction capabilities.You can hide concrete implementation details behind simple interfaces. Types in Go implicitly satisfy interfaces by implementing the required methods. This enables loose coupling between components.Interfaces are everywhere in Go - , , and more, are part of the standard library and help abstract away technical details. They are a key part of idiomatic Go code to decouple software elements and enable clean architecture and design.Unlike some languages, Go compiles down to static machine code rather than bytecode or being purely interpreted.This means you can trivially build Go programs for many platforms like Linux, macOS, and Windows. There's no need for virtual machines or interpreters. The compiled executables can be linked into C programs too.Go's cross-platform abilities make deploying and shipping software simple. A binary with no dependencies can run anywhere. Cross-compilation even allows compiling a binary on Windows for Linux, or on Linux for BSD, or on macOS for Windows, without the need for installing toolchains of the target platform. That's a massive advantage for ops and infrastructure management.I love Go's toolchain. It is fairly complete and well-thought-out, and it significantly improves the development experience as a whole, as well as in small details.For example, the built-in formatting tool  automatically styles your code consistently. No more arguing over tabs versus spaces, or the placement of curlies!Dependency management, testing, benchmarking, profiling, and more, are built-in features.The Go toolchain is a one-stop shop for all development tooling requirements. It integrates seamlessly into your development flow, so you can focus on programming rather than setup.12. Built-in testing and benchmarkingTesting and benchmarking are built right into Go with the go test command.Writing unit tests in Go is easy and idiomatic. The testing package provides everything you need for basic tests up to complex mocked objects and code coverage reporting. No need for learning a new domain language: Go tests are written in plain Go.By making testing ergonomic, Go supports development practices like Test-Driven Design (TDD). And the benchmarking tools allow comparing different versions of your code for performance.With Go, continuous unit testing quickly becomes second nature to you, leading to more confidence in your code and much fewer regressions.13. A thriving open-source ecosystemGo has a thriving open-source ecosystem with high-quality packages for nearly any need.The rich standard library provides the foundations, but Go's package ecosystem adds incredible value on top.Packages like Gin or Echo for web frameworks, sqlc, jet, bob, Ent, or GORM for database access, and Go SDKs for countless public APIs give you ready-made building blocks for the task at your hand. Sharing and discovery are easy with the central package repository at pkg.go.dev. And Go's package management handles dependencies seamlessly.Bottom line: Go's fantastic community and ecosystem make development productive and enjoyable.The language and standard library are carefully evolved between releases. New versions of Go work with older codebases seamlessly in nearly all cases. You can update to the latest Go confident your software will compile and run without issue.Backward compatibility enables a stable base to build on. Technical debt and rewrite costs are avoided. Compared to other languages, Go takes compatibility seriously to support long-term projects. Knowing your code will continue working with new Go releases is a weight off your shoulders.And it gets even better. Since Go 1.21, modules can request to be compiled with a particular version of the Go toolchain, and the installed Go command will happily download and use that toolchain. This is a level of backward and forward compatibility other languages can only dream of.All dependencies of a build are fully determined. No dependency can change unless explicitly updated in the  file. Moreover, the Go Sum database guarantees that module versions are immutable, making it impossible to inject modified code into an existing version. Finally, Go doesn't allow dependencies to execute code during builds, limiting the attack surface significantly.With these measures in place, and considering other security-friendly features of Go such as static linking, focus on clear code, static typing, and a rich standard library that reduces the need for using external modules, Go is an obvious choice for security-aware projects.It took only three of these properties to have me sold on Go: The clear syntax and semantics, concurrency, and the fast compiler. I quickly learned to know and admire the rest of these properties during my first steps in Go. So even if you can only check off three of these properties as relevant for you, give Go a try.]]></content:encoded></item><item><title>Why Rust compiler (1.77.0 to 1.85.0) reserves 2x extra stack for large enum?</title><link>https://www.reddit.com/r/rust/comments/1k3ouyq/why_rust_compiler_1770_to_1850_reserves_2x_extra/</link><author>/u/Helpful_Garbage_7242</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 20 Apr 2025 15:18:48 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Almost a year ago I found an interesting case with Rust compiler version <= 1.74.0 reserving stack larger than needed to model Result type with boxed error, the details are available here - Rust: enum, boxed error and stack size mystery. I could not find the root cause that time, only that updating to Rust >= 1.75.0 fixes the issue.Today I tried the code again on Rust 1.85.0, https://godbolt.org/z/6d1hxjnMv, and to my surprise, the method  now reserves  bytes (4096 + 4096 + 24), but it feels that around 4096 bytes should be enough.example::fib2: push r15 push r14 push r12 push rbx sub rsp,0x1000 ; reserve 4096 bytes on stack mov QWORD PTR [rsp],0x0 sub rsp,0x1000 ; reserve 4096 bytes on stack mov QWORD PTR [rsp],0x0 sub rsp,0x18 ; reserve 24 bytes on stack mov r14d,esi mov rbx,rdi ... add rsp,0x2018 pop rbx pop r12 pop r14 pop r15 ret I checked all the versions from 1.85.0 to 1.77.0, and all of them reserve  bytes. However, the version 1.76.0 reserves  bytes, https://godbolt.org/z/o9reM4dW8 use std::hint::black_box; use thiserror::Error; #[derive(Error, Debug)] #[error(transparent)] pub struct Error(Box<ErrorKind>); #[derive(Error, Debug)] pub enum ErrorKind { #[error("IllegalFibonacciInputError: {0}")] IllegalFibonacciInputError(String), #[error("VeryLargeError:")] VeryLargeError([i32; 1024]) } pub fn fib0(n: u32) -> u64 { match n { 0 => panic!("zero is not a right argument to fibonacci_reccursive()!"), 1 | 2 => 1, 3 => 2, _ => fib0(n - 1) + fib0(n - 2), } } pub fn fib1(n: u32) -> Result<u64, Error> { match n { 0 => Err(Error(Box::new(ErrorKind::IllegalFibonacciInputError("zero is not a right argument to Fibonacci!".to_string())))), 1 | 2 => Ok(1), 3 => Ok(2), _ => Ok(fib1(n - 1).unwrap() + fib1(n - 2).unwrap()), } } pub fn fib2(n: u32) -> Result<u64, ErrorKind> { match n { 0 => Err(ErrorKind::IllegalFibonacciInputError("zero is not a right argument to Fibonacci!".to_string())), 1 | 2 => Ok(1), 3 => Ok(2), _ => Ok(fib2(n - 1).unwrap() + fib2(n - 2).unwrap()), } } fn main() { use std::mem::size_of; println!("Size of Result<i32, Error>: {}", size_of::<Result<i32, Error>>()); println!("Size of Result<i32, ErrorKind>: {}", size_of::<Result<i32, ErrorKind>>()); let r0 = fib0(black_box(20)); let r1 = fib1(black_box(20)).unwrap(); let r2 = fib2(black_box(20)).unwrap(); println!("r0: {}", r0); println!("r1: {}", r1); println!("r2: {}", r2); } Is this an expected behavior? Do you know what is going on?]]></content:encoded></item><item><title>TensorFlow implementation for optimizers</title><link>https://github.com/NoteDance/optimizers</link><author>/u/NoteDancing</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 14:57:45 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Jagged AGI: o3, Gemini 2.5, and everything after</title><link>https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything</link><author>ctoth</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 14:55:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Plus, our recent paper testing prompting techniquesa new paper shows that AI passes the Turing Testreally solid 26 page summary on the topicthis post  large leap in benchmarksmy bookCome up with 20 clever ideas for marketing slogans for a new mail-order cheese shop. Develop criteria and select the best one. Then build a financial and marketing plan for the shop, revising as needed and analyzing competition. Then generate an appropriate logo using image generator and build a website for the shop as a mockup, making sure to carry 5-10 cheeses that fit the marketing plan.” Reasoner “figure out what this is and generate a report examining the implications statistically and give me a well-formatted PDF with graphs and details”You might find yourself “feeling the AGI” as well. Or maybe not. Maybe the AI failed you, even when you gave it the exact same prompt I used. If so, you just encountered the jagged frontier.My co-authors and I coined the term “Jagged Frontier”Colin FraserRiley Goodside"A young boy who has been in a car accident is rushed to the emergency room. Upon seeing him, the surgeon says, "I can operate on this boy!" How is this possible?"“A father and son are in a car crash, the father dies, and the son is rushed to the hospital. The surgeon says, 'I can't operate, that boy is my son,' who is the surgeon?”solve much harder brainteasersmatters much to our lives in the near terma normal technologyAnd there's a deeper uncertainty here: are there capability thresholds that, once crossed, fundamentally change how these systems integrate into society? Or is it all just gradual improvement? Or will models stop improving in the future as LLMs hit a wall? The honest answer is we don't know.faster take-off]]></content:encoded></item><item><title>Why is OpenAI buying Windsurf?</title><link>https://theahura.substack.com/p/tech-things-openai-buys-windsurf</link><author>theahura</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 14:28:20 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Built a simple UI tool for node group-level observability in AWS EKS — KubePeek</title><link>https://www.reddit.com/r/kubernetes/comments/1k3nas5/built_a_simple_ui_tool_for_node_grouplevel/</link><author>/u/captain_sangam</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 20 Apr 2025 14:06:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hey folks! I’ve been working on  — a lightweight web UI that gives real-time visibility into your .While there are other observability tools out there, most skip or under-serve the . This is a simple V1 focused on that gap — with more features on the way.Roadmap includes GKE, AKS, AI-powered optimization, pod interactions, and moreWould love feedback, feature requests, or contributions.]]></content:encoded></item><item><title>The Joy of Linux Theming in the Age of Bootable Containers</title><link>https://blues.win/posts/joy-of-linux-theming/</link><author>dopple</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 13:56:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Having spent a couple of decades in the Linux world, I have always had an interest in Linux desktop environments and how they are themed.
I would often come across a post on /r/unixporn that inspired me to try to customize the look and feel of my desktop environment. So I would install Xfce, LXQt or Sway and try to recreate components that I like from other users or create my own. I would end up installing different kinds of panels, plugins, docks and launchers as well as random themes, fonts and sounds.A portion of this process would be documented, initially as random shell scripts in my home directory, before graduating to Ansible playbooks – with a brief detour into Nix that I will not elaborate on. Some of the customizations would live in my home directory, but there were often system-wide modifications to  required.Eventually, the constant churn and randomly broken desktop components such as a panel that mysteriously vanished or a non-functional dock led me to stick with the stock configuration of whatever desktop environment I was using at the time.
The major desktop environments, KDE Plasma and GNOME, are both well polished and great out of the box. The desktop experience that they have delivered over the last few years has contributed to desktop Linux being the best it has ever been, in my opinion.But the itch to customize and tweak my desktop environment in fun and interesting ways is still there. Eventually, I was introduced to the concept of bootable containers.Bootc As A Themer’s Playground⌗Once written, you can build the container locally and instruct your bootc-aware system to use the new image.After a reboot, the system’s deployment is defined by the new container.With Fedora Atomic systems,  is mounted read-only and because your operating system is defined by an OCI container, it is incredibly easy to revert to a previous tag of that container. I can easily create a throwaway container where I test out ideas for a theme, reboot into the new deployment and test it out on bare-metal. I can roll back to the previous container if necessary or create a new container with follow-up modifications.One downside is that this reboot-heavy workflow can obviously cause some friction. This can be mitigated somewhat by enabling “Development Mode” with , which creates a temporary writable overlayfs on top of . I often find myself using this mode to temporarily install some package, theme or configuration in . After verifying that it works as expected, I can add that functionality to the Containerfile. If it doesn’t work, I can either reboot to get rid of the changes, or (more likely) just forget about the change and it will remove itself whenever I reboot normally. The lack of cruft that accumulates over time in a typical Linux installation is one of the major advantages of this approach.Of course, there are other ways to achieve similar results without using a bootable container model:You can write shell scripts or Ansible playbooks and hope that they accurately capture changes to the system so that they can be reliably undone. Typically, configuration drift that occurs as software gets updated is not addressed.With systemd-sysext(8), you can create a squashfs image of a filesystem containing your theming changes for  and overlay it on top of the root filesystem. An ecosystem around how these images should be created, maintained, deployed and updated has yet to emerge.You can inscribe your custom theming as runes in an arcane and inscrutable functional language known only to the elders as N̸̘̏͑̕͝į̸̈́̂x̸͙̑̅̒.In my opinion, none of the alternatives provide the same level of flexibility and tooling support as writing a Dockerfile, nor can they achieve bootc’s level of safety and reliability by making it extremely difficult to bork your  directory. And if the  directory somehow gets borked anyway, rolling back to the previously deployed container is just a reboot away.A few weeks ago, an OCI image based on Fedora Xfce Atomic that I made called Blue95 was posted to Hacker News. For the most part, the reception was warm but there were some interesting questions that were raised, such as:Is it really necessary to spin up an entirely new distro for an XFCE+GTK theme?The original poster made me question the nature of the project I created: is it a distro? In the age of bootc, the distinction between what is considered a Linux distribution and what is simply a Containerfile + CI/CD is, in my opinion, murky at best. Historically, the barrier to entry for creating what has traditionally been called a Linux distribution was orders of magnitude higher than creating and publishing an OCI container. My nights-and-weekends side project of building a bootable container is a far cry from what I imagine a Linux distribution to be.Blue95 is a collection of scripts and YAML files cobbled together to produce a Containerfile, which is built via GitHub Actions and published to the GitHub Container Registry. Which part of this process elevates the project to the status of a Linux distribution? What set of  commands in the Containerfile take the project from being merely a Fedora-based OCI image to a full-blown Linux distribution?Popular bootc-based projects like Project Bluefin and Bazzite are often labeled as Linux distributions, much to the consternation of their creators and maintainers. But if you’ve ever used Bazzite and booted directly into Steam’s Big Picture Mode, you might agree that it does indeed feel like its own Linux distribution; it is quite distinct from its twin bases of Fedora Silverblue and Fedora Kinoite.Maybe the imprecision of the term “Linux distribution” is most evident when arguments arise over what is and isn’t a distro. It has always been problematic to define a distribution as simply a curated collection of software plus a Linux kernel -— but that definition is now especially lacking, as it could just as easily describe any Containerfile for a bootable container. Ultimately, “I know it when I see it” may be the best we can do when deciding whether a project deserves the label Linux distribution or not.Finally, to address the original question about the necessity of spinning up a new distro just for a theme: creating a bootable container with a consistent visual design and curated set of applications can bring a bit of . At this moment, my laptop is booted from a container that I have created myself. The operating system being used to draft these words is the product of my own artistic and creative expression – built on the work of countless other human beings. And that brings me joy.]]></content:encoded></item><item><title>Jujutsu: different approach to versioning</title><link>https://thisalex.com/posts/2025-04-20/</link><author>/u/indeyets</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 13:27:47 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[In the previous article I told about my history with different version control systems and how I ended up using Jujutsu. This part will focus on why I think it is an important improvement over the git's status-quo and why I use it daily.Let's start by defining the aforementioned status-quo. Typical user sees interaction with Git as moving files between 3 stages:Files are edited in the file system. Git doesn't know anything about them. Some of the commands can compare the changed parts against other stages (think  or ).Some [parts] of the files are added to the "staging area" using  command. These changes are not yet part of the tracked history, but Git knows about them.Proper content-snapshot with identifier and metadata is injected into the repository using  command.Linus called this system "the stupid content tracker" because at its core it doesn't try to do anything smart. It doesn't try to understand the content, it doesn't try to understand the changes, it doesn't try to understand the history. It just stores the snapshots and the metadata. In the grand scheme of things, it is up to the user to make sense of it. And to be effective with git, users  to make sense of it.Jujutsu, a VCS which I started to use recently, makes some very different choices. Let's see how it handles this. I will be talking about the concepts and showing the commands. If you want to follow along, you can install it and try it out in parallel.On macOS with Homebrew it is as simple as . You can find instructions for other platforms on the official site.Jujutsu allows you to work with arbitrary git repositories. You can use both  and  at the same time. Adding jujutsu support to repo is as simple as running  in it. Jujutsu will detect existing git repository and will configure itself to transparently sync with it.Jujutsu has plans to provide several opensource backend implementations: git-based one and the native one. Google has a backend powered by Piper, their internal VCS system, but it is not public. Native backend is not ready yet, and while it will have some interesting properties, for now it is not a practical topic. The rest of this post would be talking only about git-backend.Everything's a part of "change"Git, as I shown above, expects user to work on files and then to "commit" their snapshot to the repository. Thus, the verb became a noun and we say that Git-repository stores commits.Jujutsu uses a different terminology: repository stores changes. At any point in time, whenever you edit, add or remove a file, these modifications become a part of the "current change". You never need to commit them explicitly.As soon as we connected jj to the repository it has created a new  change with . We can check this by running  (or ) command. is an id of the change. it would remain static no matter how we change the contents in the future. is a content-hash which corresponds to the id of the git commit. As soon as we add a single new letter to the files it will change. means that this change is allocated, but it doesn't have any content yet. Again, as soon as we modify a single letter in files, this marker will disappear. means that we didn't bother to set the description of the change yet (it corresponds to the "commit message" in git).Jujutsu doesn't require you to set descriptions of changes until you want to make them public. In Jujutsu it is perfectly fine to have multiple non-described changes. I still prefer to set descriptions early on, but it is not a requirement.The command to set the description is . This command does not create a new change. It modifies the description of current change. You can call it several times and it will be modifying description of the same change.How do you start a new change? Well, you just use  for that.Branches don't have namesLet's take a look at the log of our changes. We'll use  for that.The thing I'd like to focus on is the  marker, which stayed at exactly the same place where it was in this git repository initially. Branches in jujutsu do not have names, which might be contra-intuitive at first. Previously, we created a single branch of changes starting from git's "main" branch, but we can create more like this:  and they wouldn't be lost, as Jujutsu's log would give us access to all of them without any issues. Strictly speaking, this is exactly what was done as part of  under the hood: it created one empty change for us automatically.And, as we can create multiple independent branches of changes from any point of the tree, Jujutsu can not meaningfully make assumptions about moving the marker.In previous paragraphs I said "marker", but the proper Jujutsu term for this is "bookmark". Literally, the "main" is a bookmark in tree, which can be referenced by name, and moved as we need (just as we move a bookmark, when we read the book).So, whenever we're ready to tell: "this is, what everyone should consider  from now on!", we just type jj bookmark move main --to=@. At this moment, git's "main" branch would start to point at this commit ("current" commit has special id ) and the whole branch would become visible to the git. Other branches we created would be mostly hidden from git tools (but the commits would still be stored in git's repository).The next step would be able to push the  to the upstream repository (such as github) using  command. Except that we still have some undescribed changes. It is not prohibited to have that in git, but it would definitely look strange. Let's play nice and that.One way to do it is to use . A lot of jj's commands accept  argument which allows to specify the change-id, which is the target of command. In this case, change-id is , but we can use  as it was hilighted in the log. Jujutsu always highlights shortest non-ambiguous prefix of the change-id in the log.Another approach is more generic. We can use  to resume work on the change and just call  without arguments while we do it.Whether we modify the description or the contents of the change it's content-hash would change, but it's change-id would not. And, as descendent commits are linked via change-ids and bookmarks are attached to change-ids as well, the branch would maintain it's shape after the changes. So, now we should finally be able to push our changes.Editing pre-existing changes of the upstream branch and pushing them to git again would result in a force-push. From Git's point of view these new commits are not connected to the old ones and history rewriting happens.Jujutsu uses several heuristics to prevent "dangerous" situations such as "moving" the branch which was already merged into something else, but overall it is much more forgiving in this regard.In cases when several people work on the same branch such pushes would result in the need to re-sync local branches. In the future, when we have forges with native jujutsu support, these operations would be trivial and boring (in a good way).In the previous section we were modifying the change in the middle of the branch. How would Jujutsu react if we introduced the conflict? Let's check!You see that the upper change is marked with a red "conflict" word now, but Jujutsu still allows us to work on whatever we want. It doesn't force us to resolve conflict now. The most interesting part is, that we do not have to resolve the conflict at the point where it was introduced.I'll create a new change at the top of the branch:  and will edit the code to resolve the conflict there. Take a look at the log now:We have intermediate change in "conflicted" state, but above it the branch is clean again. And we could leave it at that, but let's apply the fix to the broken change instead to get a clean history. We will use  command for that: gathers the modifications from the current change and makes them a part of the specified change instead. Done. Conflict is gone. New empty change is automatically created for us. We can continue working on the branch. without  argument applies modifications to the parent change. It can be used to emulate a staging area of git. The basic idea is that you keep the parts which you're sure about in one change and experimental modifications in the next one. And as soon as you're sure in the quality of your experimental code you squash it into the proper change. A lot of people consider this to be the recommended flow for Jujutsu.Until this point I was talking about linear changes and diverging branches, but nothing about merges. Two reasons: a). I wanted to lay the groundwork; b). There isn't much to talk about :)In Jujutsu, one doesn't make a merge of branches. Instead, one makes a change with several parents: , where , etc. are either change-ids or branch-names (or some other interesting things we did not talk about yet).Resulting change would reside on it's own anonymous branch, as usually and you'll need to assign some bookmark to it to make it visible in git.Thus, equivalent of  would look like this:It is more verbose, but gives you more control over the process.I hope I managed to spark your interest in jujutsu. This post is definitely not an exhaustive guide, but I wanted to give you a taste of what it is like to work with it.If you want to learn more, I encourage you to check out:]]></content:encoded></item><item><title>A small dive into Virtual Memory</title><link>https://www.youtube.com/watch?v=ultz9m0n0GE</link><author>/u/1337axxo</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 13:04:29 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hey guys! I recently made this small introduction to virtual memory. I plan on making a follow up that's more practical if it interests some people :)   submitted by    /u/1337axxo ]]></content:encoded></item><item><title>F1 Race Prediction Algorithm (WIP): A sophisticated Formula 1 race simulation tool that models and predicts F1 race outcomes with realistic parameters based on driver skills, team performance, track characteristics, and dynamic weather conditions.</title><link>https://github.com/mehmetkahya0/f1-race-prediction</link><author>/u/mehmettkahya</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 13:02:34 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gemma 3 QAT Models: Bringing AI to Consumer GPUs</title><link>https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/</link><author>emrah</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 12:22:06 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.Understanding performance, precision, and quantizationThe chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.Why BFloat16 for this comparison? BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we'll discuss next.It's important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.The Need for AccessibilityWhile top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We're committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.Performance Meets Accessibility with Quantization-Aware Training in Gemma 3This is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model's parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).Using int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.How do we maintain quality? We use QAT. Instead of just quantizing the model after it's fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.See the Difference: Massive VRAM SavingsThe impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights: Drops from 54 GB (BF16) to just  (int4) Shrinks from 24 GB (BF16) to only  (int4) Reduces from 8 GB (BF16) to a lean  (int4) Goes from 2 GB (BF16) down to a tiny  (int4)This figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context lengthRun Gemma 3 on Your DeviceThese dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware: Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally. Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines. Offer even greater accessibility for systems with more constrained resources, including phones and toasters (if you have a good one).Easy Integration with Popular ToolsWe want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints: Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command. Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface. Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon. Use our dedicated C++ implementation for highly efficient inference directly on the CPU. Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.More Quantizations in the GemmaverseOur official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant Gemmaverse offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as Bartowski, Unsloth, and GGML readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.Bringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.Explore the quantized models and start building:We can't wait to see what you build with Gemma 3 running locally!]]></content:encoded></item><item><title>Linux battery life on laptops</title><link>https://www.reddit.com/r/linux/comments/1k3kncj/linux_battery_life_on_laptops/</link><author>/u/Tiny-Satisfaction-40</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 11:39:33 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I'm thinking about switching to Mint on my laptop, but found out in most cases the battery life was worse on Linux than on Windows, though the posts I tound were from 2-3 years ago.Has battery life on Linux improved?]]></content:encoded></item><item><title>🎡 Kubernetes Deployments, Pods, and Services explained through a theme park analogy</title><link>https://www.reddit.com/r/kubernetes/comments/1k3kcl4/kubernetes_deployments_pods_and_services/</link><author>/u/mmk4mmk_simplifies</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 20 Apr 2025 11:19:55 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi everyone — as someone helping my team ramp up on Kubernetes, I’ve been experimenting with simpler ways to explain how things work.I came up with this :🎡  = the ride managers ensuring rides stay available🎟️  = the ticket counters connecting guests to the ridesAnd I've added a visual I created to map it out: I’m curious how others here explain these concepts — or if you’d suggest improvements to this analogy.]]></content:encoded></item><item><title>How often do you delete kafka data stored on brokers?</title><link>https://www.reddit.com/r/kubernetes/comments/1k3kcit/how_often_do_you_delete_kafka_data_stored_on/</link><author>/u/Appropriate_Club_350</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 20 Apr 2025 11:19:46 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I was thinking if all the records are saved to data lake like snowflake etc. Can we automate deleting the data and notify the team? Again use kafka for this? (I am not experienced enough with kafka). What practices do you use in production to manage costs? ]]></content:encoded></item><item><title>Hookah - literally passes the hook around</title><link>https://www.reddit.com/r/golang/comments/1k3ihra/hookah_literally_passes_the_hook_around/</link><author>/u/halal-goblin69</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 20 Apr 2025 09:03:07 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I've developed , a lightweight webhook router, with rule based routing!,]]></content:encoded></item><item><title>IDE Survey</title><link>https://www.reddit.com/r/golang/comments/1k3ibvb/ide_survey/</link><author>/u/rashtheman</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 20 Apr 2025 08:50:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[What IDE do you use when developing Go applications and why?   submitted by    /u/rashtheman ]]></content:encoded></item><item><title>KSail - An open-source Kubernetes SDK</title><link>https://www.reddit.com/r/kubernetes/comments/1k3i8rm/ksail_an_opensource_kubernetes_sdk/</link><author>/u/nikolaidamm</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sun, 20 Apr 2025 08:44:18 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I am, u/devantler, the maintainer of KSail. KSail is a CLI tool built with the vision of becoming a full-fledged SDK for Kubernetes. KSail strives to bridge the gaps between usability, productivity, and functionality for Kubernetes development. It is easy to use and relies on mainstream approaches like GitOps, declarative configurations, and concepts known from the Kubernetes ecosystem. Today KSail works quite well locally with clusters that can run in Docker or Podman:> ksail init \ # to create a new custom project (★ is default) --provider <★Docker★|Podman> \ --distribution <★Native★|K3s> \ --deployment-tool <★Kubectl★|Flux> \ --cni <★Default★|Cilium> \ --csi <★Default★> \ --ingress-controller <★Default★> \ --gateway-controller <★Default★> \ --secret-manager <★None★|SOPS> \ --mirror-registries <★true★|false> > ksail up # to create the cluster > ksail update # to apply new manifests to the cluster with your chosen deployment tool If this seems interesting to you, I hope that you will give it a spin, and help me on the journey to making the DevEx for Kubernetes better. If not, I am still interested in your feedback! Check out KSail here:I am also actively looking for maintainers/contributions, so if you feel this project aligns with your inner ambitions, and you find joy in using a few hobby hours writing code, this might be an option for you! 🧑‍🔧Feel free to share the project with your friends and colleagues! 👨‍👨‍👦‍👦🌍]]></content:encoded></item><item><title>8 Kubernetes Deployment Strategies and How They Work</title><link>https://www.groundcover.com/blog/kubernetes-deployment-strategies</link><author>/u/tapmylap</author><category>dev</category><category>reddit</category><pubDate>Sun, 20 Apr 2025 08:24:43 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[One of Kubernetes's killer features is that it offers a flexible way to deploy applications. Admins can choose from a variety of deployment strategies, each of which offers a different approach to application lifecycle management. Depending on factors like application availability requirements or how carefully you want to be able to test a new deployment before entrusting mission-critical workloads to it, one Kubernetes deployment strategy may be a better fit than another.To provide guidance on how to select the best deployment strategy for a given workload, this article compares eight popular Kubernetes deployment techniques, explaining their pros and cons. It also offers tips on optimizing your Kubernetes deployment strategy no matter which type or types of deployments you choose.What is a Kubernetes deployment strategy?A Kubernetes deployment strategy is the configuration that manages how Kubernetes runs and manages an application. Deployment strategies are typically defined in YAML files, which describe how Kubernetes should deploy the initial pods and containers associated with an app, as well as how it should manage updates over the course of the application’s lifecycle.Having different deployment strategy options is part of what makes Kubernetes so powerful and flexible. Depending on what an application does, you may need to manage its deployment in a specific way.For example, with some applications, it’s possible to run multiple versions of the app at the same time within the same Kubernetes cluster. In that case, you could use a deployment strategy that updates application instances one by one. But this typically wouldn’t work if all application instances need to connect to a shared database or maintain a shared global state. An appropriate deployment strategy for that scenario would require updating all application instances at the same time, in order to maintain consistency between versions.Top 8 Kubernetes deployment strategiesTo illustrate what Kubernetes deployment strategies look like in practice, here are eight examples of popular deployment patterns.A recreate deployment tells Kubernetes to delete all existing instances of a pod before creating a new one. Recreate deployment strategies are useful for situations where you need all application instances to run the same version at all times.To configure a recreate deployment, include a spec like the following in your deployment configuration:spec:
  replicas: 3
  strategy:
	type: RecreateThis creates a deployment with three pod replicas and uses the recreate deployment strategy to maintain a consistent version across each replica.A rolling deployment (which is the default deployment strategy that Kubernetes uses if you don’t specify an alternative) manages pod updates by applying them incrementally to each pod instance. In other words, it works by restarting Kubernetes pods one by one.Rolling updates are a useful deployment strategy when it’s important to avoid downtime. Since this approach updates pod instances incrementally, it ensures that while one pod instance is being updated, other instances remain available to handle requests.The following spec configures a rolling deployment strategy:spec:
  replicas: 3
  strategy:
	type: RollingUpdate
	rollingUpdate:
  	maxUnavailable: 1    	# Maximum number of Pods that can be unavailable during the update
  	maxSurge: 1          	# Maximum number of Pods that can be created over the desired numberIn a blue/green deployment strategy, you maintain two distinct Kubernetes deployments – a blue deployment and a green one – and switch traffic between them. The advantage of this approach is that it allows you to test one version of your deployment and confirm that it works properly before directing traffic to it.To implement a blue/green deployment, first create two Kubernetes deployments. Use the deployment metadata field to apply a unique label to each one.Then, define a Kubernetes service that specifies which of the two Kubernetes deployments should receive requests. For example, the following service sends traffic to the blue deployment by matching the label “blue”:apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
	app: my-app
	color: blue  # Switch this to "green" during the cutover
  ports:
  - protocol: TCP
	port: 80
	targetPort: 80
  type: ClusterIPAs noted in the comment within the service definition, you can modify the selector to “green” in order to switch traffic to your other deployment.Blue/green deployments minimize the risk of downtime because they allow you to vet a new deployment fully before using it to handle production traffic. A downside, however, is that blue/green deployments require you to run two complete instances of your application at the same time. This is not an efficient use of resources, since only one of the instances is handling traffic.A canary deployment strategy switches traffic between distinct deployments gradually. It’s similar to a blue/green strategy in that it requires two different deployments. But whereas a blue/green deployment cuts traffic over from one deployment to the other all at once, the canary method directs some requests to one deployment while sending others to the other deployment.The advantage of this approach is that it allows you to detect problems with one of the deployments before they impact all users. It’s called a “canary” deployment because it’s analogous to using canaries in coal mines to detect the buildup of toxic gases before they reach a level that would harm humans, since canaries are especially sensitive to gases like carbon monoxide.To set up a canary deployment, first create two deployments for your application. The number of pod replicas for each deployment should reflect which percentage of traffic you want the deployment to handle. For instance, if you want one deployment to receive 60 percent of your traffic and the other to receive 40 percent, create 6 replicas in the first deployment and 4 in the second. Both deployments should match the same application label.Then, create a service that directs traffic to the matching application based on the deployment metadata. For example:apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
	app: my-app
  ports:
  - protocol: TCP
	port: 80
	targetPort: 80
  type: ClusterIPTo modify the balance between traffic over time in order to switch traffic gradually from one deployment to the other, scale the replicas within each deployment accordingly using the kubectl scale deployment command.5. A/B testing deploymentIn an A/B testing deployment, you run two distinct deployments and route traffic between them based on request type or user characteristics.For example, imagine you want to distinguish between requests from “testing” users and requests from “production” users. You could do this by differentiating between user types in request headers and routing requests on this basis. Requests with the header  would go to one deployment, while those with  would route to another.To implement an A/B testing deployment strategy, first create two deployments. Then, install a service mesh or ingress controller, such as Istio, and configure it with a routing rule that selects a deployment based on header strings.For instance, the following Istio virtual service (which targets the app  based on the deployment metadata) sends requests with  in the header to one deployment, while routing all others to the other deployment:apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-app
spec:
  hosts:
  - my-app
  http:
  - match:
	- headers:
    	    end-user:
      	       exact: testing     	# A/B testing condition
	route:
	- destination:
    	  host: my-app
    	  subset: v2
  - route:
	- destination:
    	  host: my-app
    	  subset: v1A shadow deployment strategy involves running two deployments. One deployment handles all requests and routes responses back to users. Meanwhile, the second deployment runs “in the shadows” and also processes some or all requests, but the responses don’t go back to users. Instead, they’re analyzed for testing purposes and then dropped.The value of a shadow deployment is that it lets you test a deployment by feeding it genuine user requests without relying on it to process those requests for production users. If the deployment is buggy, it won’t impact users, but you’ll still be able to detect the issue.To create a shadow deployment in Kubernetes, first set up two deployments for your application. Next, create a virtual service that mirrors traffic to the shadow instance, while also sending traffic to the primary instance. For example:apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-app
spec:
  hosts:
  - my-app
  http:
  - route:
	- destination:
    	    host: my-app
    	    subset: v1
	mirror:
  	    host: my-app
  	    subset: v2
	mirrorPercentage:
  	   value: 100.0   # 100% of traffic is mirrored to v27. Best-effort controlled rolloutA best-effort controlled rollout is similar to a canary deployment in that it gradually switches traffic from one deployment to another. But it does so in a more controlled manner, often by using predefined conditions (like crossing a certain CPU usage threshold for one deployment) to determine when to change the amount of traffic going to each deployment.To implement a best-effort controlled rollout, you would typically use a third-party deployment controller like Argo and configure it to manage your rollout based on predefined criteria. For example:spec:
  replicas: 5
  strategy:
	canary:
  	  steps:
    	    - setWeight: 20
    	    - pause: { duration: 2m }
    	    - setWeight: 40
    	    - pause: { duration: 5m }
    	    - setWeight: 100This configures a type of canary deployment. But unlike a generic canary deployment strategy where you simply add replicas to each deployment over time, the rollout in this case is carefully controlled based on specific criteria.A ramped slow rollout replaces pod instances incrementally, with delays between the replacements. It’s similar to a rolling deployment, but a key difference is that with a ramped slow deployment, there is a pause between each update. This provides time to run tests and confirm that the previous update was successful before moving on to the next one.To implement a ramped slow rollout, use a controller like Argo and configure pauses between each rollout event. For instance:spec:
  replicas: 5
  strategy:
	canary:
  	  steps:
  	    - setWeight: 20
  	    - pause:
      	        duration: 2m
  	- setWeight: 40
  	- pause:
      	       duration: 5m
  	- setWeight: 100Choosing the right deployment strategy by use caseThe main consideration for deciding which deployment strategy to adopt is the type of use case you need to support. Here’s a look at common use cases and the best deployment strategies for each one.For stateless applications, a simple rolling deployment usually makes the most sense. If there is no application, it’s not typically important to keep pod instances in sync, so you can update them one by one without causing problems.For most stateful applications, a recreate deployment strategy works best. Recreate deployments ensure that application versions remain consistent across all instances, helping to keep state in sync.Note as well that typically, you’d use a StatefulSet instead of a deployment to run a stateful application. (For details, check out our article on Kubernetes StatefulSet vs deployment.) But you can apply most types of deployment strategies to StatefulSets as well as to deployments.High-traffic applicationsFor applications that receive a lot of traffic on a continuous basis, a canary deployment or one of its variants (like a best-effort controlled rollout or a ramped slow rollout) is usually the best fit. These methods make it possible to route traffic between multiple deployments, which in turn helps to balance load and ensure that no one deployment becomes overwhelmed.Mission-critical and zero-downtime appsFor use cases where you can’t tolerate any downtime, consider a blue/green deployment. This approach allows you to validate a new deployment fully before sending traffic to it.A shadow deployment could also be a good choice for this use case. It would allow you to real-user perform testing on a new deployment before directing requests to it.A/B testing deployment strategies may also work well for certain mission-critical apps, especially if only certain users or requests are critical. In that case, you can send the high-value requests to one deployment that you’ve carefully tested, while routing less critical ones to another deployment.Batch processing and background jobsFor use cases that involve processing data in batches or running background jobs, a simple recreate or rollout deployment strategy typically works well. More complex and advanced deployment strategies aren’t usually necessary for these use cases because you usually don’t need to worry as much about the potential for downtime or running multiple versions of an application at the same time.Factors to consider when selecting a deployment strategyBeyond aligning deployment strategies with use cases, it’s also important to consider the following factors:Deployment downtime tolerance: The less downtime you can accept for a workload, the more important it is to use a low-risk deployment strategy, like rolling or blue/green Kubernetes deployments.Traffic flow and load management: If you need fine-grained control over traffic flow and load balancing, consider an A/B testing or controlled rollout deployment, which allows you to route traffic based on predefined rules.Failover, rollback, and reversal mechanisms: If it’s important to be able to revert to a previous version of a deployment, use either a blue/green or canary deployment method. These approaches make it possible to switch back to one deployment in the event that a newer deployment turns out to be buggy.Security and compliance considerations: Some Kubernetes deployments are subject to specific security and compliance requirements. For instance, if you need to ensure that security patches roll out to all application instances (to avoid leaving some vulnerable instances), you’d want a recreate deployment method. Or, if you need to route requests for users based in a certain area to a specific deployment to meet compliance rules that apply to those users, you could use an A/B testing deployment strategy.Scalability and auto-healing capabilities: Canary deployments are useful because they provide control over deployment scalability. They can also offer some auto-healing capabilities because Kubernetes will automatically attempt to maintain the number of replicas specified for each deployment – so if some replicas fail, Kubernetes can self-heal by restoring them.Best practices for a seamless Kubernetes deploymentNo matter which deployment strategy you choose, the following best practices can help minimize risk and simplify administration:: Some deployment strategies (like recreate and rolling Kubernetes deployments) are much simpler than others (like best-effort controlled rollouts). In general, simpler is better. Don’t implement a complex deployment strategy, or one that requires the use of additional tools (like Istio or Argo) unless you need the special capabilities it provides.: Prior to entrusting production traffic to a deployment, it’s a best practice to test it first. You can do this using synthetic requests, or you can direct real user requests to a deployment via a method like shadow deployments.Monitor and observe Kubernetes deployments: To detect issues with a deployment, it’s critical to monitor and observe all application instances. Methods like blue/green and canary deployments are only useful if you have the monitoring and observability data necessary to detect problems with one deployment and route traffic appropriately.Consider resource overhead: Deployment strategies that require you to maintain multiple Kubernetes deployments at the same time create more resource overhead (because more deployments require more resources to run). This can lead to poorer performance because Kubernetes cluster resources are tied up with redundant deployments. For this reason, it’s important to evaluate how many spare resources your Kubernetes cluster has and choose a deployment method accordingly.Optimizing and monitoring Kubernetes deployments with groundcoverWhen it comes to monitoring Kubernetes deployments and troubleshooting problems, groundcover as you covered. Using hyper-efficient eBPF-based observability, groundcover clues you in - in real time - to deployment performance issues like dropped requests or high latency rates. We also continuously track CPU, memory, and other performance metrics, so you’ll know right away if any of your Kubernetes deployments are at risk of becoming overwhelmed.We can’t tell you exactly which deployment strategy is best for a given workload. But we can give you the observability data you need to make an informed decision about Kubernetes deployment strategies.A balanced approach to Kubernetes deploymentUltimately, Kubernetes deployment strategies boil down to balancing performance and control on the one hand with risk and complexity on the other. If you just want to deploy an application simply and quickly, Kubernetes lets you do that – although simple deployment methods are sometimes more risky.You can also opt for more complex and fine-tuned deployment strategies that – like a complex chess move – require more expertise to carry out, but that can pay off in the long run by delivering a better balance between risk and performance.]]></content:encoded></item><item><title>Built my first microservices projects in Go using gRPC 🚀</title><link>https://www.reddit.com/r/golang/comments/1k3fujw/built_my_first_microservices_projects_in_go_using/</link><author>/u/Financial_Job_1564</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 20 Apr 2025 05:54:35 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Over the past few weeks, I've developed an interest in microservices and decided to learn how to build them using Go.In this project, I've implemented auth, order, and product services, along with an API Gateway to handle client requests. I’m using gRPC for internal service-to-service communication. While I know the code is still far from production-ready, I’d really appreciate any feedback you might have.]]></content:encoded></item><item><title>100 Years to Solve an Integral (2020)</title><link>https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html</link><author>blobcode</author><category>dev</category><category>hn</category><pubDate>Sun, 20 Apr 2025 03:16:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Say &quot;no&quot; to overly complicated package structures</title><link>https://laurentsv.com/blog/2024/10/19/no-nonsense-go-package-layout.html</link><author>/u/ldemailly</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sun, 20 Apr 2025 02:20:40 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It’s a recurring question on gopher slack and discord: «How should I set up my go project repository?».
Unfortunately, there are a  of both outdated and overly complicated blogs, sample repositories and old projects out there.One key philosophy of Go, is keeping things simple. More code writing, less structuring ahead of time (before it’s actually needed).The official guide on this topic is go.dev/doc/modules/layout - which you should read if you haven’t already, then come back here. Despite that reference (and maybe in part because of it) people still tend to over complicate. For instance it says packages or commands  benefit from splitting off some functionality into supporting packages.  it’s recommended placing such packages into a directory named internal; […] Since other projects cannot import code from our internal directory, we’re free to refactor its API and generally move things around without breaking external users.Emphasis on  and  is mine. And seems to be missed by many. 99% of people do not need  when, be real, most of what gets written will never be reused much and even if it is, don’t worry about over publishing. And the  is, in my opinion wrong. Initially, make something useful and don’t worry that it’ll get such great adoption that you’re stuck into maintaining your early layout forever, you’re not… (see semver point below too).Likewise, you don’t need to use  when you only have 1 binary (or even a few and no library).In general because a feature exists, doesn’t mean to have to use it (e.g ). Or because some people have a convention doesn’t mean you should follow (like your mom probably used to say “if they jump off a bridge… will you?”, remember? ).By the way, stick to 0.x semver (another pet peeve of mine is the v2/v3/… in go) for as long as you can and document what you change and why, rather than under-publishing and forcing people to fork to access what they really need and you didn’t foresee.So one infamous example of the nonsense is the fake “golang-standards” - see github.com/golang-standards/project-layout/issues/117 and other closed issues for why it’s  a standard. This led folks to come up with a counter point at github.com/go-standard/project-layout (which I contributed a few additions/corrections) but also got flak for having the word “standard” in it… so here you go, below is  (based on decades of professional experience in many languages, including 7 years of go at top companies and OSS projects) - Some of the text comes from there:Main package in the root is great, it makes for the shortest and cleanest install/run:go github.com/you/project@latest
Which is typically the priority if you do have a CLI or server to ship. Pure libraries obviously have no main at the root.If you do have both, you could move the main out of the way in a child directory (people like  but the  part is actually not needed) or move the library into a directory that will make importing and using it readable. Consider also you may have more than 1 library package to publish in which case  and  makes total sense and keep the top level for the main binary.For many small to medium-sized projects, these are the only packages you need.Just in case you do not know already, as a refresher,  is not a convention, it’s a feature of the go tool chain: code in or below such directory is importable only by code in the directory tree rooted at the parent of .But you don’t need that, unless you’re shipping code to  of third party users and have a lot of code that can’t just be lower case not exported, yet used in many exported packages, you don’t need to hide your code… it’s not so precious or such a huge commitment.Any package you put into a pkg directory can be moved to the root.
 is a very outdated convention from before  (and you saw what I think about internal/).Some people think that having packages at the top, “clutters”, the directory. First what does get cluttered if you add unnecessary directory layers in every single import in dozen of files and possibly hundreds of dependencies… think about them! Second, feel free to move the various docs, ancillary files, etc to some subdirectory. Lastly what is ‘clutter’ really, navigation through the code probably comes through your “godoc” (the excellent pkg.go.dev, or your self hosted equivalent for private repositories).Btw don’t get me started with repos that have dozens of packages and files and where to see what is going on without an IDE, you have to go: main.go -> some other file -> some init file -> some start function -> some other files… while all this could be in 1 or 2 places, easier to follow and find.Those functions, types, and methods which you think belong in a separate  (or , or , or ) package simply don’t. Try to put them where they belong, probably near where they are used, or in their own meaningfully named package (e.g applog for your application logging utils).But Laurent, I’m more comfortable with…Let me guess, you have more experience writing Java, Python, Ruby, JavaScript, TypeScript, or C# than you do writing C, C++, or Perl. Please consider that package structuring is more an art than hard rules.You can have more than 1 file in a package/directory in go. You don’t need to make a new package each time to separate code/types/… And the more packages the more chances to create a loop or having to later move code around. This being said, moving code around is good and an easy way to grow your code base - later, when needed.Also please don’t make many files with only a few lines in them either, it makes it painful to look at your code.There are lots of example of overly complicated repos. I’d give my own repos but they also evolved organically and aren’t actually that good/or simple, but here we go anyway:Library + CLIs: that one is somewhat oddly set up as it has 2 libraries, one at the root where it started and a newer one, so not exactly best practices github.com/fortio/terminalThis post is partially in response to what I see is over complication in many small projects, which isn’t to say it’s always right or applicable either, but if it gets folks to pause and consider, even to disagree. I think it’s mission accomplished.Ping me on gopher slack (Laurent Demailly) or discord (_dl) if you disagree, have comments etc… or open an issue
or comment directly below if you’re on facebook.]]></content:encoded></item><item><title>[Media] Corust - A collaborative Rust Playground</title><link>https://www.reddit.com/r/rust/comments/1k3au5e/media_corust_a_collaborative_rust_playground/</link><author>/u/_byl</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 20 Apr 2025 00:51:58 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Corust is an open source collaborative code editor for Rust with support for code execution.While Rust Playground has been the go to way for me to test code snippets, when pair programming, I've found collaborative features useful for prototyping/reviewing code, so I thought it would be useful (and interesting!) to implement a collaborative playground for Rust. Much inspiration taken from Shepmaster (kirby) and the Rust Playground in code execution design, and collaborative editors like Rustpad.Like the Rust Playground, Corust supports execution on stable/nightly/beta channels and cargo test/build/run in debug/release, and many top crates (~250 crates from lib.rs/std.atom, thanks to Kornel for quickly adding this!). Unlike the Playground, Corust does not yet support sharing gists, or extra tooling like viewing assembly, clippy, or rustfmt.Stack is an Axum server, Next JS UI, CodeMirror editor, and docker for containerized execution. Collaboration uses operational transform (OT) for conflict resolution and the OT client is compiled to WebAssembly on the front end.Added some Rust related easter eggs too. Hope Rustaceans find it useful!]]></content:encoded></item><item><title>Hexagonal Architecture Questions</title><link>https://www.howtocodeit.com/articles/master-hexagonal-architecture-rust</link><author>/u/roughly-understood</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sun, 20 Apr 2025 00:32:35 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Getting started with hexagonal architectureOur transition to hexagonal architecture begins here. We'll move from a tightly coupled, untestable nightmare to a happy place where production doesn't fall over at 3am.We're going to transform the Very Bad Application gradually, zooming out a little at a time until you see the whole hexagon. Then I'll answer "why hexagons?". Promise.I've omitted details like module names and folder structure for simplicity. Don't worry, though. Before this guide is over, you'll have a complete application template you can reuse across all your projects.The repository pattern in RustThe worst part of the Very Bad Application is undoubtedly having an HTTP handler making direct queries to an SQL database. This is a plus-sized violation of the Single Responsibility Principle.Code that understands the HTTP request-response cycle shouldn't also understand SQL. Code that needs a database doesn't need to know how that database is implemented. These could not be more different concerns.Hard-coding your handler to manage SQL transactions will come back to bite you if you switch to Mongo. That Mongo connection will need ripping out if you move to event streaming, to querying a CQRS service, or to making an intern do it by hand.All of these are valid data stores. If you overcommit by hard-wiring any one of them into your system, you guarantee future pain when you can least afford it – when you need to scale.Repository is the general term for "some store of data". Our first step is to move the  handler away from SQL and towards the abstract concept of a repository.A handler that says "give me any store of data" is much better than a handler that says "give me this specific store of data, because it's the only one I know how to use".Your mind has undoubtedly turned to traits as Rust's way of defining behaviour as opposed to structure. How very astute of you. Let's define an  trait:An  is some store of author data with (currently) one method: . takes a reference to the data required to create an author , and returns a  containing either a saved , or a specific error type describing everything that might go wrong while creating an author .  Right now, that's just the existence of duplicate authors, but we'll come back to error handling. is what's known as a domain trait. You might also have heard the term "port" before – a point of entry to your business logic. A concrete implementation of a port (say, an SQLite ) is called an adapter. Starting to sound familiar?For any code that requires access to a store of author data, this port is the source of truth for how every implementation behaves. Callers no longer have to think about SQL or message queues, they just invoke this API, and the underlying adapter does all the hard work.,  and  are all examples of .Domain models are the canonical representations of data accepted by your business logic. Nothing else will do. Let's see some definitions:Now, these aren't very exciting models (they'll get more exciting when we talk about identifying the correct domain boundaries and the special concern of authentication in ). But they demonstrate how the domain defines what data flowing through the system must look like.If you don't construct a valid  from the raw parts you've received over the wire (or from that intern), you can't call AuthorRepository::create_author. Sorry, jog on. 🤷This pattern of newtyping should be familiar if you've read . If you haven't, I'll wait for you here.Four special properties arise from this:Your business domain becomes the single source of truth for what it means to be an author, user, bank transaction or stock trade.The flow of dependencies in your application points in only one direction: towards your domain.Data structures within your domain are guaranteed to be in a valid state.You don't allow third-party implementation details, like SQL transactions or RPC messages to flow through unrelated code.And this has immediate practical benefits:Easier navigation of the codebase for veterans and new joiners.It's trivial to implement new data stores or input sources – you just implement the corresponding domain trait.Refactoring is dramatically simplified thanks to the absence of hard-coded implementation details. If an implementation of a domain trait changes, nothing about the domain code or anything downstream from it needs to change.Testability skyrockets, because any domain trait, like  can be mocked. We'll see this in action shortly.Why do we distinguish  from ? Surely we could represent both saved and unsaved s asRight now, with this exact application, this would be fine. It might be annoying to check whether  is  or  to distinguish whether  is saved or unsaved, but it would work.However, we'd be mistaken in assuming that the data required to create an  and the representation of an existing  will never diverge. This is not at all true of real applications.I've done a lot of work in onboarding and ID verification for fintechs. The data required to fully represent a customer is extensive. It can take many weeks to collect it and make an account fully operational.This is pretty poor as a customer experience, and abandonment would be high if you took an all-or-nothing approach to account creation.Instead, an initial outline of a customer's details is usually enough to create a basic profile. You get the customer interacting with the platform as soon as possible, and stagger the collection of the remaining data, fleshing out the model over time.In this scenario, you don't want to represent some  and  in the same way.  may contain dozens of optional fields and relations that aren't required to create a record in the database. It would be brittle and inefficient to pass such a needlessly large struct when creating a customer.What happens when the domain representation of a customer changes, but the data required to create one remains the same? You'd be forced to change your request handling code too. Or, you'd be forced to do what you should have done from the start – decouple these models.Hexagonal architecture is about building for change. Although these models may look like duplicative boilerplate to begin with, don't be fooled. Your application will change. Your API  diverge from your domain representation.By modeling persistent entities separately from requests to create them, you encode an incredible capacity to scale.Error types and hexagonal architectureLet's zoom in on . It reveals some important properties of domain models and traits. doesn't define failure cases such as an input name being invalid. This is the responsibility of the  constructor (which in this case delegates to the  constructor). Here's more on  if you're unclear on this point. defines failures that arise from coordinating the action of adapters. There are two categories: violations of business rules, like attempting to create a duplicate author, and unexpected errors that the domain doesn't know how to handle.Much as our domain would like to pretend the real world doesn't exist,  things can go wrong when calling a database. We could fail to start a transaction, or fail to commit it. The database could literally catch fire in the course of a request.The domain doesn't know anything about database implementations. It doesn't know about transactions. It doesn't know about the fire hazards posed by large datacenters and your pyromaniac intern. It's oblivious to retry strategies, cache layers and dead letter queues (we'll talk about these in ).But it needs some mechanism to propagate unexpected errors back up the call chain. This is typically achieved with a catch-all variant, , which wraps a general error type.  is particularly convenient for this, since it includes a backtrace for any error it wraps.As a result (no pun intended),  is a complete description of everything that can go wrong when creating an author.This is incredible news for callers of domain traits – immensely powerful. Any code calling a port has a complete description of every error scenario it's expected to handle, and the compiler will make sure that it does.But enough theorizing! Let's see this in practice.Implementing Here, I move the code required to interact with an SQLite database out of the Very Bad Application's  handler and into an implementation of .We start by wrapping an sqlx connection pool in our own  type. Module paths for sqlx types are fully qualified to avoid confusion:Wrapping types like  has the benefit of encapsulating a third-party dependencies within code of our own design. Remember the Very Bad Application's leaky  function ? Wrapping external libraries and exposing only the functionality your application needs is how we plug the leaks.Again, don't worry about module structure for now. Get comfortable with the type definitions, then we'll assemble the pieces.This constructor does what you'd expect, with the possible exception of the result it returns. This constructor isn't part of the  trait, so we're not bound by its strict opinions on the types of allowable error. is an excellent crate for working with non-specific errors.  is equivalent to , and  says we don't care  error occurred, just that one did.At the point where most applications are instantiating databases, the only reasonable thing to do with an error is log it to  or some log aggregation service.  simply wraps any sqlx error it encounters with some extra context .Now, the exciting stuff – the implementation of :Look! Transaction management is now encapsulated within our  implementation of . The HTTP handler no longer has to know about it. invokes the  method on , which isn't specified by the  trait, but gives  the freedom to set up and pass around transactions as it requires.This is the beauty of abstracting implementation details behind traits. The trait defines what needs to happen, and the implementation decides how. None of the  is visible to code calling a trait method.'s implementation of  knows all about SQLite error codes, and transforms any error corresponding to a duplicate author into the domain's preferred representation .Of course, , not being part of the domain's Garden of Eden, may encounter an error that the domain can't do much with .This is a 500 Internal Server Error in the making, but repositories shouldn't know about HTTP status codes. We need to pass it back up the chain in the form of CreateAuthorError::Unknown, both to inform the end user that something fell over, and to capture for debugging.This is a situation that the program – or at least the request handler – can't recover from. Couldn't we ? The domain can't do anything useful here, so why not skip the middleman and let the panic recovery middleware handle it?Until very recently, I would have said yes – if the domain can't do any useful work with an error, panicking will save you from duplicating error handling logic between your request handler and your panic-catching middleware.However, thanks to  and a horrible realization I had in the shower, I've reversed my position.Whether or not you consider the database falling over a recoverable error, there are two incontrovertible reasons not to panic:Panicking poisons held mutexes. If your application state is protected by an , panicking while you hold the guard will mean no other thread will ever be able to acquire it again. Your program is dead, and no amount of panic recovery middleware will bring it back.Other Rust devs won't expect you to panic. Most likely, you won't be the person woken at 3am to debug your code. Strive to make it as unsurprising as possible. Follow established error handling conventions diligently. Return errors, don't panic.What about retry handling? Good question. We'll cover that in .Everything but the kitchen asyncHave you spotted it? The mismatch between our repository implementation and the trait definition.Ok, you caught me. I simplified the definition of . There's actually more to it, because of course we want database calls to be async.Writing to a file or calling a database server is precisely the kind of slow, blocking IO that we don't want to stall on.We need to make  an async trait. Unfortunately, it's not quite as simple as writingRust understands this, and it will compile, but probably won't do what you expect.Although writing  will cause your method's return value to be sugared into Future<Output = Result<Author, CreateAuthorError>>, it  get an automatic  bound.As a result, your future can't be sent between threads. For web applications, this is useless.Let's spell things out for the compiler!Since our  and  are both , a  that wraps them can be too .But what good is a repository if its methods return thread-safe s, but the repo itself is bound to a single thread? Let's ensure  is  too.Ugh, we're not done. Remember about 4,000 words ago when we wrapped our application state in an  to inject into an HTTP handler? Well, trust me, we did. requires its contents to be both  and  to be either  itself!  on the topic if you'd like to know more.Your instinct might now be to implement  for  instead of , since  is immutable and therefore . However, sqlx's connection pools are themselves , meaning  is too.Naturally, if we're shuffling a repo between threads, Rust wants to be sure it won't be dropped unexpectedly. Let's reassure the compiler that every  will live for the whole program:Finally, our web server, axum, requires injected data to be , giving our final trait definition:From the Very Bad Application to the merely Bad ApplicationIt's time to start putting these pieces together. Let's reassemble our  HTTP handler to take advantage of the  abstraction.First, the definition of , which is the struct that contains the resources that should be available to every HTTP handler. This pattern should be familiar to users of both  and . is now generic over . That is,  provides HTTP handlers with access to "some store of author data", giving them the ability to create authors without knowledge of the implementation.We wrap whatever instance of  we receive in an , because axum is going to share it between as many async tasks as there are requests to our application.This isn't our final destination – eventually our HTTP handler won't even know it has to save something (ah, sweet oblivion).We're not quite there yet, but this is a vast improvement. Check out the handler!Doesn't your nervous system feel calmer to behold it?Go on, take some deep breaths. Enjoy the moment.  if you need a reminder.Ok, the walkthrough.  has access to an , which it makes good use of. But first, it converts the raw CreateAuthorHttpRequestBody it received from the client into the holy domain representation . Here's how:Nothing fancy! Boilerplatey, you might think. This is by design. We have preemptively decoupled the HTTP API our application exposes to the world from the internal domain representation.As you scale, you will thank this so-called boilerplate. You will name your firstborn child for it.These two things can now change independently. Changing the domain doesn't necessarily force a new web API version. Changing the HTTP request structure does not require any change to the domain. Only the mapping in CreateAuthorHttpRequestBody::into_domain and its corresponding unit tests get updated.This is a very special property. Changes to transport concerns or business logic no longer spread through your program like wildfire. Abstraction has been achieved.Thanks to the pains we took to define all the errors an  is allowed to return, constructing an HTTP response is dreamy. In the error case, we map seamlessly to a serializable  using :If the author was found to be a duplicate, it means the client's request was correctly structured, but that the contents were unprocessable. Hence, we're aiming to respond .Important detail alert! Do you see how we're manually building an error message at , even though CreateAuthorError::to_string could have produced this error for us?This is another instance of aggressive decoupling of our transport concern (JSON over HTTP) from the domain. Returning full-fat, unpasteurised domain errors to users is an easy way to leak private details of your application. It also results in unexpected changes to HTTP responses when domain implementation details change!If we get an error the domain didn't expect – CreateAuthorError::Unknown here – that maps straight to .The finer points of how you log the underlying cause will vary according to your needs. Crucially, however, the error itself is not exposed to the end user.Finally, our success case . We take a reference to the returned  and transform it into its public API counterpart. It gets sent on its way with status .Testing HTTP handlers with injected repositoriesPreviously, our handler code was impossible to unit test, because we needed a real database instance to call them. Trying to exercise every failure mode of a database call with a real database is pure pain.Those days are over. By injecting any type that implements , we open our HTTP handlers to unit testing with mock repositories. is defined to hold the  it should return in response AuthorRepository::create_author calls .The rather nasty type signature at  is due to the fact that  has a  bound, which means  must be .Unfortunately for us,  isn't , because its  variant contains .  isn't  since it's designed to wrap unknown errors, which may not be  themselves.  is one common non- error.Rather than passing  a convenient Result<Author, CreateAuthorError>, we need to give it something cloneable – . But, as discussed, 's contents need to be  for  to be , so we're forced to wrap the  in a . (I'm using a  here, hence the , but  also works with minor changes to the supporting code).The mock implementation of  then deals with swapping a dummy value with the real result in order to return it to the test caller.Here's the test for the case where the repository call succeeds. I leave the error case to your powerful imagination, but if you crave more Rust testing pearls, I'll have a comprehensive guide to unit testing for you soon!At , we construct a  with an arbitrary success . We expect that a  from the repo should produce a Result::Ok(ApiSuccess<CreateAuthorResponseData>) from the handler .This situation is simple to set up – we just call the  handler with a  object constructed from the  in place of a real one . The assertions are self-explanatory.I know, I know – you're itching to see what  looks like with these improvements, but we're about to take a much bigger and more important leap in our understanding of hexagonal architecture.In Part III, coming next, I'll introduce you to the beating heart of an application domain: the .We'll ratchet up the complexity of our example application to understand how to set domain boundaries. We'll confront the tricky problem of  through the lens of authentication, and explore the interface between hexagonal applications and distributed systems.And yes, we'll finally answer, "why hexagons?".]]></content:encoded></item><item><title>[UPDATE] Successfully fixed the problems of QCA9377 802.11ac Wireless Network Adapter!</title><link>https://www.reddit.com/r/linux/comments/1k387ef/update_successfully_fixed_the_problems_of_qca9377/</link><author>/u/lonelyroom-eklaghor</author><category>dev</category><category>reddit</category><pubDate>Sat, 19 Apr 2025 22:36:01 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This is a guide. If you have a Qualcomm Atheros QCA9377 802.11ac Wireless Network Adapter and you're facing issues in Linux, this is for you only.I have switched from KDE Neon to Fedora Workstation, and honestly, it works mostly fine (except the Night Light). However, I faced the same Wi-Fi problem initially. As I was trying out everything, I noted down the quirks of all the techniques out there on the Internet.The issue here is that there are two kinds of problems with this particular WLAN adapter: the disconnection problem and the network speed problem. In my case, I'll be mainly dealing with the disconnection problem, but in case anyone knows about the network problem (especially how to implement Roaming Aggressiveness in Linux), then I'll cover it in a separate post. Experts are encouraged to chime in :)A simple note that some of these methods might work in one distro, but not for the other ones. However, I'll only be stating the ones which worked for me in Fedora 41 & 42.In your terminal, open this file/etc/NetworkManager/conf.d/wifi-powersave.conf using whatever editor you prefer. (Neovim or Nano or Emacs or whatever)Write this down or change it appropriately:[connection] wifi.powersave=2 Restart your computer after that.For me, it absolutely didn't work. The wlp1s0 network interface was disappearing as a whole.This one might not actually work because linux-firmware has already merged the last commit, so this might not be the fix.At first, check if this is the file tree:/lib/firmware/ath10k/QCA9377 ├── firmware-6.bin.xz └── hw1.0 ├── board-2.bin ├── board-2.bin.xz ├── board.bin ├── board.bin.xz ├── CNSS.TF.1.0 ├── firmware-5.bin.xz ├── firmware-6.bin.xz ├── firmware-sdio-5.bin.xz ├── notice_ath10k_firmware-5.txt.xz ├── notice_ath10k_firmware-6.txt.xz -> ../../QCA6174/hw3.0/notice_ath10k_firmware-6.txt.xz ├── notice_ath10k_firmware-sdio-5.txt.xz -> notice_ath10k_firmware-5.txt.xz ├── untested ├── WLAN.TF.1.0 └── WLAN.TF.2.1 You just need to ensure that there is content within this  directory; it's optional for the files to match.Click on the Code icon in blue, then scroll down to "Download this directory". Under that section, you can download in any format.Download that archive, then extract it.Through your terminal, use  to go to the folder where you have extracted it all.Go to the directory/folder named . Under that directory, there will only be one item called .While being under this  directory in the terminal, as a protective measure, write ls /lib/firmware/ath10k/QCA9377/. Check if there's only  or not.Press the up arrow, then replace thatwith  . Then it becomessudo cp -rv * /lib/firmware/ath10k/QCA9377/.Press Enter. Wait for the files to go.Just so you know, it didn't work in this case.3) Copying firmware files (didn't work but this can fix your issue)As usual, check what ls /lib/firmware/ath10k/QCA9377/hw1.0/ leads to. What are the names of the firmware files?I think you guys have seen it... the names are like firmware-6, firmware-5. Basically, the one with the highest number is the one being run.Suppose N is the highest number. Then, you will use cd /lib/firmware/ath10k/QCA9377/hw1.0/ .Notice the file you see resembling firmware-N.whatever.extensions . Copy it to the parent directory. In simpler terms: sudo cp -v firmware-N.whatever.extensions ..Even this one didn't quite work. At first, it could resolve the network interface disappearance issue for some time. I even attended a class through Google Meet. But just after classes ended, I used Suspend/S3 Sleep. After waking, the Wi-Fi wasn't working at all, just like the previous solutions. On a different note, you guys can try this out if you can make a startup script with root access (but this might be tedious): https://github.com/pop-os/pop/issues/1470#issuecomment-20291191164) ath10k-custom.conf (hyphen) and ath10k_core.conf (underscore) (Read it carefully, skip_otp is an important aspect after all)At first, I tried to create ath10k-custom.conf. That's what helped someone in the previous post. However, my problems were resolved ONLY after wrting ath10k_core.conf.Just execute these commands ONCE and you'll be fine. Note that the following commands are case-sensitive.For ath10k-custom.conf: echo -e "options ath10k_core skip_otp=y\noptions ath10k_core rawmode=0" | sudo tee -a /etc/modprobe.d/ath10k-custom.confFor ath10k_core.conf: echo "options ath10k_core skip_otp=y" | sudo tee -a /etc/modprobe.d/ath10k_core.confRestart your PC after executing the first command, and after executing the second command (basically twice).I have tried my best to propose all the solutions to this problem I could find, and now I'm tired. It's already 3:58 AM. To the firmware/NetworkManager experts, it'd be a pleasure for me to know how can roaming aggressiveness be increased. To the normal users, in case you find anything problematic, you can ask me in the comments.]]></content:encoded></item><item><title>A full data pipeline in Rust to explore how politicians use words</title><link>https://www.reddit.com/r/rust/comments/1k37v1v/a_full_data_pipeline_in_rust_to_explore_how/</link><author>/u/sufjanfan</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 19 Apr 2025 22:18:46 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I've built a little tool that allows you to search through transcripts of the most recent session of the Canadian House of Commons to generate breakdowns of how often members of parliament use your search term by party, gender, province, etc. Check it out here!It started with a very basic web scraper to download the Hansard transcripts in HTML format - didn't even need selenium. From there I populated a MariaDB database of MPs and other speakers mostly manually, and built a hacky translator to convert the transcripts into speech strings with a time and matchable name attached.I hadnt scoped out the project much by that point and was just going to poke through the numbers myself with some SQL, but I had the silly idea to make it accessible through a web app, so I threw together an axum server and a frontend with yew and plotters. I added a few more graphs and features, jazzed up the style a bit, and tried to make the backend not waste too much processing time.Eventually I'd like to have the scraper and translator work in a live pipeline to keep this thing updating as the house sits again after our election coming up. A time series selector, or at least a session selector, would be a good add in that case.If you're a statistician you're probably horrified at this point, but I'm having fun and I think there's something worthwhile to play around with here even if none of this is rigorous enough to draw hard conclusions. This is a unique space and I'd like to explore it a bit more.]]></content:encoded></item><item><title>Shout out to nautilus-scripts (which, despite the name, work in Caja, Dolphin, Nemo, PCManFM-Q, and Thunar, too)</title><link>https://www.reddit.com/r/linux/comments/1k37s2f/shout_out_to_nautilusscripts_which_despite_the/</link><author>/u/JimmyRecard</author><category>dev</category><category>reddit</category><pubDate>Sat, 19 Apr 2025 22:14:39 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This project is probably my single most used tool outside of the core OS software, and after it saved me a bunch of time yet again, I figured I'd rave about it a bit, if you'll indulge me.I'm not much of a customisation devotee. I rawdog basically vanilla Gnome with only a few strategic extensions, and that's the way I like it. But the one place where this radical turn towards simplicity has presented challenges are file managers.Back years ago in my Windows days, I used to us Directory Opus and loved it, but none of the third party file managers really stuck with me on Linux. But I still missed some of the cool features. Well, this project fills the gap.It is a set of scripts that you can invoke from context click to execute all kinds of useful actions. The selection is extensive, and I use the following the most:copy filepath to clipboard (the path box doesn't contain name of the specific file, this lets me yoink the path and the file name in one go)paste from clipboard as a file (paste text directly into a file, without needing to create the file first)list the largest files/directoriescombine multiple PDFs into one (great for merging multiple PDFs into one before feeding it to my document storage solution)optimise PDFs/images for webstrip exif data via ExifToolverify checksum files (to verify my linux .isos, naturally)convert webps to pngs/jpgspaste as hard links (recursively paste whole folder as hard links, equivalent of , my MVP)permanently delete via git operations, especially pullThere are a bunch more too. If you find the sheer number overwhelming, you don't have to use them all, the install script lets you pick what you want.If you ever felt your file manager needed a bit more oomph, give it a look.]]></content:encoded></item><item><title>Show HN: I built an AI that turns GitHub codebases into easy tutorials</title><link>https://github.com/The-Pocket/Tutorial-Codebase-Knowledge</link><author>zh2408</author><category>dev</category><category>hn</category><pubDate>Sat, 19 Apr 2025 21:04:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Raspberry Pi Lidar Scanner</title><link>https://github.com/PiLiDAR/PiLiDAR</link><author>Venn1</author><category>dev</category><category>hn</category><pubDate>Sat, 19 Apr 2025 18:53:32 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Arch Linux Is the Latest Distro Replacing Redis with Valkey</title><link>https://news.slashdot.org/story/25/04/19/0413205/arch-linux-is-the-latest-distro-replacing-redis-with-valkey?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sat, 19 Apr 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[In NoSQL database news, Arch Linux "is the latest Linux distribution replacing its Redis packages with the Valkey fork," reports Phoronix. 

Valkey is backed by the Linux Foundation, Google, Amazon Web Services, and Oracle, which the article points out is due to Redis's decision last year to shift the upstream Redis license from a BSD 3-clause to RSALv2 and SSPLv1.


Valkey is replacing Redis in the Arch Linux extra repository and after a two week period the Redis package will be moved out to AUR and receive no further updates. Users are encouraged to migrate to Valkey as soon as possible.
]]></content:encoded></item><item><title>Librarians are dangerous</title><link>https://bradmontague.substack.com/p/librarians-are-dangerous</link><author>mooreds</author><category>dev</category><category>hn</category><pubDate>Sat, 19 Apr 2025 14:49:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Undercutf1 – F1 Live Timing TUI with Driver Tracker, Variable Delay</title><link>https://github.com/JustAman62/undercut-f1</link><author>deltaknight</author><category>dev</category><category>hn</category><pubDate>Sat, 19 Apr 2025 07:50:36 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[undercutf1 is a F1 live timing app, built as a TUI. It contains traditional timing pages like a Driver Tracker, Timing Tower, Race Control, along with some more detailed analysis like lap and gap history, so that you can see strategies unfolding.I started to build undercutf1 almost two years ago, after becoming increasingly frustrated with the TV direction and lack of detailed information coming out of the live feed. Overtakes were often missed and strategies were often ill-explained or missed. I discovered that F1 live timing data is available over a simple SignalR stream, so I set out building an app that would let me see all the information I could dream of. Now undercutf1 serves as the perfect companion (like a second Martin Brundle) when I'm watching the sessions live.If you want to test it out, you replay the Suzuka race easily by downloading the timing data, then starting a simulated session:1. Download undercutf1 using the installation instructions in the README.2. Import the Suzuka race session data using `undercutf1 import 2025 -m 1256 -s 10006`.3. Start the app (`undercutf1`) then press S (Session) then F (Simulated Session), then select Suzuka then Race using the arrow keys, then press Enter.4. Use arrow keys to navigate between the timing pages, and use N / Shift+N to fast-forward through the session.If you want to test it out during this weekends Jeddah GP, simply install as in the README then start a live session by pressing S (Session) then L (Live Session).The app is built for a terminal of roughly 110x30 cells, which probably seems an odd size but just so happens to be the size of a fullscreen terminal on a MBP zoomed in far enough that the text is easily glanceable when the laptop is placed on a coffee table some distance away from me :) Other terminal sizes will work fine, but information density/scaling may not be ideal.If you're using the TUI during a live session, you'll want to synchronise the delay of the timing feed to your TV feed. Use the N/M keys to increase/decrease the delay. During non-race session, I find it fairly easy to sync the session clock on TV with the session clock on the bottom left of the timing screen. For race sessions, synchronisation is a little harder. I usually aim to sync the start of the race time (e.g. 13:00 on the timing screen clock) with the start of the formation lap, where the live feed helpfully shows the clock tick over to 0 minutes. I usually delay the feed by 30 to 60 seconds.]]></content:encoded></item><item><title>Show HN: Goldbach Conjecture up to 4*10^18+7*10^13</title><link>https://medium.com/@jay_gridbach/grid-computing-shatters-world-record-for-goldbach-conjecture-verification-1ef3dc58a38d</link><author>jay_gridbach</author><category>dev</category><category>hn</category><pubDate>Sat, 19 Apr 2025 06:11:37 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[A new challenge to a 280-year-old unsolved mathematical problemI’ve achieved a new world record in verifying the Goldbach Conjecture, a famous unsolved problem in mathematics, by extending the verification up to 4 quintillion (4×10¹⁸) + 70 trillion (7×10¹³). This article introduces Gridbach, the grid computing system I developed for this computation.Check out my live grid computing system here:https://gridbach.comNo login is required. You can immediately see the computation results on both PC and mobile.I am @jay_gridbach, a freelance engineer and consultant based in Kanagawa, Japan. Currently, I work as a PreSales engineer at a company in Tokyo, while also developing web applications and my own services. Gridbach is a project I’ve been nurturing since my corporate days, and I’m thrilled to release it in April 2025.The Goldbach Conjecture, proposed by Prussian mathematician Christian Goldbach in 1742, remains an unsolved problem in mathematics. Despite the efforts of numerous modern mathematicians, no one has proven it mathematically. The conjecture itself is simple enough for a middle school student to understand:“Every even natural number greater than 2 is the sum of two prime numbers”4 = 2 + 26 = 3 + 3………1000000000001092576 = 1913 + 1000000000001090663…While it’s believed to be true, a mathematical proof for all even numbers is still elusive.In 2013, T. Oliveira e Silva from Portugal verified the conjecture up to 4×10¹⁸ using computers. This was the previous world record.https://sweet.ua.pt/tos/goldbach.htmlMy newly developed Gridbach system has slightly surpassed this record by adding 70 trillion to the verified range. I aim to push this further to 5 quintillion by increasing the number of participating machines and improving my algorithms. I’m not sure how to get this recognized as an official record, but I’m willing to write a paper if necessary. If anyone has insights on this, please let me know.Gridbach is a cloud-based distributed computing system accessible from any PC or smartphone.It requires no login or app installation. The high-performance WASM (WebAssembly) binary code is downloaded as browser content, enabling computation on the user’s browser.Each computation job covers a range of 100 million (50 million even numbers), taking about 5–10 seconds on a PC and 10–20 seconds on a smartphone.Inspired by SETI@home, I’ve aimed to create a system that anyone can easily join.My system uses a combination of high-performance WASM for efficient computation and a highly scalable JAMStack architecture.The challenges and lessons learned in choosing and setting up this stack are worth a separate post.The app is simple, offering two main features: running computations on your machine and viewing the collective results from all Gridbach users. It’s also mobile-friendly.For details on the dashboard data, please visit - https://app.gridbach.com/I define “Goldbach Ridge” as the maximum value of the smaller prime in the prime pairs satisfying the Goldbach Conjecture within a given range.While T. Oliveira e Silva refers to these as the smaller prime  in the Goldbach partition, I named them as “ridge” as graphing these looks like mountain peaks and cols.Oliveira e Silva al. discovered a large Goldbach ridge of 9781:Feel free to join in and see if you can find some bigger peaks! So far, 6421 is the largest one found on my system. Your top 30 Goldbach Peaks are displayed on the My Calculation screen.The core computation logic of mine is open-sourced as a Go command-line tool under the MIT license.https://github.com/nakatahr/gridbach-coreHere’s a snippet of the code. I’ve upgraded the Sieve of Eratosthenes to use bitwise operations on byte arrays for faster prime number generation, and optimized it for given ranges.I also plan to write a separate article about the computation algorithm.Thank you for reading! Please take 5 minutes to try the computation on gridbach.com. I aim to continue breaking records and improving the system, hoping to spark interest in mathematics and IT.]]></content:encoded></item><item><title>SIGMOD Programming Contest Archive: In-Memory Join Pipeline (2025)</title><link>https://transactional.blog/sigmod-contest/2025.html</link><author></author><category>dev</category><pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate><source url="https://transactional.blog/">Dev - Transactional</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>