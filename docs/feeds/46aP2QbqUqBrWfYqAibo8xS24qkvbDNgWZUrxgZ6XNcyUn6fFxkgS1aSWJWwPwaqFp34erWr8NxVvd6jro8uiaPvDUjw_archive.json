{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Event driven restart of Pods?","url":"https://www.reddit.com/r/kubernetes/comments/1iax3fa/event_driven_restart_of_pods/","date":1737943849,"author":"/u/ButterscotchWeak1192","guid":384,"unread":true,"content":"<p>Context: we have a particular Pod which likes to hang, for unknown to us reasons and conditions (it's external software, we can't modify, and logs don't show anything).</p><p>The most accurate way to tell when it's happening is by checking a liveness probe. We have monitoring set up for particular URL and we can check for non 2xx status.</p><p>This chart we talk about deploys  Pod as well as  Pods. Each is separate Deployment.</p><p>The issue: when  Pod fails it's liveness probe, it gets restarted by k8s. But we also need to restart  nodes, because for some reason it looks like they lose connection in such way that they don't pick up work, and only restart helps. And  in this case .  Pod first, then .</p><p>Restart in case of liveness probe restarts only affected Pod. Currently, to restart workers too, I installed <a href=\"https://keda.sh/\">KEDA</a> in cluster and created <a href=\"https://keda.sh/docs/2.14/concepts/scaling-jobs/\">ScaleJob</a> object to trigger deployment restart. As trigger we use <code>kube_pod_container_status_restarts_total</code> Prometheus query:</p><pre><code>apiVersion: keda.sh/v1alpha1 kind: ScaledJob metadata: name: n8n-restart-job-scaler namespace: company spec: jobTargetRef: kind: Job name: n8n-worker-restart-job spec: jobTargetRef: template: spec: containers: - name: kubectl image: bitnami/kubectl:latest # imagePullPolicy: Always command: [\"/bin/sh\", \"-c\"] args: [\"kubectl rollout restart deployment n8n-worker -n company\"] backoffLimit: 4 pollingInterval: 15 # Check every 15 seconds (default: 30) successfulJobsHistoryLimit: 1 # How many completed jobs should be kept. failedJobsHistoryLimit: 1 # How many failed jobs should be kept. triggers: - type: prometheus metadata: serverAddress: https://&lt;DOMAIN&gt;.com/select/0/prometheus metricName: pod_liveness_failure threshold: \"1\" # Triggers when any liveness failure alert is active query: increase(kube_pod_container_status_restarts_total{pod=~\"^n8n-[^worker].*$\"}[1m]) &gt; 0 </code></pre><p>This kind of works. I mean it succesfully triggers restarts. But: - in current setup it triggers multiple restarts when there was only single liveness probe failure. This extends downtime<p> - depending on different settings for check time, there might be a slight delay between time of event, and time of triggering</p></p><p>I've been thinking about more event-driven workflow. So that when event in cluster happens, I can perform matching action. but I don't know what options would be most suitable for this task.</p><p>What do you suggest here? Maybe you've had such problem? How would you deal with it?</p><p>if something is unclear or I didn't provide something, ask below and I'll provide more info.</p>","contentLength":2504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"k3s pods networking","url":"https://www.reddit.com/r/kubernetes/comments/1iaszwq/k3s_pods_networking/","date":1737932042,"author":"/u/crewman4","guid":380,"unread":true,"content":"<p>im not used to \"onprem\" k8s and am testing setting up an k3s in my homelab and i cant get it to work. ive been testing this on debian server and whatever i do, fresh installs and such, i cant enter a pod and wget an external internet site. all sites point to some IP (213.163.146.142:443)</p><p>Non-authoritative answer:</p><p>i can resolve dns , but thats hosted internally. everything else works from debian server and no firewalls active. ive been chatGPTing for hours but im stuck. ive rolled new servers and tested everything :P</p>","contentLength":519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Way to Collect Traces for Tempo","url":"https://www.reddit.com/r/kubernetes/comments/1iaq420/best_way_to_collect_traces_for_tempo/","date":1737924977,"author":"/u/Sule2626","guid":382,"unread":true,"content":"<p>I'm currently using Prometheus, Grafana, and Loki in my stack, and I'm planning to integrate Tempo for distributed tracing. However, I'm still exploring the best way to collect traces efficiently.</p><p>I've looked into Jaeger and OpenTelemetry:</p><ul><li> seems to require a relatively large infrastructure, which feels like overkill for my use case.</li><li> looks promising, but it overlaps with some functionality I already have covered by Prometheus (metrics) and Loki (logs).</li></ul><p>Does anyone have recommendations or insights on the most efficient way to implement tracing with Tempo? I'm particularly interested in keeping the setup lightweight and complementary to my existing stack.</p>","contentLength":658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Has the behaviour of maxUnavailable and maxSurge for RollingUpdates changed since v1.21.9","url":"https://www.reddit.com/r/kubernetes/comments/1iam71l/has_the_behaviour_of_maxunavailable_and_maxsurge/","date":1737916265,"author":"/u/mrnadaara","guid":381,"unread":true,"content":"<p>We've deployed a new cluster with v1.30.7 and tried to deploy a deployment with a maxSurge of 1 and maxUnavailable with 0. The new pod remains stuck in Pending with the following reasons:</p><p>0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.</p><p>Changing maxUnavailable to 1 fixes it but I'm curious as to why it fails with the new version when it worked fine in the old version. It exceeds the replica count when doing a rolling update so it makes sense the pod wouldn't be scheduled until the old one is deleted, but since we've set the maxUnavailable to 0 the old pods are never deleted. This in theory shouldn't have worked in the old version as well. Am I misconstruing things here?</p>","contentLength":757,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes EKS course","url":"https://www.reddit.com/r/kubernetes/comments/1iaawqa/kubernetes_eks_course/","date":1737886342,"author":"/u/caiolagreca","guid":379,"unread":true,"content":"<p>Hi everyone, I’m looking to learn Kubernetes and Amazon EKS. I haven’t found many good tutorials on yotube, and the Udemy courses that I had checked have not so good reviews. Could you recommend any good courses based on your experience? Thank you!</p>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Patching 3rd party chart to support secrets - ideas","url":"https://www.reddit.com/r/kubernetes/comments/1iaal1o/patching_3rd_party_chart_to_support_secrets_ideas/","date":1737885196,"author":"/u/0x4ddd","guid":383,"unread":true,"content":"<div><p>I need to install 3rd party Helm chart, unfortunately it expects some of the secret values (like API keys and database credentials) to be provided via plain Helm values. No secret support at all.</p><p>This doesn't natively align very nicely with storing desired state in Git.</p><p>What do you typically do in such scenario?</p><ul><li>utilize helm-secrets (we don't use it at the moment)</li><li>create some ugly Kustomize patches to make it work with External Secret Operator (we are already using ESO for other charts to sync secrets from cloud KMS)</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/0x4ddd\"> /u/0x4ddd </a>","contentLength":547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}