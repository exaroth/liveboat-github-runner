{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Smarter Scheduling for AI Workloads: Topology-Aware Scheduling","url":"https://www.reddit.com/r/kubernetes/comments/1p8ku4a/smarter_scheduling_for_ai_workloads_topologyaware/","date":1764301188,"author":"/u/Electronic_Role_5981","guid":510,"unread":true,"content":"<div><p><strong>TL;DR — Topology-Aware Scheduling (Simple Summary)</strong></p><ol><li><strong>AI workloads need good hardware placement.</strong> GPUs, CPUs, memory, PCIe/NVLink all have different “distances.” Bad placement can waste 30–50% performance.</li><li><strong>Traditional scheduling isn’t enough.</strong> Kubernetes normally just counts GPUs. It doesn’t understand NUMA, PCIe trees, NVLink rings, or network topology.</li><li><strong>Topology-Aware Scheduling fixes this.</strong> The scheduler becomes aware of full hardware layout so it can place pods where GPUs and NICs are closest.</li><li><ul><li><strong>DRA (Dynamic Resource Allocation)</strong></li><li> These let Kubernetes make smarter placement choices.</li></ul></li><li><ul><li>Simple single-GPU jobs → normal scheduling is fine.</li><li>Multi-GPU or distributed training → topology-aware scheduling gives big performance gains</li></ul></li></ol></div>   submitted by   <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"developing k8s operators","url":"https://www.reddit.com/r/kubernetes/comments/1p8gmv6/developing_k8s_operators/","date":1764288005,"author":"/u/TraditionalJaguar844","guid":511,"unread":true,"content":"<div><p>I’m doing some research on how people and teams are using Kubernetes Operators and what might be missing.</p><p>I’d love to hear about your experience and opinions:</p><ul><li>Which operators are you using today? </li><li>Which of them are running in production vs non-prod?</li><li>Have you ever needed an operator that didn’t exist? How did you handle it — scripts, GitOps hacks, Helm templating, manual ops?</li><li>Have you considered writing your own custom operator? </li><li>If yes, why? if you didn't do it, what stopped you ? </li><li>If you could snap your fingers and have a new Operator exist today, what would it do?</li></ul><p>Trying to understand the gap between what exists and what teams really need day-to-day.</p><p>Thanks! Would love to hear your thoughts</p></div>   submitted by   <a href=\"https://www.reddit.com/user/TraditionalJaguar844\"> /u/TraditionalJaguar844 </a>","contentLength":743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Running Kubernetes in the homelab","url":"https://www.reddit.com/r/kubernetes/comments/1p89ywo/running_kubernetes_in_the_homelab/","date":1764269491,"author":"/u/AlertKangaroo6086","guid":512,"unread":true,"content":"<p>I’ve been wanting to dip my toes into Kubernetes recently after making a post over at <a href=\"https://www.reddit.com/r/homelab\">r/homelab</a></p><p>It’s been on a list of things to do for years now, but I am a bit lost on where to get started. There’s so much content out there regarding Kubernetes - some of which involves running nodes on VMs via Proxmox (this would be great for my set up whilst I get settled)</p><p>Does anyone here run Kubernetes for their lab environment? Many thanks!</p>","contentLength":437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automating Talos on Proxmox with Self-Hosted Sidero Omni (Declarative VMs + K8s)","url":"https://www.reddit.com/r/kubernetes/comments/1p87rdz/automating_talos_on_proxmox_with_selfhosted/","date":1764264039,"author":"/u/aceofskies05","guid":514,"unread":true,"content":"<p>I’ve been testing out  (running self-hosted) combined with their new <strong>Proxmox Infrastructure Provider</strong>, and it has completely simplified how I bootstrap clusters. I've probably tried over 10+ way to bootstrap / setup k8s and this method is by far my favorite. There is a few limitations as the Proxmox Infra Provider is in beta technically. </p><p>The biggest benefit I found is that I didn't need to touch Terraform, Ansible, or manual VM templates. Because Omni integrates directly with the Proxmox API, it handles the infrastructure provisioning and the Kubernetes bootstrapping in one go.</p><p>I recorded a walkthrough of the setup showing how to:</p><ul><li>Run Sidero Omni self-hosted (I'm running it via Docker)</li><li>Register Proxmox as a provider directly in the UI/CLI</li><li>Define \"Machine Classes\" (templates for Control Plane/Worker/GPU nodes)</li><li>Spin up the VMs and install Talos automatically without external tools</li></ul>","contentLength":887,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WAF for nginx-ingress (or alternatives?)","url":"https://www.reddit.com/r/kubernetes/comments/1p7zmpl/waf_for_nginxingress_or_alternatives/","date":1764241755,"author":"/u/marahin","guid":513,"unread":true,"content":"<p>I'm self-hosting a Kubernetes cluster at home. Some of the services are exposed to the internet. <strong>All http(s) traffic is only accepted from Cloudflare IPs.</strong></p><p>This is fine for a general web app, but when it comes to media hosting it's an issue, since Cloudflare has limitations on how much can you push through to the upstream (say, a big docker image upload to my registry will just fail).</p><p>Also I can still see _some_ malicious requests. For example, I receive some checking for ,  files, etc.</p><p>I'm running  which has some support for paid license WAF (F5 WAF) which I'm not interested in. I'd much rather run with <a href=\"https://coraza.io/\">Coraza</a> or something similar. However, I don't see clear integrations documented in the web.</p><ul><li>have something filtering the HTTP(s) traffic that my cluster receives - <strong>it has to run in the cluster</strong>,</li><li>be able to securely receive traffic from outside of Cloudflare, <ul><li>a big plus would be if I could do it based on the domain (host), e.g. <a href=\"http://host-A.com\">host-A.com</a> will only handle traffic coming through CF, and <a href=\"http://host-B.com\">host-B.com</a> will handle traffic from wherever,</li><li>some services in mind: docker-registry, nextcloud</li></ul></li></ul><p>If we go by an nginx-ingress alternative, it has to:</p><ul><li>support cert-manager &amp; LetsEncrypt cluster issuers (or something similar - basically HTTPS everywhere),</li><li>support retrieving real ip from headers (from traffic coming from Cloudflare)</li><li>support retrieving real ip (replacing the local router gateway the traffic was forwarded from)</li></ul><p>What do you use? What should I be using?</p>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1p7zhaw/weekly_this_week_i_learned_twil_thread/","date":1764241233,"author":"/u/gctaylor","guid":509,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","k8s"]}