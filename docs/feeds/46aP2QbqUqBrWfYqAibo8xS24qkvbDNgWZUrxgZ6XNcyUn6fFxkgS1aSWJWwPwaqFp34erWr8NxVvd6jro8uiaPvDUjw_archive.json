{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Installing operators and CRs in automated way?","url":"https://www.reddit.com/r/kubernetes/comments/1ilitg3/installing_operators_and_crs_in_automated_way/","date":1739119213,"author":"/u/Such_Relative_9097","guid":421,"unread":true,"content":"<p>Hi, maybe I’m wrong but I see some technologies officially provide their k8s installation with operators and CRs (being installed after) instead of official helm chart. We all know the cons/pros using helm… and the advantages of operators.. but how the operator installation will work in automation? I mean, seem to be the CR yaml must be deployed after the operator yaml to function properly. In my case I do not mind using operators but I need an automated way to deploy them.. Maybe I grasp the concept all wrong… how you guys tackle this? Which tools? (Ansible for instance) … my case is very specific one because I must provide to the customer a bundle of charts (umbrella) .. so I can’t even use ansible and etc.. ok I can create helm chart that will deploy the operator and the CR but it feels weird and definitely I need your opinion and guidance about the matter. Thank you ..</p>","contentLength":895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DOKS vs GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ilhsz1/doks_vs_gke/","date":1739116613,"author":"/u/Impossible-Night4276","guid":423,"unread":true,"content":"<p>I used GKE at my job but I'm starting a personal project now so I'm shopping around for a managed cluster</p><p>I can get a basic cluster on DOKS for $12/month while GKE charges about $100/month?</p><p>I understand the sentiment \"DigitalOcean is for hobbyists\" and \"GCP is for enterprises\" but why is that? What does GKE provide that DOKS doesn't?</p>","contentLength":333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubeconfig Operator: Create restricted kubeconfigs as custom resources","url":"https://www.reddit.com/r/kubernetes/comments/1ild0hg/kubeconfig_operator_create_restricted_kubeconfigs/","date":1739101547,"author":"/u/ASBroadcast","guid":420,"unread":true,"content":"<p>There recently was a post by the Reddit engineer <a href=\"https://www.reddit.com/u/keepingdatareal\">u/keepingdatareal</a> about their new SDK to build operators: <a href=\"https://www.reddit.com/r/RedditEng/comments/1gp11ui/open_source_of_achilles_sdk/\">Achilles SDK</a>. It allows you to specify Kubernetes operators as finite state machines. Pretty neat!</p><p>I used it to build a <a href=\"https://github.com/klaudworks/kubeconfig-operator\">Kubeconfig Operator.</a> It is useful for anybody who quickly wants to hand out limited access to a cluster without having OIDC in place. I also like to create a \"daily-ops\" kubeconfig to protect myself from accidental destructive operations. It usually has readonly permissions + deleting pods + creating/deleting portforwards.</p><p>Unfortunately, I can just add a single image but check out the repo's <a href=\"https://github.com/klaudworks/kubeconfig-operator\">README.md</a> to see a graphic of the operator's behavior specified as a FSM. Here is a sample Kubeconfig manifest:</p><pre><code> apiVersion: kind: Kubeconfig metadata: name: restricted-access spec: clusterName: local-kind-cluster # specify external endpoint to your kubernetes API. # You can copy this from your other kubeconfig. server: https://127.0.0.1:52856 expirationTTL: 365d clusterPermissions: rules: - apiGroups: - \"\" resources: - namespaces verbs: - get - list - watch namespacedPermissions: - namespace: default rules: - apiGroups: - \"\" resources: - configmaps verbs: - '*' - namespace: kube-system rules: - apiGroups: - \"\" resources: - configmaps verbs: - get - list - watchklaud.works/v1alpha1 </code></pre><p>If you like the operator I'd be happy about a Github star ⭐️. The core logic is already fully covered by tests. So feel free to use it in production. Should any issue arise, just open a Github issue or text me here and I'll fix it.</p>","contentLength":1545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minikube versus Kind: GPU Support","url":"https://www.reddit.com/r/kubernetes/comments/1ilb8v2/minikube_versus_kind_gpu_support/","date":1739093973,"author":"/u/MaximumNo4105","guid":422,"unread":true,"content":"<p>I come from a machine learning background with some, little, DevOps experience. I am trying to deploy a local Kubernetes cluster with NVIDIA GPU support.</p><p>I have so far been using Kind to do so, deploying three services and exposing them via an ingress controller locally, but I stumbled upon what seems to be an ongoing issue with providing GPU support to the containers when using kind. I have already set the container runtime to use NVIDIA's runtime. I have followed guides on installing NVIDIA plugin into the cluster, mounting the correct GPU devices paths, providing tolerations as to where a deployment which requires GPU access can be deployed to, I have tried everything, but still I am unable to access the GPUs from</p><p>Is this a known issue within the DevOps community?</p><p>If so, would switching to minikube make gaining access to the GPUs any easier? Has anyone got any experience deploying a minikube cluster locally and successfully gaining access to the GPUs?</p><p>I appreciate your help and time to read this.</p><p>Any help whatsoever is welcomed.</p>","contentLength":1042,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fluxcd useful features","url":"https://www.reddit.com/r/kubernetes/comments/1il9d9q/fluxcd_useful_features/","date":1739085759,"author":"/u/Upper-Aardvark-6684","guid":424,"unread":true,"content":"<p>I have been using fluxcd as gitops tool since 6 months at my job. The most useful features I found was the dependson and wait parameters that help me better manage dependencies. I want to know if there are more such features that I might have missed or not used and have been useful to you. Let me know how flux has helped you in your k8s deployments.</p>","contentLength":351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Securing Kubernetes Secrets & Disaster Recovery with SOPS and FluxCD — My Journey","url":"https://www.reddit.com/r/kubernetes/comments/1ikrydu/securing_kubernetes_secrets_disaster_recovery/","date":1739034242,"author":"/u/mustybatz","guid":425,"unread":true,"content":"<div><p>I recently explored <strong>securing Kubernetes secrets and disaster recovery</strong> using  in a GitOps setup, and I thought this could be helpful for others working with Kubernetes (home labs or production).</p><ul><li>Encrypt and store secrets directly in Git with .</li><li>Automatically decrypt and deploy them using .</li><li>Disaster recovery using GitOps workflows + backup strategies with NAS and Velero.</li></ul><ul><li>Do you prefer  or ?</li><li>What’s your go-to strategy for persistent data backups?</li></ul><p>Let me know your thoughts or feedback! </p></div>   submitted by   <a href=\"https://www.reddit.com/user/mustybatz\"> /u/mustybatz </a>","contentLength":514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","k8s"]}