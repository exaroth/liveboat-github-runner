{"id":"2Qhhdda6Qnbf8RCfUPd4nB9sSt2WDQfEpF7H3gCnZZ4AsfbGMy3RmrCa6gigGY6TkbrrJn4wmHXXNYcVj1bK","title":"top scoring links : rust","displayTitle":"Reddit - Rust","url":"https://www.reddit.com/r/rust/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/rust/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Automatic Server Reloading in Rust on Change: What is listenfd/systemfd?","url":"https://lucumr.pocoo.org/2025/1/19/what-is-systemfd/","date":1737319322,"author":"/u/mitsuhiko","guid":452,"unread":true,"content":"<p>written on Sunday, January 19, 2025</p><p>When I developed <a href=\"https://werkzeug.palletsprojects.com/\">Werkzeug</a> (and\nlater <a href=\"https://flask.palletsprojects.com/\">Flask</a>), the most\nimportant part of the developer experience for me was enabling fast, automatic\nreloading.  Werkzeug (and with it Flask), this is achieved by using two\nprocsses at all times.  The parent process holds on to the file descriptor\nof the socket on which the server listens, and a subprocess picks up that\nfile descriptor.  That subprocess restarts when it detects changes.  This\nensures that no matter what happens, there is no window where the browser\nreports a connection error.  At worst, the browser will hang until the\nprocess finishes reloading, after which the page loads successfully.  In\ncase the inner process fails to come up during restarts, you get an error\nmessage.</p><p>A few years ago, I wanted to accomplish the same experience for working\nwith Rust code which is why I wrote <a href=\"https://github.com/mitsuhiko/systemfd\">systemfd</a> and <a href=\"https://github.com/mitsuhiko/listenfd\">listenfd</a>.  I however realized that I\nnever really wrote here about how they work and disappointingly I think\nthose crates, and a good auto-reloading experience in Rust are largely\nunknown.</p><div><p>Firstly one needs to monitor the file system for changes.  While in theory\nI could have done this myself, there was already a tool that could do\nthat.</p><p>At the time there was <a href=\"https://crates.io/crates/cargo-watch\">cargo watch</a>.  Today one might instead use it\ntogether with the more generic <a href=\"https://github.com/watchexec/watchexec\">watchexec</a>.  Either one monitor your\nworkspace for changes and then executes a command.  So you can for\ninstance tell it to restart your program.  One of these will work:</p><pre>watchexec -r -- cargo run\ncargo watch -x run\n</pre><p>You will need a tool like that to do the watching part.  At this point I\nrecommend the more generic  which you can find on <a href=\"https://github.com/watchexec/watchexec/blob/main/doc/packages.md\">homebrew and\nelsewhere</a>.</p></div><div><p>But what about the socket?  The solution to this problem I picked comes\nfrom <a href=\"https://en.wikipedia.org/wiki/Systemd\">systemd</a>.  Systemd has a\n“protocol” that standardizes passing file descriptors from one process to\nanother through environment variables.  In systemd parlance this is called\n“socket activation,” as it allows systemd to only launch a program if\nsomeone started making a request to the socket.  This concept was\noriginally introduced by Apple as part of launchd.</p><p>To make this work with Rust, I created two crates:</p><ul><li><a href=\"https://github.com/mitsuhiko/systemfd\">systemfd</a> is the command\nline tool that opens sockets and passes them on to other programs.</li><li><a href=\"https://crates.io/crates/listenfd\">listenfd</a> is a Rust crate that\naccepts file descriptors from systemd or .</li></ul><p>It's worth noting that systemfd is not exclusivly useful to Rust.  The\nsystemd protocol can be implemented in other languages as well, meaning\nthat if you have a socket server written in Go or Python, you can also use\nsystemfd.</p><p>So here is how you use it.</p><p>First you need to add  to your project:</p><p>Then, modify your server code to accept sockets via listenfd before\nfalling back to listening itself on ports provided through command-line\narguments or configuration files.  Here is an example using  in\naxum:</p><div><pre>::::::::-&gt; -&gt; ::::::::::::::::</pre></div><p>The key point here is to accept socket 0 from the environment as a TCP\nlistener and use it if available.  If the socket is not provided (e.g.\nwhen launched without systemd/), the code falls back to opening a\nfixed port.</p></div><div><p>Finally you can use  /  together with :</p><pre>systemfd --no-pid -s http::8888 -- watchexec -r -- cargo run\nsystemfd --no-pid -s http::8888 -- cargo watch -x run\n</pre><p>This is what the parameters mean:</p><ul><li> needs to be first it's the program that opens the sockets.</li><li> is a flag prevents the PID from being passed.  This is necessary\nfor  to accept the socket.  This is a departure of the socket\npassing protocol from systemd which otherwise does not allow ports to be\npassed through another program (like ).  In short: when the\nPID information is not passed, then listenfd will accept the socket\nregardless.  Otherwise it would only accept it from the direct parent\nprocess.</li><li> tells  to open one TCP socket on port 8888.\nUsing  instead of  is a small improvement that will cause\nsystemfd to print out a URL on startup.</li><li> makes  restart the process when something\nchanges in the current working directory.</li><li> is the program that watchexec will start and re-start onm\nchanges.  In Rust this will first compile the changes and then run the\napplication.  Because we put  in, it will try to first accept\nthe socket from .</li></ul><p>The end result is that you can edit your code, and it will recompile\nautomatically and restart the server without dropping any requests.  When\nyou run it, and perform changes, it will look a bit like this:</p><pre>$ systemfd --no-pid -s http::5555 -- watchexec -r -- cargo run\n~&gt; socket http://127.0.0.1:5555/ -&gt; fd #3\n[Running: cargo run]\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.02s\n     Running `target/debug/axum-test`\n[Running: cargo run]\n   Compiling axum-test v0.1.0 (/private/tmp/axum-test)\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.52s\n     Running `target/debug/axum-test`\n</pre><p>For easier access, I recommend putting this into a  or similar\nso you can just run  and it runs the server in watch mode.</p><p>To install  you can use curl to bash:</p><pre>curl -sSfL https://github.com/mitsuhiko/systemfd/releases/latest/download/systemfd-installer.sh | sh\n</pre></div><div><p>Now how does this work on Windows?  The answer is that  and\n have a custom, proprietary protocol that also makes socket\npassing work on Windows.  That's a more complex system which involves a\nlocal RPC server.  However the system does also support Windows and the\ndetails about how it works are largely irrelevant for you as a user\n—&nbsp;unless you want to implement that protocol for another programming\nlanguage.</p></div><div><p>I really enjoy using this combination, but it can be quite frustrating to\nrequire so many commands, and the command line workflow isn't optimal.\nIdeally, this functionality would be better integrated into specific Rust\nframeworks like axum and provided through a dedicated cargo plugin.  In a\nperfect world, one could simply run , and everything\nwould work seamlessly.</p><p>However, maintaining such an integrated experience is a much more involved\neffort than what I have.  Hopefully, someone will be inspired to further\nenhance the developer experience and achieve deeper integration with Rust\nframeworks, making it more accessible and convenient for everyone.</p></div>","contentLength":6129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i58gmt/automatic_server_reloading_in_rust_on_change_what/"},{"title":"Rust Ray Tracer","url":"https://www.reddit.com/r/rust/comments/1i589gw/rust_ray_tracer/","date":1737318822,"author":"/u/thebigjuicyddd","guid":448,"unread":true,"content":"<p>First post in the community! I've seen a couple of ray tracers in Rust so I thought I'd share mine: <a href=\"https://github.com/PatD123/rusty-raytrace\">https://github.com/PatD123/rusty-raytrace</a> I've really only implemented Diffuse and Metal cuz I feel like they were the coolest. It's not much but it's honest work.</p><p>Anyways, some of the resolutions are 400x225 and the others are 1000x562. Rendering the 1000x562 images takes a very long time, so I'm trying to find ways to increase rendering speed. A couple things I've looked at are async I/O (for writing to my PPM) and multithreading, though some say these'll just slow you down. Some say that generating random vectors can take a while (rand). What do you guys think?</p>","contentLength":669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Tantivy, a Rust-Based Search Library, with PostgreSQL Block Storage","url":"https://www.paradedb.com/blog/block_storage_part_one","date":1737311903,"author":"/u/philippemnoel","guid":451,"unread":true,"content":"<h2>A New Postgres Block Storage Layout for Full Text Search</h2><div><img loading=\"lazy\" width=\"512\" height=\"512\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fming_headshot.81b98356.png&amp;w=640&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fming_headshot.81b98356.png&amp;w=1080&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fming_headshot.81b98356.png&amp;w=1080&amp;q=75\"><p>By Ming Ying on January 16, 2025</p></div><img loading=\"lazy\" width=\"2400\" height=\"1260\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fopengraph-image.5f0dce05.png&amp;w=3840&amp;q=75 1x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fopengraph-image.5f0dce05.png&amp;w=3840&amp;q=75\"><p><a href=\"https://github.com/paradedb/paradedb/tree/dev/pg_search\"></a> is now fully on Postgres block storage.</p><p>Prior to ,  operated outside of Postgres’ block storage and buffer cache. This means that the extension created files which were not managed by Postgres and could read the contents of those files directly from disk.\nWhile it’s not uncommon for Postgres extensions to do this, block storage has enabled  to simultaneously achieve:</p><ol><li>Postgres write-ahead log (WAL) integration, which is necessary for physical replication of the index</li><li>Crash and point-in-time recovery</li><li>Full support for Postgres MVCC (multi-version concurrency control)</li><li>Integration with Postgres’ buffer cache, which has led to massive improvements in index creation times and write throughput</li></ol><img loading=\"lazy\" width=\"1488\" height=\"432\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_create_index.de1454d3.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_create_index.de1454d3.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_create_index.de1454d3.png&amp;w=3840&amp;q=75\"><img loading=\"lazy\" width=\"1960\" height=\"434\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_tps.598e54c0.png&amp;w=2048&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_tps.598e54c0.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_tps.598e54c0.png&amp;w=3840&amp;q=75\"><p>Moving to block storage has been one our biggest engineering bets. At first, we weren’t sure if reconciling the data access patterns and concurrency model of Postgres and <a href=\"https://github.com/quickwit-oss/tantivy\">Tantivy</a> — 's underlying search library — was possible without drastic changes to Tantivy.</p><p>This is part one of a three-part series. In this part, we’ll dive into how we architected 's new block storage layout and data access patterns. Part two will discuss how we designed and tested  to be MVCC-safe in update-heavy scenarios. Part three will dive into how we customized the block storage layout for analytical workloads (e.g. faceted search, aggregates).</p><p>Block storage is Postgres’ storage API that backs all of Postgres’ tables and built-in index types. The fundamental unit of block storage is a block: a chunk of 8192 bytes. When executing a query, Postgres reads blocks into buffers, which are stored in Postgres’ buffer cache.</p><p>DML (, , , ) statements do not modify the physical block. Instead, their changes are written to the underlying buffers, which are later flushed to disk when evicted from the buffer cache or during a checkpoint.</p><p>If Postgres crashes, modifications to buffers that have not been flushed can become lost. To guard against this, any changes to the index must be written to the write-ahead log (WAL). During crash recovery, Postgres replays the WAL to restore the database to its most recent state.</p><p>pg_search is a Postgres extension that implements a custom index for full text search and analytics. The extension is powered by Tantivy, a search library written in Rust and inspired by Lucene.</p><h3>Why Migrate To Block Storage?</h3><p>A custom Postgres index has two choices for persistence: use Postgres block storage or the filesystem. At first, using the filesystem may seem like the easier option. Integrating with block storage requires solving a series of problems:</p><ol><li>Some data structures may not fit within a single 8KB block. Splitting data across multiple blocks can create lock contention, garbage collection, and concurrency challenges.</li><li>Once a block has been allocated to a Postgres index, it cannot be physically deleted — only recycled. This means that the size of an index strictly increases until a  or  is run. The index must be careful to return blocks that have been tombstoned by deletes or vacuums to Postgres’ free space map for reuse.</li><li>In update-heavy scenarios, the index can become dominated by space that once belonged to dead (i.e. deleted) rows. This may increase the number of I/O operations required for searches and updates, which degrades performance. The index must find ways to reorganize and compact the index during vacuums.</li><li>Because Postgres is single-threaded, multiple threads cannot concurrently read from block storage. The index may need to leverage Postgres’ parallel workers.</li></ol><p>Once the index overcomes these hurdles, however, Postgres block storage does an incredible amount of heavy lifting. After a year of working with the filesystem, it became clear that block storage was the way forward.</p><ol><li>Being able to use the buffer cache means a huge reduction in disk I/O and massive improvements to read and write throughput.</li><li>Postgres provides a simple, battle-tested API to write buffers to the WAL. Without block storage, extensions must define custom WAL record types and implement their own WAL replay logic, which drastically increases complexity and surface area for bugs.</li><li>Postgres handles the physical creation and cleanup of files for us. The index doesn’t need to clean up after aborted transactions or  statements.</li></ol><h3>Tantivy’s File-Based Index Layout</h3><img loading=\"lazy\" width=\"1546\" height=\"1138\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftantivy_layout.e206f50d.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftantivy_layout.e206f50d.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftantivy_layout.e206f50d.png&amp;w=3840&amp;q=75\"><p>The first challenge was migrating <a href=\"https://www.paradedb.com/blog/(https://github.com/quickwit-oss/tantivy)\">Tantivy’s</a> file-based index layout to block storage. Let’s quickly examine how Tantivy’s index is structured.</p><p>A Tantivy index is comprised of multiple segments. A segment is like a database shard — it contains a subset of the documents in the index. Each segment, in turn, is made up of multiple files:</p><ol><li>: Stores a mapping of terms to document IDs and term frequencies, allowing Tantivy to efficiently retrieve documents that contain a specific term. This is the backbone of the inverted index.</li><li>: Tracks the positions of terms within documents, enabling phrase queries by identifying where terms appear in relation to each other.</li><li>: Contains the list of unique terms in the index and metadata for each term, such as document frequency and offsets into the postings file.</li><li>: Stores normalization factors for each field in a document, which are used to adjust term scores during ranking.</li><li>: Columnar storage for numeric and categorical fields, enabling fast filtering and sorting.</li><li>: A bitset that tracks which documents in the segment have been deleted.</li><li>: Stores the original document. This file is not used by  since the heap table already contains the original value.</li></ol><p>New segments are created whenever new documents are committed to the index. To maintain a target segment count, Tantivy’s merge process combines smaller segments into a single, larger segment.</p><p>When a segment is created, Tantivy assigns it a unique UUID. Segments are tracked across two files. The first file contains a  of all files in the index. The second file contains a list of segment UUIDs that are currently visible. If a segment is present in first file but not the second, that means that the segment has been tombstoned by the merge process and is subject to being removed by garbage collection.</p><p>In addition, the second file also stores index’s schema and settings.</p><p>Tantivy uses a file-based locking approach — if a lock file exists, then the lock is being held by another process. Locks are important for Tantivy because Tantivy is not a database capable of handling concurrent readers and writers. They ensure that only one writer exists per index, and that reads and writes to the metadata files are atomic. In Part 2, we’ll discuss how we used Postgres MVCC controls to lift Tantivy’s “one writer per index” limitation.</p><h2>Migrating to a Block Storage Layout</h2><img loading=\"lazy\" width=\"1506\" height=\"949\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_layout.e32a6151.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_layout.e32a6151.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_layout.e32a6151.png&amp;w=3840&amp;q=75\"><p>Rather than being written to a file, segments are serialized and written to blocks. Large segments that spill past a single block are stored in a linked list of blocks.</p><p>Separate blocks are used to store the index schema, settings, and list of segment UUIDs.</p><p>Postgres MVCC visibility information is stored alongside each segment UUID. At query time, the extension uses MVCC visibility rules to construct a snapshot of the list of all visible segments, which eliminates the need for a second list of visible segments.</p><p>Tantivy’s lock files are no longer needed since Postgres provides buffer-level, interprocess locking mechanisms.</p><h3>Challenge 1: Large Files Can Spill Over a Single Block</h3><img loading=\"lazy\" width=\"1632\" height=\"772\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_linked_list.1fcd2db4.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_linked_list.1fcd2db4.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_linked_list.1fcd2db4.png&amp;w=3840&amp;q=75\"><p>A segment file can exceed an 8KB block. To accommodate these files, we implement a linked list over block storage, where each block is a node.</p><p>The linked list starts with a header block that contains a bitpacked representation of all subsequent block numbers. This structure enables O(1) lookups by directly mapping the starting offset of any byte range to its position in the list.</p><p>After the header block, the next block stores the file’s serialized data. Once the block becomes full, a new block is allocated. In Postgres, every block has an area reserved for metadata called special data. The current block's special data section is updated to store the block number of the newly allocated block, forming the linked list.</p><h3>Challenge 2: Blocks Cannot Be Memory Mapped</h3><p>Tantivy’s data access patterns for fast fields assume that the underlying file can be memory mapped, which means that Tantivy can leverage zero-copy access for the entire fast field. This is not the case for block storage — the buffer cache can only provide a pointer to the contents of a single block. If a fast field spans multiple blocks, each block must be copied into memory, introducing significant overhead.</p><p>To address this problem, we <a href=\"https://github.com/paradedb/tantivy/commit/b42a45dd4aa29ce880864c95f2b6e69ad26cdc06\">modified Tantivy</a> to defer dereferencing large slices of bytes up front. Instead, bytes are dereferenced lazily and\ncached in memory to avoid re-reading blocks that have been previously accessed.</p><h3>Challenge 3: The Segment Count Explodes in Update-Heavy Scenarios</h3><p>Because segments are immutable, every DML statement in Tantivy creates at least one new segment. Having too many segments can degrade performance because there is a cost to opening a segment reader, searching over the segment, and merging the results with other segments. While the ideal number of segments depends on the dataset and the underlying hardware, having more than a hundred segments is generally not optimal.</p><p>If a table experiences a high volume of updates, the number of segments quickly explodes. To address this, we introduce a step called  , which looks for merge opportunities after an  completes.</p><p>It is critical that only one merge process runs concurrently. If two merge processes run at the same time, they could both see the same segments, merge them together, and create duplicate segments. To guard against this, every merge process atomically writes its transaction ID to a metadata block. Subsequent merge attempts first read this transaction ID, and are only allowed to proceed if the effects of that transaction ID are MVCC-visible.</p><p>The following parts of this blog post series will dive into some more exciting challenges we faced with block storage, with a focus on concurrency and analytical performance.</p><ol><li><a href=\"https://github.com/zombodb/zombodb\">ZomboDB</a> and <a href=\"https://github.com/CrunchyData/pg_parquet\">pg_parquet</a> are examples of Postgres extensions that directly interact with the filesystem.</li><li>Fortunately this ended up not being the case.</li><li>This will be covered in detail in Parts 2 and 3.</li></ol>","contentLength":10322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i55ggv/integrating_tantivy_a_rustbased_search_library/"},{"title":"Crates doing protocol programming well","url":"https://www.reddit.com/r/rust/comments/1i51chr/crates_doing_protocol_programming_well/","date":1737301584,"author":"/u/mjaakkola","guid":449,"unread":true,"content":"<p>I've been programming with Rust a couple years now but haven't created a pattern that I feel comfortable for protocol programming. Protocol programming includes decode/encode (can this be done with declarative manner), state-machines and abstracting the stack with links (traits vs. generics vs. ipc) so there are multiple aspects to think about. </p><p>I wanted to ping community about what crates do feel are having nice clean patters of doing these things. Especially, in the non-async world. Obviously, the pattern cleaness may be impacted zero copying and other performance requirements, but right now I'm looking clean good examples from the existing crates so no need for anyone to create examples. </p>","contentLength":699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are some lesser-known Rust books worth reading?","url":"https://www.reddit.com/r/rust/comments/1i4wv95/what_are_some_lesserknown_rust_books_worth_reading/","date":1737287666,"author":"/u/EightLines_03","guid":453,"unread":true,"content":"<p>What are your favorite hidden gems? Bonus points for saying why you think they're worth reading in addition to the standard works.</p>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"metaheuRUSTics v0.2.0 Released! (under MIT License this time)","url":"https://www.reddit.com/r/rust/comments/1i4u8hx/metaheurustics_v020_released_under_mit_license/","date":1737276266,"author":"/u/aryashah2k","guid":450,"unread":true,"content":"<p>After a good response and a month of reading papers and implementing them, metaheuRUSTics v0.2.0 is here, there are two new algorithms: an improvement/variant of Grey Wolf and the Firefly algorithm, along with two new test functions: greiwank and beal</p><p>The error handling has improved along with plots and I have added great benchmark scripts to evaluate all the algorithms present in this package</p><p>I would love it if this community can give inputs, contribute to the code and collaborate together in the spirit of open source.</p><p>There have been lots of papers that I want to implement as algorithms in this library and if any veteran or fellow rookie wishes to learn and contribute to this project at the same time, nothing else will make me happier!</p><p><a href=\"https://www.reddit.com/r/rust/comments/1hdetdq/metaheurustics/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">Prev. Reddit Post(v0.1.0)</a> p.s v0.1.0 was released under GPL license but I pledge to continue development and release code under the MIT License. If you end up using this in your research, do cite this package (while the paper is still being written)</p><pre><code>Shah, A. S. (2025). MetaheuRUSTics: A comprehensive collection of metaheuristic optimization algorithms implemented in Rust. https://github.com/aryashah2k/metaheuRUSTics </code></pre>","contentLength":1163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}]}