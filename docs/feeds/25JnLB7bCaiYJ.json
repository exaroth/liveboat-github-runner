{"id":"25JnLB7bCaiYJ","title":"Tech News","displayTitle":"Tech News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":80,"items":[{"title":"AWS Introduces DNS Failover Feature for Its Notoriously Unreliable US East Region","url":"https://it.slashdot.org/story/25/11/28/0118256/aws-introduces-dns-failover-feature-for-its-notoriously-unreliable-us-east-region?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764324060,"author":"msmash","guid":203,"unread":true,"content":"Amazon Web Services has rolled out a DNS resilience feature that allows customers to make domain name system changes within 60 minutes of a service disruption in its US East region, a direct response to the long history of outages at the cloud giant's most troubled infrastructure. \n\nAWS said customers in regulated industries like banking, fintech and SaaS had asked for additional capabilities to meet business continuity and compliance requirements, specifically the ability to provision standby resources or redirect traffic during unexpected regional disruptions. The 60-minute recovery time objective still leaves a substantial window for outages to cascade, and the timing of the announcement -- less than six weeks after an October 20th DynamoDB incident and a subsequent VM problem drew criticism -- underscores how persistent US East's reliability issues have been.","contentLength":875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seven Years Later, Airbus is Still Trying To Kick Its Microsoft Habit","url":"https://it.slashdot.org/story/25/11/28/019235/seven-years-later-airbus-is-still-trying-to-kick-its-microsoft-habit?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764315060,"author":"msmash","guid":202,"unread":true,"content":"Breaking free from Microsoft is harder than it looks. Airbus began migrating its 100,000-plus workforce from Office to Google Workspace more than seven years ago and it still hasn't completed the switch. The Register: As we exclusively revealed in March 2018, the aerospace giant told 130,000 employees it was ditching Microsoft's productivity tools for Google's cloud-based alternatives. Then-CEO Tom Enders predicted migration would finish in 18 months, a timeline that, in hindsight, was \"extremely ambitious,\" according to Catherine Jestin, Airbus's executive vice president of digital. \n\nToday, more than two-thirds of Airbus's 150,000 employees have fully transitioned, but significant pockets continue to use Microsoft in parallel. Finance, for example, still relies on Excel because Google Sheets can't handle the necessary file sizes, as some spreadsheets involve 20 million cells. \"Some of the limitations was just the number of cells that you could have in one single file. We'll definitely start to remove some of the work,\" Jestin told The Register.","contentLength":1062,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anduril’s autonomous weapons stumble in tests and combat, WSJ reports","url":"https://techcrunch.com/2025/11/27/andurils-autonomous-weapons-stumble-in-tests-and-combat-wsj-reports/","date":1764306467,"author":"Connie Loizos","guid":298,"unread":true,"content":"<article>Defense tech startup Anduril Industries has faced numerous setbacks during testing of its autonomous weapons systems, according to new reporting by the WSJ. The problems cited include more than a dozen drone boats that failed during a Navy exercise off California in May, with sailors warning of safety violations and potential loss of life; a […]</article>","contentLength":349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Canada Rolls Back Climate Rules To Boost Investments","url":"https://news.slashdot.org/story/25/11/28/0042205/canada-rolls-back-climate-rules-to-boost-investments?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764306060,"author":"msmash","guid":201,"unread":true,"content":"Canada's Prime Minister Mark Carney has signed an agreement with Alberta's premier that will roll back certain climate rules to spur investment in energy production, while encouraging construction of a new oil pipeline to the West Coast. From a report: Under the agreement, which was signed on Thursday, the federal government will scrap a planned emissions cap on the oil and gas sector and drop rules on clean electricity in exchange for a commitment by Canada's top oil-producing province to strengthen industrial carbon pricing and support a carbon capture-and-storage project. \n\nThe deal, which was hailed by the country's oil industry but panned by environmentalists, signaled a shift in Canada's energy policy in favour of fossil fuel development and is already creating tensions within Carney's minority government. Steven Guilbeault, who served as environment minister under Carney's predecessor Justin Trudeau, said he was quitting the cabinet over concerns that Canada's climate plan was being dismantled.","contentLength":1016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Black Friday Reminder For Those Hating Ads But Loving Linux Hardware/Software","url":"https://www.phoronix.com/news/Black-Friday-2025","date":1764306001,"author":"Michael Larabel","guid":686,"unread":true,"content":"<article>As the one and last friendly reminder, if you enjoy the daily and original content found on Phoronix.com but not liking ads and wanting to view multi-page articles on a single page, native dark mode, and more: the 2025 Black Friday / Cyber Week deal is ending Monday to help support the site while enjoying a discounted rate...</article>","contentLength":327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulkan 1.4.335 Released With The Very Notable VK_EXT_present_timing","url":"https://www.phoronix.com/news/Vulkan-1.4.335-Released","date":1764306000,"author":"Michael Larabel","guid":685,"unread":true,"content":"<article>Vulkan 1.4.335 released a few hours ago as the latest iteration of this high performance graphics and compute API. With being just a week since the prior update and given the US Thanksgiving week, it's on the lighter side in terms of issues addressed. There is one new extension though and it's a big one: VK_EXT_present_timing is finally merged...</article>","contentLength":348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Patent Office Issues New Guidelines For AI-Assisted Inventions","url":"https://yro.slashdot.org/story/25/11/28/0037205/us-patent-office-issues-new-guidelines-for-ai-assisted-inventions?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764297000,"author":"msmash","guid":200,"unread":true,"content":"The U.S. Patent and Trademark Office has issued new guidelines outlining when inventions created with the help of AI can be patented. From a report: USPTO Director John Squires said on Wednesday in a notice set to be published Friday, that the office considers generative AI systems to be \"analogous to laboratory equipment, computer software, research databases, or any other tool that assists in the inventive process.\" \n\n\"They may provide services and generate ideas, but they remain tools used by the human inventor who conceived the claimed invention,\" the office said. \"When one natural person is involved in creating an invention with the assistance of AI, the inquiry is whether that person conceived the invention under the traditional conception standard.\" \n\nThe office reiterated its guidance from last year that AI itself cannot be considered an inventor under U.S. patent law. However, it rejected the approach taken by the PTO during former President Joe Biden's administration for deciding when AI-assisted inventions are patentable, which relied on a standard normally used to determine when multiple people can qualify as joint inventors.","contentLength":1155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One Of Intel's Xe Open-Source Linux Graphics Driver Maintainers Is Departing","url":"https://www.phoronix.com/news/Intel-Xe-Maintainer-Leaving","date":1764293278,"author":"Michael Larabel","guid":684,"unread":true,"content":"<article>It's been two months since there were any notable Intel Linux engineering departures to note following various layoffs and voluntary departures this year that have unfortunately impacted their Linux/open-source talent. Sadly this US Thanksgiving is a new departure to note: one of Intel's maintainers for the Xe open-source Linux kernel graphics driver is leaving the company. This is for the modern Xe driver used by default since Lunar Lake and playing a pivotal role for Intel Linux graphics moving forward...</article>","contentLength":512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Epic's Sweeney Says Platforms Should Stop Tagging Games Made With AI","url":"https://games.slashdot.org/story/25/11/28/0033246/epics-sweeney-says-platforms-should-stop-tagging-games-made-with-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764292200,"author":"msmash","guid":199,"unread":true,"content":"The CEO of Epic Games, Tim Sweeney, has argued that platforms like Steam should not label games that are made using AI. From a report: Responding to a post on Twitter from a user who suggested that storefronts drop this tag, the industry exec said that it \"makes no sense\" to flag such content. Sweeney added that soon AI will be a part of the way all games are made. \"The AI tag is relevant to art exhibits for authorship disclosure, and to digital content licensing marketplaces where buyers need to understand the rights situation,\" Sweeney said. \"It makes no sense for game stores, where AI will be involved in nearly all future production.\"","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Social Media Giants Liable For Financial Scams Under New EU Law","url":"https://tech.slashdot.org/story/25/11/28/0028207/social-media-giants-liable-for-financial-scams-under-new-eu-law?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764289680,"author":"msmash","guid":198,"unread":true,"content":"Platforms including Meta and TikTok will be held liable for financial fraud for the first time under new rules agreed by EU lawmakers in the early hours of Thursday. From a report: The Parliament and Council agreed on the package of rules after eight hours of negotiations to strengthen safeguards against payment fraud. The deal adds another layer of EU regulatory risk for U.S. tech giants, which have lobbied the White House to confront Brussels' anti-monopoly and content moderation rules. \n\n[...] Social media has become rife with financial scams, and MEPs pushed hard to hold both Big Tech and banks liable during legislative negotiations. EU governments, meanwhile, believed banks should be held responsible if their safeguards aren't strong enough. As a compromise, lawmakers agreed that banks should reimburse victims if a scammer, impersonating the bank, swindles them out of their money, or if payments are processed without consent.","contentLength":944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ETH Could Drop Below $2,600 as Mutuum Finance (MUTM) Nears Phase 6 100% Allocation","url":"https://hackernoon.com/eth-could-drop-below-$2600-as-mutuum-finance-mutm-nears-phase-6-100percent-allocation?source=rss","date":1764278204,"author":"BTCWire","guid":408,"unread":true,"content":"<p>Ethereum is entering a difficult stretch as selling pressure rises and investor confidence cools. The price has slipped toward the lower end of its range, and many analysts now warn that ETH could be on track for another drop if support fails. </p><p>At the same time, a new DeFi crypto token priced at $0.035 is gaining strong attention, climbing 250% from early levels and approaching a full Phase 6 sellout. As Ethereum loses momentum, the shift in interest toward Mutuum Finance is becoming harder to ignore.</p><h3>Ethereum Faces Strong Resistance</h3><p>Ethereum trades near $2,795 with a market cap of about $329B. The chart has been weakening for several weeks. Heavy resistance is stacked between $3,300 and $3,450, with a second resistance zone close to $3,800. ETH has tested these levels multiple times, but each attempt ended with strong rejection. The pressure above remains intense.</p><p><img src=\"https://cdn.hackernoon.com/images/FS1PiuQb1sWxoW2ESuJpZswu0xk2-ay0373h.jpeg\" alt=\"\">Support near $2,700 is now critical. Many analysts say that if this level breaks, Ethereum could slide toward the $2,150 to $2,200 region. That scenario would represent a deeper correction that matches the trend seen in previous cycles. With BTC slowing and sentiment dipping across the market, this outcome has become more likely.</p><p>Ethereum’s size also limits its upside. With a market cap above $300B, large % gains require massive inflows. Many investors searching for the best crypto to buy now are starting to look away from large assets and toward smaller tokens that offer higher growth potential. Some price models predict that ETH may only see a 5% to 15% bounce in the near term, which is far from attractive for traders aiming for strong returns entering Q2 and Q3 2025.</p><p>This stagnant outlook has pushed attention toward early stage tokens that still have room to grow. Mutuum Finance is now one of the fastest rising projects in that category.</p><p> is developing a decentralized lending and borrowing protocol built around chain liquidity and collateral backed loans. The platform uses a Peer to Contract model where users deposit assets into a shared pool and receive mtTokens in return. These mtTokens increase in value as borrowers pay interest, giving depositors predictable APY without complex tracking.</p><p>Borrowers access funds using clear Loan to Value rules. If the collateral becomes unsafe, the system triggers liquidations. Liquidators repay part of the debt and receive collateral at a discount. This keeps the protocol healthy and protects lenders during volatile conditions.</p><p>Mutuum Finance also confirmed an important milestone through its  account. The team stated that the V1 protocol is scheduled for launch on the Sepolia Testnet in Q4 2025. The post noted that Halborn Security is reviewing the lending and borrowing contracts and that the code is finalized. </p><p>V1 will include the liquidity pool, mtTokens, the debt token and the liquidation bot. ETH and USDT will be the first supported assets. This early delivery provides strong confidence for investors looking for what crypto to invest in with real development behind it.</p><h3>Presale Growth Shows Rapid Acceleration</h3><p>The MUTM presale has been one of the most active in 2025. The token entered the market at $0.01 and rose to $0.035 as each phase sold out. That is a 250% climb driven by constant demand. </p><p>The project has raised about $18.85M, brought in more than 18,100 holders and sold around 805M tokens. Out of the total 4B supply, about 45.5% is allocated to the presale.</p><p>Phase 6 is now close to 100% allocation. Only a very small amount of tokens remain at $0.035, and traders expect them to sell out soon. This tight supply has created urgent activity as investors rush to secure the final low entry point. Many traders searching for the best crypto to invest in or cheap crypto with strong upside are moving fast before the price increases again.</p><p>The 24 hour leaderboard adds fuel to the rush. The top contributor each day receives $500 worth of MUTM. Large buyers often compete for this reward, which raises daily volume and accelerates the presale pace. Card payments are also active, making the process easier for new participants.</p><p>Ethereum continues to face heavy resistance and weakening momentum. Analysts warn that ETH may drop below $2,600 if support breaks. With limited upside in the near term, many top crypto investors are shifting their attention toward early stage tokens that offer more room for growth.</p><p>Mutuum Finance has emerged as one of the strongest new options. With a nearly complete Phase 6 sellout, a 250% presale climb, a working V1 on the way and a full security review in progress, the project is gaining serious traction. Only a small amount of allocation remains at $0.035. When it sells out, the next price increase takes effect. For investors looking for the best crypto to buy before the next cycle, the final stage of the Mutuum Finance presale may be the last chance to secure the lowest entry point before 2026.</p><p>For more information about Mutuum Finance (MUTM) visit the links below:</p><strong><p>:::tip\n<em>This story was published as a press release by Btcwire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p></strong>","contentLength":5123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quttera Launches \"Evidence-as-Code\" API to Automate Security Compliance For SOC 2 and PCI DSS v4.0","url":"https://hackernoon.com/quttera-launches-evidence-as-code-api-to-automate-security-compliance-for-soc-2-and-pci-dss-v40?source=rss","date":1764277384,"author":"CyberNewswire","guid":407,"unread":true,"content":"<p>Tel Aviv, Israel, November 27th, 2025, CyberNewsWire/--New API capabilities and AI-powered Threat Encyclopedia eliminate manual audit preparation, providing real-time compliance evidence and instant threat intelligence. </p><p>\\\nQuttera today announced major enhancements to its Web Malware Scanner API that transform static security scanning into automated compliance evidence. The update introduces real-time evidence streaming and compliance mapping, directly addressing the manual burden of audit preparation that costs organizations 30-40 hours per audit cycle.</p><h3>Automating the Manual Evidence Chase</h3><p>Organizations preparing for SOC 2, ISO 27001, and PCI DSS v4.0 audits traditionally spend dozens of hours manually collecting security evidence—exporting reports, capturing screenshots, and mapping findings to compliance controls. This approach creates outdated evidence, doesn't scale across frameworks, and fails to prove continuous monitoring.</p><blockquote><p>\"Security teams are exhausted by the manual 'evidence chase' required before every audit,\" said Michael Novofastovsky, CTO of Quttera. \"We're transforming malware detection into 'Evidence-as-Code'—structured, real-time security data that flows automatically into compliance workflows. Whether organizations use Drata, Vanta, or custom GRC systems, our API provides continuous proof without human intervention.\"</p></blockquote><p>Quttera's API converts threat detection into structured JSON with embedded compliance metadata, mapping findings to controls across SOC 2 (CC6.1, CC7.2), PCI DSS v4.0 (Requirements 6.4.3, 11.6.1), ISO 27001, and GDPR simultaneously.</p><h3>Addressing PCI DSS v4.0's New Requirements</h3><p>The update specifically targets PCI DSS v4.0 requirements mandatory since March 2025, particularly Requirements 6.4.3 (script authorization on payment pages) and 11.6.1 (file integrity monitoring). These requirements demand continuous automated detection—capabilities manual processes cannot provide at scale.</p><blockquote><p>\"PCI DSS v4.0 requires real-time detection of unauthorized changes to payment scripts,\" Novofastovsky explained. \"Our API provides timestamped evidence that monitoring is active 24/7, changes are detected automatically, and controls are continuously validated.\"</p></blockquote><h3>AI-Powered Threat Intelligence</h3><p>The Threat Encyclopedia addresses the context gap security teams face when responding to detections. Integrated directly into scan reports, it provides:</p><ul><li>Technical breakdown of malware behavior</li><li>Business impact and risk classification</li><li>Step-by-step remediation guidance</li><li>Connections to known attack campaigns</li></ul><blockquote><p>\"We're automating both sides of the problem,\" said Novofastovsky. \"The API handles compliance proof. The Threat Encyclopedia handles operational response. Together, they eliminate manual evidence collection and research overhead.\"</p></blockquote><p>The Encyclopedia currently documents 80+ web malware categories, with AI-assisted expansion based on emerging threats.</p><ul><li>Automated Control Mapping: Detections tagged for multiple compliance frameworks simultaneously</li><li>Real-Time Evidence Streaming: Continuous JSON feeds replace static PDF reports</li><li>Behavioral Detection: Heuristic scanning identifies zero-day and polymorphic threats</li><li>Integration Flexibility: Works with existing GRC platforms via standard REST API</li></ul><p>Enhanced capabilities are available immediately to all Quttera API subscribers.</p><p>Quttera provides automated website security and malware detection solutions, delivering compliance-ready evidence for organizations across financial services, healthcare, e-commerce, and technology sectors. </p><p>Its comprehensive suite includes advanced heuristic scanning, blacklist monitoring, and remediation services, helping businesses worldwide protect their digital assets and reputation.</p><p>:::tip\n<em>This story was published as a press release by CyberNewswire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":3852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Australia Spent $62 Million To Update Its Weather Web Site and Made It Worse","url":"https://slashdot.org/story/25/11/27/2013252/australia-spent-62-million-to-update-its-weather-web-site-and-made-it-worse?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764277200,"author":"msmash","guid":197,"unread":true,"content":"quonset writes: Australia last updated their weather site a decade ago. In October, during one of the hottest days of the year, the Bureau of Meteorology (BOM) revealed its new web site and was immediately castigated for doing so. Complaints ranged from a confusing layout to not being able to find information. Farmers were particularly incensed when they found out they could no longer input GPS coordinates to find forecasts for a specific location. When it was revealed the cost of this update was A$96.5 million ($62.3 million), 20 times the original cost estimate, the temperature got even hotter. \n\nWith more than 2.6 billion views a year, Bom tried to explain that the site's refresh -- prompted by a major cybersecurity breach in 2015 -- was aimed at improving stability, security and accessibility. It did little to satisfy the public. Some frustrated users turned to humour: \"As much as I love a good game of hide and seek, can you tell us where you're hiding synoptic charts or drop some clues?\" \n\nMalcolm Taylor, an agronomist in Victoria, told the Australian Broadcasting Corporation (ABC) that the redesign was a complete disaster. \"I'm the person who needs it and it's not giving me the information I need,\" the plant and soil scientist said. As psychologist and neuroscientist Joel Pearson put it, \"First you violate expectations by making something worse, then you compound the injury by revealing the violation was both expensive and avoidable. It's the government IT project equivalent of ordering a renovation, discovering the contractor has made your house less functional, and then learning they charged you for a mansion.\"","contentLength":1646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bitrise Mobile Report: New Benchmarks & 28% Faster Build Times","url":"https://hackernoon.com/bitrise-mobile-report-new-benchmarks-and-28percent-faster-build-times?source=rss","date":1764276614,"author":"Bitrise","guid":406,"unread":true,"content":"<article>The Bitrise Mobile Insights 2025 report (analyzing 10M+ builds) reveals that while mobile CI pipelines are 23% more complex, leading teams cut build times by 28% through automation like build caching. React Native dominates cross-platform builds (83%). Top performers ship biweekly or faster and adopt new Xcode releases rapidly. The report highlights the rising cost of flaky tests and confirms that strategic CI/CD investment offers a major competitive advantage.</article>","contentLength":465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Decentralized Finance and Gaming are Reshaping Digital Economies","url":"https://hackernoon.com/how-decentralized-finance-and-gaming-are-reshaping-digital-economies?source=rss","date":1764275918,"author":"Bernard","guid":405,"unread":true,"content":"<p>Decentralized Finance (DeFi)and gaming continue to transform the structure of digital economies as blockchain networks merge programmable finance with user-driven virtual worlds. The shift strengthens transparency, ownership, and verifiable logic across connected ecosystems already forming around new Web3 infrastructure.</p><h2>Convergence of Decentralized Finance and Game-Based Economies</h2><p>The concept of decentralized DeFi and gaming reflects a growing movement where financial protocols and game environments function inside shared frameworks. GameFi models allow players to earn, trade, or stake assets rather than limit participation to entertainment alone. These structures create digital economies with transferable, user-owned resources.</p><p>Tokenized items introduce scarcity and verifiable ownership, allowing characters, skins, or in-game tools to exist as NFTs managed on-chain. Players maintain custody of their items rather than depend on central servers. This change supports a stronger foundation for long-term asset value across multiple platforms.</p><p>The connection also brings new roles for economic tools. Users can expand the utility of their assets across connected platforms thanks to the staking, lending, and liquidity functions. As assets can be transferred across different blockchain ecosystems, the value flow is enhanced by further improving cross-game interoperability.</p><h2>Growing Utility Through Transparent and Portable In-Game Assets</h2><p>The structure of GameFi relies on the tokenized items as the building blocks of open economies. Players earn rewards and can funnel these rewards into different DeFi channels for increased financial utility. This creates a rewarding loop as users are incentivized to engage in the activity of the game while also accessing decentralized markets.</p><p>Liquidity is enhanced as users are able to utilize their NFTs or tokens as collateral without having to sell their assets Borrowing systems allow them to access additional resources or expand their presence in game environments while maintaining ownership. The outcome helps both developers and users as the market expands.</p><p>Cross-chain environments also are important. Digital assets are enabled moving beyond the original game and into the associated game, resulting in valuable assets in different experiences. This strengthens the ecosystem to stay in motion and supports developers to create assets with a longer perceived cycle of value.</p><ol><li>On-Chain Strategy Game Built on the Nexus zkVM</li></ol><p>A Web3 studio building a large-scale strategy title adopted the Nexus zkVM to ensure fair and transparent gameplay. The team needed verifiable randomness for battles, item drops, and resource processes while preserving scalability. Traditional structures were not able to support these requirements at the desired speed.</p><p>The zkVM allowed complete x game logic to run efficiently. Each action produced a zero-knowledge proof confirming the integrity of results. Players could check the fairness of outcomes without exposing sensitive data or risking exploits inside the environment.</p><p>The ecosystem was further extended with the in-game assets being tokenized and linked to the Nexus DEX. The economy of the game was connected to DeFi by liquid pools of rare items, allowing the staking and borrowing of items. The activity level rose tremendously when the users got involved in the gameplay and financial mediums within a single environment.</p><ol start=\"2\"><li><strong>Autonomous On-Chain Agents for DeFi Portfolio Management</strong></li></ol><p>A DeFi fund explored automated methods for managing portfolios but encountered issues linked to centralized bots and opaque logic. The team required transparent execution with verifiable processes to maintain trust among participants. Nexus provided a structure that could meet those needs.</p><p>Fund used the agency model to distribute completely autonomous agents fully on-chain. These agents managed portfolio rebalancing, risk evaluation, trade execution, and the Nexus DEX. Each of these actions led to a proof of action occurring and that the action was performed properly and in accordance with request parameters. The model not only led to lower operational implementation costs by eliminating centralized infrastructure but also drastically increased performance through continuous execution.</p><p>The model allowed investors access to verifiable logs to evaluate progress, as opposed to opaque summaries (the investors had to trust us that all was done properly). The model improved mutual trust and convicted a more transparent operating model that maximized acceptable risks in a sustainable way as investors adopted long-term models.</p><ol start=\"3\"><li><strong>Kamirai's Dual-Utility Architecture for Web3 and Console Markets.</strong></li></ol><p>Kamirai pursued a major initiative to build an institutional-quality DEX with a full console gaming release. The Kamirex platform addresses liquidity fragmentation in major Asian markets and operates as a high-throughput exchange built on a proprietary structure. Its token powers governance, trading rebates, premium data access, and staking features.</p><p>The second component of the ecosystem focuses on a AAA Action-RPG that is a confirmed release that will take place on the major console systems. The main currency of the economy of the game is the Kamirai token. It allows purchasing items, in-game exchange, authenticating the digital assets, and voting on development decisions.</p><p>The architecture uses controlled supply reduction tied to both Kamirex activity and gaming purchases. Transaction fees and high-value game actions feed the burn model. This framework seeks to build steady demand through financial usage and mainstream entertainment channels rather than market speculation.</p><h2>Research Advancements Supporting Emerging Digital Economies</h2><p>The recent research analyses how intelligent agents are used within GameFi spaces. These agents can manage resources, respond to market changes, or assist users through automated decisions. Their actions are still transparent because each one relates to an on-chain process.</p><p>Another proposed model, ServerFi, provides a different dimension of user rewards in gaming economies. Instead of only earning value through play tasks, players earn value when they take part in the wider ecosystem processes, with the goal being a more balanced reward cycle.</p><p>Also, researchers are developing lending models that allow players to borrow tokens to access certain modes or levels in the game, with payback occurring through future value, thus enabling even greater participation. All models capture both accessibility and a finance normalization model in a consistent and verifiable manner.</p><h2>Cross-Chain Marketplaces and Expanding Interoperability</h2><p>The third sector connected  Decentralized DeFi and Gaming involves large-scale asset marketplaces. Developers across different chains face challenges linking their items into unified platforms. Nexus developed a solution by building a marketplace that was able to accept assets from different networks.</p><p>Over 120 games integrated their items into this unified structure, with users allowed to stake, borrow, or trade items in and across different blockchains through shared liquidity pools. Authenticity would be verified and fraud minimized through zero-knowledge proofs.</p><p>The presence of one shared market also increased overall trading volume and gave players and developers more consistent value for their assets. This environment supports broader participation and encourages creators to design items with longer market presence.</p><h2>Where the DeFi–Gaming Relationship Is Moving Next</h2><p>The way ahead of Decentralized DeFi and Gaming is broader integration of zk-based elements into real-time settings. Games are expected to integrate provable fairness systems as part of standard design rather than optional tools.</p><p>This might also extend to autonomous agents in gaming environments, where agents support players or maintain parts of the in-game economy without any sort of centralized oversight. Such agents could process tasks continuously, thus creating a predictable operational layer across connected systems.</p><p>Meanwhile, the cross-chain frameworks continue to be relevant as developers build networks to support liquidity and portable assets. This encourages cooperation across platforms and helps users manage holdings with more flexibility within decentralized settings.</p>","contentLength":8315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technance Introduces Institutional-Grade Infrastructure for Exchanges, Fintech Platforms & Web3 Apps","url":"https://hackernoon.com/technance-introduces-institutional-grade-infrastructure-for-exchanges-fintech-platforms-and-web3-apps?source=rss","date":1764275689,"author":"Chainwire","guid":404,"unread":true,"content":"<p>Dubai, United Arab Emirates, November 27th, 2025/Chainwire/--, a global provider of digital asset and trading infrastructure, has announced the launch of its expanded enterprise technology stack designed for crypto exchanges, neobanks, brokerages, and Web3-native platforms. </p><p>The company aims to bridge the gap between traditional finance and digital asset markets by offering a unified suite of high-performance trading and liquidity solutions.</p><p>Technance’s modular infrastructure enables businesses to deploy and scale digital asset products without the need to build complex in-house systems. The platform includes:</p><p>• Liquidity Providing &amp; Multi-Source Aggregation</p><p>Access to deep liquidity pools through intelligent routing and aggregation across external and internal sources.</p><p>A high-speed, low-latency execution engine engineered for derivatives markets, built with advanced risk and margin controls.</p><p>Optimized for handling large order volumes with stability, precision, and institutional reliability.</p><p>• Web3-Ready Architecture</p><p>Native integration with blockchain networks, wallets, and digital asset rails, enabling seamless support for Web3 products.</p><p>With its infrastructure-as-a-service model, Technance allows financial institutions and digital asset platforms to launch trading systems, upgrade their liquidity stack, and expand into new asset classes without operational overhead.</p><blockquote><p>“Fintech companies, exchanges, and Web3 projects are rapidly shifting toward modular infrastructure,” said Mohammad Haghshenas, Founder and CEO of Technance . “Our mission is to empower them with enterprise-grade technology that accelerates product development while maintaining the highest levels of performance and security.”</p></blockquote><p>Technance currently powers next-generation platforms across global markets, supporting spot and derivatives trading, liquidity routing, and digital asset integrations. The company continues to expand its international footprint as demand for reliable fintech and Web3 infrastructure grows worldwide.</p><p> is a global fintech infrastructure provider specializing in high-performance trading systems, liquidity solutions, and Web3-ready financial technology.</p><p>The company delivers modular enterprise components—including futures and spot trading engines, liquidity aggregation, and digital asset integration—that enable exchanges, fintech companies, and Web3 platforms to launch and scale digital-asset products with institutional reliability. </p><p>Technance powers next-generation financial applications across global markets.</p><p>:::tip\n<em>This story was published as a press release by Btcwire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":2700,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Custom Logic in Webflow: How to Push No-Code to Its Limits","url":"https://hackernoon.com/custom-logic-in-webflow-how-to-push-no-code-to-its-limits?source=rss","date":1764275545,"author":"Che IT Group","guid":403,"unread":true,"content":"<p>For many growing businesses, standard software once seemed like the fastest and most affordable path to digital transformation. But as operations expand, processes become more complex, and data volumes increase, these universal tools often reach their limits.</p><p>For years, Webflow has been regarded as the “designer’s playground” - ideal for creating quick landing pages, but often considered too limited for complex, logic-driven systems. That assumption couldn’t be further from the truth.</p><p>As digital products evolve, companies need sites that are fast, dynamic, and easy to maintain - without sacrificing flexibility. The no-code movement gave teams freedom, but for developers who understand how to extend it, Webflow becomes something much more powerful: a low-code framework for custom logic, automation, and scalable front-end systems.</p><p>At , we’ve spent years exploring that frontier - building projects that blur the line between visual editing and true engineering.</p><h2>The Myth of “No-Code = No Control”</h2><p>Webflow’s drag-and-drop interface makes it easy to start, but that same simplicity often leads people to underestimate its depth.</p><p>The truth is, no-code doesn’t mean no logic. Behind the visual layer, Webflow supports conditions, CMS-driven visibility, and dynamic content rendering. A skilled team can orchestrate these components, such as a front-end architecture, controlling visibility, relationships, and even logic flow - all within the platform’s visual editor.</p><p>The difference between a casual user and an engineering-minded one isn’t tools - it’s mindset. A marketer drags and drops; a developer designs systems that scale.</p><h2>Webflow Logic Layer - What’s Possible Without Code</h2><p>Webflow includes a surprisingly rich native logic system.</p><p>You can conditionally display content based on CMS fields, switch layouts depending on categories, or time content visibility automatically - all without writing a line of JavaScript.</p><ul><li>Show or hide entire sections depending on a “featured” flag in your CMS;</li><li>Automatically reveal or archive content based on a scheduled date;</li><li>Bind interactions to CMS data to affect animations or visibility dynamically.</li></ul><p>This layer alone can handle up to 80% of what many teams assume requires custom scripting, proving that, in the right hands, “no-code” can rival traditional development for flexibility.</p><h2>Dynamic Behavior Without JavaScript</h2><p>Even without coding, Webflow’s data attributes, visibility conditions, and native interactions allow you to simulate reactive behavior.</p><p>Designers can build multi-step forms that reveal sections based on user input, or create dashboards that filter content using CMS connections. When used strategically, this combination produces a “pseudo-reactive” UI - the experience of dynamic interactivity, powered entirely by native tools.</p><p>The beauty is that everything remains manageable through Webflow’s visual interface. Teams can iterate fast without developer bottlenecks, yet the site feels engineered, not templated.</p><h2>Integrating Custom Code - The Low-Code Sweet Spot</h2><p>There comes a point where you need to go further - when user personalization, dynamic filtering, or live API calls become essential. This is where Webflow transitions from no-code to low-code.</p><p>Through embedded scripts or before  injections, developers can extend functionality while keeping the structure intact. It’s the sweet spot: enough code to enable real logic, not so much that it breaks maintainability.</p><p>Typical low-code enhancements include:</p><ul><li>Dynamic filters for CMS collections;</li><li>Conditional redirects based on user roles;</li><li>Fetch requests to external APIs for real-time content updates.</li></ul><p>By using lightweight, modular code instead of heavy libraries, Webflow projects can achieve the complexity of a full web app - without losing speed or SEO health.</p><h2>When to Go Beyond Webflow</h2><p>Every platform has limits, and Webflow is no exception.</p><p>When projects involve large-scale content, complex user accounts, or backend logic, extending Webflow with headless backends or APIs becomes the natural next step.</p><p>Some of the best hybrid setups we’ve built include:</p><ul><li> for structured data synchronization;</li><li> for user authentication and custom dashboards;</li><li> or Make for workflow automation.</li></ul><p>In these models, Webflow serves as the presentation layer - fast, beautiful, and intuitive - while external services handle the logic, data, and scaling behind the scenes.</p><p>It’s how startups turn a simple Webflow prototype into a long-term digital ecosystem.</p><h2>Performance, SEO &amp; Maintainability</h2><p>Custom logic must remain lightweight. Too many embedded scripts can block rendering, negatively impact SEO, or cause layout flickering.</p><p>After building dozens of logic-driven Webflow systems, we’ve learned a few golden rules:</p><ul><li>Keep scripts page-specific, not global;</li><li>Test every custom behavior in Lighthouse and Webflow Audit;</li><li>Prefer CSS transitions over JavaScript animations for visual stability;</li><li>Use async or deferred loading to avoid render blocking.</li></ul><p>A fast site doesn’t just rank higher - it also converts better. Simplicity in logic equals stability in growth.</p><h2>Real Projects from Che IT Group</h2><h3>1. 1-800-D2C - Scaling No-Code with Logic</h3><p>We helped 1-800-D2C build a platform that cataloged over 500 D2C tools using Webflow, Memberstack, and Airtable.</p><p>The result: gated premium content, seamless user management, and automated data updates - all launched in just six weeks.</p><p>This project demonstrated that with thoughtful architecture, no-code solutions can scale effectively like enterprise software.</p><h3>2. Ganz Security - Custom Logic for Smarter UX</h3><p>For Ganz Security, we rebuilt their digital presence on Webflow with advanced product filters, multilingual support, and AWS integrations.</p><p>The new structure increased user engagement by 125% and reduced quote request time by 70%, demonstrating how well-planned logic can transform a corporate website into an interactive, high-performance system.</p><p>Both examples demonstrate a single truth: Webflow’s limits are not technical - they’re creative.</p><p>The no-code movement was never about eliminating developers; it was about removing friction. Webflow has evolved into a genuine front-end environment where creativity and engineering intersect.</p><p>Used right, it’s not a design toy - it’s a rapid development framework that gives teams the power to move faster without losing precision. You don’t need to choose between flexibility and simplicity - with Webflow, you can have both.</p><p>Because “no-code” doesn’t mean “no control.” It means building smarter.</p><p>Che IT Group helps companies build scalable Webflow ecosystems that strike a balance between creativity and logic. Explore our  to see how we combine design freedom with engineering precision.</p>","contentLength":6704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Models Keep Breaking in Production; Strong Documentation Can Fix It","url":"https://hackernoon.com/ai-models-keep-breaking-in-production-strong-documentation-can-fix-it?source=rss","date":1764275028,"author":"Sapan Pandya","guid":402,"unread":true,"content":"<p>AI systems often fail for reasons that seem unpredictable. A model works during development but behaves differently in production. An upstream field changes format without notice. A feature arrives late. A threshold shifts because old assumptions stayed in place for too long. When problems like these appear, teams search through logs and dashboards without a clear guide. One cause can account for many issues. The model was deployed without proper documentation.</p><p>Strong documentation gives teams a shared reference that explains how the model should act. It supports audits. It reduces the time needed to diagnose issues. It improves handoffs between teams. It raises long-term confidence in the system. The following sections describe a practical method for documenting model behavior in a way that supports reliability and trust.</p><p>\\\nEvery model needs a clear statement of intention. This section defines the decision the model supports. It identifies the outcome the model produces and the action that follows the prediction.</p><p>A model intention statement uses direct language. It avoids broad claims and vague descriptions. If the model classifies events, the statement explains what the classification means. If the model generates a score, the statement explains how it is used downstream. This clarity prevents incorrect assumptions about the model’s role.</p><p>The section lists the inputs the model expects. Each input is described with its field name, format, and purpose. The outputs are described in the same way. Response time expectations are recorded here. Some models run in low-latency environments. Others run in scheduled jobs. Recording these constraints helps teams understand where the model fits in the workflow.</p><p>Environmental details also belong here. A model may run on cloud infrastructure, on a local server, or on a constrained device. Each environment shapes how the model behaves under load. Recording these details prevents deployment in situations the model cannot support.</p><p>Input behavior is the source of many production failures. A changed field can cause silent drift. A delayed pipeline can shift predictions. A value outside the normal range can trigger unexpected actions. Documenting input behavior reduces the impact of these events.</p><p>This section begins with a list of all input fields. Each field description includes acceptable ranges, valid formats, and any transformation applied before prediction. Recording the origin of each field is essential. Many fields depend on upstream systems that evolve. Knowing where each field comes from helps teams identify causes of unexpected changes.</p><p>A short example makes the concept clear. A model may receive a field named device_load. The valid range may be zero to one hundred. Values above this limit should trigger a fallback path. Recording this detail helps teams catch corrupted or noisy input before it reaches production traffic.</p><p>This section documents common data risks. These risks include delayed updates, missing values, placeholder entries, and inconsistent sample rates. Recording these risks presents teams a realistic view of the data. It also helps reviewers understand where drift or instability may begin.</p><p>An example dataset strengthens this part of the document. The dataset should reflect real patterns. It should include typical ranges, common outliers, and realistic distributions. The content should be non-sensitive and simplified. The goal is to support quick tests, validation checks, and local experimentation. This dataset becomes a reusable resource that helps prevent repeated data errors.</p><p>Teams rely on predictable decision behavior to maintain stable systems. When decision behavior is unclear, the model appears unpredictable. Such behavior slows audits. It adds uncertainty during incidents. It increases the time needed to review features and resolve issues.</p><p>This section describes how the model reaches an output. This section documents thresholds, numeric cutoffs, category rules, and decision points. If the model uses rules before or after prediction, they are included. The goal is to show the full path from input to final action.</p><p>Examples bring clarity to this part of the document. Realistic examples demonstrate how specific inputs produce specific outputs. These examples show normal cases, boundary cases, and atypical inputs. They help teams understand the decision process without searching through codes.</p><p>The section also explains how the model handles invalid or unexpected input. Many incidents begin with a single bad record. When fallback rules are documented, teams can respond quickly. This reduces guesswork and protects the system during irregular events.</p><p>If the system provides confidence scores, ranking levels, or reason codes, these elements are defined here. Clear definitions help readers interpret results correctly. They also support consistent decisions across teams.</p><p>Operational control protects the system after deployment. Many teams focus on training and testing but overlook the conditions that affect long-term performance. Strong operational documentation prevents drift, reduces downtime, and improves system resilience.</p><p>This section starts with performance limits. These limits include throughput, latency under load, retry rules, and timeout behavior. Recording these details helps teams plan scaling strategies and load tests.</p><p>Monitoring checks follow. These checks track data quality, distribution changes, input drift, output stability, and model health. Each check is described with the source of the metric, the alert rule, and the actions teams should take when an alert triggers. Clear monitoring reduces confusion during incidents and keeps responses consistent.</p><p>Rollback steps belong in this section. Rollbacks often restore stability faster than incremental fixes. Documenting the process prevents mistakes during high-pressure moments. The description includes the version used for fallback, the systems affected by the rollback, the steps needed to complete it, and the conditions required before starting.</p><p>Ownership is the final part of operational control. This section lists the teams responsible for updates, monitoring, reviews, and incident response. Clear ownership prevents gaps in responsibility. A review schedule keeps the documentation current.</p><p>A fraud detection model evaluated a large volume of transactions. The model used several features provided by upstream sources. One field tracked user movement across regions. The documentation noted the source of this field, its expected range, and the known delays during heavy traffic.</p><p>A rise in false declines appeared in one region. The behavior looked random until the team reviewed the documentation. The input behavior section pointed to the movement field as a high-risk input during peak load. The team reviewed upstream logs and found a delay large enough to move the value outside its normal range. The model assigned higher risk scores because of the shift. The rollback process restored normal behavior quickly. The documentation reduced investigation time and protected the rest of the workflow.</p><h2>Integration Into Team Workflows</h2><p>\\\nThis method can be added to any software development process. Teams can begin by creating a template with the sections described above. Filling the template requires accurate information. Once created, the document becomes part of the model release process.</p><p>After deployment, the document supports audits, updates, incident reviews, and training for new engineers. It reduces maintenance cost because changes are easier to evaluate. When teams work with a shared reference, they spend less time debating assumptions and more time improving quality.</p><p>AI systems become more predictable when teams understand how the model behaves in real conditions. Clear documentation makes that possible. It supports audits. It reduces incident impact. It improves communication across teams. A simple template is enough to begin. Use this structure on one active model in your environment and refine the process with each new release.</p>","contentLength":8064,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stablecoin Regulation as a Threat to Stablecoins Themselves","url":"https://hackernoon.com/stablecoin-regulation-as-a-threat-to-stablecoins-themselves?source=rss","date":1764274658,"author":"Christopher Louis Tsu","guid":401,"unread":true,"content":"<h2>The Paradox of Legalization: How Global Regulation May Empower Banks and Undermine USDT and USDC.</h2><p>Right now, something unusual is happening in the stablecoin market. Governments around the world are finally starting to regulate digital dollars, which is what the industry has wanted. Europe’s MiCA  took effect in December 2024. Hong Kong put its Stablecoins Ordinance in place in . The UK and Singapore are set to establish their own rules. Even the US passed the GENIUS Act this year.</p><p>However, here's the thing: the moment regulation arrived, the ground started shifting under the feet of private issuers.</p><p>Take Europe. Since March 2025, exchanges across the European Economic Area have been systematically delisting USDT. Coinbase  it in December 2024. Crypto.com followed in January. Binance pulled all USDT spot trading pairs by the end of March. The reason? Tether doesn't hold an EU e-money license and hasn't applied for MiCA authorization. Without that stamp, even the world's largest stablecoin – with its $140 billion market cap – simply cannot be offered to European retail investors anymore.</p><p>This isn't theoretical compliance theater. It's capital reallocation at scale. Circle's USDC, which secured MiCA approval early, has been the primary beneficiary. But the real competition isn't coming from another fintech. It's coming from traditional banks.</p><p>In September 2025, nine major European lenders – ING, UniCredit, CaixaBank, Danske Bank, SEB, and others – announced a  to launch a MiCA-compliant euro stablecoin by mid-2026. Not a partnership with crypto companies. Not an endorsement of existing tokens. This is a direct alternative, regulated by the Dutch Central Bank, built on decades of institutional trust.</p><p>The irony is hard to miss. Stablecoins were supposed to disrupt traditional finance by offering faster, cheaper, more transparent value transfer. Now, banks are adopting the same technology but keeping control of the issuance, the reserves, and the regulatory relationships. They're issuing tokenized deposits that operate under existing banking frameworks while delivering the same blockchain-based efficiency that made stablecoins attractive in the first place.</p><p>Singapore offers another data point. The Monetary Authority of Singapore finalized its stablecoin framework in 2023, explicitly excluding tokenized bank liabilities from the regulatory regime for private stablecoins. Why? Because banks already operate under comprehensive supervision. MAS Managing Director Chia Der Jiun said it plainly at Singapore Fintech Festival: while stablecoins have a role, \"tokenized bank liabilities benefit from current central bank and regulatory arrangements that underpin value stability.\"</p><p>Translation: if you're a bank, you don't need to comply with stablecoin rules – your existing license covers it. If you're Tether or Circle, you face new reserve requirements, monthly attestations, redemption timelines, and operational restrictions that banks simply don't.</p><p>Hong Kong's approach is even more explicit. The Stablecoins Ordinance requires HK$25 million in paid-up capital, full segregation of reserve assets, and daily reconciliation. Banks issuing tokenized deposits? They can operate under their existing licenses with lighter oversight. The Hong Kong Monetary Authority has indicated that only a handful of stablecoin licenses will be granted initially, and they're setting a high bar.</p><p>The UK is heading in a similar direction. The Bank of England's November 2025 consultation on systemic stablecoins proposed allowing issuers to hold up to 60% of backing assets in short-term government debt, with the remainder in unremunerated central bank deposits. But the proposals also acknowledged that banks issuing tokenized deposits would remain subject to conventional banking regulations, not stablecoin regimes. Another regulatory moat.</p><p>Let's be clear about what's happening. Regulation isn't making stablecoins safer or more legitimate. It's making them harder to scale and more expensive to operate than bank-issued alternatives. Private issuers face strict reserve rules, capital buffers, business restrictions, and compliance costs that incumbent financial institutions don't. Banks, meanwhile, get to tokenize their existing deposit base with minimal friction.</p><p>Citi Institute projects that tokenized bank deposits could support $100 to $140 trillion in  – potentially surpassing stablecoin circulation entirely. McKinsey estimates stablecoin circulation could reach , but that assumes regulatory frameworks don't systematically favor banks. Given what we've seen in Europe and Asia, that assumption looks shaky.</p><p>The European delisting of USDT is just the opening act. As more jurisdictions implement their regimes, we'll see this pattern repeat. Private issuers will face mounting compliance costs, operational constraints, and market access barriers. Banks will issue their own tokenized liabilities, often in consortium structures that reduce competition and increase costs for users.</p><p>The real question is whether private stablecoins can maintain their operational edge – speed, scale, interoperability – under regulatory frameworks explicitly designed to favor banks. The technology itself isn't the issue. Blockchain infrastructure has matured significantly. At Venom Foundation, we've built our platform secure by design and scalable by nature precisely to handle institutional-grade throughput with proper compliance controls. The problem is that regulation isn't evaluating technology on merit, it's creating structural advantages for incumbents.</p><p>Composability matters. Network effects matter. But so does regulatory arbitrage, and right now, banks have it. Stablecoins won the technology argument. The question is whether they can survive winning the regulatory one.</p>","contentLength":5808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Clean Code in Go (Part 4): Package Architecture, Dependency Flow, and Scalability","url":"https://hackernoon.com/clean-code-in-go-part-4-package-architecture-dependency-flow-and-scalability?source=rss","date":1764274325,"author":"Vladimir Yakovlev","guid":400,"unread":true,"content":"<p>This is the fourth article in the \"Clean Code in Go\" series. </p><p>I've spent countless hours helping teams untangle circular dependencies in their Go projects. \"Can't load package: import cycle not allowed\" — if you've seen this error, you know how painful it is to refactor tangled dependencies. Go is merciless: no circular imports, period. And this isn't a bug, it's a feature that forces you to think about architecture. \\n  \\n Common package organization mistakes I've seen: \\n - Circular dependencies attempted: ~35% of large Go projects \\n - Everything in one package: ~25% of small projects \\n - Utils/helpers/common packages: ~60% of codebases \\n - Wrong interface placement: ~70% of packages \\n - Over-engineering with micropackages: ~30% of projects</p><p>After 6 years working with Go and reviewing architecture in projects from startups to enterprise, I've seen projects with perfect package structure and projects where everything imports everything (spoiler: the latter don't live long). Today we'll explore how to organize packages so your project scales without pain and new developers understand the structure at first glance.</p><h2>Anatomy of a Good Package</h2><pre><code>// BAD: generic names say nothing\npackage utils\npackage helpers  \npackage common\npackage shared\npackage lib\n\n// GOOD: name describes purpose\npackage auth      // authentication and authorization\npackage storage   // storage operations\npackage validator // data validation\npackage mailer    // email sending\n</code></pre><h3>Project Structure: Flat vs Nested</h3><pre><code> BAD: Java-style deep nesting\n/src\n  /main\n    /java\n      /com\n        /company\n          /project\n            /controllers\n            /services\n            /repositories\n            /models\n\n# GOOD: Go flat structure\n/cmd\n  /api         # API server entry point\n  /worker      # worker entry point\n/internal      # private code\n  /auth        # authentication\n  /storage     # storage layer\n  /transport   # HTTP/gRPC handlers\n/pkg          # public packages\n  /logger     # reusable\n  /crypto     # crypto utilities\n</code></pre><h2>Internal: Private Project Packages</h2><p>Go 1.4+ has a special `internal` directory whose code is accessible only to the parent package:</p><pre><code>// Structure:\n// myproject/\n//   cmd/api/main.go\n//   internal/\n//     auth/auth.go\n//     storage/storage.go\n//   pkg/\n//     client/client.go\n\n// cmd/api/main.go - CAN import internal\nimport \"myproject/internal/auth\"\n\n// pkg/client/client.go - CANNOT import internal\nimport \"myproject/internal/auth\" // compilation error!\n\n// Another project - CANNOT import internal\nimport \"github.com/you/myproject/internal/auth\" // compilation error!\n</code></pre><h3>Rule: internal for Business Logic</h3><pre><code>// internal/user/service.go - business logic is hidden\npackage user\n\ntype Service struct {\n    repo Repository\n    mail Mailer\n}\n\nfunc NewService(repo Repository, mail Mailer) *Service {\n    return &amp;Service{repo: repo, mail: mail}\n}\n\nfunc (s *Service) Register(email, password string) (*User, error) {\n    // validation\n    if err := validateEmail(email); err != nil {\n        return nil, fmt.Errorf(\"invalid email: %w\", err)\n    }\n\n    // check existence\n    if exists, _ := s.repo.EmailExists(email); exists {\n        return nil, ErrEmailTaken\n    }\n\n    // create user\n    user := &amp;User{\n        Email:    email,\n        Password: hashPassword(password),\n    }\n\n    if err := s.repo.Save(user); err != nil {\n        return nil, fmt.Errorf(\"save user: %w\", err)\n    }\n\n    // send welcome email\n    s.mail.SendWelcome(user.Email)\n\n    return user, nil\n}\n</code></pre><h2>Dependency Inversion: Interfaces on Consumer Side</h2><h3>Rule: Define Interfaces Where You Use Them</h3><pre><code>// BAD: interface in implementation package\n// storage/interface.go\npackage storage\n\ntype Storage interface {\n    Save(key string, data []byte) error\n    Load(key string) ([]byte, error)\n}\n\n// storage/redis.go\ntype RedisStorage struct {\n    client *redis.Client\n}\n\nfunc (r *RedisStorage) Save(key string, data []byte) error { /*...*/ }\nfunc (r *RedisStorage) Load(key string) ([]byte, error) { /*...*/ }\n\n// PROBLEM: service depends on storage\n// service/user.go\npackage service\n\nimport \"myapp/storage\" // dependency on concrete package!\n\ntype UserService struct {\n    store storage.Storage\n}\n</code></pre><pre><code>// GOOD: interface in usage package\n// service/user.go\npackage service\n\n// Interface defined where it's used\ntype Storage interface {\n    Save(key string, data []byte) error\n    Load(key string) ([]byte, error) \n}\n\ntype UserService struct {\n    store Storage // using local interface\n}\n\n// storage/redis.go\npackage storage\n\n// RedisStorage automatically satisfies service.Storage\ntype RedisStorage struct {\n    client *redis.Client\n}\n\nfunc (r *RedisStorage) Save(key string, data []byte) error { /*...*/ }\nfunc (r *RedisStorage) Load(key string) ([]byte, error) { /*...*/ }\n\n// main.go\npackage main\n\nimport (\n    \"myapp/service\"\n    \"myapp/storage\"\n)\n\nfunc main() {\n    store := storage.NewRedisStorage()\n    svc := service.NewUserService(store) // storage satisfies service.Storage\n}\n</code></pre><h2>Import Graph: Wide and Flat</h2><h3>Problem: Spaghetti Dependencies</h3><pre><code>// BAD: everyone imports everyone\n// models imports utils\n// utils imports config  \n// config imports models // CYCLE!\n\n// controllers imports services, models, utils\n// services imports repositories, models, utils\n// repositories imports models, database, utils\n// utils imports... everything\n</code></pre><h3>Solution: Unidirectional Dependencies</h3><pre><code>// Application layers (top to bottom)\n// main\n//   ↓\n// transport (HTTP/gRPC handlers)\n//   ↓\n// service (business logic)\n//   ↓\n// repository (data access)\n//   ↓\n// models (data structures)\n\n// models/user.go - zero dependencies\npackage models\n\ntype User struct {\n    ID       string\n    Email    string\n    Password string\n}\n\n// repository/user.go - depends only on models\npackage repository\n\nimport \"myapp/models\"\n\ntype UserRepository interface {\n    Find(id string) (*models.User, error)\n    Save(user *models.User) error\n}\n\n// service/user.go - depends on models and defines interfaces\npackage service\n\nimport \"myapp/models\"\n\ntype Repository interface {\n    Find(id string) (*models.User, error)\n    Save(user *models.User) error\n}\n\ntype Service struct {\n    repo Repository\n}\n\n// transport/http.go - depends on service and models\npackage transport\n\nimport (\n    \"myapp/models\"\n    \"myapp/service\"\n)\n\ntype Handler struct {\n    svc *service.Service\n}\n</code></pre><h2>Organization: By Feature vs By Layer</h2><h3>By Layers (Traditional MVC)</h3><pre><code>project/\n  /controllers\n    user_controller.go\n    post_controller.go\n    comment_controller.go\n  /services\n    user_service.go\n    post_service.go\n    comment_service.go\n  /repositories\n    user_repository.go\n    post_repository.go\n    comment_repository.go\n  /models\n    user.go\n    post.go\n    comment.go\n\n# Problem: changing User requires edits in 4 places\n</code></pre><h3>By Features (Domain-Driven)</h3><pre><code>project/\n  /user\n    handler.go     # HTTP handlers\n    service.go     # business logic\n    repository.go  # database operations\n    user.go       # model\n  /post\n    handler.go\n    service.go\n    repository.go\n    post.go\n  /comment\n    handler.go\n    service.go\n    repository.go\n    comment.go\n\n# Advantage: all User logic in one place\n</code></pre><pre><code>project/\n  /cmd\n    /api\n      main.go\n  /internal\n    /user          # user feature\n      service.go\n      repository.go\n    /post          # post feature\n      service.go\n      repository.go\n    /auth          # auth feature\n      jwt.go\n      middleware.go\n    /transport     # shared transport layer\n      /http\n        server.go\n        router.go\n      /grpc\n        server.go\n    /storage       # shared storage layer\n      postgres.go\n      redis.go\n  /pkg\n    /logger\n    /validator\n</code></pre><h2>Dependency Management: go.mod</h2><h3>Minimal Version Selection (MVS)</h3><pre><code>// go.mod\nmodule github.com/yourname/project\n\ngo 1.21\n\nrequire (\n    github.com/gorilla/mux v1.8.0\n    github.com/lib/pq v1.10.0\n    github.com/redis/go-redis/v9 v9.0.0\n)\n\n// Use specific versions, not latest\n// BAD:\n// go get github.com/some/package@latest\n\n// GOOD:\n// go get github.com/some/package@v1.2.3\n</code></pre><h3>Replace for Local Development</h3><pre><code>// go.mod for local development\nreplace github.com/yourname/shared =&gt; ../shared\n\n// For different environments\nreplace github.com/company/internal-lib =&gt; (\n    github.com/company/internal-lib v1.0.0 // production\n    ../internal-lib                        // development\n)\n</code></pre><h2>Code Organization Patterns</h2><h3>Pattern: Options in Separate File</h3><pre><code>package/\n  server.go      # main logic\n  options.go     # configuration options\n  middleware.go  # middleware\n  errors.go      # custom errors\n  doc.go         # package documentation\n</code></pre><pre><code>// options.go\npackage server\n\ntype Option func(*Server)\n\nfunc WithPort(port int) Option {\n    return func(s *Server) {\n        s.port = port\n    }\n}\n\nfunc WithTimeout(timeout time.Duration) Option {\n    return func(s *Server) {\n        s.timeout = timeout\n    }\n}\n\n// errors.go\npackage server\n\nimport \"errors\"\n\nvar (\n    ErrServerStopped = errors.New(\"server stopped\")\n    ErrInvalidPort   = errors.New(\"invalid port\")\n)\n\n// doc.go\n// Package server provides HTTP server implementation.\n//\n// Usage:\n//   srv := server.New(\n//     server.WithPort(8080),\n//     server.WithTimeout(30*time.Second),\n//   )\npackage server\n</code></pre><pre><code>// crypto/facade.go - simple API for complex package\npackage crypto\n\n// Simple functions for 90% of use cases\nfunc Encrypt(data, password []byte) ([]byte, error) {\n    return defaultCipher.Encrypt(data, password)\n}\n\nfunc Decrypt(data, password []byte) ([]byte, error) {\n    return defaultCipher.Decrypt(data, password)\n}\n\n// For advanced cases - full access\ntype Cipher struct {\n    algorithm Algorithm\n    mode      Mode\n    padding   Padding\n}\n\nfunc NewCipher(opts ...Option) *Cipher {\n    // configuration\n}\n</code></pre><h3>Test Packages for Black Box Testing</h3><pre><code>// user.go\npackage user\n\ntype User struct {\n    Name string\n    age  int // private field\n}\n\n// user_test.go - white box (access to private fields)\npackage user\n\nfunc TestUserAge(t *testing.T) {\n    u := User{age: 25} // access to private field\n    // testing\n}\n\n// user_blackbox_test.go - black box\npackage user_test // separate package!\n\nimport (\n    \"testing\"\n    \"myapp/user\"\n)\n\nfunc TestUser(t *testing.T) {\n    u := user.New(\"John\") // only public API\n    // testing\n}\n</code></pre><h2>Anti-patterns and How to Avoid Them</h2><h3>Anti-pattern: Models Package for Everything</h3><pre><code>// BAD: all models in one package\npackage models\n\ntype User struct {}\ntype Post struct {}\ntype Comment struct {}\ntype Order struct {}\ntype Payment struct {}\n// 100500 structs...\n\n// BETTER: group by domain\npackage user\ntype User struct {}\n\npackage billing\ntype Order struct {}\ntype Payment struct {}\n</code></pre><h3>Anti-pattern: Leaking Implementation Details</h3><pre><code>// BAD: package exposes technology\npackage mysql\n\ntype MySQLUserRepository struct {}\n\n// BETTER: hide details\npackage storage\n\ntype UserRepository struct {\n    db *sql.DB // details hidden inside\n}\n</code></pre><p>1. — don't split into micropackages immediately \\n 2.<strong>internal for all private code</strong>— protection from external dependencies \\n 3.<strong>Define interfaces at consumer</strong>— not at implementation \\n 4., not by file types \\n 5. **One package = one responsibility \\ 6. <strong>Avoid circular dependencies</strong>through interfaces \\n 7. in doc.go</p><h2>Package Organization Checklist</h2><p>- Package has clear, specific name \\n - No circular imports \\n - Private code in internal \\n - Interfaces defined at usage site \\n - Import graph flows top to bottom \\n - Package solves one problem \\n - Has doc.go with examples \\n - Tests in separate test package</p><p>Proper package organization is the foundation of a scalable Go project. Flat import graph, clear responsibility boundaries, and Dependency Inversion through interfaces allow project growth without the pain of circular dependencies. \\n  \\n In the final article of the series, we'll discuss concurrency and context — unique Go features that make the language perfect for modern distributed systems. \\n  \\n What's your approach to package organization? Do you prefer organizing by feature or by layer? How do you handle the temptation to create a \"utils\" package? Let me know in the comments!</p>","contentLength":11955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Treating Risk Assessment Like Corporate Horoscopes","url":"https://hackernoon.com/stop-treating-risk-assessment-like-corporate-horoscopes?source=rss","date":1764273974,"author":"Hui","guid":399,"unread":true,"content":"<p>Most risk assessments are nothing more than \"risk theater.\"</p><p>We gather in a conference room, brainstorm a list of things that&nbsp;&nbsp;go wrong, assign arbitrary numbers (1-5) to their probability and impact, multiply them to get a \"risk score,\" and color-code a spreadsheet. Green means safe. Red means panic.</p><p>Then we file it away and never look at it again until the project explodes for a reason that wasn't on the spreadsheet.</p><p>It's the business equivalent of reading tea leaves. We do it because it gives us the&nbsp;. We feel better believing that because we put \"Server Crash\" in cell C4, we have tamed the chaos of reality.</p><p>But reality doesn't care about your 5x5 matrix.</p><p>The problem isn't the matrix itself; it's the biological hardware running the simulation:&nbsp;.</p><p>Humans are evolutionarily wired for \"Optimism Bias.\" It’s a survival mechanism. If our ancestors stopped to calculate the exact statistical probability of being eaten by a lion every time they left the cave, they’d never have gone hunting.</p><p>In modern business, this bias is fatal.</p><ul><li>\"The vendor promised they'd deliver by Q3.\" (They won't.)</li><li>\"Our users will adapt to the new interface quickly.\" (They'll revolt.)</li><li>\"Regulatory changes take years.\" (Not anymore.)</li></ul><p>We are terrible at imagining \"Black Swans\"—high-impact, low-probability events that rewrite the rules. We stick to the \"Known Unknowns\" (what we know we don't know) and completely miss the \"Unknown Unknowns.\"</p><p>This is where we need a silicon partner.</p><h2>Enter the \"Chief Pessimist\"</h2><p>We don't need AI to be creative here. We don't need it to write marketing copy or code a website.</p><p>We need AI to be the coldest, most paranoid actuary in the room.</p><p>Large Language Models (LLMs) like Claude 3 or GPT-4 don't care about your project's success. They don't have a bonus tied to the launch date. They don't fear offending the VP of Product by pointing out a glaring flaw in the strategy.</p><p>They are the perfect candidate for&nbsp;—the practice of rigorously challenging plans to find weaknesses.</p><p>But you can't just ask, \"What are the risks?\" You'll get a generic list: \"Budget overruns, timeline delays, scope creep.\" Useless.</p><p>To get value, you need to force the AI into a specific persona: a veteran Risk Assessment Specialist who has seen everything fail and knows exactly why.</p><p>I’ve developed a \"Red Team\" prompt that strips away the optimism and forces a brutal, systematic analysis of your project. It uses frameworks like ISO 31000 and FAIR (Factor Analysis of Information Risk) to ground the output in reality, not guesswork.</p><p>Here is the prompt I use to shatter the illusion of control:</p><pre><code># Role Definition\nYou are a Senior Risk Assessment Specialist with 15+ years of experience in enterprise risk management. Your expertise spans:\n\n- **Core Competencies**: Quantitative and qualitative risk analysis, risk matrix development, mitigation strategy design\n- **Professional Background**: Certified in ISO 31000, COSO ERM Framework, and FAIR methodology\n- **Specialized Domains**: Financial risk, operational risk, strategic risk, compliance risk, cybersecurity risk, and project risk management\n\nYou approach risk assessment with a systematic, evidence-based methodology while maintaining practical applicability for business decision-making.\n\n# Task Description\nConduct a comprehensive risk assessment for the provided scenario, project, or business context. Your analysis should:\n\n- Identify and categorize all relevant risks\n- Evaluate probability and impact using standardized frameworks\n- Develop actionable mitigation strategies\n- Provide clear prioritization for risk response\n\n**Input Information** (Please provide):\n- **Context/Scenario**: [Describe the project, initiative, or business situation requiring risk assessment]\n- **Scope**: [Define boundaries - what's included and excluded from assessment]\n- **Time Horizon**: [Short-term (&lt; 1 year), Medium-term (1-3 years), Long-term (&gt; 3 years)]\n- **Risk Appetite**: [Conservative, Moderate, Aggressive]\n- **Industry/Domain**: [Specific industry context if applicable]\n- **Existing Controls**: [Current risk mitigation measures in place, if any]\n\n# Output Requirements\n\n## 1. Content Structure\n\n### Section A: Executive Risk Summary\n- High-level risk overview (2-3 paragraphs)\n- Top 5 critical risks with brief descriptions\n- Overall risk rating (Critical/High/Medium/Low)\n- Key recommendations summary\n\n### Section B: Risk Identification Matrix\n- Comprehensive list of identified risks\n- Risk categorization (Strategic, Operational, Financial, Compliance, Reputational, Technical)\n- Risk source and trigger events\n- Affected stakeholders and business areas\n\n### Section C: Risk Analysis &amp; Evaluation\n- Probability assessment (1-5 scale with justification)\n- Impact assessment across multiple dimensions (Financial, Operational, Reputational, Legal)\n- Risk score calculation (Probability × Impact)\n- Heat map visualization recommendations\n\n### Section D: Mitigation Strategy Framework\n- Risk response options (Avoid, Transfer, Mitigate, Accept)\n- Specific control measures for each significant risk\n- Resource requirements and implementation timeline\n- Residual risk assessment post-mitigation\n\n### Section E: Monitoring &amp; Review Plan\n- Key Risk Indicators (KRIs) for ongoing monitoring\n- Review frequency recommendations\n- Escalation triggers and protocols\n- Reporting structure\n\n## 2. Quality Standards\n- **Comprehensiveness**: Cover all relevant risk categories without significant gaps\n- **Specificity**: Provide concrete, actionable recommendations rather than generic advice\n- **Evidence-Based**: Support assessments with logical reasoning and industry benchmarks where applicable\n- **Practicality**: Ensure recommendations are feasible within typical organizational constraints\n- **Clarity**: Use clear language accessible to both technical and non-technical stakeholders\n\n## 3. Format Requirements\n- Use structured headers and subheaders (H2, H3, H4)\n- Include risk assessment tables with consistent formatting\n- Provide numbered lists for action items\n- Use bullet points for supporting details\n- Include a risk matrix table (5×5 format)\n- Total length: 2,000-4,000 words depending on complexity\n\n## 4. Style Constraints\n- **Language Style**: Professional, authoritative, yet accessible\n- **Expression Mode**: Third-person objective analysis\n- **Technical Depth**: Balance technical rigor with business readability\n- **Tone**: Confident but measured; avoid alarmist language\n\n# Quality Checklist\n\nBefore completing your output, verify:\n- [ ] All major risk categories relevant to the context have been addressed\n- [ ] Each risk has clear probability and impact ratings with justification\n- [ ] Mitigation strategies are specific, actionable, and resource-conscious\n- [ ] Risk prioritization is logical and defensible\n- [ ] The assessment is balanced - neither overly pessimistic nor dismissive\n- [ ] Key Risk Indicators are measurable and monitorable\n- [ ] Executive summary accurately reflects the detailed analysis\n- [ ] Recommendations align with stated risk appetite\n\n# Important Notes\n- Focus on risks that are material and actionable; avoid listing trivial or highly improbable scenarios\n- Consider interdependencies between risks (risk clusters)\n- Acknowledge uncertainty where data is limited; distinguish between known unknowns and assumptions\n- Avoid regulatory or legal advice beyond general compliance risk identification\n- Update assessments as new information becomes available\n\n# Output Format\nDeliver the complete risk assessment as a structured document following the section framework above. Begin with the Executive Risk Summary and proceed through each section systematically. Conclude with a clear action priority list.\n</code></pre><h2>How to Run a \"Pre-Mortem\"</h2><p>The best time to use this isn't when things are going wrong. It's when everyone thinks things are going right.</p><p>Psychologist Gary Klein invented the concept of a \"Pre-Mortem.\" Unlike a post-mortem (where you figure out why the patient died), a pre-mortem assumes the patient&nbsp;&nbsp;and asks, \"What killed them?\"</p><p>Here’s how to pair this technique with the prompt:</p><ol><li>: Write down your sunny, optimistic project plan.</li><li>: Paste the plan into the&nbsp;&nbsp;section of the prompt.</li><li>: Under&nbsp;, add this line:&nbsp;<em>\"Assume the project has failed catastrophically 6 months from now. Reverse engineer the most likely causes.\"</em></li><li>: The AI will generate a detailed breakdown of your blind spots.</li></ol><p>I used this recently for a client launching a fintech app. Their internal risk log was full of technical worries: \"API latency,\" \"Server downtime,\" \"Buggy UI.\"</p><p>The AI, prompted to think like a COSO-certified expert, flagged something completely different:</p><ul><li>: \"Regulatory ambiguity regarding new SEC crypto custody rules.\"</li><li>: \"Possible cease-and-desist order post-launch.\"</li></ul><p>The team had been so focused on the code they forgot the law. That insight alone saved them months of development on a feature that would have been illegal by the time it shipped.</p><p>Don't use this prompt to tick a box for your boss. Use it to protect your work.</p><p>In a world that rewards speed, the ultimate competitive advantage isn't moving fast. It's not crashing.</p>","contentLength":9108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Crypto is Getting Centralized (and What We Can Do About It)","url":"https://hackernoon.com/how-crypto-is-getting-centralized-and-what-we-can-do-about-it?source=rss","date":1764273728,"author":"Obyte","guid":398,"unread":true,"content":"<p>Let’s remember that centralization occurs when one or a few entities control a platform or service from behind the scenes. This can, and has, created its own set of problems. Without decentralization, we risk repeating the failures of traditional finance, with gatekeepers, censorship, and single points of failure. Yet over time, parts of crypto’s infrastructure are drifting toward centralized control.</p><p>What began as permissionless, open systems are increasingly shaped by dominant actors in key layers. The question becomes: how do we notice when centralization is creeping in, and how can we push back, even with modest steps?</p><h2>Why Decentralization Still Matters</h2><p>Decentralization isn’t just a fancy buzzword to sell digital products. It’s the principle that makes crypto worth trusting in the first place. When no single entity controls the system, no gatekeeper can decide who participates or who gets blocked. That means users can transfer value, publish data, or build tools without asking for permission. It’s freedom, but in network form.</p><p><strong>When control concentrates in a few hands, then freedom starts to fade.</strong> A centralized exchange (like Coinbase or Binance) can freeze withdrawals for any reason; a dominant “validator” or group of “validators” in a Proof-of-Stake (PoS) network can censor transactions; a government can pressure a single operator or a small group of operators instead of thousands. In those potential scenarios, decentralization stops being a theory and becomes protection. It’s what allows crypto to keep functioning even when politics, outages, or business failures strike.</p><p>From a financial angle, decentralization also spreads risk. When a system has many independent actors verifying and storing data, a single failure doesn’t take everything down. It’s like weaving a net instead of stretching a single thread: if one strand snaps, the rest still hold together.</p><p>Finally, decentralization nurtures innovation. Open participation means new developers can propose upgrades or create alternatives without seeking approval from a monopoly. The explosion of new chains and applications was possible precisely because no one was in charge of permissioning innovation. That freedom is what gave crypto its creative spark, and losing it would dim the entire ecosystem. \\n </p><h2>Where Centralization Is Appearing Today</h2><p>Despite the ideals, centralization is quietly running in several layers of crypto. Custodial exchanges  a great portion of user funds, concentrating financial control in a handful of platforms. If those platforms face pressure from regulators or suffer downtime, the effects could ripple across millions of users —as it happened with the . It’s an irony that a movement built to remove middlemen now leans on a few massive intermediaries.</p><p><strong>Another clear sign of centralization creeping into crypto is validator censorship.</strong> In theory, every transaction should have an equal chance of being processed, but in practice, “validators” can (and sometimes do) exclude transactions they consider “risky” or politically inconvenient. After the U.S. sanctioned  in 2022, many Ethereum “validators” began rejecting transactions linked to it to comply with regulations, for instance.</p><p>Since “validators” (who are actually block builders) decide what gets included, nearly half of Ethereum’s blocks excluded such transactions at some point, showing how fragile decentralization becomes when middlemen can silence parts of a network. And this happens because the network was designed that way.</p><h3>… And There Are More Places</h3><p>We’re definitely seeing more centralization in crypto now than five years ago. For example, institutional actors are holding huge chunks of the market:<a href=\"https://www.gemini.com/strategic-bitcoin-reserve\"></a> says that over 30% of all Bitcoin supply is now controlled by treasuries, ETFs and public companies. And that’s not to mention the next<a href=\"https://www.whitehouse.gov/presidential-actions/2025/03/establishment-of-the-strategic-bitcoin-reserve-and-united-states-digital-asset-stockpile/\"></a>, established this year by the Trump’s administration.</p><p><strong>Large companies are also piling into crypto in a way we didn’t see before.</strong> Corporations  roughly 855,000 BTC, or around 4% of Bitcoin’s supply, and in recent quarters their accumulations have outpaced ETFs. This might not be so bad in Bitcoin with its Proof-of-Work consensus, however in Proof-of-Stake networks like Ethereum and Solana, ownership translates to direct control of what’s allowed to be included in blocks, and it’s rapidly concentrating in these networks too. These moves not only shift influence toward fewer actors but also mean decision-making and market influence lean more toward institutions than broad networks of users.</p><p>Even DeFi is not immune.  from the Bank for International Settlements shows that behind the appearance of openness, control often sits with a limited set of token holders, usually from venture capital companies. They can even collude to alter “decentralized” apps for financial gain. Andreessen Horowitz (a16z), a major VC in crypto,  of this several times.</p><p>In short, if a supposedly decentralized platform doesn’t have enough participants, or if those participants hand over custody of their assets or votes to an intermediary, it will stay as centralized as a small bank.</p><h2>How To Spot Centralization</h2><p>Spotting centralization doesn’t require deep technical skills. It starts with asking who controls what. If a service keeps the , it’s in charge of your funds. If a network relies on a small handful of “validators” or servers, it’s more fragile than it looks. And if decisions come from a closed circle, the project may already be centralized in practice, no matter how decentralized it claims to be.</p><p><strong>A few simple checks can reveal a lot. Look at how many independent “validators” or nodes are active, and, even more importantly, how much they can control at the end of the day.</strong> If two or three entities dominate and have broad powers, that’s a sign of imbalance. Examine governance forums or websites to see who proposes and passes updates. Transparency is a good indicator: open discussions suggest power is distributed; silence from the top often means otherwise.</p><p>The architecture itself can offer clues. , for example, uses a Directed Acyclic Graph (DAG) instead of a blockchain, spreading validation across users rather than relying on miners or stakers. Its Order Providers (OPs) can’t “approve” or “reject” transactions; they only create waypoints that are used to order them, while is available to all users. Still, even in systems like Obyte, vigilance is needed to ensure that influence doesn’t cluster among a few active holders. \\n </p><h2>Some Steps to Avoid Centralization</h2><p>Avoiding the risks of centralization doesn’t require turning into a  overnight. Small steps matter. The first is self-custody. When we hold our own private keys (even on just a piece of paper), we remove an entire layer of dependency. <strong>We can still use exchanges for trading, but large balances should live in wallets we control.</strong> In Obyte, users can create a simple textcoin as a cold and self-custodial wallet, for instance.</p><p>Running or supporting  is another underrated act of decentralization. All networks offer guides for doing this, and even lighter full nodes, like pruned nodes in Bitcoin, help strengthen network resilience —and your own autonomy. Depending on the network, the requirements may not be a lot, either.</p><p>Transparency should also be rewarded. <strong>Decentralized projects should publish their code and governance proposals for everyone, even if not everyone is a programmer.</strong> Other community members will analyze this data and share their findings. The more visible a system is, the easier it becomes to avoid hidden centralization.</p><p>And finally, community participation matters: joining discussions, proposing governance changes, or voting on them keeps teams accountable. The healthier a community’s involvement is, the harder it is for control to concentrate unnoticed. \\n </p><h2>Trade-Offs and Realistic Expectations</h2><p>Complete decentralization is hard, and that’s okay. Systems that distribute power often sacrifice speed or simplicity. Self-custody shifts responsibility to the user, and smaller “validators” can face reliability issues. Still, these ‘inconveniences’ are the price of independence.</p><p><strong>We also need to accept that some centralization has value.</strong> Exchanges simplify fiat on-ramps, and centralized relays can help coordinate complex systems. The key is to rely on them wisely. Keep only what’s needed on centralized platforms, secure the rest yourself, and always have an exit path. The difference between dependence and convenience is preparation.</p><p>Progress happens incrementally. When we choose decentralized services, set up our own nodes, or speak up about governance concentration, we nudge the system back toward its original vision.  isn’t built overnight, and it’s never granted. It’s a continuous balance between usability and resilience.</p><p>In the end, avoiding an excess of centralization is about cultivating good habits. A healthy crypto ecosystem depends on millions of users making small, conscious choices that keep power distributed rather than concentrated. By staying alert and sharing responsibility, we ensure that the open, borderless spirit that gave birth to crypto stays alive —not just in code, but in the way we use it.</p><p>:::info\n<em>Featured Vector Image by vectorjuice / </em></p>","contentLength":9309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Face Transplants Promised Hope. Patients Were Put Through the Unthinkable","url":"https://science.slashdot.org/story/25/11/27/202228/face-transplants-promised-hope-patients-were-put-through-the-unthinkable?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764273600,"author":"msmash","guid":196,"unread":true,"content":"Twenty years after surgeons in France performed the world's first face transplant, the experimental field that procedure launched is now confronting a troubling record of patient deaths, buried negative data and a healthcare system that leaves recipients financially devastated and medically vulnerable. \n\nAbout 50 face transplants have been performed globally since Isabelle Dinoire received her partial face graft at University Hospital CHU Amiens-Picardie in November 2005. A 2024 JAMA Surgery study reported five-year graft survival of 85% and 10-year survival of 74%, concluding that the procedure is \"an effective reconstructive option for patients with severe facial defects.\" The study did not track psychological wellbeing, financial outcomes, employment status or quality of life. Roughly 20% of face transplant patients have died from rejection, kidney failure, or heart failure. \n\nThe anti-rejection medications that keep transplanted faces alive can destroy kidneys and weaken immune systems to the point where routine infections become life-threatening. In the United States, the Department of Defense has funded most operations, treating them as a frontier for wounded veterans, because private insurers refuse to cover the costs. Patients who survive the surgery often find themselves unable to afford medications, transportation to follow-up appointments or basic caregiving. The field's long-term grants cover surgical innovation but not the lifelong needs of the people who receive these transplants.","contentLength":1519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GCC 16 Switches To Using C++20 Standard By Default","url":"https://www.phoronix.com/news/GCC-16-Now-CPP20-Default","date":1764272856,"author":"Michael Larabel","guid":683,"unread":true,"content":"<article>Following up on the discussion from earlier this month among GCC developers over switching to C++20 by default for the GCC compiler as the default C++ standard when not otherwise set, that change has indeed happened. Merged now is the change defaulting to C++20 (well, the GNU++20 dialect) rather than C++17/GNU++17 when not otherwise specified when compiling C++ code...</article>","contentLength":371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic UX Over \"Chat\": How to Design Multi-Agent Systems People Actually Trust","url":"https://hackernoon.com/agentic-ux-over-chat-how-to-design-multi-agent-systems-people-actually-trust?source=rss","date":1764272321,"author":"Victor Churchill","guid":397,"unread":true,"content":"<p>When I was first tasked with integrating generative AI into Viamo’s IVR platform, serving millions of people across Africa and Asia’s emerging markets, it didn’t take me long to recognise that we couldn’t just stick a chat interface on it and call it a day, as much as that would simplify some of our technical and development challenges. To be clear, we were designing for people who rely on voice interfaces strictly because they need this kind of information about healthcare, agriculture, and finance, and they have little patience for AI that fails them or gives them the wrong information when they have limited time and limited bandwidth.</p><p>That project taught me a lesson about designing for AI that I think all designers should learn: Designing for agentic AI is not about making it chat-friendly, but about designing intelligent systems that can work reliably, transparently, and predictably inside of workings that people already trust. Over seven years of designing products, spanning fintech, logistics, and software platforms, I have realised that the most effective way of implementing AI is not about replacing human judgement, but about augmenting it in ways that people can, and ultimately, will trust.</p><h3>The Fatal Flaw of Chat-First Thinking</h3><p>There is a dangerous paradigm that has been ingrained in this industry because of its obsession with chat interfaces on AI products, and that is that everyone is trying to build a “ChatGPT for Y”. No one stops and says, “Well, actually, just because we can build this and chat is a part of it, that doesn’t actually have anything to do with whether or not chat interaction is actually what we need on this.”</p><p>That’s not necessarily true. Chat is perfect for open-ended exploration and creative tasks that involve journeys as much as they involve destinations. But most business tasks demand accuracy, auditability, and repeatability. When designing the supplier interface for Waypoint Commodities, a system that deals with million-dollar fertiliser and chemical trade transactions, users didn’t require a user-friendly chat interface that could facilitate exploratory conversations about their transactions. They required interfaces that enabled AI systems to point out errors, identify optimal routes, and highlight compliance concerns without clouding critical transactions with any uncertainty or vagueness.</p><p>The primary issue with chat-centric AI is that it enables decision-making under a facade of conversation. Users can’t easily inspect what information was used, what was applied, and what was explored as an alternative option. Of course, this is acceptable for low-stakes interrogations, but disastrous for consequential choices. When designing our shipment monitoring system that tracked orders all through fulfilment, our Waypoint project was facing a challenge that required users to be assured AI messages about potential delays or market fluctuations weren’t based on fictitious observations but on actual facts explored and verified by AI itself.</p><h3>Multi-Agent Systems Require Multi-Modal Interfaces</h3><p>But then, a paradigm shift occurred within my thinking as I ceased designing only for one AI model and focused on designing for environments that consisted of multiple specialised AI entities operating together as a system.</p><p>It meant that we had to forgo entirely the paradigm of a one-window chat system. Instead, we built a multi-window interface through which multiple interaction methods could be used simultaneously. Quick facts would get immediate responses through AI voice output. Troubleshooting would involve a guided interaction through which AI would answer preliminary questions before redirecting the user to an expert system. Users searching for information on government facilities would get formatted replies that would cite sources accordingly. All these methods of interaction would have distinct visual and audio signals that would build user expectation accordingly.</p><p>These outcomes proved this strategy was valid, and we experienced improved accuracy of response of more than thirty per cent and heightened user engagement levels. Far more significantly, user abandonment levels decreased by twenty per cent as users ceased leaving conversations due to frustration of expectation mismatches. Since users understood they would be speaking to an AI system that had a certain body of knowledge as compared to waiting for human expertise, they adjusted their levels of enquiry and patience accordingly.</p><h3>Designing for Verification, Not Just Automation</h3><p>One of the most important principles of agentic UX design that I uphold is that ‘automation without verification’ is merely ‘technical debt masquerading as AI.’ There should be an ‘escape hatch’ provided alongside each AI ’agent’ used in a system, allowing ’users to validate its reasoning’ and ‘override its decision’ as and when required, ’not because one lacks faith’ in AI ’abilities,’ but because one ’respects the fact’ that ’users have final responsibility’ when ’in regulated environments or high-value transactions.’</p><p>When I was responsible for designing the admin dashboard for onboarding new users at Waypoint, we had a typical case of an automation project, the kind that would enable AI processing of incorporation documents, abstracting essential information, and automatically populating user profiles, thereby reducing user onboarding from several hours into just minutes. Of course, we understood that inaccuracies could potentially lead a company into a case of non-compliance or, worse, create fraudulent user profiles. So, we realised that we don’t need more accurate AI processing as a remedy to this problem, but rather to create a system of verification that would involve AI-generated user profiles, pending activation by human admins.</p><p>In our interface, we implemented the following system for indicating AI confidence levels for each field that was extracted:</p><ul><li>Fields that had high levels of accuracy had black text colour and green tick marks;</li><li>Medium accuracy had orange colour, and a neutral symbol was used;</li><li>Fields that had low accuracy or missing information had red colour and a warning symbol.</li></ul><p>To identify any errors that AI systems had missed, thirty seconds per profile was enough time for admins, as they got enough context through this system.</p><p>But the outcome was clear: we achieved a reduction of onboarding time of forty per cent over fully manual methods and greater accuracy than human or AI approaches alone. But more significantly, the admin personnel trusted this system because they could actually follow its logic. If there was any error on the AI’s part, that was pointed out quite easily through the verification page, and this helped build that all-important trust that enabled us to successfully roll out other AI functionality later on.</p><h3>Progressive Disclosure of Agent Capabilities</h3><p>Another subtle but essential area of agentic UX that most designers struggle with is providing users with information about what their agents can and cannot accomplish without overwhelming them with possibilities and potential applications of these capabilities. Such is especially true for systems that apply generative AI, and as we struggled at work at FlexiSAF Edusoft, where I developed these systems, they have applications that range widely but are unpredictable across different tasks or activities. Users, in this case students and parents, would need direction through often complex admission procedures and, on the other hand, would need to be informed of what responses could be provided by AI and what would require human interaction.</p><p>Our implementation provided capability hints based on interaction, meaning that as one used the system, they would be provided with examples of questions that the AI was strong at answering versus questions that could be more appropriately answered by the human resources people at the institution, meaning that as a user typed questions about application deadlines, they would see examples of questions that the AI was strong at answering, such as “When is the deadline for engineering applications?” as opposed to questions they could more effectively answer, for instance, “Can I be exempted from payment of application fees?”</p><p>Additionally, we enabled a feedback cycle through which users could express whether their question had been fully answered by an AI response or not. This was not only for improving the model, but it enabled a UX feature through which users could express that they required escalation of their issue and that they had been left stranded by an AI system. Relevant resources would be provided through this system, and, if not, they would be connected with human resources as well, thus resulting in a support ticket decrease but without sacrificing user satisfaction, as people would feel that they had been listened to and that they had not been left stranded through an AI system. </p><h3>Transparency and Its Usefulness as a Trust-Building Factor</h3><p>Trust, of course, is not established by improved AI algorithms but by transparent system design that allows a user to see what the system knows, why it made its conclusions, and where its limitations are. eHealth Africa, our project involving logistics and data storage of supply chains in the medical sector, made this one of its non-negotiables: ’If AI computer agents predicted the timing of vaccine shipments or indicated optimal routes for delivery, these justifications had to be explainable, because human decision-makers would be deciding whether rural clinics received life-saving commodities on time.’</p><p>To address this, we built what I call “reasoning panels” that provided output alongside AI suggestions. These reasoning panels did not display model details of its computations, only information about why it reached its recommendations, including road conditions, previous delivery times for this route, weather, and transport capacity available. The reasoning panels enabled field operatives to quickly ascertain if they had been getting outdated advice from AI or if they had neglected an essential, more recently available fact, such as a bridge closure, and made them indispensable and transparent rather than opaque decision-makers, as would be the case for black boxes.</p><p>Transparency was required, and this was true for failure as well as success. To this end, we built helpful failure states that would describe why the AI was unable to offer its recommendation as opposed to falling back on some generic error message. If, for instance, it was unable to offer an optimal route because it lacked connectivity information, this was explicitly communicated, and the user was informed of what they could do if they still had no route recommendation available.</p><h3>Designing Handoffs Between Agents and Humans</h3><p>But perhaps one of most undeveloped themes of agentic UX is that of handover, or exactly when and exactly how an AI agent is supposed to pass control of a system or of an interaction over to a human, whether that human be a colleague or be themselves a user of that system or interaction. This is precisely where most of the loss of trust occurs within multi-agent systems, and this was actually one of the first projects that I engaged that dealt explicitly with this issue, that of Bridge Call Block for Viamo, which was a system that transferred users from IVR interactions to human customer service reps.</p><p>Our protocol for context transfer was designed such that after every interaction of the AI, a structured summary was displayed on the screen of the operator before they could greet the user, and this summary contained what was asked by the user, what the AI intended to say, and why the AI escalated this call. There was no need for users to be asked to repeat what they had asked, and all interaction context was available to the operators, and this small detail of interaction design vastly improved average handling time and user satisfaction, as people felt they had been respected and that their time had not gone to waste.</p><p>The handoff from human to AI agent had to be considered equally carefully in reverse. In cases that called on the operators to refer their users back to the automated system, user interface functionality was used effectively by the operators to communicate adequate expectations of AI autonomy based on certain tasks that would enable users to be referred back to the automated system, as opposed to doing so with expected frustrations.</p><h3>Principles of Pragmatic Design of Agentic UX</h3><p>As a practitioner designing AI-enabled systems for many years, today I have formulated some pragmatic guidelines that help me design agentic UX effectively:</p><p>Firstly, design for the workflow, not for technology. Users don’t care whether they’re being helped through AI, rules, or human intelligence. They only care about whether they can accomplish their tasks effectively and conveniently. Begin by reverse-engineering from the target outcome, identifying areas of added value and added complexity due to AI-enabled agents, and then stop and proceed accordingly.</p><p>Secondly, define meaningful boundaries of AI-enabled agents. Users need to be aware of when they are leaving one realm of intelligence and entering other realms, such as the intelligence of retrieval, model intelligence, and human intelligence, and establish consistent visual and interaction guidelines accordingly, such that they don’t wonder what kind of answer they’re going to get and when they’re going to get it.</p><p>Thirdly, build verification into your workflow design respecting user expertise. AI systems should ideally help hasten decision-making by bringing up pertinent information and suggesting courses of action, but these should ultimately be made by human users who possess context unavailable to AI systems themselves. Designing decision verification flows into AI system user interfaces that facilitate this would be ideal.</p><p>Because of projects that have successfully secured funding, boosted engagement by definite increments, and processed user figures in the thousands, we didn’t succeed because we possessed, or attempted to create, sophisticated AI systems. It is because we provided these users, through our interface, the ability to comprehend what was happening on their end of this AI system and, through that, helped them trust it enough to accomplish increasingly complex tasks over time that has truly made them successful examples of agentic UX.</p>","contentLength":14527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Thanksgiving’s real drama may be Michael Burry versus Nvidia","url":"https://techcrunch.com/2025/11/27/this-thanksgivings-real-drama-may-be-michael-burry-versus-nvidia/","date":1764271941,"author":"Connie Loizos","guid":297,"unread":true,"content":"<article>Is Burry the canary in the coal mine, warning of a collapse that's inevitable? Or could his fame, his track record, his now unrestricted voice, and a fast-growing audience trigger the very implosion he's predicting?</article>","contentLength":215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scalability Lessons From Building an AI Learning Platform for Healthcare","url":"https://hackernoon.com/scalability-lessons-from-building-an-ai-learning-platform-for-healthcare?source=rss","date":1764271932,"author":"Emma M","guid":396,"unread":true,"content":"<p>When I first&nbsp;began working on our recent project, which was about developing an AI-driven learning platform for healthcare professionals, my approach to scalability was exactly like most engineering teams. It was a problem of performance, throughput, and infrastructure.</p><p>In fact, our first diagrams revolved around APIs, databases, and cloud scaling policies. We wanted the system to handle thousands of concurrent users and millions of records without breaking a sweat.</p><p>But a few months into development, as we started testing with clinicians, our view of scalability changed completely. We learned that in healthcare, scale is not only about how much data you can process or how fast your inference runs. It is about how effectively you can expand , , and  without breaking the ethical fabric that healthcare relies on.</p><p>The project — an AI- and React-based platform designed to personalize learning paths for nurses and healthcare workers — became an eye-opener. It was meant to help medical professionals identify skill gaps, complete relevant training, and track progress through an intelligent recommendation system. But success wasn’t measured by technical benchmarks alone. It was measured by adoption, confidence, and measurable improvement in learning outcomes.</p><h2>How to Scale AI Systems for Healthcare - Based on Practical Experience</h2><p>Below I have mentioned the five principles that guided us as we learned how to architect AI systems for healthcare that truly scale.</p><h3>1. Start by Scaling Trust</h3><p>Every AI product depends on trust, but in healthcare it is everything. When we rolled out the first version of our recommendation engine, it performed impressively in internal tests. The AI could predict learning needs and suggest modules with high accuracy. Yet the clinicians who tried it were hesitant. Their first question wasn’t “How fast is it?” Instead, it was “How do I know this suggestion is right?”</p><p>That question reshaped our design philosophy. We realized that explainability had to be a feature, not a footnote. So, we made every recommendation transparent: each suggestion came with a confidence score, an explanation of what data informed the choice, and a simple breakdown of learning objectives.</p><p>Once users could see  the system thought a particular course mattered to them, skepticism turned into curiosity, and curiosity turned into trust. Only then did engagement and scalability follow.</p><p>Technical scale means nothing without emotional scale, and trust is the emotional layer that allows AI to grow within healthcare ecosystems.</p><p>In most software projects, compliance is seen as a checklist at the end of the development cycle. In healthcare, that approach is a recipe for re-architecture.</p><p>Early on, our data scientists wanted to move fast: ingest data, train models, and deploy updates weekly. But healthcare operates under strict regulations like HIPAA and GDPR. Patient-related information had to remain confidential, auditable, and tamper-proof. The architecture itself had to enforce those rules, not rely on human vigilance.</p><p>We re-engineered our data flow so that sensitive data was encrypted at rest and in transit. Personally identifiable information was pseudonymized before entering the AI pipeline, and every inference produced an immutable audit trail. These weren’t just technical decisions but&nbsp;they were architectural boundaries that allowed our system to grow safely.</p><p>By embedding compliance in architecture, we created a foundation where scaling to more clinics and locations became straightforward. Every new location didn’t require a fresh legal review of our processes, because compliance was literally built into the system’s DNA. Scalability became easier because </p><h3>3. Build Modular Systems That Learn and Evolve</h3><p>AI systems in healthcare are living organisms and&nbsp;they must adapt as medical knowledge, treatment protocols, and learning behaviors evolve. The only sustainable way to support that evolution is modularity.</p><p>We designed the platform around independent modules: a recommendation engine, a learner-behavior analyzer, and a React-based engagement dashboard. Each component communicated through APIs, allowing updates without touching the others. When our data team built an improved analytics model months later, it slotted neatly into the system without downtime.</p><p>This modularity also made scaling across different healthcare organizations effortless. One of the clinics, for example, wanted deeper analytics while another wanted simpler dashboards. We could toggle modules on or off without rewriting the entire stack.</p><p>True scalability, we realized, doesn’t mean building a single large system but building  that can grow and mutate as needs evolved. It was our insurance against change.</p><h3>4. Human-Centered Scaling Beats Pure Automation</h3><p>In AI projects, there’s constant pressure to reduce human intervention with fewer labels, faster feedback loops, and fancy, self-healing pipelines. But in healthcare, removing humans from the loop is the fastest way to lose credibility.</p><p>We built feedback as a core feature, not an afterthought. Every new model iteration was validated by clinicians who tested the recommendations in real learning scenarios. Their qualitative insights around what felt intuitive and what seemed irrelevant helped shape retraining data far more effectively than metrics alone.</p><p>One of my favorite milestones came when a group of nurses described the system as “a digital mentor” rather than “a tool.” That language shift showed real adoption. Over time, feedback from more than 500 healthcare professionals helped us refine algorithms, language tone, and even notification timing.</p><h3>5. Measure What Truly Matters</h3><p>When people talk about scalable AI, they often celebrate technical metrics: lower latency, higher throughput, and reduced compute cost. Those numbers look impressive on a slide deck, but in healthcare, they don’t mean much unless they translate into better outcomes.</p><p>We shifted our measurement strategy early in the rollout. Instead of tracking CPU utilization or response times as success indicators, we focused on human-centric metrics: how much faster did new staff onboard? How often were training modules completed? How did clinicians rate their confidence before and after using the system?</p><p>The results were rewarding: onboarding time dropped by 40 percent, course completion improved by 94 percent, and user satisfaction consistently climbed. These were the metrics that resonated with stakeholders because they reflected the real-world value of the system.</p><p>When you measure what matters to humans, you end up scaling what matters to the organization. Infrastructure follows purpose.</p><h2>Redefining Scalability in Healthcare AI</h2><p>Looking back, that project changed how I think about software architecture. Scalability is often portrayed as a purely technical victory… more users, more data, more uptime. But in healthcare AI, scaling without ethics or empathy is hollow progress.</p><p>To scale responsibly, you must align growth with integrity. That means systems that can expand without eroding privacy, algorithms that can learn without losing explanability, and platforms that grow user confidence instead of dependence.</p><p>Our AI learning platform succeeded not because it handled thousands of users, but because it earned their trust and kept it. Every technical improvement… from modular APIs to cloud elasticity, was valuable only insofar as it supported that human connection.</p><p>The more I work in this field, the clearer it becomes: healthcare doesn’t need AI that scales endlessly. It needs AI that scales  If your architecture can grow in a way that safeguards patients, empowers clinicians, and enhances understanding, then it’s already scalable enough.</p><p>Because in the end, true scalability in healthcare isn’t measured in requests per second, it is measured in trust per interaction.</p>","contentLength":7880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UK To Tax Electric Cars by the Mile Starting 2028","url":"https://news.slashdot.org/story/25/11/27/1332209/uk-to-tax-electric-cars-by-the-mile-starting-2028?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764266520,"author":"msmash","guid":195,"unread":true,"content":"The UK government will levy a pay-per-mile tax on electric and plug-in hybrid vehicles starting April 2028, UK's finance minister Rachel Reeves announced, a measure designed to offset some of the fuel duty revenue that will disappear as drivers shift away from petrol and diesel cars. Electric vehicles will be charged 3 pence per mile and plug-in hybrids 1.5 pence per mile, payable annually alongside car tax. An average driver covering 8,000 miles a year would pay around $320, roughly half what a petrol or diesel driver pays in fuel duty. \n\nThe Office for Budget Responsibility expects the tax to generate $1.45 billion in its first year and $2.51 billion by 2030-31, offsetting about a quarter of the revenue losses projected from the EV transition by 2050. The Society of Motor Manufacturers and Traders warned the new charge would \"suppress demand\" and make sales targets harder to achieve. New Zealand and Iceland have already introduced road pricing for EVs; demand dropped in the former but held steady in the latter.","contentLength":1028,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing AI with the Right Cloud Strategy: Multi-Cloud, Hybrid, and More","url":"https://hackernoon.com/optimizing-ai-with-the-right-cloud-strategy-multi-cloud-hybrid-and-more?source=rss","date":1764266238,"author":"Nathan Smith","guid":395,"unread":true,"content":"<p>AI (artificial intelligence) is a key driver of modern digital transformations, which happens only with faster processing, large-scale machine learning, and handling data in real-time. Moreover, as AI workloads become much more complex and the volume of data that is to be processed grows, there is a need for robust and scalable hosting solutions.</p><p>Cloud has emerged as the ideal solution because of its ability to provide on-demand resources, secure AI computing environments, and scale infrastructure without physical overhead. But, AI workloads are not one-size-fits-all; they vary with the scale of your business, your chosen use case, as well as the scale (and quality) of your data.</p><p>As a result, enterprises face a critical question: which cloud strategy is most suitable for their AI compute demands? Multi-cloud, hybrid cloud, or service models (IaaS, PaaS, or SaaS)?</p><p>This blog examines each cloud strategy in detail, exploring how businesses can make informed decisions, optimize cloud infrastructure, and navigate governance to maximize their AI computing potential.</p><p>Traditional cloud strategies, particularly relying on a single cloud provider, often fall short when handling the dynamic demands of AI computing. Here are key reasons why:</p><ul><li>: Single-cloud strategies (e.g., AWS, GCP, or Azure) can result in vendor lock-ins and restrict scalability, limiting your ability to choose the best-fit services for AI workloads.</li><li>: AI requires real-time processing and centralized data access for efficient performance. Single-cloud models often introduce latency, failing to meet the needs for AI orchestration and edge computing.</li><li>: As AI workloads expand, data transfer costs and vendor lock-in increase (sometimes even exceeding the <a href=\"https://www.suntecindia.com/blog/cloud-migration-costs-unveiled-what-you-should-expect-at-each-step/\">cost of migrating to the cloud</a>), reducing ROI and hindering cloud cost optimization for AI.</li><li><strong>Security &amp; Compliance Issues</strong>: Traditional cloud strategies may struggle to comply with specific data sovereignty regulations across different regions.</li><li>: Relying on a single cloud platform limits access to certain services, which are essential for AI-driven cloud workload management.</li></ul><p>Three main cloud strategies work well for complex, growing AI workloads:</p><ul><li>Service Models: Infrastructure-as-a-Service (IaaS), Platform-as-a-Service (PaaS), and Software-as-a-Service (SaaS)</li></ul><p>Let’s dig deeper into each and assess cloud strategy dominance for AI workloads in 2026.</p><h3>Multi-Cloud Strategy for AI</h3><p>In a multi-cloud strategy, businesses use certain services from one cloud platform and another set of services from the other. The bifurcation is spread across multiple clouds (e.g., AWS, Azure, and Google Cloud), improving how diverse AI workloads and applications are managed.</p><p>With this cloud strategy, you can benefit from the flexibility of choosing prominent service offerings (such as specialized ML services or computing power) of each provider.</p><p><strong>Why is a Multi-Cloud Strategy Suitable for AI Workloads?</strong></p><p>It is well-suited for AI workloads that require a combination of high-performance computing and real-time data processing capabilities/services.</p><p>For example, training large AI models may require high GPU resources from one provider, while real-time inference could be better served by edge computing services from another.</p><p><strong>Benefits of a Multi-Cloud Strategy</strong></p><ul></ul><p><strong>Cost Implications of a Multi-Cloud Strategy</strong></p><p>The cost of running your AI workloads on multiple cloud platforms depends on several factors (your data, use case, etc.), but generally is calculated through a pay-as-you-go pricing model—for all cloud platforms involved.</p><p>This can quickly add to the overall costs if not optimized carefully.</p><p>Using one cloud platform for high-performance computing and another for data storage or processing.</p><p><strong>Challenges in Implementing a Multi-Cloud Strategy for AI</strong></p><ol><li> - When you manage multiple cloud environments, data pipelines, and AI workloads, the operational complexity naturally increases.</li><li><strong>Data Governance Becomes Tough</strong> - Keeping an eye on all AI and data workloads is very tough across multiple environments when you also have to adhere to varying governance policies and compliance requirements.</li></ol><h3>Hybrid Cloud Strategy for AI</h3><p>When using a hybrid cloud strategy for AI, organizations typically keep specific workloads on-premises (or in a private cloud) and others on public cloud platforms.</p><h4><strong>Why is a Hybrid Cloud Strategy Suitable for AI Workloads?</strong></h4><p>For AI workloads that involve sensitive user data (e.g., customer PII or financial information) for training, hybrid cloud strategies are ideal. They enable businesses to maintain control over critical applications and sensitive data, while leveraging the flexibility of public clouds for computing power.</p><h4><strong>Benefits of a Hybrid Cloud Strategy</strong></h4><ul><li>Ability to scale without overhauling IT infrastructure</li><li>Tighter compliance and security</li><li>Adherence to data privacy and security regulations (GPDR, CCPA, SOC 1 and 2, etc) becomes easier</li></ul><h4><strong>Cost Implications of a Hybrid Cloud Strategy</strong></h4><p>The cost of using a hybrid cloud strategy can be slightly higher as you also have to maintain your infrastructure (including hardware) in addition to the cloud environment. But, there are still ways to optimize cloud costs.</p><p>Sensitive data management, particularly in regulated industries like finance and healthcare, is the most common use case for a hybrid cloud strategy.</p><h4><strong>Challenges in Implementing a Hybrid Cloud Strategy for AI</strong></h4><ol><li>Complexity of Integrating On-Prem Data with Cloud-Based Applications - Setting up ETL/ELT data pipelines from private infrastructure to the cloud requires several custom solutions and integration tools.</li><li>Cloud Cost Management Becomes Tough: Managing costs across both on-premises and cloud infrastructure can be tricky. You will need to monitor everything to avoid over- or under-provisioning cloud resources constantly.</li></ol><h3>Managed Service Models for AI</h3><p>Relying on service models (managed by cloud providers) like IaaS, PaaS, or SaaS is another way to benefit from flexible scaling and ease of computing.</p><ol><li><strong><strong>IaaS (Infrastructure-as-a-Service)</strong></strong>: In this cloud strategy, enterprises use virtual computing resources (compute power, storage methods, network elements, etc). They can easily scale (up or down) based on the need and pay for only what has been used.</li></ol><p> Custom AI training and development workloads.</p><ol start=\"2\"><li><strong><strong>Paas (Platform-as-a-Service)</strong></strong> In this cloud strategy, enterprises use dedicated platforms to build, run, and manage AI workloads and applications. This allows teams to quickly deploy AI models, allowing them to focus on developing the models rather than managing the infrastructure.</li></ol><p>: Ideal for AI model deployment in dynamic environments or when you need to scale quickly.</p><ol start=\"3\"><li><strong><strong>SaaS (Software-as-a-Service)</strong></strong> In this cloud strategy, enterprises use fully managed software apps hosted by the cloud provider of their choice. It could be an AI tool, an AI platform to which you subscribe, etc.</li></ol><p>: Best suited for businesses that require pre-trained AI models.</p><h4><strong>Why Service Models are Suitable for AI Workloads?</strong></h4><p>Service models provide organizations with the flexibility to adjust their cloud usage without worrying about the infrastructure.</p><h4><strong>Benefits of Service Models</strong></h4><ul><li>IaaS provides the infrastructure for training models.</li><li>PaaS simplifies model development and deployment.</li><li>SaaS provides ready-to-use AI apps and services, eliminating the need for development.</li></ul><h4><strong>Cost Implications of Service Models</strong></h4><p>IaaS and PaaS are mostly offered with pay-as-you-go cost models, while SaaS is often provided through a subscription-based pricing model.</p><p>IaaS is perfect when you have to train AI models using robust GPUs/TPUs and do not want to invest in physical hardware. Similarly, PaaS is often used when you need built-in support for analytics and machine learning, which can be achieved through CI/CD pipelines and APIs. Lastly, the SaaS model is best suited for use when you need ready-made solutions, such as chatbots and sentiment analysis engines.</p><h4><strong>Challenges in Implementing a Service-Driven Cloud Strategy for AI</strong></h4><p>While it is beneficial to manage AI workloads through a service model, it introduces many challenges.</p><ul><li>With third-party services, there are risks around data governance and compliance.</li><li>For the same reason, you may end up in a vendor lock-in situation if reliance on a particular cloud platform is heavy.</li><li>Services are cost-effective until they’re not—and this transition often occurs in a fraction of the time, as costs are usually unpredictable initially.</li><li>Service models offer pre-determined access to features. Customization scope for particular AI needs is, hence, restricted or very costly.</li></ul><h2>How to Choose the Right Cloud Strategy for Your AI Workloads?</h2><p>Selecting the best cloud strategy for AI workloads is a crucial decision that determines whether you will succeed, how much you will spend, and how quickly you can deploy AI. Here is a practical framework that you can refer to when selecting a strategy and optimizing cloud costs for AI.</p><h3>1. Make a Profile of your AI Workloads</h3><ul><li>What type of an AI workflow do you need?</li><li>Will you be working with large datasets?</li><li>How much computing power will this data require?</li><li>Do you require real-time processing, or is a modest latency acceptable?</li></ul><h3>2. Assess Data Sensitivity</h3><p>Does your AI workload require sensitive data?</p><h3>3. Consider the Financial Impact</h3><p>Have you thought of a budget? If not, determine a cloud cost optimization approach and estimate the associated financial implications.</p><p>Consider the data privacy and compliance regulations that you have to adhere to.</p><p>Finding answers to the above questions and points of consideration will help you choose an ideal cloud strategy for your AI workloads. If you are not able to make this assessment, consider seeking help from a cloud service provider. Their cloud consultants can do a thorough evaluation of your workload and recommend a suitable cloud strategy.</p><p>If you are just beginning to migrate your AI workloads to the cloud, it is advisable to work closely with a cloud migration service partner to ensure a smooth transition.</p><p>With increasingly complex AI workloads and growing data volumes, moving to the cloud is no longer a question, but an imperative. Instead, the question facing enterprises today is which cloud strategy to adopt, as this decision determines how your AI is built, managed, and deployed for maximum ROI. Whether you opt for a multi-cloud, hybrid, or service-driven approach, the key to success lies in aligning your cloud model with your specific AI needs, whether that involves high computing or stronger governance. Those who can strategically make this decision will not just stay ahead in the AI race but also unlock new levels of innovation and operational excellence in 2026 and beyond.</p>","contentLength":10600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intel Battlemage Graphics Enjoyed Nice GPU Compute Performance Gains In 2025","url":"https://www.phoronix.com/review/intel-b580-compute-one-year","date":1764264840,"author":"Michael Larabel","guid":682,"unread":true,"content":"<article>In addition to Intel Arrow Lake desktop performance evolving nicely on Linux over the course of 2025, the Intel Arc B-Series graphics that launched last December with the Arc B580 have evolved quite nicely too with their open-source driver stack. With it coming up on one year since the Arc B580 launch, here is a look at how the GPU compute performance has evolved since that point. Similar Intel Arc B580 Linux graphics comparisons are also coming up in a follow-up comparison on Phoronix.</article>","contentLength":491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best iPad apps for unleashing and exploring your creativity","url":"https://techcrunch.com/2025/11/27/best-ipad-apps-for-unleashing-and-exploring-your-creativity/","date":1764264547,"author":"Aisha Malik","guid":296,"unread":true,"content":"<article>We’ve compiled a list of some of the best iPad apps for creativity that are available on the App Store.&nbsp;</article>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Glīd won Startup Battlefield 2025 by building solutions to make logistics simpler, safer, and smarter","url":"https://techcrunch.com/2025/11/27/glid-won-startup-battlefield-2025-by-building-solutions-to-make-logistics-simpler-safer-and-smarter/","date":1764262920,"author":"Isabelle Johannessen, Maggie Nye","guid":295,"unread":true,"content":"<article>We're excited to interview the winner of Startup Battlefield 2025, and Glid co-founder Kevin Damoa, on this week's Build Mode!</article>","contentLength":126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Android's New Dual-Band Hotspot Mode Pairs 6 GHz Speed With 2.4 GHz Compatibility","url":"https://mobile.slashdot.org/story/25/11/27/1255245/androids-new-dual-band-hotspot-mode-pairs-6-ghz-speed-with-24-ghz-compatibility?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764262860,"author":"msmash","guid":194,"unread":true,"content":"Google is testing a new Wi-Fi hotspot configuration in the latest Android Canary build that pairs the 6 GHz band's superior throughput with the 2.4 GHz band's broad device compatibility, eliminating the trade-off users previously faced when choosing between speed and legacy support. Android's default hotspot setting uses 2.4 and 5 GHz frequencies, omitting 6 GHz because most devices lack support for the newer standard and because U.S. regulations previously prohibited smartphones from creating 6 GHz hotspots. Recent regulatory changes and a Pixel update unlocked standalone 6 GHz hotspots, but that option cuts off older devices entirely. The new \"2.4 and 6 GHz\" dual-band mode, spotted in Android Canary, is expected to arrive in an upcoming Android 16 QPR3 beta.","contentLength":770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What the Recent Amazon and Microsoft Cloud Outages Taught the UK Payments Industry","url":"https://hackernoon.com/what-the-recent-amazon-and-microsoft-cloud-outages-taught-the-uk-payments-industry?source=rss","date":1764262414,"author":"Noda","guid":394,"unread":true,"content":"<article>October 2025’s AWS and Azure outages showed how dependent the UK payments sector is on a small set of cloud providers. The piece unpacks the systemic risks, regulatory concerns, and lessons from blockchain—offering a roadmap for payment firms to build resilient, cloud-agnostic, multi-provider architectures that prioritise continuity over assumed perfect uptime.</article>","contentLength":367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Saurav Kant Kumar Is Using AI to Strengthen Industries—and the Workforce","url":"https://hackernoon.com/how-saurav-kant-kumar-is-using-ai-to-strengthen-industriesand-the-workforce?source=rss","date":1764260999,"author":"Jon Stojan Journalist","guid":393,"unread":true,"content":"<article>Saurav Kant Kumar drives large-scale impact across energy, manufacturing, logistics, and telecom through AI systems that predict failures, detect defects, optimize routes, and forecast demand. His projects cut costs, reduce waste, and improve reliability. Through mentorship and team development, he also helps close the global skills gap, preparing workforces for an AI-driven future.</article>","contentLength":385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Engineering Intelligence: Visionary of Autonomous Infrastructure and Fluid Digital Evolution","url":"https://hackernoon.com/engineering-intelligence-visionary-of-autonomous-infrastructure-and-fluid-digital-evolution?source=rss","date":1764260099,"author":"Jon Stojan Journalist","guid":392,"unread":true,"content":"<article>Hardik Mahant is redefining digital infrastructure with autonomous, self-healing systems that cut manual intervention by 60% and prevent failures before they occur. His machine-learning frameworks unify observability, asset intelligence, and automated decisions, saving millions in downtime and advancing resilient, human-centered engineering across global enterprises.</article>","contentLength":369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: Everyones Using the Wrong Algebra in AI (11/27/2025)","url":"https://hackernoon.com/11-27-2025-newsletter?source=rss","date":1764259311,"author":"Noonification","guid":391,"unread":true,"content":"<p>🪐 What’s happening in tech today, November 27, 2025?</p><p>By <a href=\"https://hackernoon.com/u/janluk\">@janluk</a> [ 5 Min read ] This step-by-step guide shows you how to add mobile connectivity and create a pocket-sized network powerhouse. <a href=\"https://hackernoon.com/the-diy-5g-router-hack-that-turns-a-raspberry-pi-into-a-pocket-sized-powerhouse\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/yozololo09\">@yozololo09</a> [ 5 Min read ] How to launch on product hunt and get good results for beginner makers. <a href=\"https://hackernoon.com/how-we-went-from-0-followers-to-1-product-of-the-day-on-product-hunt-in-3-weeks\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/josecrespophd\">@josecrespophd</a> [ 9 Min read ] From Tesla phantom braking to LLM hallucinations, the root bug is first-order math. We explain how dual/jet numbers unlock scalable second-order AI. <a href=\"https://hackernoon.com/everyones-using-the-wrong-algebra-in-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/sergey-baloyan\">@sergey-baloyan</a> [ 6 Min read ] Bitcoin has experienced one of its most dramatic corrections in recent history, plummeting over&nbsp;33% from its October peak of $126,000. <a href=\"https://hackernoon.com/bitcoins-november-2025-bloodbath-dissecting-the-perfect-storm-behind-the-$42000-crash\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hackmarketing\">@hackmarketing</a> [ 6 Min read ] Get 10% off all HackerNoon services this Black Friday! Business blogging, ads, contests  more. Reach 4M+ tech readers. Limited time offer. <a href=\"https://hackernoon.com/this-black-friday-and-holiday-season-hackernoon-has-your-tech-marketing-covered\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Defense Contractors Lobby To Kill Military Right-to-Repair, Push Pay-Per-Use Data Model","url":"https://tech.slashdot.org/story/25/11/27/095202/defense-contractors-lobby-to-kill-military-right-to-repair-push-pay-per-use-data-model?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764259260,"author":"msmash","guid":193,"unread":true,"content":"A bipartisan right-to-repair provision that would let the U.S. military fix its own equipment faces a serious threat from defense industry lobbyists who want to replace it with a pay-per-use model for accessing repair information. A source familiar with negotiations told The Verge that there are significant concerns that the language in the National Defense Authorization Act will be swapped out for a \"data-as-a-service\" alternative that would require the Department of Defense to pay contractors for access to technical repair data. \n\nThe provision, introduced by Sens. Elizabeth Warren (D-MA) and Tim Sheehy (R-MT) in their Warrior Right to Repair Act, passed the Senate in October and has support from Defense Secretary Pete Hegseth, the Army and the Navy. The National Defense Industrial Association published a white paper backing the data-as-a-service model, arguing it would protect contractors' intellectual property. Reps. Mike Rogers (R-AL) and Adam Smith (D-WA), who lead the House Armed Services Committee, outlined similar language in their SPEED Act. Rogers received more than $535,000 from the defense industry in 2024; Smith received over $310,550. The final NDAA is expected early next week.","contentLength":1211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Over-Explaining Your Tech Is Killing Your Content Strategy","url":"https://hackernoon.com/why-over-explaining-your-tech-is-killing-your-content-strategy?source=rss","date":1764259208,"author":"Startups Of The Week","guid":390,"unread":true,"content":"<p>It can be a hard pill to swallow, especially for builders who are in it for the love of  the game; builders for whom the elegant coming together of infrastructure and architecture and all the technical brilliance in between is the be-all and end-all. But it is the truth: your end users don’t care what your product/service is made of. They only care what it can help them do— better, quicker, or cheaper.</p><p>And that’s the way it should be.</p><p>Society, as it operates today, is propped up by the symbiosis of trust and specialized expertise. Person A commits their life to mastering a field, say Medicine, confident that person B is doing the same with Engineering. Person A trusts that, as he can attend to Person B’s medical needs, Person B can, in turn, build and maintain the systems Person A relies on.</p><p>Your users trust you in the same way. They assume you’ve figured the tech out, and unless they’re developers or you’re building for a B2B audience, they couldn’t care less about how it works. They only care about what it means for them.</p><p>Knowing this helps you build a content strategy that speaks directly to the people you want to reach. Here are a few practical ways to do that.</p><h3>1. Translate Feature into Feeling</h3><p>Reframe all your amazing tech as a human benefit.</p><p>The smartphone market leader could’ve spent years going on about aperture sizes and sensor upgrades. Instead,  showed ‘ordinary people’ making cinematic images, with the implied message: “You can make this, too.” They signaled to potential buyers that with all of this great tech (that you probably don’t care about, or even understand), you tool could make cool-looking cinematic videos on the iPhone.</p><p>When you write about your features, save the technical details for a technical audience, and focus on what you expect your ideal users to do with said feature.</p><h3>2. Replace Explanations With Demonstrations</h3><p>In short, show, don’t tell.</p><p>Cloud storage was abstract and unfamiliar when Dropbox launched. Instead of long copy, they used a simple explainer video on their homepage. That one video reportedly lifted conversions by over 10%, drove hundreds of thousands of views in a month.</p><p>If your homepage still leads with dense copy, you’re leaving money on the table. Create one piece of “show, don’t tell” content (video, interactive demo, live product tour) and make  the hero asset. Everything else can support it.</p><h3>3. Anchor Your Messaging in Real-World Scenarios</h3><p>At this point in the article, this should be apparent.</p><p>People connect much more with things they can relate to. And Airbnb’s  campaign plugged into this.</p><p>With , Airbnb stopped selling “rooms” and started selling the scenario: <em>what it feels like to live in a place, not just pass through it</em>. The campaign leaned into words like “live,” “belong,” and “local,” and the visuals focused on real neighborhoods, hosts, and daily rituals instead of generic landmarks. Case studies highlight how the campaign repositioned Airbnb from a budget lodging alternative to an experience-driven lifestyle brand and boosted engagement and advocacy.</p><p>\\\nA content strategy built on outcomes instead of internals helps your users see your product clearly and see themselves in the story you’re telling. And if you want to scale that visibility with the same clarity these companies used, HackerNoon can help with that!</p><p><strong>Starting at only $5k, you get to:</strong></p><ul><li>Publish three evergreen content pieces on HackerNoon (with canonical tags)</li><li>Translations into 76 languages for each of the three stories</li><li>Advertise your product for a week on a targeted category</li></ul><h2>HackerNoon Startups of the Week</h2><p>Skylight Ventures funds early-stage founders building in underserved communities. They pair capital with hands-on guidance, helping entrepreneurs refine their ideas, build traction, and access opportunities that are often out of reach. Their model is simple: back mission-driven founders early and give them the support they need to grow sustainably.</p><p>Based in London, this incredible Startup was the 2nd runner-up for Startups of the Year 2024 in the <a href=\"https://hackernoon.com/startups/industry/investing?stup=66f320ba6ea4677e23deee8e\">investing industry</a>. It also received honorable mention for top 10 finishes in the <a href=\"https://hackernoon.com/startups/industry/banking?stup=66f320ba6ea4677e23deee8e\">fintech</a> and <a href=\"https://hackernoon.com/startups/industry/banking?stup=66f320ba6ea4677e23deee8e\">banking</a> industries.</p><p>Cord Comms is a UK-based communications consultancy that helps brands clarify what they’re trying to say and how they say it. From messaging and PR to content strategy and internal comms, they turn complex ideas into clear, confident communication that actually lands with the intended audience.</p><p>GeoMinds Africa blends geospatial data, remote sensing, drone technology, and AI to help organizations make informed decisions across agriculture, climate, and infrastructure. Their tools provide accurate mapping, monitoring, and predictive insights that support sustainable development across the continent.</p><p>\\\nWe’ll see you at the next one!</p>","contentLength":4824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Node.js 24 LTS Is Here—Your Backend Has No Business Being Stuck in 2022","url":"https://hackernoon.com/nodejs-24-lts-is-hereyour-backend-has-no-business-being-stuck-in-2022?source=rss","date":1764259129,"author":"Mukhtar Abdussalam","guid":389,"unread":true,"content":"<article>Node.js 24 has officially entered LTS, bringing a modern runtime, new Web APIs, and long-term support through 2028. While the upgrade exposes outdated tooling and dependencies, it offers teams a chance to modernize safely, improve performance, and align their stack with today’s JavaScript ecosystem.</article>","contentLength":302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zlib-ng 2.3.1 Released With More CPU Performance Optimizations","url":"https://www.phoronix.com/news/zlib-ng-2.3.1-Released","date":1764258722,"author":"Michael Larabel","guid":681,"unread":true,"content":"<article>Zlib-ng 2.3.1 is out today as the first stable release in the v2.3 series for this Zlib replacement that carries a variety of performance optimizations for speedier compression/decompression...</article>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NASA Reduces Flights on Boeing's Starliner After Botched Astronaut Mission","url":"https://science.slashdot.org/story/25/11/27/0856219/nasa-reduces-flights-on-boeings-starliner-after-botched-astronaut-mission?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764255660,"author":"msmash","guid":192,"unread":true,"content":"An anonymous reader shares a report: NASA has slashed the number of astronaut missions on Boeing's Starliner contract and said the spacecraft's next mission to the International Space Station will fly without a crew, reducing the scope of a program hobbled by engineering woes and outpaced by SpaceX. The most recent mishap occurred during Starliner's first crewed test flight in 2024, carrying NASA astronauts Butch Wilmore and Suni Williams. Several thrusters on Starliner's propulsion system shut down during its approach to the ISS.","contentLength":536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3 Weird Things You Can Turn Into a Memristor","url":"https://spectrum.ieee.org/memristor-materials","date":1764255601,"author":"Perri Thaler","guid":87,"unread":true,"content":"<p>Yes, you might want to plug a mushroom into your circuit</p>","contentLength":56,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjI0MzU5Ny9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5ODg4MTI2OX0.Hr_J34eJleYiqVx0v80zAQJQe0rpkLuFXm3tLD-r7pA/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"AI Can Technically Perform 12% of US Labor Market's Wage Value, MIT Simulation Finds","url":"https://news.slashdot.org/story/25/11/27/0752210/ai-can-technically-perform-12-of-us-labor-markets-wage-value-mit-simulation-finds?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764252000,"author":"msmash","guid":191,"unread":true,"content":"Researchers at MIT and Oak Ridge National Laboratory have built a simulation that models all 151 million American workers and their skills, then maps those skills against the capabilities of over 13,000 AI tools currently in production to see where the two overlap. The answer, according to their analysis: 11.7% of the US labor market's total wage value, or about $1.2 trillion, sits in tasks that AI systems can technically perform [PDF]. \n\nThe researchers call this the Iceberg Index, and the name is deliberate. The visible AI disruption happening in tech jobs right now accounts for only 2.2% of labor market wage value. The remaining exposure lurks in cognitive and administrative work across finance, healthcare administration, and professional services, and unlike tech-sector disruption, it's spread across all fifty states rather than concentrated on the coasts. \n\nDelaware and South Dakota show higher Iceberg Index values than California because their economies lean heavily on administrative and financial work. Ohio and Tennessee register modest tech-sector exposure but substantial hidden risk in the white-collar functions that support their manufacturing bases. \n\nTo validate the framework, the researchers compared their predictions against Anthropic's Economic Index tracking real-world AI usage from millions of Claude users. The two measures agreed on state categorizations 69% of the time, with particularly strong alignment at the extremes. \n\nThe Iceberg Index doesn't predict job losses or adoption timelines. It measures technical capability, the overlap between what AI can do and what occupations require. Traditional economic indicators like GDP and unemployment explain less than five percent of the variation in this skill-based exposure, which is partly why the researchers argue workforce planners need new metrics.","contentLength":1847,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"For This Engineer, Taking Deep Dives Is Part of the Job","url":"https://spectrum.ieee.org/levi-unema-underwater-robotics-exploration","date":1764248402,"author":"Edd Gent","guid":86,"unread":true,"content":"<p>Levi Unema builds, maintains, and pilots underwater robots</p>","contentLength":58,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIxODg2OS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxNjkzOTAyOH0.si_LfPWgicNE_yxugmo_6cLvN6I2T6lRTyOLf-4ZnXU/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"UK Police To Trial AI 'Agents' Responding To Non-Emergency Calls","url":"https://news.slashdot.org/story/25/11/26/2344228/uk-police-to-trial-ai-agents-responding-to-non-emergency-calls?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764248400,"author":"BeauHD","guid":190,"unread":true,"content":"An anonymous reader quotes a report from the BBC: Call-handling agents powered by AI are to be trialled by Staffordshire Police in a bid to cut waiting times for the non-emergency 101 service. The force is set to become the third in the country to take part in the scheme testing the use of artificial \"agents\" to deal with calls. Under the system, the AI agent would deal with simple queries like requests for information without the need for human involvement, freeing up call handlers and reducing answering times.\n \nActing Chief Constable Becky Riggs confirmed the force would be looking to launch the AI pilot early in the new year. \"It's a piece of technology called Agentforce. It will help with our response to the public, which historically we know we haven't done well.\" The senior officer said that sometimes people are not calling to report a crime, but want more information, which the technology could help with. However, if the system detects keywords suggesting vulnerability or risk or emergency, then it will be able to divert the call to a human being.","contentLength":1071,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Valve-Backed Color Pipeline API For Linux Is Finally Ready For Upstream","url":"https://www.phoronix.com/news/Linux-Color-Pipeline-API-Ready","date":1764244170,"author":"Michael Larabel","guid":680,"unread":true,"content":"<article>For those Linux desktop users in the US needing another reason to be thankful this Thanksgiving, a huge and long-awaited accomplishment is ready for merging to the kernel: the Color Pipeline API that is important for HDR is ready for merging! As of last night the code is queued in DRM-Misc-Next for this years-in-the-making effort...</article>","contentLength":334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GNU C Library Sees Up To 12.9x Improvement With New Generic FMA Implementation","url":"https://www.phoronix.com/news/Glibc-New-Generic-FMA","date":1764243000,"author":"Michael Larabel","guid":679,"unread":true,"content":"<article>Just a few days ago I wrote about the Glibc math code seeing a 4x improvement on AMD Zen by changing the used FMA implementation. Merged overnight was a new generic FMA implementation for the GNU C Library and now yielding up to a 12.9x throughput improvement on AMD Zen 3...</article>","contentLength":275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ubuntu 26.04 \"Resolute Raccoon\" Snapshot 1 ISOs Published","url":"https://www.phoronix.com/news/Ubuntu-26.04-Snapshot-1","date":1764241292,"author":"Michael Larabel","guid":678,"unread":true,"content":"<article>Canonical announced today the Ubuntu 26.04 \"Resolute Raccoon\" Snapshot 1 images as their first monthly ISO snapshots of the upcoming Ubuntu 26.04 LTS release...</article>","contentLength":160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Asks Indian Court to Block Antitrust Law Allowing $38 Billion Fine","url":"https://yro.slashdot.org/story/25/11/26/2352256/apple-asks-indian-court-to-block-antitrust-law-allowing-38-billion-fine?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1764237600,"author":"BeauHD","guid":189,"unread":true,"content":"Apple is challenging a new Indian antitrust law that would let regulators calculate penalties based on global revenue -- a change that could expose the company to a fine of roughly $38 billion in its dispute with Tinder owner Match. The 2022 antitrust case centers on accusations that Apple abused its power by forcing developers to use its in-app purchase system. MacRumors reports: Last year, India passed a law that allows the Competition Commission of India (CCI) to use global turnover when calculating penalties imposed on companies for abusing market dominance. Apple can be fined up to 10 percent, which would result in a penalty of around $38 billion. Apple said that using global turnover would result in a fine that's \"manifestly arbitrary, unconstitutional, grossly disproportionate, and unjust.\"\n \nApple is asking India's Delhi High Court to declare the law illegal, suggesting that penalties should be based on the Indian revenue of the specific unit that violates antitrust law. [...] Apple said in today's filing that the CCI used the new penalty law on November 10 in an unrelated case, fining a company for a violation that happened 10 years ago. Apple said it had \"no choice but to bring this constitutional challenge now\" to avoid having retrospective penalties applied against it, too. Match has argued that a high fine based on global turnover would discourage companies from repeating antitrust violations. Apple's plea will be heard on December 3.","contentLength":1471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The future will be explained to you in Palo Alto","url":"https://techcrunch.com/2025/11/26/the-future-will-be-explained-to-you-in-palo-alto/","date":1764223720,"author":"Connie Loizos","guid":294,"unread":true,"content":"<article>On Wednesday evening at PlayGround Global in Palo Alto, some very smart people who are building things you don’t understand yet will explain what’s coming. This is the final StrictlyVC event of 2025, and truly, the lineup is ridiculous. The series has traveled around the globe under the auspices of TechCrunch. Steve Case rented a […]</article>","contentLength":341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Common Desktop Environment \"CDE\" 2.5.3 Released After Two Years","url":"https://www.phoronix.com/news/CDE-2.5.3-Desktop","date":1764207800,"author":"Michael Larabel","guid":677,"unread":true,"content":"<article>Two years and one week since the prior point release, Common Desktop Environment 2.5.3 is now available as the latest iteration of this Unix desktop environment built around the Motif toolkit. CDE has been open-source for more than a decade now but its development not exactly brisk. But for those resisting the likes of Wayland and other modern display tech -- especially with KDE announcing today Plasma 6.8 will be Wayland-exclusive -- CDE 2.5.3 is now available...</article>","contentLength":468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulkan's VK_EXT_present_timing Merged After Five Years In The Making","url":"https://www.phoronix.com/news/VK_EXT_present_timing-Merged","date":1764206647,"author":"Michael Larabel","guid":676,"unread":true,"content":"<article>Originally opened in September 2020 by NVIDIA Linux engineer James Jones, tonight the Vulkan VK_EXT_present_timing extension was finally merged! Five years in the making and incorporating contributions from Google, NVIDIA, AMD, Collabora, Samsung, Unity, and Red Hat is this prominent new addition to the Vulkan API...</article>","contentLength":318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nordic founders are taking bigger swings, and it’s paying off","url":"https://techcrunch.com/video/nordic-founders-are-taking-bigger-swings-and-its-paying-off/","date":1764190800,"author":"Theresa Loconsolo","guid":293,"unread":true,"content":"<article>Ten years ago, raising €1 million in Copenhagen was enough to make waves in the region’s tech scene. Today, the Nordics are turning out billion-dollar companies like Lovable —&nbsp;which hit $200 million in revenue&nbsp;just 12 months after launching.&nbsp; Dennis Green-Lieber, founder of AI-powered customer intelligence platform Propane, has had a front-row seat to that shift […]</article>","contentLength":379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Here are the 49 US AI startups that have raised $100M or more in 2025","url":"https://techcrunch.com/2025/11/26/here-are-the-49-us-ai-startups-that-have-raised-100m-or-more-in-2025/","date":1764190138,"author":"Rebecca Szkutak","guid":292,"unread":true,"content":"<article>Last year was monumental for the AI industry in the U.S. and beyond. How will 2025 compare?</article>","contentLength":91,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"JustiGuide wants to use AI to help people navigate the US immigration system","url":"https://techcrunch.com/2025/11/26/justiguide-wants-to-use-ai-to-help-people-navigate-the-u-s-immigration-system/","date":1764189119,"author":"Lorenzo Franceschi-Bicchierai","guid":291,"unread":true,"content":"<article>The startup uses AI to help immigrants understand the system, connect with lawyers, and reduce associated costs.</article>","contentLength":112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI claims teen circumvented safety features before suicide that ChatGPT helped plan","url":"https://techcrunch.com/2025/11/26/openai-claims-teen-circumvented-safety-features-before-suicide-that-chatgpt-helped-plan/","date":1764188796,"author":"Amanda Silberling","guid":290,"unread":true,"content":"<article>In August, parents Matthew and Maria Raine sued OpenAI and its CEO, Sam Altman, over their 16-year-old son Adam's suicide, accusing the company of wrongful death. On Tuesday, OpenAI responded to the lawsuit with a filing of its own, arguing that it shouldn't be held responsible for the teenager's death.</article>","contentLength":304,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Radeon Software for Linux 25.20.3 Released - \"Exclusively Open-Source\" With RADV","url":"https://www.phoronix.com/news/Radeon-Software-25.20.3-Linux","date":1764188176,"author":"Michael Larabel","guid":675,"unread":true,"content":"<article>With the great upstream support for AMD Radeon graphics in the Linux kernel and Mesa, most desktop users / gamers / enthusiasts are best off just using the latest code shipped by their distributions or via the enthusiast-supported third-party archives/repositories. But for those on older enterprise Linux distributions, Radeon Software for Linux 25.20.3 was recently released for shipping that packaged AMD Linux graphics driver stack. This 25.20 series is the big one where they are now officially supporting the Mesa RADV Vulkan driver in place of their own former Vulkan Linux driver...</article>","contentLength":590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Redwood Materials reportedly cuts 5% of staff after $350M raise","url":"https://techcrunch.com/2025/11/26/redwood-materials-reportedly-cuts-5-of-staff-after-350m-raise/","date":1764188085,"author":"Sean O'Kane","guid":289,"unread":true,"content":"<article>The battery recycling and cathode production company, founded by former Tesla CTO JB Straubel, has started using recycled battery materials to create energy storage solutions for AI data centers.</article>","contentLength":195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatGPT: Everything you need to know about the AI-powered chatbot","url":"https://techcrunch.com/2025/11/26/chatgpt-everything-to-know-about-the-ai-chatbot/","date":1764185619,"author":"Kyle Wiggers, Cody Corrall, Kate Park, Alyssa Stringer","guid":288,"unread":true,"content":"<article>A timeline of ChatGPT product updates and releases, starting with the latest, which we’ve been updating throughout the year.</article>","contentLength":126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A comprehensive list of 2025 tech layoffs","url":"https://techcrunch.com/2025/11/26/tech-layoffs-2025-list/","date":1764185285,"author":"Cody Corrall, Alyssa Stringer, Kate Park","guid":287,"unread":true,"content":"<article>A complete list of all the known layoffs in tech, from Big Tech to startups, broken down by month throughout 2024.</article>","contentLength":114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiple London councils report disruption amid ongoing cyberattack","url":"https://techcrunch.com/2025/11/26/multiple-london-councils-report-disruption-amid-ongoing-cyberattack/","date":1764184633,"author":"Zack Whittaker","guid":286,"unread":true,"content":"<article>Three London councils reported disruption, prompting officials to shut down phone lines and networks and activate emergency plans.</article>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD ROCm 7.1.1 Released With RHEL 10.1 Support, More Models Working On RDNA4","url":"https://www.phoronix.com/news/AMD-ROCm-7.1.1","date":1764182878,"author":"Michael Larabel","guid":674,"unread":true,"content":"<article>Following the release of ROCm 7.1 from just under one month ago, ROCm 7.1.1 is now available with expanded Linux operating system support, continued Instinct MI350 series work, more large language models working on RDNA4 GPUs, and other enhancements...</article>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GM tech executive shakeup continues on software team","url":"https://techcrunch.com/2025/11/26/gm-tech-executive-shakeup-continues-on-software-team/","date":1764182718,"author":"Kirsten Korosec","guid":285,"unread":true,"content":"<article>Three of GM's top software executives have left the automaker in recent months, as the company moves to combine its disparate technology businesses. </article>","contentLength":149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breaking down the boom in the Nordic’s startup ecosystem","url":"https://techcrunch.com/2025/11/26/breaking-down-the-boom-in-the-nordics-startup-ecosystem/","date":1764179265,"author":"Dominic-Madori Davis","guid":284,"unread":true,"content":"<article>On this week’s Equity, we sat down with Dennis Green-Lieber, the founder of the AI company Propane, to talk about the booming success of the Nordic ecosystem.</article>","contentLength":160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HP plans to save millions by laying off thousands, ramping up AI use","url":"https://arstechnica.com/information-technology/2025/11/hp-plans-to-save-millions-by-laying-off-thousands-ramping-up-ai-use/","date":1764177575,"author":"Scharon Harding","guid":326,"unread":true,"content":"<p>HP Inc. said that it will lay off 4,000 to 6,000 employees in favor of AI deployments, claiming it will help save $1 billion in annualized gross run rate by the end of its fiscal 2028.</p><p>HP expects to complete the layoffs by the end of that fiscal year. The reductions will largely hit product development, internal operations, and customer support, HP CEO Enrique Lores <a href=\"https://www.fool.com/earnings/call-transcripts/2025/11/25/hp-hpq-q4-2025-earnings-call-transcript/\">said</a> during an earnings call on Tuesday.</p><p>Using AI, HP will “accelerate product innovation, improve customer satisfaction, and boost productivity,” Lores said.</p>","contentLength":529,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-987512596-1152x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"TraffickCam Uses Computer Vision to Counter Human Trafficking","url":"https://spectrum.ieee.org/traffickcam-human-trafficking-hotel-ai","date":1764177566,"author":"Perri Thaler","guid":85,"unread":true,"content":"<p>Hotels offer particular challenges for AI image recognition</p>","contentLength":59,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIzOTQ5OS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5ODAyNjc1MH0.1OGTqJdesJTFYcdymBzECJMAxOZZp2as_kw5ICfQJuU/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Musk’s xAI to build small solar farm adjacent to Colossus data center","url":"https://techcrunch.com/2025/11/26/musks-xai-to-build-small-solar-farm-adjacent-to-colossus-data-center/","date":1764177544,"author":"Tim De Chant","guid":283,"unread":true,"content":"<article>The artificial intelligence company said it was working with a developer to build a solar farm on 88 acres next to its Memphis site. Given the proposed size, the solar farm would likely produce around 30 megawatts of electricity, only about 10% of the data center’s estimated power use.</article>","contentLength":288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are you balding? There’s an AI for that","url":"https://techcrunch.com/2025/11/26/are-you-balding-theres-an-ai-for-that/","date":1764175448,"author":"Dominic-Madori Davis","guid":282,"unread":true,"content":"<article>MyHair AI examines photos of the scalp to help diagnose hair loss and suggest scientifically validated clinics and treatments.</article>","contentLength":126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bug in jury systems used by several US states exposed sensitive personal data","url":"https://techcrunch.com/2025/11/26/bug-in-jury-systems-used-by-several-us-states-exposed-sensitive-personal-data/","date":1764172814,"author":"Lorenzo Franceschi-Bicchierai","guid":281,"unread":true,"content":"<article>An easy-to-exploit vulnerability in a jury system made by Tyler Technologies exposed the personally identifiable data of jurors, including names, home addresses, emails, and phone numbers. </article>","contentLength":189,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intel Core Ultra 9 285K \"Arrow Lake\" Linux Performance Up ~9% One Year Later At ~85% Power Use","url":"https://www.phoronix.com/review/core-ultra-9-285k-2025","date":1764171900,"author":"Michael Larabel","guid":673,"unread":true,"content":"<article>It's been just over one year now since the launch of the Core Ultra 9 285K and other Arrow Lake desktop processors. For those that may be considering an Arrow Lake CPU this holiday season for a Linux desktop or just curious how the power and performance has evolved one year later, here are some leading-edge benchmarks of the Intel Core Ultra 9 285K compared to the launch-day performance last October.</article>","contentLength":403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Crypto hoarders dump tokens as shares tumble","url":"https://arstechnica.com/information-technology/2025/11/crypto-hoarders-dump-tokens-as-shares-tumble/","date":1764171467,"author":"Nikou Asgari in London and Jill R Shah in New York","guid":325,"unread":true,"content":"<p>Crypto-hoarding companies are ditching their holdings in a bid to prop up their sinking share prices, as the craze for “digital asset treasury” businesses unravels in the face of a $1 trillion&nbsp;cryptocurrency rout.</p><p>Shares in Michael Saylor-led Strategy, the world’s biggest corporate bitcoin holder, have tumbled 50 percent over the past three months, dragging down scores of copycat companies.</p><p>About $77 billion&nbsp;has been wiped from the stock market value of these companies, which raise debt and equity to fund purchases of crypto, since their peak of $176 billion&nbsp;in July, according to industry data publication The Block.</p>","contentLength":630,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-1368998806-1152x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"KDE Plasma 6.8 Will Go Wayland-Exclusive In Dropping X11 Session Support","url":"https://www.phoronix.com/news/KDE-Plasma-68-Wayland-Exclusive","date":1764171251,"author":"Michael Larabel","guid":672,"unread":true,"content":"<article>KDE developers announced they are going \"all-in on a Wayland future\" and with the Plasma 6.8 desktop it will become Wayland-exclusive. The Plasma X11 session is going away...</article>","contentLength":174,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fedora SIG Proposed To Improve Production Stability","url":"https://www.phoronix.com/news/Fedora-SIG-Production-Stability","date":1764167071,"author":"Michael Larabel","guid":671,"unread":true,"content":"<article>A Fedora special interest group is being proposed to help improve production stability of Fedora Linux and better handling incident management when problems do arise...</article>","contentLength":168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event Sensors Bring Just the Right Data to Device Makers","url":"https://spectrum.ieee.org/event-sensors-to-the-edge","date":1764165602,"author":"Christoph Posch","guid":84,"unread":true,"content":"<p>They’re ultraefficient because they detect only change and motion</p>","contentLength":67,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MjIzNjM0Mi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2NjIzMjQ5MH0.jEYHdpc8Y6zblqWAVBlvBWPfxytt069uPckiM4OmB9s/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Urgent ACPI Revert For Linux 6.18 To Deal With Some Hardware Crashing","url":"https://www.phoronix.com/news/Urgent-ACPI-Revert-Linux-6.18","date":1764163900,"author":"Michael Larabel","guid":670,"unread":true,"content":"<article>The Linux 6.18 kernel is anticipated for release this coming Sunday while this week a last-minute crisis was averted following reports of a kernel crash from recent ACPI code changes...</article>","contentLength":185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Onton raises $7.5M to expand its AI-powered shopping site beyond furniture","url":"https://techcrunch.com/2025/11/26/onton-raises-7-5m-to-expand-its-ai-powered-shopping-site-beyond-furniture/","date":1764158400,"author":"Ivan Mehta","guid":280,"unread":true,"content":"<article>Onton, which changed its name from Deft, is giving users an infinite canvas with AI-powered image generation to nudge them toward making quicker purchase decisions. </article>","contentLength":165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NVIDIA Is Interested In Helping Bring Vulkan Video To Chrome","url":"https://www.phoronix.com/news/NVIDIA-Vulkan-Video-Chrome-Help","date":1764156845,"author":"Michael Larabel","guid":669,"unread":true,"content":"<article>NVIDIA engineers are interested in helping Google bring Vulkan Video accelerated GPU video decoding to the Chrome/Chromium web browser...</article>","contentLength":137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.19 Overhauling The Intel TDX Locking Code For KVM","url":"https://www.phoronix.com/news/Linux-6.19-Fixing-Intel-TDX-KVM","date":1764156037,"author":"Michael Larabel","guid":668,"unread":true,"content":"<article>Sean Christopherson of Google sent out the pull requests to the KVM tree of the various x86_64-related areas of virtualization he oversees. With these updates ahead of the Linux 6.19 merge window there is a significant overhaul of Intel's Trust Domain Extensions (TDX) code to address various outstanding problems...</article>","contentLength":316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rocky Linux 10.1 Released As Community Alternative To RHEL 10.1","url":"https://www.phoronix.com/news/Rocky-Linux-10.1-Released","date":1764155399,"author":"Michael Larabel","guid":667,"unread":true,"content":"<article>Following the release of Red Hat Enterprise Linux 10.1 earlier this month, Rocky Linux 10.1 is now available for this popular community-driven alternative to RHEL 10.1...</article>","contentLength":170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["tech"]}