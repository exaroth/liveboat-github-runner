{"id":"82kPqomaPXmNomrHzpZWfbkQxiiNUBTAYKxHR5qZBEpf","title":"Hacker News: Show HN","displayTitle":"HN Show","url":"https://hnrss.org/show?points=60","feedLink":"https://news.ycombinator.com/shownew","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":18,"items":[{"title":"Show HN: Clyp – Clipboard Manager for Linux","url":"https://github.com/murat-cileli/clyp","date":1755878606,"author":"timeoperator","guid":183,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44986205"},{"title":"Show HN: Splice – CAD for Cable Harnesses and Electrical Assemblies","url":"https://splice-cad.com/","date":1755810634,"author":"djsdjs","guid":182,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44978140"},{"title":"Show HN: ChartDB Cloud – Visualize and Share Database Diagrams","url":"https://app.chartdb.io/","date":1755781271,"author":"Jonathanfishner","guid":181,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44972238"},{"title":"Show HN: Using Common Lisp from Inside the Browser","url":"https://turtleware.eu/posts/Using-Common-Lisp-from-inside-the-Browser.html","date":1755778110,"author":"jackdaniel","guid":180,"unread":true,"content":"<p> Written on 2025-08-21 by Daniel Kochmański </p><p>Web Embeddable Common Lisp is a project that brings Common Lisp and the Web\nBrowser environments together. In this post I'll outline the current progress of\nthe project and provide some technical details, including current caveats and\nfuture plans.</p><p>It is important to note that this is not a release and none of the described\nAPIs and functionalities is considered to be stable. Things are still changing\nand I'm not accepting bug reports for the time being.</p><p>The easiest way to use Common Lisp on a website is to include WECL and insert\nscript tags with a type \"text/common-lisp\". When the attribute src is present,\nthen first the runtime loads the script from that url, and then it executes the\nnode body. For example create and run this HTML document from localhost:</p><pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Web Embeddable Common Lisp&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://turtleware.eu/static/misc/wecl-20250821/easy.css\" /&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/boot.js\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/wecl.js\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/easy.lisp\" id='easy-script'&gt;\n(defvar *div* (make-element \"div\" :id \"my-ticker\"))\n(append-child [body] *div*)\n\n(dotimes (v 4)\n  (push-counter v))\n\n(loop for tic from 6 above 0\n      do (replace-children *div* (make-paragraph \"~a\" tic))\n         (js-sleep 1000)\n      finally (replace-children *div* (make-paragraph \"BOOM!\")))\n\n(show-script-text \"easy-script\")\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>We may use Common Lisp that can call to JavaScript, and register callbacks to be\ncalled on specified events. The source code of the script can be found here:</p><p>Because the runtime is included as a script, the browser will usually cache the\n~10MB WebAssembly module.</p><p>The initial foreign function interface has numerous macros defining wrappers\nthat may be used from Common Lisp or passed to JavaScript.</p><p>Summary of currently available operators:</p><ul><li> an inlined expression, like </li><li> an object referenced from the object store</li><li> a function</li><li> a method of the argument, like </li><li> a slot reader of the argument</li><li> a slot writer of the first argument</li><li> combines define-js-getter and define-js-setter</li><li> template for JavaScript expressions</li><li> Common Lisp function reference callable from JavaScript</li><li> anonymous Common Lisp function reference (for closures)</li></ul><p>Summary of argument types:</p><table border=\"2\" cellspacing=\"0\" cellpadding=\"6\" rules=\"groups\" frame=\"hsides\"><thead><tr></tr></thead><tbody><tr><td>Common Lisp object reference</td></tr><tr><td>JavaScript object reference</td></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>All operators, except for  have a similar lambda list:</p><blockquote><p>(DEFINE-JS NAME-AND-OPTIONS [ARGUMENTS [,@BODY]])</p></blockquote><p>The first argument is a list  that is common to all\ndefining operators:</p><ul><li> Common Lisp symbol denoting the object</li><li> a string denoting the JavaScript expression, i.e \"innerText\"</li><li> a type of the object returned by executing the expression</li></ul><pre><code>(define-js-variable ([document] :js-expr \"document\" :type :symbol))\n;; document\n(define-js-object ([body] :js-expr \"document.body\" :type :js-ref))\n;; wecl_ensure_object(document.body) /* -&gt; id   */\n;; wecl_search_object(id)            /* -&gt; node */\n</code></pre><p>The difference between a variable and an object in JS-FFI is that variable\nexpression is executed each time when the object is used (the expression is\ninlined), while the object expression is executed only once and the result is\nstored in the object store.</p><p>The second argument is a list of pairs . Names will be used in the\nlambda list of the operator callable from Common Lisp, while types will be used\nto coerce arguments to the type expected by JavaScript.</p><pre><code>(define-js-function (parse-float :js-expr \"parseFloat\" :type :js-ref)\n    ((value :string)))\n;; parseFloat(value)\n\n(define-js-method (add-event-listener :js-expr \"addEventListener\" :type :null)\n    ((self :js-ref)\n     (name :string)\n     (fun :js-ref)))\n;; self.addEventListener(name, fun)\n\n(define-js-getter (get-inner-text :js-expr \"innerText\" :type :string)\n    ((self :js-ref)))\n;; self.innerText\n\n(define-js-setter (set-inner-text :js-expr \"innerText\" :type :string)\n    ((self :js-ref)\n     (new :string)))\n;; self.innerText = new\n\n(define-js-accessor (inner-text :js-expr \"innerText\" :type :string)\n    ((self :js-ref)\n     (new :string)))\n;; self.innerText\n;; self.innerText = new\n\n(define-js-script (document :js-expr \"~a.forEach(~a)\" :type :js-ref)\n    ((nodes :js-ref)\n     (callb :object)))\n;; nodes.forEach(callb)\n</code></pre><p>The third argument is specific to callbacks, where we define Common Lisp body of\nthe callback. Argument types are used to coerce values from JavaScript to Common\nLisp.</p><pre><code>(define-js-callback (print-node :type :object)\n    ((elt :js-ref)\n     (nth :fixnum)\n     (seq :js-ref))\n  (format t \"Node ~2d: ~a~%\" nth elt))\n\n(let ((start 0))\n  (add-event-listener *my-elt* \"click\"\n                      (lambda-js-callback :null ((event :js-ref)) ;closure!\n                        (incf start)\n                        (setf (inner-text *my-elt*)\n                              (format nil \"Hello World! ~a\" start)))\n</code></pre><p>Note that callbacks are a bit different, because  does not\naccept  option and  has unique lambda list. It is\nimportant for callbacks to have an exact arity as they are called with, because\nJS-FFI does not implement variable number of arguments yet.</p><p>Callbacks can be referred by name with an operator .</p><p>While working on FFI I've decided to write an adapter for SLIME/SWANK that will\nallow interacting with WECL from Emacs. The principle is simple: we connect with\na websocket to Emacs that is listening on the specified port (i.e on localhost).\nThis adapter uses the library  written by Andrew Hyatt.</p><p>It allows for compiling individual forms with , but file compilation\ndoes not work (because files reside on a different \"host\"). REPL interaction\nworks as expected, as well as SLDB. The connection may occasionally be unstable,\nand until Common Lisp call returns, the whole page is blocked. Notably waiting\nfor new requests is not a blocking operation from the JavaScript perspective,\nbecause it is an asynchronous operation.</p><pre><code>;;; Patches for SLIME 2.31 (to be removed after the patch is merged).\n;;; It is assumed that SLIME is already loaded into Emacs.\n(defun slime-net-send (sexp proc)\n  \"Send a SEXP to Lisp over the socket PROC.\nThis is the lowest level of communication. The sexp will be READ and\nEVAL'd by Lisp.\"\n  (let* ((payload (encode-coding-string\n                   (concat (slime-prin1-to-string sexp) \"\\n\")\n                   'utf-8-unix))\n         (string (concat (slime-net-encode-length (length payload))\n                         payload))\n         (websocket (process-get proc :websocket)))\n    (slime-log-event sexp)\n    (if websocket\n        (websocket-send-text websocket string)\n      (process-send-string proc string))))\n\n(defun slime-use-sigint-for-interrupt (&amp;optional connection)\n  (let ((c (or connection (slime-connection))))\n    (cl-ecase (slime-communication-style c)\n      ((:fd-handler nil) t)\n      ((:spawn :sigio :async) nil))))\n</code></pre><pre><code>;;; lime.el --- Lisp Interaction Mode for Emacs -*-lexical-binding:t-*-\n;;; \n;;; This program extends SLIME with an ability to listen for lisp connections.\n;;; The flow is reversed - normally SLIME is a client and SWANK is a server.\n\n(require 'websocket)\n\n(defvar *lime-server* nil\n  \"The LIME server.\")\n\n(cl-defun lime-zipit (obj &amp;optional (start 0) (end 72))\n  (let* ((msg (if (stringp obj)\n                  obj\n                (slime-prin1-to-string obj)))\n         (len (length msg)))\n    (substring msg (min start len) (min end len))))\n\n(cl-defun lime-message (&amp;rest args)\n  (with-current-buffer (process-buffer *lime-server*)\n    (goto-char (point-max))\n    (dolist (arg args)\n      (insert (lime-zipit arg)))\n    (insert \"\\n\")\n    (goto-char (point-max))))\n\n(cl-defun lime-client-process (client)\n  (websocket-conn client))\n\n(cl-defun lime-process-client (process)\n  (process-get process :websocket))\n\n;;; c.f slime-net-connect\n(cl-defun lime-add-client (client)\n  (lime-message \"LIME connecting a new client\")\n  (let* ((process (websocket-conn client))\n         (buffer (generate-new-buffer \"*lime-connection*\")))\n    (set-process-buffer process buffer)\n    (push process slime-net-processes)\n    (slime-setup-connection process)\n    client))\n\n;;; When SLIME kills the process, then it invokes LIME-DISCONNECT hook.\n;;; When SWANK kills the process, then it invokes LIME-DEL-CLIENT hook.\n(cl-defun lime-del-client (client)\n  (when-let ((process (lime-client-process client)))\n    (lime-message \"LIME client disconnected\")\n    (slime-net-sentinel process \"closed by peer\")))\n\n(cl-defun lime-disconnect (process)\n  (when-let ((client (lime-process-client process)))\n    (lime-message \"LIME disconnecting client\")\n    (websocket-close client)))\n\n(cl-defun lime-on-error (client fun error)\n  (ignore client fun)\n  (lime-message \"LIME error: \" (slime-prin1-to-string error)))\n\n;;; Client sends the result over a websocket. Handling responses is implemented\n;;; by SLIME-NET-FILTER. As we can see, the flow is reversed in our case.\n(cl-defun lime-handle-message (client frame)\n  (let ((process (lime-client-process client))\n        (data (websocket-frame-text frame)))\n    (lime-message \"LIME-RECV: \" data)\n    (slime-net-filter process data)))\n\n(cl-defun lime-net-listen (host port &amp;rest parameters)\n  (when *lime-server*\n    (error \"LIME server has already started\"))\n  (setq *lime-server*\n        (apply 'websocket-server port\n               :host host\n               :on-open    (function lime-add-client)\n               :on-close   (function lime-del-client)\n               :on-error   (function lime-on-error)\n               :on-message (function lime-handle-message)\n               parameters))\n  (unless (memq 'lime-disconnect slime-net-process-close-hooks)\n    (push 'lime-disconnect slime-net-process-close-hooks))\n  (let ((buf (get-buffer-create \"*lime-server*\")))\n    (set-process-buffer *lime-server* buf)\n    (lime-message \"Welcome \" *lime-server* \"!\")\n    t))\n\n(cl-defun lime-stop ()\n  (when *lime-server*\n   (websocket-server-close *lime-server*)\n   (setq *lime-server* nil)))\n</code></pre><p>After loading this file into Emacs invoke <code>(lime-net-listen \"localhost\" 8889)</code>.\nNow our Emacs listens for new connections from SLUG (the lisp-side part adapting\nSWANK, already bundled with WECL). There are two SLUG backends in a repository:</p><ul><li> for web browser environment</li><li> for Common Lisp runtime (uses )</li></ul><p>Now you can open a page listed here and connect to SLIME:</p><pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Web Embeddable Common Lisp&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"easy.css\" /&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/boot.js\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/wecl.js\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/slug.lisp\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/wank.lisp\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/easy.lisp\"&gt;\n      (defvar *connect-button* (make-element \"button\" :text \"Connect\"))\n      (define-js-callback (connect-to-slug :type :null) ((event :js-ref))\n        (wank-connect \"localhost\" 8889)\n        (setf (inner-text *connect-button*) \"Crash!\"))\n      (add-event-listener *connect-button* \"click\" (js-callback connect-to-slug))\n      (append-child [body] *connect-button*)\n    &lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>This example shows an important limitation –  does not allow for\nmultiple asynchronous contexts in the same thread. That means that if Lisp call\ndoesn't return (i.e because it waits for input in a loop), then we can't execute\nother Common Lisp statements from elsewhere because the application will crash.</p><p>Here's another example. It is more a cool gimmick than anything else, but let's\ntry it. Open a console on this very website (on firefox C-S-i) and execute:</p><pre><code>function inject_js(url) {\n    var head = document.getElementsByTagName('head')[0];\n    var script = document.createElement('script');\n    head.appendChild(script);\n    script.type = 'text/javascript';\n    return new Promise((resolve) =&gt; {\n        script.onload = resolve;\n        script.src = url;\n    });\n}\n\nfunction inject_cl() {\n    wecl_eval('(wecl/impl::js-load-slug \"https://turtleware.eu/static/misc/wecl-20250821\")');\n}\n\ninject_js('https://turtleware.eu/static/misc/wecl-20250821/boot.js')\n    .then(() =&gt; {\n        wecl_init_hooks.push(inject_cl);\n        inject_js('https://turtleware.eu/static/misc/wecl-20250821/wecl.js');\n    });\n</code></pre><p>With this, assuming that you've kept your LIME server open, you'll have a REPL\nonto uncooperative website. Now we can fool around with queries and changes:</p><pre><code>(define-js-accessor (title :js-expr \"title\" :type :string)\n  ((self :js-ref)\n   (title :string)))\n\n(define-js-accessor (background :js-expr \"body.style.backgroundColor\" :type :string)\n  ((self :js-ref)\n   (background :string)))\n\n(setf (title [document]) \"Write in Lisp!\")\n(setf (background [document]) \"#aaffaa\")\n</code></pre><p>The first thing to address is the lack of threading primitives. Native threads\ncan be implemented with web workers, but then our GC wouldn't know how to stop\nthe world to clean up. Another option is to use cooperative threads, but that\nalso won't work, because Emscripten doesn't support independent asynchronous\ncontexts, nor ECL is ready for that yet.</p><p>I plan to address both issues simultaneously in the second stage of the project\nwhen I port the runtime to WASI. We'll be able to use browser's GC, so running\nin multiple web workers should not be a problem anymore. Unwinding and rewinding\nthe stack will require tinkering with ASYNCIFY and I have somewhat working green\nthreads implementation in place, so I will finish it and upstream in ECL.</p><p>Currently I'm focusing mostly on having things working, so JS and CL interop is\nbrittle and often relies on evaluating expressions, trampolining and coercing.\nThat impacts the performance in a significant way. Moreover all loaded scripts\nare compiled with a one-pass compiler, so the result bytecode is not optimized.</p><p>There is no support for loading cross-compiled files onto the runtime, not to\nmention that it is not possible to precompile systems with ASDF definitions.</p><p>JS-FFI requires more work to allow for defining functions with variable number\nof arguments and with optional arguments. There is no dynamic coercion of\nJavaScript exceptions to Common Lisp conditions, but it is planned.</p>","contentLength":14560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971744"},{"title":"Show HN: I replaced vector databases with Git for AI memory (PoC)","url":"https://github.com/Growth-Kinetics/DiffMem","date":1755757211,"author":"alexmrv","guid":179,"unread":true,"content":"<p>Hey HN! I built a proof-of-concept for AI memory using Git instead of vector databases.</p><p>The insight: Git already solved versioned document management. Why are we building complex vector stores when we could just use markdown files with Git's built-in diff/blame/history?</p><p>Memories stored as markdown files in a Git repo\nEach conversation = one commit\ngit diff shows how understanding evolves over time\nBM25 for search (no embeddings needed)\nLLMs generate search queries from conversation context\nExample: Ask \"how has my project evolved?\" and it uses git diff to show actual changes in understanding, not just similarity scores.</p><p>This is very much a PoC - rough edges everywhere, not production ready. But it's been working surprisingly well for personal use. The entire index for a year of conversations fits in ~100MB RAM with sub-second retrieval.</p><p>The cool part: You can git checkout to any point in time and see exactly what the AI knew then. Perfect reproducibility, human-readable storage, and you can manually edit memories if needed.</p><p>Stack: Python, GitPython, rank-bm25, OpenRouter for LLM orchestration. MIT licensed.</p><p>Would love feedback on the approach. Is this crazy or clever? What am I missing that will bite me later?</p>","contentLength":1223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44969622"},{"title":"Show HN: PlutoPrint – Generate PDFs and PNGs from HTML with Python","url":"https://github.com/plutoprint/plutoprint","date":1755722278,"author":"sammycage","guid":178,"unread":true,"content":"<p>Hi everyone, I built PlutoPrint because I needed a simple way to generate beautiful PDFs and images directly from HTML with Python. Most of the tools I tried felt heavy, tricky to set up, or produced results that didn’t look great, so I wanted something lightweight, modern, and fast. PlutoPrint is built on top of PlutoBook’s rendering engine, which is designed for paged media, and then wrapped with a Python API that makes it easy to turn HTML or XML into crisp PDFs and PNGs. I’ve used it for things like invoices, reports, tickets, and even snapshots, and it can also integrate with Matplotlib to render charts directly into documents.</p><p>I’d be glad to hear what you think. If you’ve ever had to wrestle with generating PDFs or images from HTML, I hope this feels like a smoother option. Feedback, ideas, or even just impressions are all very welcome, and I’d love to learn how PlutoPrint could be more useful for you.</p>","contentLength":932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44966170"},{"title":"Show HN: Anchor Relay – A faster, easier way to get Let's Encrypt certificates","url":"https://anchor.dev/relay","date":1755706398,"author":"geemus","guid":177,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963226"},{"title":"Show HN: Luminal – Open-source, search-based GPU compiler","url":"https://github.com/luminal-ai/luminal","date":1755705673,"author":"jafioti","guid":176,"unread":true,"content":"<p>Hi HN, I’m Joe. My friends Matthew, Jake and I are building Luminal (<a href=\"https://luminalai.com/\">https://luminalai.com/</a>), a GPU compiler for automatically generating fast GPU kernels for AI models. It uses search-based compilation to achieve high performance.</p><p>We take high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.</p><p>You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: <a href=\"https://youtu.be/P2oNR8zxSAA\" rel=\"nofollow\">https://youtu.be/P2oNR8zxSAA</a></p><p>Our approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.</p><p>We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.</p><p>We aim to radically simplify the ML ecosystem while improving performance and hardware utilization. Please check out our repo: <a href=\"https://github.com/luminal-ai/luminal\" rel=\"nofollow\">https://github.com/luminal-ai/luminal</a> and I’d love to hear your thoughts!</p>","contentLength":1654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963135"},{"title":"Show HN: What country you would hit if you went straight where you're pointing","url":"https://apps.apple.com/us/app/leascope/id6608979884","date":1755703381,"author":"brgross","guid":175,"unread":true,"content":"<p>\n    The developer, , indicated that the app’s privacy practices may include handling of data as described below. For more information, see the <a href=\"https://gist.github.com/poopmanchu/a7d61f69ff9aeb3a0a37c61beb3962fd\">developer’s privacy policy</a>.\n  </p><div><div><p>The developer does not collect any data from this app.</p></div></div><p>Privacy practices may vary, for example, based on the features you use or your age. <a href=\"https://apps.apple.com/story/id1538632801\">Learn&nbsp;More</a></p>","contentLength":327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962767"},{"title":"Show HN: Pinch – macOS voice translation for real-time conversations","url":"https://www.startpinch.com/","date":1755691858,"author":"christiansafka","guid":173,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44961153"},{"title":"Show HN: Project management system for Claude Code","url":"https://github.com/automazeio/ccpm","date":1755685932,"author":"aroussi","guid":172,"unread":true,"content":"<p>I built a lightweight project management workflow to keep AI-driven development organized.</p><p>The problem was that context kept disappearing between tasks. With multiple Claude agents running in parallel, I’d lose track of specs, dependencies, and history. External PM tools didn’t help because syncing them with repos always created friction.</p><p>The solution was to treat GitHub Issues as the database. The \"system\" is ~50 bash scripts and markdown configs that:</p><p>- Brainstorm with you to create a markdown PRD, spins up an epic, and decomposes it into tasks and syncs them with GitHub issues\n- Track progress across parallel streams\n- Keep everything traceable back to the original spec\n- Run fast from the CLI (commands finish in seconds)</p><p>It’s still early and rough around the edges, but has worked well for us. I’d love feedback from others experimenting with GitHub-centric project management or AI-driven workflows.</p>","contentLength":918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44960594"},{"title":"Show HN: OpenAI/reflect – Physical AI Assistant that illuminates your life","url":"https://github.com/openai/openai-reflect","date":1755632918,"author":"Sean-Der","guid":171,"unread":true,"content":"<p>I have been working on making WebRTC + Embedded Devices easier for a few years. This is a hackathon project that pulled some of that together. I hope others build on it/it inspires them to play with hardware. I worked on it with two other people and I had a lot of fun with some of the ideas that came out of it.</p><p>* Extendable/hackable - I tried to keep the code as simple as possible so others can fork/modify easily.</p><p>* Communicate with light. With function calling it changes the light bulb, so it can match your mood or feelings.</p><p>* Populate info from clients you control. I wanted to experiment with having it guide you through yesterday/today.</p><p>* Phone as control. Setting up new devices can be frustrating. I liked that this didn't require any WiFi setup, it just routed everything through your phone. Also cool then that they device doesn't actually have any sensitive data on it.</p>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44955576"},{"title":"Show HN: Fractional jobs – part-time roles for engineers","url":"https://www.fractionaljobs.io/","date":1755551439,"author":"tbird24","guid":170,"unread":true,"content":"<div>Fractional work is part-time work, typically paid on a monthly retainer, by experts in their field. In this post we’ll break down exactly what fractional work is in more detail, and why it’s exploding in popularity.</div>","contentLength":219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44945379"},{"title":"Show HN: Strix - Open-source AI hackers for your apps","url":"https://github.com/usestrix/strix","date":1755549793,"author":"ahmedallam2","guid":169,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44945113"},{"title":"Show HN: I built a toy TPU that can do inference and training on the XOR problem","url":"https://www.tinytpu.com/","date":1755546734,"author":"evxxan","guid":168,"unread":true,"content":"<p>When we started this project, all we knew was that the equation y = mx + b is the foundational building block for neural networks. However, we needed to fully UNDERSTAND the math behind neural networks to build other modules in our TPU. So before we started writing any code, each of us worked out the math of a simple 2 -&gt; 2 -&gt; 1 multi-layer perceptron (MLP).</p><p>The reason we chose this specific network is because we were targeting inference and training for the XOR problem (the \"hello world\" of neural networks). The XOR problem is one of the simplest problems a neural network can solve. All other gates (AND, OR, etc) can predict the outputs from its inputs using just one linear line (one neuron) to separate which inputs correspond to a 0 and which ones correspond to a 1. But to classify all XOR, an MLP is needed, since it requires curved decision boundaries, which can't be achieved with ONLY linear equations. For a geometric and first-principles treatment, the free book<a href=\"https://udlbook.github.io/udlbook/\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding Deep Learning</a>is excellent.</p><p>Now, say we want to do continuous inference (i.e. self driving car making multiple predictions a second). That would imply that we're sending multiple pieces of data at once. Since data is inherently multidimensional and has many features, we would have matrices with very large dimensions. However, the XOR problem simplifies the dimensions for us, as there are only two features (0 or 1) and 4 possible pieces of input data (four possible binary combinations of 0 and 1). This gives us a 4x2 matrix, where 4 is the number of rows (batch size) and 2 is the number of columns (feature size).</p><div><p>The XOR input matrix and target outputs:</p><p>Each row represents one of the four possible XOR inputs, and the output vector shows the expected XOR results</p></div><p>Another simplification we're making for our systolic array example here is that we'll use a 2x2 instead of the 256x256 array used in the TPUv1. However, the math is still faithful so nothing is actually dumbed down, rather scaled down instead.</p><p>The first step in the equation is multiplying m with x, which, in matrix form, would be.</p><div><p>whereis our input matrix,is our weight matrix, andis our bias vector</p></div><p>How can we perform matrix multiplication in hardware? Well, we can use a unit called the systolic array!</p><p>The heart of a TPU is a unit called the systolic array.It consists of individual building blocks called Processing Elements (PE) which are connected together in a grid-like structure. Each PE performs a multiply-accumulate operation, meaning it multiplies an incoming input X with a stationary weight Wand adds it to an incoming accumulated sum, all in the same clock cycle.</p><div><pre><code> @ or ;\n            ;\n            ;\n        ;\n        ;\n            ;\n        </code></pre></div><h3>Systolic matrix multiplication</h3><p>When these PEs are connected together, they can be used to perform matrix multiplication systolically, meaning multiple elements of the output matrix can be calculated every clock cycle. The inputs enter the systolic array from the left and move to the neighbouring PE to the right, every clock cycle. The accumulated sums start with the multiplication output from the first row of PEs, move downwards, and get added to the products of each successive PE, until they up at the last row of PEs where they become an element of the output matrix.</p><p>Because of this single unit (and the fact that matrix multiplications dominate the computations performed in models), TPUs can very easily inference and train any model.</p><p>Now let's walk through the example of our XOR problem:</p><p>Our systolic array takes two inputs: the input matrix and the weight matrix. For our XOR network, we initialize with the following weights and biases:</p><h3>Input and weight scheduling</h3><p>To input our input batch within the systolic array, we need to:</p><p>To input our weight matrix: we need to:</p><p>Note that the rotating and staggering don't have any mathematical significance — they are simply required to make the systolic array work. The transpoing too is just for mathematical bookkeeping – it's required to make the matrix math work because of how we set up our weight pointers within the neural network drawing.</p><p>To perform the staggering, we designed near-identical accumulators for the weights and inputs that would sit above and to the left of the systolic array, respectively.</p><p>Since the activations are fed into the systolic array one-by-one, we thought a first-in-first-out queue (FIFO) would be the optimal data storage option. There was a slight difference between a traditional FIFO and the accumulators we built, however. Our accumulators had 2 input ports — one for writing weights manually to the FIFO and one for writing the previous layer's outputs from the activation modules BACK into the input FIFOs (the previous layer's outputs are inputs for the current layer).</p><p>We also needed to load the weights in a similar fashion for every layer, so we replicated the logic for the weight FIFOs, without the second port.</p><p>The next step in the equation is adding the bias. To do this in hardware, we need to create a bias module under each column of the systolic array. We can see that as the sums move out of the last row within the systolic array, we can immediately stream them into our bias modules to compute our pre-activations.<b> We will denote these values with the variable Z.</b></p><div><p>The bias vector  is broadcast across all rows of the matrix — meaning it's added to each row of </p></div><p>Now our equation is starting to look a lot like what we've learned in high school –but just in multidimensional form, where each column that streams out of the systolic array represents its own feature!</p><p>Next we have to apply the activation, for which we chose Leaky ReLU.This is also an element-wise operation, similar to the bias, meaning we need an activation module under every bias module (and by proxy under every column of the systolic array) and we can stream the outputs of our bias modules into the activation modules immediately.<b>We will denote these post-activation values with H</b>.</p><div><p>The Leaky ReLU function applies element-wise:</p><p>where  is our leak factor. For matrices, this applies to each element independently.</p></div><div><p>For our XOR example, let's see how Layer 1 processes the data. First, the systolic array computes:</p><p>Finally, LeakyReLU is applied element-wise:</p><p>Negative values are multiplied by 0.5, positive values pass through unchanged.</p></div><p>Now you might be asking – why don't we merge the bias term and the activation term in one clock cycle? Well, this is because of something called pipelining! Pipelining allows multiple operations to be executed simultaneously across different stages of the TPU —instead of waiting for one complete operation to finish before starting the next, you break the work into stages that can overlap. Think of it like an assembly line: while one worker (activation module) processes a part, the previous worker (bias module) is already working on the next part. This keeps all of the modules busy rather than having them sit idle waiting for the previous stage to complete. It also affects the speed at which we can run our TPU — if we have one module that tries to squeeze many operations in a single cycle, our clock speed will be bottlenecked by that module, as the other modules can only run as fast as that single module. Therefore, it's efficient and best practice to split up operations into individual clock cycles as much as possible.</p><p>Another mechanism we used to run our chip as efficiently as possible, was a propagating \"start\" signal, which we called a travelling chip enable (denoted by the purple dot). Because everything in our design was staggered, we realized that we could very elegantly assert a start signal for a single clock cycle at the first accumulator and have it propagate to neighbouring modules exactly when they needed to be turned on.</p><p>This would extend into the systolic array and eventually the bias and activation modules, where neighbouring PEs and modules, moving from the top left to the bottom right, were turned on in consecutive clock cycles. This ensured that every module was only performing computations when it was required to and wasn't wasting power in the background.</p><p>Now, we know that starting a new layer means we must compute the same  using a new weight matrix. How can we do this if our systolic array is weight-stationary? How can we change the weights?</p><p>While thinking about this problem, we came across the idea of double buffering, which originates from video games. The reason why double buffering exists is to prevent something called screen tearing on your monitor. Ultimately, pixels take time to load and we'd like to \"hide away\" that time somehow. And if you paid attention, this is the exact same problem we're currently facing with the systolic array. Fortunately, video game designers have already come up with a solution for this problem. By adding a second \"shadow\" buffer, which holds the weights of the next layer while the current layer is being computed on, we can load in new weights during computation, cutting the total clock cycle count in half.</p><p>To make this work, we also needed to add some signals to move the data. First, we needed a signal to indicate when to switch the weights in the shadow buffer and the active buffer. We called this signal the \"switch\" signal (denoted by the blue dot) and it copied the values in the shadow buffer to the active buffer. It propagated from the top left of the systolic array to the bottom right (the same path as the travelling chip enable, but only within the systolic array). We then needed one more signal to indicate when we wanted to move the weights down by one row and we called this the \"accept\" flag (denoted by the green dot) because each row is ACCEPTING a new set of weights. This would move the new weights into the top row of the systolic array, as well as each row of weights down into the next row of the systolic array. These two control flags worked in tandem to make our double buffering mechanism work.</p><p>If you haven't already noticed, this allows the systolic array to do something powerful…continuous inference!!! We can continuously stream in new weights and inputs and compute forward pass for as many layers as we want. This touches into a core design philosophy of the systolic array: we want to maximize PE usage.<b>We always want to keep the systolic array fed!</b></p><div><p>For Layer 2, the outputs from Layer 1 () now become our inputs:</p><p>Adding bias and applying activation:</p><p>All values are positive, so they pass through unchanged. These are our final predictions for the XOR problem!</p></div><p>Our final step for inference was making a control unit to use a custom instruction set (ISA) to assert all of our control flags and load data through a data bus. Including the data bus, our ISA was 24 bits long and it made our testbench more elegant as we could pass a single string of bits every clock cycle, rather than individually setting multiple flags.</p><p>We then put everything together and got inference completely working! This was a big milestone for us and we were very proud about what we had accomplished.</p><h2>Backpropagation and training</h2><div><p>Ok we've solved inference — but what about training? Well here's the beauty: We can use the same architecture we use for inference for training! Why? Because training is just matrix multiplications with a few extra steps.</p><p>Here's where things get really exciting. Let's say we just ran inference on the XOR problem and got a prediction that looks something like [0.8, 0.3, 0.1, 0.9] when we actually wanted [1, 0, 0, 1]. Our model is performing poorly! We need to make it better. This is where training comes in. We're going to use something called a loss function to tell our model exactly how poorly it's doing. For simplicity, we chose Mean Squared Error (MSE) — think of it like measuring the \"distance\" between what we predicted and what we actually wanted, just like how you might measure how far off target your basketball shot was.<b>Let's denote the loss with L.</b></p><div><p>where  is the target output, is our prediction, and is the number of samples</p></div><div><p>For our XOR example, with predictionsand targets :</p><p>This loss value tells us how far off our predictions are from the true XOR outputs.</p></div><p>So right after we finish computing our final layer's activations (let's call them), we immediately stream them into a loss module to calculate just how bad our predictions are. These loss modules sit right below our activation modules, and we only use them when we've reached our final layer. But here's the key insight: you don't actually need to calculate the loss value itself to train. You just need its derivative. Why? Because that derivative tells us which direction to adjust our weights to make the loss smaller. It's like having a compass that points toward \"better performance.\"</p><h3>The magic of the chain rule</h3><p>This is where calculus enters the picture. To make our model better, we need to figure out how changing each weight affects our loss. The chain rule lets us break this massive calculation into smaller, manageable pieces.</p><div><p>The chain rule for gradients:</p><p>This allows us to compute gradients layer by layer, propagating them backwards through the network</p></div><p>Let's naively trace through what happens step by step.</p><ol><li>Calculate- how much the loss changes with respect to our final activations.</li><li>Computeby taking the derivative of the activation (leaky ReLU in our case).</li><li>Compute,</li><li>Compute</li><li>Rinse and repeat for the n-1 layer.</li></ol></div><div><p>Propagating gradients to the hidden layer:</p><p>And through the first layer's activation:</p><p>With mixed positive and negative values in, the gradient is:</p></div><p>Once we have all of these individual derivatives, we can multiply them together to find any derivative with respect of the loss (i.e.gives us).</p><p>After that, we have to compute the activation derivative, for which the formula is. This is also an element-wise computation, meaning we can structure it exactly like the loss module (and bias and activation modules), but it will perform a different calculation. One important note about this module, however, is that it requires the activations we computed during forward pass.</p><p>Now you might be wondering — how do we actually compute derivatives in hardware? Let's look at Leaky ReLU as an example, since it's beautifully simple but demonstrates the key principles. Remember that Leaky ReLU applies different operations based on whether the input is positive or negative. The derivative follows the same pattern: it outputs 1 for positive inputs and a small constant (we used 0.01) for negative inputs.</p><pre><code> @;\n    .01 ;\n    </code></pre><p>What's beautiful about this is that it's just a simple comparison – no complex arithmetic needed. The hardware can compute this derivative in a single clock cycle, keeping our pipeline flowing smoothly. This same principle applies to other activation functions: their derivatives often simplify to basic operations that hardware can execute very efficiently.</p><p>You'll notice a really cool pattern emerging: all these modules that sit underneath the systolic array process column vectors that stream out one by one. This gave us the idea to unify them into something we called a <b>vector processing unit (VPU)</b> – because that's exactly what they're doing, processing vectors element-wise!<p>Not only is this more elegant to work with, it's also useful when we scale our TPU beyond a 2x2 systolic array, as we'll have N number of these modules (N being the size of the systolic array), each of which we would have to interface with individually. Unifying these modules under a parent module makes our design more scalable and elegant!</p></p><p>Additionally, by incorporating control signals for each module, which we call the VPU pathway bits, we can selectively enable or skip specific operations. This makes the VPU flexible enough to support both inference and training. For instance, during the forward pass, we want to apply biases and activations but skip computing loss or activation derivatives. When transitioning to the backward pass, all modules are engaged, but within the backward chain we only need to compute the activation derivative. Due to pipelining, all values that flow through the VPU pass through each of the four modules, and any unused modules simply act as registers, forwarding their inputs to outputs without performing computation.</p><p>The next few derivatives are interesting because we can actually use matrix multiplication (and the systolic array!) to compute the derivatives with the help of these three identities:</p><ol><li>If we haveand take its derivative with respect to the weights, we get:</li><li>If we haveand take its derivative with respect to the inputs, we get:<div>(just the weight matrix transposed)</div></li><li>For the bias term, the derivative is simply 1.</li></ol><p>This means that we can multiply the previouswith ,, and 1 to get,, and, respectively, and we can multiply all of these byto get the gradients of the loss with respect to all of our second layer parameters. And because all of the gradients are actually gradient matrices, we can use the systolic array!</p><p>Now something to note about the activation derivativeand the weight derivativeis that they both require the post-activations (H) we calculate during forward pass. This means we need to store the outputs of every layer in some form of memory to be able to perform training. Here's where we created a new scratchpad memory modulewhich we called the unified buffer (UB).This lets us store our H values immediately after we compute them during forward pass.</p><p>We realized that we can also get rid of the input and weight accumulators, as well as manually loading the bias and leak factors into their respective modules, by using the UB to store them. This is also better practice, rather than loading in new data every clock cycle with the instruction set. Since we want to access two values (2 inputs or 2 weights for each row/col of the systolic array) at the same time, we added TWO read and write ports. We did this for each data primitive (inputs, weights, bias, leak factor, post activations) to minimize data contention since we have many different types of data.</p><p>To read values, we supply a starting address and the number of locations we want the UB to read and it will read 2 values every clock cycle. Writing is a similar mechanism, where we specify which values we want to write to each of the two input ports. The beauty in the read mechanism is that it runs in the background once we supply a starting address until the number of locations given are read, meaning we only need to provide an instruction for this every few clock cycles.</p><p>At the end of the day, not having these mechanisms wouldn't break the TPU — but they allow us to always keep the systolic array fed, which is a core design principle we couldn't compromise.</p><p>While we were working on this, we realized we could make one last small optimization for the activation derivative module — since we only use the  values once (for computing), we created a tiny cache within the VPU instead of storing them in the UB. The rest of the  values will be stored in the UB because they're needed to compute multiple derivatives.</p><p>This is what the new TPU architecture, modified to perform training, looks like:</p><p>Now we can do backpropagation!</p><h3>The beautiful symmetry of forward and backward pass</h3><p>Going back to the computational graph, we discovered something remarkable: the longest chain in backpropagation closely resembles forward pass! In forward pass, we multiply activation matrices with transposed weight matrices. In backward pass, we multiply gradient matrices with weight matrices (untransposed). It's like looking in a mirror!</p><p>This insight led us to compute the long chain of the computational graph first (highlighted in yellow) – getting all ourgradients just like we computed activations in forward pass. We could cache these gradients and reuse them, following the same efficient pattern we'd already mastered.</p><p>We create a loop where we:</p><ol><li>Fetch a bridge node () from our unified buffer</li><li>Fetch the corresponding matrix, also from unified buffer</li><li>Stream these through our systolic array to compute the weight gradients</li></ol><p>And here's where something really magical happens: we can stream these weight gradients directly into a gradient descent module while we're still computing them! This module takes the current weights stored in memory and updates them using the gradients.</p><div><p>The gradient descent update rule:</p><p>where  is the learning rate and represents any parameter (weights or biases)</p></div><div><p>Computing weight gradients for our XOR network:</p><p>Bias gradients (sum over samples):</p><p>Applying gradient descent with learning rate:</p></div><p>No waiting around — everything flows like water through our pipeline.</p><p>You might be wondering: \"We've used our matrix multiplication identities for the long chain and weight gradients — how do we calculate bias gradients?\" Well, we've actually already done most of the work! Since we're processing batches of data, we can simply sum (the technical term is \"reduce\") thegradients across the batch dimension. The beauty is that we can do this reduction right when we're computing the long chain — no extra work required!</p><p>With all these new changes and control flags, our instruction is significantly longer — 94 bits in fact! But we can confirm that every single one of these bits is needed and we ensured that we couldn't make the instruction set any smaller without compromising the speed and efficiency of the TPU.</p><p>By continuing this same process iteratively – forward pass, backward pass, weight updates – we can train our network until it performs exactly how we want. The same systolic array that powered our inference now powers our training, with just a few additional modules to handle the gradient computations.</p><p>What started as a simple idea about matrix multiplication has grown into a complete training system. Every component works together in harmony: data flows through pipelines, modules operate in parallel, and our systolic array stays fed with useful work.</p>","contentLength":21867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44944592"},{"title":"Show HN: Chroma Cloud – serverless search database for AI","url":"https://trychroma.com/cloud","date":1755544801,"author":"jeffchuber","guid":167,"unread":true,"content":"<div>client </div><div>collection name</div><div>    ids</div><div>results </div><div>    query_texts</div><div>    where</div><div>    include</div>","contentLength":72,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44944241"},{"title":"Show HN: We started building an AI dev tool but it turned into a Sims-style game","url":"https://www.youtube.com/watch?v=sRPnX_f2V_c","date":1755543083,"author":"maxraven","guid":166,"unread":true,"content":"<p>We started out building an AI agent dev tool, but somewhere along the way it turned into Sims for AI agents.</p><p>The original idea was simple: make it easy to create AI agents. We started with Jupyter Notebooks, where each cell could be callable by MCP—so agents could turn them into tools for themselves. It worked well enough that the system became self-improving, churning out content, and acting like a co-pilot that helped you build new agents.</p><p>But when we stepped back, what we had was these endless walls of text. And even though it worked, honestly, it was just boring. We were also convinced that it would be swallowed up by the next model’s capabilities. We wanted to build something else—something that made AI less of a black box and more engaging. Why type into a chat box all day if you could look your agents in the face, see their confusion, and watch when and how they interact?</p><p>Both of us grew up on simulation games—RollerCoaster Tycoon 3, Age of Empires, SimCity—so we started experimenting with running LLM agents inside a 3D world. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we’d done before.</p><p>The very first version was small: a single Unity room, an MCP server, and a chat box. Even getting two agents to take turns took weeks. Every run surfaced quirks—agents refusing to talk at all, or only “speaking” by dancing or pulling facial expressions to show emotion. That unpredictability kept us building.</p><p>Now it’s a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs—prompts, decision logic, even how they see chat history—without rebuilding.</p><p>On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies—tuning how “chatty” agents are versus how much they move or manipulate the environment.</p><p>We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There’s community sharing built in, so you can post rooms you make.</p><p>Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, “win” and “lose” tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.</p><p>Under the hood, Unity’s ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.</p><p>Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons.\nThe bigger vision is to build an open-ended, AI-native sim-game where you can build and interact with anything or anyone. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.</p><p>The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.</p><p>The plan is to keep expanding—bigger rooms, more in-world tools for agents, and then multiplayer hosting. It’s live now, no waitlist. Free to play. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.</p><p>We’d love feedback on scenarios worth testing and what to build next. Tell us the weird stuff you’d throw at this—we’ll be in the comments.</p>","contentLength":3846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44943986"},{"title":"Show HN: Whispering – Open-source, local-first dictation you can trust","url":"https://github.com/epicenter-so/epicenter/tree/main/apps/whispering","date":1755535949,"author":"braden-w","guid":165,"unread":true,"content":"<p>Hey HN! Braden here, creator of Whispering, an open-source speech-to-text app.</p><p>I really like dictation. For years, I relied on transcription tools that were  good, but they were all closed-source. Even a lot of them that claimed to be “local” or “on-device” were still black boxes that left me wondering where my audio really went.</p><p>So I built Whispering. It’s open-source, local-first, and most importantly, transparent with your data.   Your data is stored locally on your device, and your audio goes directly from your machine to a local provider (Whisper C++, Speaches, etc.) or your chosen cloud provider (Groq, OpenAI, ElevenLabs, etc.). For me, the features were good enough that I left my paid tools behind (I used Superwhisper and Wispr Flow before).</p><p>Productivity apps should be open-source and transparent with your data, but they also need to match the UX of paid, closed-software alternatives. I hope Whispering is near that point. I use it for several hours a day, from coding to thinking out loud while carrying pizza boxes back from the office.</p><p>There are plenty of transcription apps out there, but I hope Whispering adds some extra competition from the OSS ecosystem (one of my other OSS favorites is Handy <a href=\"https://github.com/cjpais/Handy\" rel=\"nofollow\">https://github.com/cjpais/Handy</a>). Whispering has a few tricks up its sleeve, like a voice-activated mode for hands-free operation (no button holding), and customizable AI transformations with any prompt/model.</p><p>I’m basically obsessed with local-first open-source software. I think there should be an open-source, local-first version of every app, and I would like them all to work together. The idea of Epicenter is to store your data in a folder of plaintext and SQLite, and build a suite of interoperable, local-first tools on top of this shared memory. Everything is totally transparent, so you can trust it.</p><p>Whispering is the first app in this effort. It’s not there yet regarding memory, but it’s getting there. I’ll probably write more about the bigger picture soon, but mainly I just want to make software and let it speak for itself (no pun intended in this case!), so this is my Show HN for now.</p><p>I just finished college and was about to move back with my parents and work on this instead of getting a job…and then I somehow got into YC. So my current plan is to cover my living expenses and use the YC funding to support maintainers, our dependencies, and people working on their own open-source local-first projects. More on that soon.</p>","contentLength":2479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44942731"}],"tags":["dev","hn"]}