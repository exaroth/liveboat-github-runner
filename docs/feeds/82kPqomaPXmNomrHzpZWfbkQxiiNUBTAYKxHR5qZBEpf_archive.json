{"id":"82kPqomaPXmNomrHzpZWfbkQxiiNUBTAYKxHR5qZBEpf","title":"Hacker News: Show HN","displayTitle":"HN Show","url":"https://hnrss.org/show?points=60","feedLink":"https://news.ycombinator.com/shownew","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":18,"items":[{"title":"Show HN: I built a zero-browser, pure-JS typesetting engine for bit-perfect PDFs","url":"https://github.com/cosmiciron/vmprint","date":1772367911,"author":"cosmiciron","guid":117,"unread":true,"content":"<p>Hi HN, I'm a film director by trade, and I prefer writing my stories in plain text rather than using clunky screenplay software. Standard markup like Fountain doesn't work for me because I write in mixed languages, so I use Markdown with a custom syntax I invented to resemble standard screenplay structures.</p><p>This workflow is great until I need to actually generate an industry-standard screenplay PDF. I got tired of manually copying and pasting my text back into the clunky software just to export it, so I decided to write a script to automate the process. That's when I hit a wall.</p><p>I tried using React-pdf and other high-level libraries, but they failed me on two fronts: true multilingual text shaping, and complex contextual pagination. Specifically, the strict screenplay requirement to automatically inject (MORE) at the bottom of a page and (CONT'D) at the top of the next page when a character's dialogue is split across a page break.</p><p>You can't really do that elegantly when the layout engine is a black box. So, I bypassed them and built my own typesetting engine from scratch.</p><p>VMPrint is a deterministic, zero-browser layout VM written in pure TypeScript. It abandons the DOM entirely. It loads OpenType fonts, runs grapheme-accurate text segmentation (Intl.Segmenter), calculates interval-arithmetic spatial boundaries for text wrapping, and outputs a flat array of absolute coordinates.</p><p>Zero dependencies on Node.js APIs or the DOM (runs in Cloudflare Workers, Lambda, browser).</p><p>Performance: On a Snapdragon Elite ARM chip, the engine's \"God Fixture\" (8 pages of mixed CJK, Arabic RTL, drop caps, and multi-page spanning tables) completes layout and rendering in ~28ms.</p><p>The repo also includes draft2final, the CLI tool I built to convert Markdown into publication-grade PDFs (including the screenplay flavor) using this engine.</p><p>This is my first open-source launch. The manuscript is still waiting, but the engine shipped instead. I’d love to hear your thoughts, answer any questions about the math or the architecture, and see if anyone else finds this useful!</p><p>---\nA note on AI usage: To be fully transparent about how this was built, I engineered the core concept (an all-flat, morphable box-based system inspired by game engines, applied to page layouts), the interval-arithmetic math, the grapheme segmentation, and the layout logic entirely by hand. I did use AI as a coding assistant at the functional level, but the overall software architecture, component structures, and APIs were meticulously designed by me.</p><p>For a little background: I’ve been a professional systems engineer since 1992. I’ve worked as a senior system architect for several Fortune 500 companies and currently serve as Chief Scientist at a major telecom infrastructure provider. I also created one of the world's first real-time video encoding technologies for low-power mobile phones (in the pre-smartphone era). I'm no stranger to deep tech, and a deterministic layout VM is exactly the kind of strict, math-heavy system that simply cannot be effectively constructed with a few lines of AI prompts.</p>","contentLength":3086,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47206082"},{"title":"Show HN: Xmloxide – an agent-made Rust replacement for libxml2","url":"https://github.com/jonwiggins/xmloxide","date":1772322281,"author":"jawiggins","guid":116,"unread":true,"content":"<p>Recently several AI labs have published experiments where they tried to get AI coding agents to complete large software projects.</p><p>I have been wondering if there are software packages that can be easily reproduced by taking the available test suites and tasking agents to work on projects until the existing test suites pass.</p><p>After playing with this concept by having Claude Code reproduce redis and sqlite, I began looking for software packages where an agent-made reproduction might actually be useful.</p><p>I found libxml2, a widely used, open-source C language library designed for parsing, creating, and manipulating XML and HTML documents. Three months ago it became unmaintained with the update, \"This project is unmaintained and has\n[known security issues](<a href=\"https://gitlab.gnome.org/GNOME/libxml2/-/issues/346\" rel=\"nofollow\">https://gitlab.gnome.org/GNOME/libxml2/-/issues/346</a>). It is foolish to use this software to process untrusted data.\".</p><p>With a few days of work, I was able to create xmloxide, a memory safe rust replacement for libxml2 which passes the compatibility suite as well as the W3C XML Conformance Test Suite. Performance is similar on most parsing operations and better on serialization. It comes with a C API so that it can be a replacement for existing uses of libxml2.</p><p>While I don't expect people to cut over to this new and unproven package, I do think there is something interesting to think about here in how coding agents like Claude Code can quickly iterate given a test suite. It's possible the legacy code problem that COBOL and other systems present will go away as rewrites become easier. The problem of ongoing maintenance to fix CVEs and update to later package versions becomes a larger percentage of software package management work.</p>","contentLength":1695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47201816"},{"title":"Show HN: Claude-File-Recovery, recover files from your ~/.claude sessions","url":"https://github.com/hjtenklooster/claude-file-recovery","date":1772209582,"author":"rikk3rt","guid":114,"unread":true,"content":"<p>Claude Code deleted my research and plan markdown files and informed me: “I accidentally rm -rf'd real directories in my Obsidian vault through a symlink it didn't realize was there: I made a mistake. “</p><p>Unfortunately the backup of my documentation accidentally hadn’t run for a month. So I built claude-file-recovery, a CLI-tool and TUI that is able to extract your files from your ~/.claude session history and thankfully I was able to recover my files. It's able to extract any file that Claude Code ever read, edited or wrote. I hope you will never need it, but you can find it on my GitHub and pip. Note: It can recover an earlier version of a file at a certain point in time.</p><p>pip install claude-file-recovery</p>","contentLength":717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47182387"},{"title":"Show HN: Badge that shows how well your codebase fits in an LLM's context window","url":"https://github.com/qwibitai/nanoclaw/tree/main/repo-tokens","date":1772205282,"author":"jimminyx","guid":113,"unread":true,"content":"<p>Small codebases were always a good thing. With coding agents, there's now a huge advantage to having a codebase small enough that an agent can hold the full thing in context.</p><p>Repo Tokens is a GitHub Action that counts your codebase's size in tokens (using tiktoken) and updates a badge in your README. The badge color reflects what percentage of an LLM's context window the codebase fills: green for under 30%, yellow for 50-70%, red for 70%+. Context window size is configurable and defaults to 200k (size of Claude models).</p><p>It's a composite action. Installs tiktoken, runs ~60 lines of inline Python, takes about 10 seconds. The action updates the README but doesn't commit, so your workflow controls the git strategy.</p><p>The idea is to make token size a visible metric, like bundle size badges for JS libraries. Hopefully a small nudge to keep codebases lean and agent-friendly.</p>","contentLength":875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47181471"},{"title":"Show HN: RetroTick – Run classic Windows EXEs in the browser","url":"https://retrotick.com/","date":1772197610,"author":"lqs_","guid":112,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47180083"},{"title":"Show HN: Unfucked - version all changes (by any tool) - local-first/source avail","url":"https://www.unfudged.io/","date":1772141419,"author":"cyrusradfar","guid":111,"unread":true,"content":"<div data-index=\"0\"><h3>Agent mass-overwrote your source</h3><p>Your AI agent refactored 30 Rust files, hit an error on file 27, and reverted everything to stale versions. Three hours of good work — gone.</p><div>$ unf log --since 3h --include \"*.rs\" --stats\n$ unf diff --at 10m\n$ unf restore --at 10m -y</div></div><div data-index=\"1\"><p>The agent decided  was \"generated\" and deleted it. API keys, database URLs, local config. Not in git. Not anywhere.</p><div>$ unf log .env\n$ unf cat .env --at 5m\n$ unf restore --at 5m .env -y</div></div><div data-index=\"2\"><h3>Agent's cleanup script went wrong</h3><p>You asked the agent to \"clean up build artifacts.\" It wrote a shell script that 'd  instead of .</p><div>$ unf diff --at 1m\n$ unf restore --at 2m --dry-run\n$ unf restore --at 2m -y</div></div><div data-index=\"3\"><h3>Agent \"fixed\" your dependencies</h3><p>The agent removed 6 \"unused\" crates from . Four were behind feature flags. CI is red.</p><div>$ unf log Cargo.toml --stats\n$ unf cat Cargo.toml --at 1h\n$ unf restore --at 1h Cargo.toml -y</div></div><div data-index=\"4\"><h3>Agent reformatted everything</h3><p>The agent ran Prettier with the wrong config and rewrote 200 TypeScript files. It committed before you noticed.  gives you one commit. UNF* has every file.</p><div>$ unf log --since 30m --include \"*.ts\" --stats\n$ unf diff --at 30m\n$ unf restore --at 30m -y</div></div><div data-index=\"5\"><h3>Agent replaced your test fixtures</h3><p>Your hand-crafted SQL seed data and JSON fixtures got overwritten with generic placeholders. A week of edge cases, gone.</p><div>$ unf log --include \"fixtures/*\" --stats\n$ unf diff --at 20m\n$ unf restore --at 20m -y</div></div><div data-index=\"6\"><h3>Agent deleted your migration files</h3><p>The agent saw 47 SQL migration files and decided they were \"redundant.\" Production depends on them running in order.</p><div>$ unf log --include \"migrations/*.sql\"\n$ unf diff --at 15m\n$ unf restore --at 15m -y</div></div><div data-index=\"7\"><h3>Squash merge ate intermediate work</h3><p>You squash-merged a feature branch. Git only has the final result. The 40 intermediate versions across 3 days? Git doesn't know they existed.</p><div>$ unf log --since 3d --include \"*.py\"\n$ unf diff --from 3d --to 1d\n$ unf cat app/models.py --at 2d</div></div><div data-index=\"8\"><h3>Agent lost context mid-session</h3><p>Context window overflow. The agent crashed 2 hours into a refactor across 4 repos. The new agent needs to pick up exactly where the old one left off.</p><div>$ unf recap --global --json\n$ unf log --sessions --since 2h\n$ unf diff --session</div></div><div data-index=\"9\"><h3>What happened while you were away?</h3><p>You left an agent running overnight. It touched 80 files across 3 projects. What did it do?</p><div>$ unf log --global --since 8h --stats\n$ unf diff --at 8h\n$ unf restore --at 8h --dry-run</div></div>","contentLength":2353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47172238"},{"title":"Show HN: Deff – Side-by-side Git diff review in your terminal","url":"https://github.com/flamestro/deff","date":1772128446,"author":"flamestro","guid":110,"unread":true,"content":"<p>deff is an interactive Rust TUI for reviewing git diffs side-by-side with syntax highlighting and added/deleted line tinting. It supports keyboard/mouse navigation, vim-style motions, in-diff search (/, n, N), per-file reviewed toggles, and both upstream-based and explicit --base/--head comparisons. It can also include uncommitted + untracked files (--include-uncommitted) so you can review your working tree before committing.</p><p>Would love to get some feedback</p>","contentLength":460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47169518"},{"title":"Show HN: Agent Swarm – Multi-agent self-learning teams (OSS)","url":"https://github.com/desplega-ai/agent-swarm","date":1772108138,"author":"tarasyarema","guid":109,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47165046"},{"title":"Show HN: Terminal Phone – E2EE Walkie Talkie from the Command Line","url":"https://gitlab.com/here_forawhile/terminalphone","date":1772102445,"author":"smalltorch","guid":108,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47164270"},{"title":"Show HN: I ported Tree-sitter to Go","url":"https://github.com/odvcencio/gotreesitter","date":1772044117,"author":"odvcencio","guid":107,"unread":true,"content":"<p>This started as a hard requirement for my TUI-based editor application, it ended up going in a few different directions.</p><p>I think this has some pretty big potential! I think there's many classes of application (particularly legacy architecture) that can benefit from these kinds of analysis tooling. My next post will be about composing all these together, an exciting project I call GotHub. Thanks!</p>","contentLength":397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47155597"},{"title":"Show HN: I ported Manim to TypeScript (run 3b1B math animations in the browser)","url":"https://github.com/maloyan/manim-web","date":1772043307,"author":"maloyan","guid":106,"unread":true,"content":"<p>Hi HN, I'm Narek. I built Manim-Web, a TypeScript/JavaScript port of 3Blue1Brown’s popular Manim math animation engine.</p><p>The Problem: Like many here, I love Manim's visual style. But setting it up locally is notoriously painful - it requires Python, FFmpeg, Cairo, and a full LaTeX distribution. It creates a massive barrier to entry, especially for students or people who just want to quickly visualize a concept.</p><p>The Solution: I wanted to make it zero-setup, so I ported the engine to TypeScript. Manim-Web runs entirely client-side in the browser. No Python, no servers, no install. It runs animations in real-time at 60fps.</p><p>How it works underneath:\n- Rendering: Uses Canvas API / WebGL (via Three.js for 3D scenes).\n- LaTeX: Rendered and animated via MathJax/KaTeX (no LaTeX install needed!).\n- API: I kept the API almost identical to the Python version (e.g., scene.play(new Transform(square, circle))), meaning existing Manim knowledge transfers over directly.\n- Reactivity: Updaters and ValueTrackers follow the exact same reactive pattern as the Python original.</p><p>Because it's web-native, the animations are now inherently interactive (objects can be draggable/clickable) and can be embedded directly into React/Vue apps, interactive textbooks, or blogs. I also included a py2ts converter to help migrate existing scripts.</p><p>It's open-source (MIT). I'm still actively building out feature parity with the Python version, but core animations, geometry, plotting, and 3D orbiting are working great. I would love to hear your feedback, and I'll be hanging around to answer any technical questions about rendering math in the browser!</p>","contentLength":1631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47155375"},{"title":"Show HN: Django Control Room – All Your Tools Inside the Django Admin","url":"https://github.com/yassi/dj-control-room","date":1772029895,"author":"yassi_dev","guid":105,"unread":true,"content":"<p>Over the past year I’ve been building a set of operational panels for Django:</p><p>- Redis inspection\n- cache visibility\n- Celery task introspection\n- URL discovery and testing</p><p>All of these tools have been built inside the Django admin.</p><p>Instead of jumping between tools like Flower, redis-cli, Swagger, or external services, I wanted something that sits where I’m already working.</p><p>I’ve grouped these under a single umbrella: Django Control Room.</p><p>The idea is pretty simple: the Django admin already gives you authentication, permissions, and a familiar interface. It can also act as an operational layer for your app.</p><p>Each panel is just a small Django app with a simple interface, so it’s easy to build your own and plug it in.</p><p>I’m working on more panels (signals, errors, etc.) and also thinking about how far this pattern can go.</p><p>Curious how others think about this. Does it make sense to consolidate this kind of tooling inside the admin, or do you prefer keeping it separate?</p>","contentLength":975,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47151995"},{"title":"Show HN: Respectify – A comment moderator that teaches people to argue better","url":"https://respectify.org/","date":1772029279,"author":"vintagedave","guid":104,"unread":true,"content":"<p data-astro-cid-vs4kwel2=\"\">Sometimes people write things that sound like they're saying one thing, but their words are 'coded' — to mean something else to some readers.<p>For example, someone might write: 'Those polar bears are always ruining our porridge.' To most readers, this seems like a complaint about bears and food. But to certain groups, it's actually saying something else entirely. (The real comments are not about bears.)</p><p>You can avoid this by telling Respectify what not to allow. Tailor it for your site, topics, and audience.</p></p>","contentLength":512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47151842"},{"title":"Show HN: Clocksimulator.com – A minimalist, distraction-free analog clock","url":"https://www.clocksimulator.com/","date":1772029034,"author":"user_timo","guid":103,"unread":true,"content":"<table><tbody><tr><td>Switch between dark and light mode</td></tr><tr><td> (monitor icon)</td><td>Prevent the display from sleeping</td></tr><tr><td> (stopwatch icon)</td><td>Switch between ticking and smooth second hand</td></tr><tr><td>Show embed link, this help, and contact info</td></tr></tbody></table><h3>Screen burn-in protection</h3><p>To protect OLED and AMOLED displays, the clock subtly shifts its position every 10 minutes in a slow circular pattern. Dark Mode further reduces the risk of burn-in. Protection does not apply in embedded mode.</p><p>This site works as a Progressive Web App. You can install it on your phone or computer for a full-screen clock and offline use. Use your browser’s menu (e.g. “Add to Home Screen” or “Install app”) to install.</p><pre>www.clocksimulator.com/?tz=America/New_York\nwww.clocksimulator.com/?tz=Europe/London\nwww.clocksimulator.com/?tz=Asia/Tokyo</pre><p>Embed the clock on any website via an . Use the built-in embed panel ( → ) to configure and copy the code.</p><table><tbody><tr></tr><tr></tr><tr></tr></tbody></table>","contentLength":870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47151784"},{"title":"Show HN: A real-time strategy game that AI agents can play","url":"https://llmskirmish.com/","date":1772013765,"author":"__cayenne__","guid":102,"unread":true,"content":"<section><ul><li>LLM Skirmish is a benchmark where LLMs play 1v1 RTS (real-time strategy) games against each other</li><li>LLMs write their battle strategies in code, which is then executed in the game environment</li><li>LLM Skirmish tests in-context learning, as each tournament lasts five rounds and LLMs are able to alter strategies between rounds</li></ul></section><section><p>\n              It's been great to see the energy in the last year around using games to evaluate LLMs. Yet there's \n              a weird disconnect between frontier LLMs one-shotting full coding projects and \n              those same models struggling to get out of Pokemon Red's <a href=\"https://www.twitch.tv/claudeplayspokemon\" target=\"_blank\" rel=\"noopener\">Mt. Moon</a>.\n            </p><p>\n              We wanted to create an LLM game benchmark that put this generation of frontier LLMs' superpower, \n              coding, on full display. Ten years ago, a team released a game called <a href=\"https://github.com/screeps/screeps\" target=\"_blank\" rel=\"noopener\">Screeps</a>. It was described \n              as an \"MMO RTS sandbox for programmers.\" In Screeps, human players write javascript strategies \n              that get executed in the game's environment. Players gain resources, lose territory, and have \n              units wiped out. It's a traditional RTS, but controlled entirely through code. \n            </p><p>\n              The Screeps paradigm, writing code and having it execute in a real-time game environment, is well suited \n              for an LLM benchmark. Drawing on a version of the Screeps open source API, LLM Skirmish pits \n              LLMs head-to-head in a series of 1v1 real-time strategy games.\n            </p></section><section><p>\n              In LLM Skirmish, each player begins with a \"spawn\" (a building that can create units), one \n              military unit, and three economic units. The objective of each LLM Skirmish match is to \n              eliminate your opponent's spawn. If a player is not eliminated within 2,000 game frames \n              (each player is allowed up to one second of runtime computation per frame), the game ends \n              and the victor is determined based on score.\n            </p></section><section><p>\n              Every LLM Skirmish tournament consists of five rounds. In each round, each LLM is asked to \n              write a script implementing its strategy. For all rounds after the first, each LLM can see \n              the results of all its matches from the previous round and use that information to make \n              changes to the script it submits for the next round. In every round, every player plays all \n              other players once. This means there are 10 matches per round and 50 matches per tournament.\n            </p></section><section><p>\n              LLM Skirmish was conducted using <a href=\"https://opencode.ai\" target=\"_blank\" rel=\"noopener\">OpenCode</a>, \n              an open source general purpose agentic coding harness. OpenCode was selected because it was not \n              designed for any of the evaluated models and is fully open source to aid in replicability.\n            </p><p>\n              Each LLM agent runs in an isolated Docker container with OpenCode providing the coding environment. \n              The orchestrator coordinates the tournament by sending prompts to each agent, which then uses \n              OpenCode's tools (file editing, shell commands, etc.) to write and submit their game scripts.\n            </p><p>\n              At the start of each round, agents receive \n              <a href=\"https://github.com/llmskirmish/skirmish/blob/main/prompts/OBJECTIVE.md\" target=\"_blank\" rel=\"noopener\">OBJECTIVE.md</a> \n              (the game rules, API documentation, and instructions for writing a game script) and \n              <a href=\"https://github.com/llmskirmish/skirmish/blob/main/prompts/NEXT_ROUND.md\" target=\"_blank\" rel=\"noopener\">NEXT_ROUND.md</a> \n              (instructions for reviewing match logs from the previous round, rounds 2-5 only). \n              Agents are also provided with <a href=\"https://github.com/llmskirmish/skirmish/tree/main/example_strategies\" target=\"_blank\" rel=\"noopener\">two example strategies</a> as reference.\n            </p><p>\n              After each agent creates their strategy, the orchestrator validates the script. If validation fails, the agent \n              receives the error message and has up to 3 attempts to fix the issue before the round proceeds.\n            </p></section><section><p>\n              LLM Skirmish tests in-context learning, as each tournament lasts five rounds and models are \n              able to alter strategies between rounds. One would hypothesize that if a model is successfully \n              learning in context, scripts written after seeing previous results (as in rounds 2–5) would be \n              of higher quality compared to scripts written in round 1.\n            </p><p>\n              Across all tournaments, each model submits 25 scripts for a total of 250 matches. In a tournament, \n              we consider each model to be a player. If we treat each script as a player and have all scripts \n              play against each other, we can simulate 7,750 matches to get a robust per-round average win rate \n              (a proxy for script quality).\n            </p><div><div><h3>Script Round vs Performance</h3></div></div><p>\n              We can see that four of the five models evaluated have notable increases in average win rate \n              between round 1 and round 5 (Claude Opus 4.5 +20%, GLM 4.7 +16%, GPT 5.2 +7%, Grok 4.1 Fast +6%).\n            </p><p>\n              Gemini 3 Pro's performance presents an anomaly. Its round 1 average win rate was 70% (higher \n              than all four other evaluated models), while its round 2-5 average win rate was 15% (lower than \n              all four other evaluated models). Gemini 3 Pro's round 1 scripts are approximately four times \n              shorter than those of top-performing models Claude 4.5 Opus and GPT 5.2. A qualitative review of \n              Gemini 3 Pro's scripts suggests it had success with simplistic strategies in round 1. In rounds \n              2-5, compared to the other four models evaluated, Gemini 3 Pro most aggressively populated its \n              context with previous round results before submitting its script for that round, suggesting that \n              context rot was a notable contributor to the performance variance. Whether this context rot reflects \n              other models being better at planning tool use than Gemini 3 Pro, or whether OpenCode is a \n              uniquely inhospitable harness for Gemini 3 Pro, is worth investigating further in future versions \n              of LLM Skirmish.\n            </p></section><section><p>\n              API costs vary significantly across models. The chart below plots each model's \n              average cost per round against its ELO rating. Claude Opus 4.5 achieved the highest \n              ELO (1778) but at the highest cost ($4.12/round). GPT 5.2 delivers nearly 1.7x more \n              ELO per dollar than Claude Opus 4.5.\n            </p></section>","contentLength":6380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47149586"},{"title":"Show HN: Context Mode – 315 KB of MCP output becomes 5.4 KB in Claude Code","url":"https://github.com/mksglu/claude-context-mode","date":1772000610,"author":"mksglu","guid":101,"unread":true,"content":"<p>Every MCP tool call dumps raw data into Claude Code's 200K context window. A Playwright snapshot costs 56 KB, 20 GitHub issues cost 59 KB. After 30 minutes, 40% of your context is gone.</p><p>I built an MCP server that sits between Claude Code and these outputs. It processes them in sandboxes and only returns summaries. 315 KB becomes 5.4 KB.</p><p>It supports 10 language runtimes, SQLite FTS5 with BM25 ranking for search, and batch execution. Session time before slowdown goes from ~30 min to ~3 hours.</p><p>MIT licensed, single command install:</p><p>/plugin marketplace add mksglu/claude-context-mode</p><p>/plugin install context-mode@claude-context-mode</p><p>Would love feedback from anyone hitting context limits in Claude Code.</p>","contentLength":698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47148025"},{"title":"Show HN: Linex – A daily challenge: placing pieces on a board that fights back","url":"https://www.playlinex.com/","date":1771976038,"author":"Humanista75","guid":100,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47145082"},{"title":"Show HN: Moonshine Open-Weights STT models – higher accuracy than WhisperLargev3","url":"https://github.com/moonshine-ai/moonshine","date":1771970047,"author":"petewarden","guid":99,"unread":true,"content":"<p>I wanted to share our new speech to text model, and the library to use them effectively. We're a small startup (six people, sub-$100k monthly GPU budget) so I'm proud of the work the team has done to create streaming STT models with lower word-error rates than OpenAI's largest Whisper model. Admittedly Large v3 is a couple of years old, but we're near the top the HF OpenASR leaderboard, even up against Nvidia's Parakeet family. Anyway, I'd love to get feedback on the models and software, and hear about what people might build with it.</p>","contentLength":540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47143755"}],"tags":["dev","hn"]}