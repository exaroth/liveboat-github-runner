{"id":"2bP4pJr4wVimh6yine63HqNeafoyheTFz1gqCrQ4h7Z","title":"AlgoMaster Newsletter","displayTitle":"Dev - Algomaster","url":"https://blog.algomaster.io/feed","feedLink":"https://blog.algomaster.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"300+ Engineering Articles to Level Up Your System Design Skills","url":"https://blog.algomaster.io/p/300-engineering-articles-to-level-up-system-design","date":1772340064,"author":"Ashish Pratap Singh","guid":744,"unread":true,"content":"<p>I’m excited to share a  where I’ve curated 300+ high-quality engineering articles, organized by top tech companies.</p><p>These articles cover various topics including:</p><ul><li><p>Real-World System Design and Architecture</p></li><li><p>Databases and Performance</p></li><li><p>Infrastructure and Security</p></li></ul><p>A lot of people tell you which engineering blogs to follow. Almost nobody tells you which articles are actually worth your time.</p><p>So I did the hard part: I went through the last 5–6 years of popular company engineering blogs and pulled out the articles that are genuinely worth reading.</p><p>My goal is to make this repo a one-stop resource for the most interesting engineering writing across the internet.</p><p>If you find it valuable, consider giving it a star (⭐️) and share it with others.</p><p>Contributions are welcome too. If you think a company or article is missing, feel free to open a pull request.</p>","contentLength":853,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!7Ld9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7552b6-aa48-4fa2-83aa-f01d1c0d27aa_1600x1200.png","enclosureMime":"","commentsUrl":null},{"title":"20 AI Concepts Explained Simply","url":"https://blog.algomaster.io/p/20-ai-concepts-explained-simply","date":1772112178,"author":"Ashish Pratap Singh","guid":743,"unread":true,"content":"<p>Learning AI can feel overwhelming. If you are not working directly in AI, it can feel like learning an entirely new language.</p><p>But like any technical topic, AI becomes much easier once you understand the fundamentals behind large language models (LLMs) and the modern tools built around them.</p><p>In this article, we will break down <strong>20 of the most important AI concepts</strong> in the simplest way possible, with clear explanations and intuitive examples.</p><p>Many developer tools promise context-aware AI, but having data access doesn’t automatically mean agents know when to use it.</p><p>Real context requires understanding.  synthesizes knowledge from your codebase, PRs, discussions, docs, project trackers, and runtime signals. It connects past decisions to current work, resolves conflicts between outdated docs and actual practice, respects data permissions, and surfaces what matters for the task at hand.</p><ul><li><p>Coding agents like Cursor, Claude, and Copilot generate output that aligns with your actual architecture and conventions</p></li><li><p>Code review focuses on real bugs rather than stylistic nits</p></li><li><p>You find instant answers without interrupting teammates</p></li></ul><p>At its core, a neural network is a stack of connected layers made up of simple units called neurons. Data enters through the input layer, moves through one or more hidden layers where the model learns useful patterns, and then produces a final prediction at the output layer.</p><p>A helpful way to picture this is as a series of refinement steps. The same input gets transformed again and again, with each layer extracting slightly higher-level features than the one before it. In image models, early layers often pick up simple signals like edges and textures. Deeper layers combine those signals into shapes and parts, and later layers can represent full objects.</p><p>The connections between neurons have weights, numbers that control how strongly one neuron influences another. Training is essentially the process of adjusting these weights so the network’s outputs become more accurate. </p><p>Modern large language models contain an enormous number of such weights, often in the tens or even hundreds of billions.</p><p>Training a neural network from scratch takes a huge amount of data and compute. Transfer learning changes the game. Instead of starting over, you take a model that has already been trained on a broad task and adapt it for a new, more specific one. Most of what the model learned still applies, so you get a strong head start.</p><p>A simple way to think about it is skill reuse. If you have already learned the basics, you can pick up a new variation much faster because the fundamentals do not need to be relearned. In the same way, a pretrained model already understands many general patterns in data, so fine-tuning it for your use case takes far less effort.</p><p>This is actually how most modern AI works. Someone trains a massive general-purpose model (the “foundation model”), and then you adapt it for your specific task.</p><p>Before a model can work with text, it first has to convert that text into smaller units called . A token can be a full word, a part of a word, a punctuation mark, or sometimes even a single character. This is the model’s “alphabet” for reading and writing language.</p><p>For example, the word  might be split into subword tokens like , , and , while a common word like  might remain a single token. In general, common words tend to map cleanly to one token, while longer or rarer words get broken into smaller pieces.</p><h4>Why not just use whole words?</h4><p>Because the number of possible words is enormous, and it keeps growing. Proper nouns, slang, typos, domain-specific terms, and words from different languages would explode the vocabulary size. </p><p>Tokenization solves this by using a  (often around ) built from common words and reusable subword chunks. That way, even if the model has never seen a word before, it can still represent it by combining familiar pieces.</p><p>Once text is split into tokens, each token gets converted into an , a vector (basically a list of numbers) that captures what the token means.</p><p>You can think of embeddings as coordinates in a high-dimensional map. Words with similar meanings end up close to each other, while unrelated words are far apart. For example, “king” and “queen” are located near each other in this space, whereas “king” and “refrigerator” are much farther apart.</p><p>These vectors usually have hundreds or even thousands of dimensions. While that sounds abstract, those dimensions capture meaningful patterns. The difference between “king” and “queen” is similar to the difference between “man” and “woman.”</p><p>The model does not understand language symbolically the way humans do. Instead, it learns meaning through geometry, by organizing words in a space where relationships become distances and directions.</p><p>Here’s the problem: the meaning of a word depends on context. “Bank” means something different in “river bank” versus “bank account”. Embeddings alone don’t solve this because they start as fixed vectors per token.</p><p> is the mechanism that addresses this. It lets each token look at every other token in the input and decide which ones matter. When processing “bank” in “I sat by the river bank”, the attention mechanism focuses on “river” and adjusts the representation of “bank” accordingly.</p><p>This was the key innovation that unlocked modern AI. Before attention, models processed words one at a time, left to right. Attention lets the model see everything at once and figure out what’s relevant.</p><p>The transformer is the architecture that ties all of this together. It was introduced in a 2017 paper titled <em>“Attention Is All You Need”</em> and replaced the strictly sequential, word-by-word processing used in earlier models with attention as the central mechanism.</p><p>A transformer stacks multiple layers of attention and feed-forward networks. Each layer refines the representation of the input. Early layers tend to capture basic grammar. Middle layers pick up relationships between concepts. Later layers handle more complex reasoning.</p><p>One of the biggest advantages of transformers is that they process all tokens in  instead of one at a time. This makes training far more efficient on modern hardware like GPUs and enables models to scale to massive sizes. GPT, Claude, Gemini, Llama, and most other leading AI systems today are all built on transformer architectures.</p><p>In simple terms, text is converted into tokens, tokens become vectors, and stacked attention layers learn how those vectors relate and interact. That process forms the backbone of modern language models.</p><h2>7. LLM (Large Language Model)</h2><p>A large language model (LLM) is essentially a  trained on an enormous amount of text, often <strong>hundreds of billions to trillions of tokens</strong> pulled from books, websites, code, and many other sources. The training objective sounds almost too simple: .</p><p>That’s it. That’s the whole trick. But by doing this across trillions of examples, the model develops something that looks a lot like understanding, language, facts, reasoning patterns, even a degree of common sense. Most well-known systems today fall into this category, including GPT-style models, Claude, Gemini, and Llama.</p><p>The “large” part refers to the parameter count. Frontier models today have hundreds of billions of parameters, and training them costs tens of millions of dollars. But what you get is a model that can write code, answer questions, translate languages, and reason through problems it was never explicitly taught.</p><p>The  is the maximum number of tokens a model can process at once, including both your input and the model’s generated output. You can think of it as the model’s working memory.</p><p>Early GPT models had a context window of 4,096 tokens (roughly 3,000 words). That felt limiting fast. Current models have pushed way beyond that. Claude supports 200K tokens. Gemini goes up to 1M. More context means the model can handle longer documents, longer conversations, and more information when forming a response.</p><p>However, there is a trade-off. Bigger context windows require significantly more memory and compute, which increases cost and latency. And even with large windows, models do not treat every part of the input equally. </p><p>Research has shown that performance often drops for information buried in the middle of very long inputs, a phenomenon commonly referred to as “lost in the middle.”</p><p>When an LLM generates the next token, it doesn’t just pick one. It calculates a probability for every token in its vocabulary. Temperature controls how it chooses from those probabilities.</p><p>At temperature 0, it always picks the most probable token. The result is deterministic, focused, and predictable. At temperature 1, it samples proportionally to the probabilities, so you get more variety and surprise. Above 1, things get increasingly random, more creative maybe, but also less coherent.</p><p>In practice, a few simple rules work well:</p><ul><li><p><strong>Low temperature (0 to 0.3):</strong> best for precise tasks like code generation, structured extraction, summaries, and anything where correctness matters more than originality.</p></li><li><p><strong>Medium temperature (0.5 to 0.8):</strong> good for brainstorming, alternative phrasings, marketing copy, and creative writing where variety is useful.</p></li><li><p> can be fun for playful ideation, but it often reduces coherence and can quickly produce nonsense, especially in longer outputs.</p></li></ul><p>This is one of the most common issues people encounter with language models. A  happens when an LLM produces something that sounds confident and believable, but is actually incorrect. It might reference a research paper that does not exist, invent a function in a software library, or present a fabricated statistic as if it were widely known.</p><p>Because LLMs are pattern completion engines at heart. They’re optimized to produce fluent, probable text, not to verify whether that text is true. If the most “natural” next sentence happens to be false, the model will produce it with the exact same confidence as a true one.</p><p>This makes hallucination one of the biggest practical challenges when using LLMs in real applications. Common ways to reduce it include retrieval-augmented generation (RAG), grounding responses in trusted source documents, and prompting the model to provide citations or acknowledge uncertainty.</p><p>At this point we have a powerful LLM, but it’s a generalist. How do you make it better at your specific task? And how do you shrink it enough to actually deploy?</p><p>These four techniques cover the spectrum from heavy customization to lightweight compression.</p><p>Fine-tuning starts with a pretrained model and continues training it on a smaller, task-specific dataset. The base model already understands general language patterns. Fine-tuning simply nudges it toward a particular domain, style, or behavior.</p><p>For example, if you want a model that performs well on medical question answering, you might fine-tune a general-purpose LLM on thousands of high-quality medical conversations or clinical explanations.</p><p>The trade-off is cost and infrastructure. Traditional fine-tuning updates most or all of the model’s parameters. That requires enough GPU memory to load the entire model along with optimizer states and gradients during training. </p><p>For a 70B parameter model, this typically means multiple high-end GPUs and significant compute resources. Fine-tuning can be powerful, but it is not lightweight.</p><h2>12. RLHF (Reinforcement Learning from Human Feedback)</h2><p>RLHF (Reinforcement Learning from Human Feedback) is one of the main techniques that turns “a model that predicts the next token” into “a model that feels helpful, polite, and safe to use.” It is a big reason modern chatbots feel like they are having a conversation, not just doing high-quality autocomplete.</p><p>At a high level, the process works like this:</p><ol><li><p><strong>Generate candidate answers.</strong> For a given prompt, the model produces multiple possible responses.</p></li><li><p> Human reviewers compare those responses and rank them based on qualities like helpfulness, correctness, clarity, and safety.</p></li><li><p> A separate model learns to predict which responses humans would prefer, based on those rankings.</p></li><li><p><strong>Tune the LLM using that reward signal.</strong> The LLM is then optimized to produce answers that score higher according to the reward model.</p></li></ol><p>This teaches the model behaviors that plain next-token prediction does not reliably produce: following instructions, refusing unsafe requests, being more balanced, and avoiding harmful or toxic directions. </p><p>Without RLHF (or other alignment methods), an LLM would be far more likely to continue text in whatever direction seems statistically plausible, even when that direction is unhelpful, misleading, or unsafe.</p><h2>13. LoRA (Low-Rank Adaptation)</h2><p>Fine-tuning every parameter in a large model, like a 70B parameter LLM, is extremely expensive. <strong>LoRA (Low-Rank Adaptation)</strong> takes a much more efficient approach.</p><p>Instead of updating the model’s original weights, it  and adds small, trainable adapter matrices into selected layers. These adapters usually contain only about <strong>0.1% to 1% of the total parameters</strong>.</p><p>The key insight is that the weight changes during fine-tuning tend to be “low-rank,” meaning they can be approximated by much smaller matrices without losing much quality. Instead of updating a 4096x4096 weight matrix, LoRA adds two small matrices (4096x8 and 8x4096) that together approximate the same change.</p><h4>Why does this matter in practice? </h4><p>You can fine-tune on a single GPU what would otherwise require a whole cluster. And you can swap different LoRA adapters in and out of the same base model, so you get multiple specialized versions without storing a full copy of the model for each one.</p><p>Quantization is a way to make models smaller and cheaper to run by <strong>storing their weights with fewer bits</strong>. In full precision, weights are often stored as 32-bit floating point values. Quantization reduces that to <strong>16-bit, 8-bit, or even 4-bit</strong>, cutting memory usage dramatically.</p><p>The basic math is straightforward: if you go from , each weight uses 8× less memory, so the whole model becomes roughly . </p><p>For example, a 70B-parameter model can require well over  at full precision, but a  version can fit into a few dozen GB. The quality drop is often smaller than you would expect, especially with , which tends to preserve performance well for many tasks.</p><p>This is one of the main reasons large models can run on consumer hardware. When you see someone running a 70B model on a desktop GPU or even a laptop, it is almost always a  rather than the full-precision model.</p><p>You’ve got a trained, optimized model. Now the question is: how do you actually get good output from it?</p><p>Turns out, a lot of it comes down to how you ask.</p><p>Prompt engineering is the art of crafting your input to get better output. The same underlying question, phrased differently, can lead to dramatically different responses.</p><p>For example, a vague prompt like “explain databases” usually results in a high-level overview. But something more specific like “explain how B-tree indexes work in PostgreSQL, with a concrete example using a users table” gives the model clear direction.</p><p>Here are few techniques that consistently improve results:</p><ul><li><p> “You are a senior database engineer.”</p></li><li><p> Provide sample inputs and outputs to show the format and depth you expect.</p></li><li><p><strong>Step-by-step decomposition:</strong> Break complex tasks into smaller, explicit steps.</p></li><li><p> Limit length, specify structure, or define the desired tone.</p></li></ul><p>Prompt engineering is not a workaround. It is the primary interface for interacting with LLMs. The difference between a vague prompt and a carefully constructed one can mean the difference between shallow, generic output and something that is accurate, structured, and production-ready.</p><h2>16. Chain of Thought (CoT)</h2><p>Chain of thought is a prompting technique where you ask the model to show its reasoning step by step before giving a final answer. Simple idea, but it makes a surprisingly big difference on math, logic, and multi-step reasoning.</p><p>Here’s a concrete example.</p><p> “What is 47 x 23?”</p><p>The model might output 1,071 (wrong) because it’s pattern-matching rather than actually computing.</p><p> “What is 47 x 23? Think step by step.”</p><p>The model walks through: 47 x 20 = 940, 47 x 3 = 141, 940 + 141 = 1,081 (correct).</p><p>By making the model generate intermediate steps, you’re giving it “scratch space” to work through the problem instead of jumping straight to an answer. Research shows this can improve accuracy on reasoning benchmarks by 20-40%.</p><p>Better answers come from letting the model think out loud, rather than forcing it to jump straight to a conclusion.</p><p>Knowing individual concepts is useful, but real AI products are systems, not solo models. This last section covers the building blocks engineers use to put it all together.</p><h2>17. RAG (Retrieval-Augmented Generation)</h2><p>Remember the hallucination problem? <strong>RAG (Retrieval-Augmented Generation)</strong> is one of the most effective ways to reduce it.</p><p>The idea is simple. Before the model generates a response, the system first <strong>retrieves relevant documents</strong> from a knowledge base and inserts them into the prompt as context. The model then answers using that information, instead of relying only on what it learned during training.</p><p>For example, imagine a customer support bot. When a user asks about your refund policy, the system first fetches the actual policy document, then the model generates an answer based on what it reads. The response is grounded in real, up-to-date information rather than vague recollection.</p><p>What makes RAG especially powerful is that it <strong>separates knowledge from reasoning</strong>.</p><ul><li><p>The LLM handles understanding and explanation.</p></li><li><p>Your document store provides the facts.</p></li></ul><p>If the information changes, you do not need to retrain the model. You simply update the documents it retrieves. This makes RAG practical, scalable, and far more reliable for real-world applications where accuracy matters.</p><p>So how does RAG actually find the right documents? That is where  come in. They store embeddings, the numerical vectors we discussed earlier, and allow you to search by meaning rather than exact keywords.</p><p>Here is how the flow works. Your documents are first split into smaller chunks. Each chunk is converted into an embedding, and those embeddings are stored in a vector database. </p><p>When a user asks a question, the query is also converted into an embedding. The database then searches for the stored vectors that are closest to the query vector and returns the most relevant chunks.</p><p>This creates a fundamentally different kind of search. A keyword search for “how to cancel my subscription” might miss a document titled “account termination process.” A vector search can still find it because the underlying meaning is similar, even though the wording is different.</p><p>Some widely used vector databases include Pinecone, Weaviate, Qdrant, and Chroma. PostgreSQL can also support vector search through the  extension, which makes it possible to add semantic search capabilities to a familiar relational database.</p><p>An  is an LLM that does more than generate text. It can , make decisions, and interact with external tools to complete tasks. While a chatbot mainly responds with language, an agent can browse the web, run code, query databases, call APIs, and chain these actions together to achieve a goal.</p><p>Most agents follow a simple control loop:</p><ol><li><p> the current state or new information.</p></li><li><p> about what to do next.</p></li><li><p> by using a tool or taking a step.</p></li><li><p> until the task is complete.</p></li></ol><p>The LLM serves as the decision-making engine that drives this loop.</p><p>For example, a coding agent might read a bug report, search the codebase for relevant files, analyze the logic, write a fix, run tests, see failures, revise the fix, and run the tests again until everything passes. Each step involves the model deciding what action to take next based on the latest results.</p><p>The biggest challenge is . Every step has some chance of failure, and those risks multiply across long action chains. If a 10-step task has 95% accuracy per step, the chance of everything working perfectly end-to-end drops to about 60%. </p><p>That is why modern agent frameworks invest heavily in planning, validation, retries, and self-correction to keep multi-step workflows on track.</p><p>Diffusion models are the engine behind many modern image generators, including DALL·E, Midjourney, and Nano Banana (Gemini). The core idea is a bit counterintuitive: they learn to generate images by first learning how to corrupt them.</p><p>During training, you start with real images and gradually add random noise until the image becomes almost pure static. The model is then trained to reverse that process, step by step, learning how to remove a little noise at a time and reconstruct the original image.</p><p>At generation time, you flip the pipeline. You begin with pure noise, and the model iteratively denoises it into a coherent image, guided by your text prompt. Each step makes the image slightly more structured, typically over something like 20–50 denoising steps.</p><p>The term “diffusion” comes from physics, where randomness spreads through a medium like ink dispersing in water. In diffusion models, noise spreads through the image in a similar way, and the model learns the reverse trajectory: how to go from randomness back to signal.</p><p>This same idea now extends beyond images. Diffusion-style approaches are used for video generation, audio, 3D assets, and even scientific domains like molecule and protein structure generation.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions/suggestions, feel free to leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/20-ai-concepts-explained-simply?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div>","contentLength":21643,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!kpKk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6903c7ca-215d-4727-be3a-a905bc20c93a_1152x824.png","enclosureMime":"","commentsUrl":null},{"title":"12 OOP Concepts EVERY Developer Should Know","url":"https://blog.algomaster.io/p/12-oop-concepts-every-developer-should-know","date":1770870446,"author":"Ashish Pratap Singh","guid":742,"unread":true,"content":"<p><strong>Object-Oriented Programming (OOP)</strong> gives you a practical way to structure software around real-world “things” like Users, Orders, Payments, and Notifications.</p><p>Instead of scattering data across variables and wiring behavior through unrelated functions, you <strong>bundle state and behavior</strong> into self-contained units. That makes code easier to reason about, extend, test, and maintain as the project grows.</p><p>But OOP is not just about writing classes. It is about understanding a small set of  that help you model complexity, control change, and avoid turning your codebase into a fragile mess.</p><p>In this article, we’ll cover <strong>12 OOP concepts every developer should know</strong>, with real-world examples and code. These concepts also appear frequently in . I’ve also included links to help you explore each concept in more depth.</p><p>A  is a blueprint that defines the structure and behavior of objects. It specifies what data something will hold (fields) and what actions it can perform (methods).</p><blockquote><p> Think of it like an architectural blueprint for a house. The blueprint specifies the number of rooms, doors, and windows. But you can’t live in a blueprint. You need to build an actual house from it.</p></blockquote><pre><code>public class User {\n    private String username;\n    private String email;\n    private String role;\n\n    public User(String username, String email, String role) {\n        this.username = username;\n        this.email = email;\n        this.role = role;\n    }\n\n    public boolean isAdmin() {\n        return \"ADMIN\".equals(role);\n    }\n\n    public String getDisplayName() {\n        return username + \" (\" + role + \")\";\n    }\n}</code></pre><p>In the above example, the  class bundles , , and  together with the methods that operate on them.</p><p>But a class by itself doesn’t do anything. It’s just a template. To actually use it, you need to create objects.</p><p>An  is a concrete instance of a class. It has actual values for the fields defined in the class.</p><p>If the class is a template, each object is a filled-in copy. You can create many objects from the same class, and each one is independent.</p><pre><code>// Creating objects from the User class\nUser alice = new User(\"alice\", \"alice@example.com\", \"ADMIN\");\nUser bob = new User(\"bob\", \"bob@example.com\", \"DEVELOPER\");\nUser carol = new User(\"carol\", \"carol@example.com\", \"DEVELOPER\");\n\nalice.isAdmin();          // true\nbob.isAdmin();            // false\nalice.getDisplayName();   // alice (ADMIN)</code></pre><p>Each object has its own copy of the fields. Changing ‘s role doesn’t affect . They’re independent instances built from the same template.</p><p>Classes and objects let you group related data and behavior together. But in larger systems, you often need to define what behaviors must exist without specifying how they work. That’s where interfaces come in.</p><p>An  is a contract. It defines a set of methods that a class must implement, without specifying how they should work.</p><p>Think about payment processing in an e-commerce app. You need to charge customers, but you don’t want to be locked into a single payment provider. So, you define a contract that says “any payment gateway must support charging and refunding,” and then Stripe, PayPal, Razorypay or any future provider can plug in.</p><pre><code>public interface PaymentGateway {\n    PaymentResult charge(String customerId, double amount);\n    PaymentResult refund(String transactionId);\n}\n\npublic class StripeGateway implements PaymentGateway {\n    private String apiKey;\n\n    public StripeGateway(String apiKey) {\n        this.apiKey = apiKey;\n    }\n\n    @Override\n    public PaymentResult charge(String customerId, double amount) {\n        // Stripe-specific API call\n        System.out.println(\"Charging $\" + amount + \" via Stripe\");\n        return new PaymentResult(true, \"txn_stripe_123\");\n    }\n\n    @Override\n    public PaymentResult refund(String transactionId) {\n        System.out.println(\"Refunding \" + transactionId + \" via Stripe\");\n        return new PaymentResult(true, transactionId);\n    }\n}\n\npublic class PayPalGateway implements PaymentGateway {\n    @Override\n    public PaymentResult charge(String customerId, double amount) {\n        // PayPal-specific API call\n        System.out.println(\"Charging $\" + amount + \" via PayPal\");\n        return new PaymentResult(true, \"txn_paypal_456\");\n    }\n\n    @Override\n    public PaymentResult refund(String transactionId) {\n        System.out.println(\"Refunding \" + transactionId + \" via PayPal\");\n        return new PaymentResult(true, transactionId);\n    }\n}</code></pre><p>The beauty of interfaces is that your checkout service can work with  without knowing whether it’s talking to Stripe or PayPal. Swapping providers means changing one line of configuration, not rewriting your business logic.</p><p>Interfaces tell you  classes must do. The four pillars of OOP tell you  to design those classes well.</p><p> is the practice of <strong>bundling data and methods</strong> together in a class while restricting direct access to the internal data. You expose a controlled public interface and hide everything else.</p><p>Consider a rate limiter. Other parts of your system only need to ask “can this user make another request?” They shouldn’t be able to directly mess with the internal counters or reset the time window.</p><p>Here’s what happens without encapsulation:</p><pre><code>public class RateLimiter {\n    public int requestCount;       // Anyone can modify directly\n    public long windowStartTime;   // Anyone can reset the window\n    public int maxRequests;\n}\n\nRateLimiter limiter = new RateLimiter();\nlimiter.requestCount = -100;       // Invalid state\nlimiter.windowStartTime = 0;       // Window broken</code></pre><pre><code>public class RateLimiter {\n    private int requestCount;\n    private long windowStartTime;\n    private final int maxRequests;\n    private final long windowSizeMs;\n\n    public RateLimiter(int maxRequests, long windowSizeMs) {\n        this.maxRequests = maxRequests;\n        this.windowSizeMs = windowSizeMs;\n        this.windowStartTime = System.currentTimeMillis();\n        this.requestCount = 0;\n    }\n\n    public boolean allowRequest() {\n        resetWindowIfExpired();\n        if (requestCount &lt; maxRequests) {\n            requestCount++;\n            return true;\n        }\n        return false;\n    }\n\n    public int getRemainingRequests() {\n        resetWindowIfExpired();\n        return maxRequests - requestCount;\n    }\n\n    private void resetWindowIfExpired() {\n        long now = System.currentTimeMillis();\n        if (now - windowStartTime &gt;= windowSizeMs) {\n            requestCount = 0;\n            windowStartTime = now;\n        }\n    }\n}</code></pre><p>Now nobody can corrupt the internal state. The only way to interact with the limiter is through  and . The window-reset logic is completely internal. If you later switch from a fixed window to a sliding window algorithm, none of the calling code needs to change.</p><p>Encapsulation hides a class’s internal data. But there’s a closely related concept that hides complexity at a higher level.</p><p> is about <strong>hiding unnecessary complexity</strong> and exposing only what the user needs. While encapsulation hides data, abstraction hides implementation details.</p><blockquote><p> Think about sending a message through Slack. You type a message and hit send. Behind the scenes, there’s WebSocket management, message serialization, retry logic, delivery confirmation, and push notifications. You don’t deal with any of that. The complexity is abstracted away behind a simple action.</p></blockquote><p>In code, abstraction typically uses abstract classes or interfaces to define simplified interactions:</p><pre><code>public abstract class CloudStorage {\n    // What the caller sees - one simple method\n    public String upload(String fileName, byte[] data) {\n        validate(fileName, data);\n        String path = generatePath(fileName);\n        String url = doUpload(path, data);\n        logUpload(fileName, url);\n        return url;\n    }\n\n    // Each provider implements its own upload logic\n    protected abstract String doUpload(String path, byte[] data);\n\n    private void validate(String fileName, byte[] data) {\n        if (fileName == null || data.length == 0) {\n            throw new IllegalArgumentException(\"Invalid file\");\n        }\n    }\n\n    private String generatePath(String fileName) {\n        return \"uploads/\" + System.currentTimeMillis() + \"/\" + fileName;\n    }\n\n    private void logUpload(String fileName, String url) {\n        System.out.println(\"Uploaded \" + fileName + \" to \" + url);\n    }\n}\n\npublic class S3Storage extends CloudStorage {\n    @Override\n    protected String doUpload(String path, byte[] data) {\n        // AWS SDK calls, multipart upload, encryption...\n        return \"https://s3.amazonaws.com/bucket/\" + path;\n    }\n}\n\npublic class GcsStorage extends CloudStorage {\n    @Override\n    protected String doUpload(String path, byte[] data) {\n        // Google Cloud SDK calls, resumable upload...\n        return \"https://storage.googleapis.com/bucket/\" + path;\n    }\n}</code></pre><p>The caller just invokes . They don’t need to know about path generation, validation, or provider-specific SDK calls. All that complexity is abstracted away.</p><p>Abstraction simplifies how you interact with objects. But what if multiple classes share the same data and behavior? That’s where inheritance steps in.</p><p> lets a new class (child)  from an existing class (parent), inheriting its fields and methods. The child class can reuse the parent’s code, add new behavior, or override existing behavior.</p><p>In an event-driven system, every event needs a timestamp, an event ID, and a source. But each specific event type carries its own payload. Instead of duplicating the common fields in every event class, you define them once in a base class.</p><pre><code>public class DomainEvent {\n    protected String eventId;\n    protected String source;\n    protected long timestamp;\n\n    public DomainEvent(String source) {\n        this.eventId = UUID.randomUUID().toString();\n        this.source = source;\n        this.timestamp = System.currentTimeMillis();\n    }\n\n    public String getEventId() {\n        return eventId;\n    }\n\n    public long getTimestamp() {\n        return timestamp;\n    }\n}\n\npublic class UserRegisteredEvent extends DomainEvent {\n    private String userId;\n    private String email;\n\n    public UserRegisteredEvent(String userId, String email) {\n        super(\"user-service\");\n        this.userId = userId;\n        this.email = email;\n    }\n\n    public String getUserId() {\n        return userId;\n    }\n}\n\npublic class OrderPlacedEvent extends DomainEvent {\n    private String orderId;\n    private double totalAmount;\n\n    public OrderPlacedEvent(String orderId, double totalAmount) {\n        super(\"order-service\");\n        this.orderId = orderId;\n        this.totalAmount = totalAmount;\n    }\n\n    public String getOrderId() {\n        return orderId;\n    }\n}</code></pre><p> and  both get , , , and  from  without writing that code again. They also add their own unique fields.</p><p>Use inheritance when there’s a clear  relationship. A . A . Avoid inheriting just to reuse code. If there’s no natural “is-a” relationship, use composition instead.</p><p>Inheritance lets classes share structure and behavior. But what happens when you call the same method on different child classes and get different results? </p><p>Polymorphism means “many forms.” It allows objects of different types to be treated through a common interface, with each type providing its own behavior.</p><ul><li><p> (method overloading): same method name, different parameters</p></li><li><p> (method overriding): same method signature, different implementations in child classes</p></li></ul><p>Runtime polymorphism is the more powerful concept. Imagine a notification system that sends alerts through different channels:</p><pre><code>public interface NotificationChannel {\n    void send(String recipient, String message);\n}\n\npublic class EmailChannel implements NotificationChannel {\n    @Override\n    public void send(String recipient, String message) {\n        // SMTP setup, HTML formatting, attachment handling...\n        System.out.println(\"Email to \" + recipient + \": \" + message);\n    }\n}\n\npublic class SlackChannel implements NotificationChannel {\n    @Override\n    public void send(String recipient, String message) {\n        // Slack API call, channel lookup, markdown formatting...\n        System.out.println(\"Slack to #\" + recipient + \": \" + message);\n    }\n}\n\npublic class SmsChannel implements NotificationChannel {\n    @Override\n    public void send(String recipient, String message) {\n        // Twilio API, phone number validation, character limits...\n        System.out.println(\"SMS to \" + recipient + \": \" + message);\n    }\n}\n\n// Polymorphism in action\nList&lt;NotificationChannel&gt; channels = List.of(\n    new EmailChannel(), new SlackChannel(), new SmsChannel()\n);\n\nfor (NotificationChannel channel : channels) {\n    channel.send(\"ops-team\", \"Server CPU above 90%\");\n    // Each channel sends the alert its own way\n}</code></pre><p>The loop doesn’t know or care whether it’s sending an email, a Slack message, or an SMS. It calls  on each one, and the right implementation runs automatically. If you add a  tomorrow, the loop works without any changes.</p><p>This is the real power of polymorphism: you can write code that works with abstractions, and it automatically handles new types as they’re added.</p><p>Now that we understand how individual classes are structured and designed, let’s look at how objects relate to each other.</p><p> represents a “knows-about” relationship between objects. Both objects exist independently. Neither owns or controls the other.</p><p>Think of a developer and a repository on GitHub. A developer contributes to multiple repositories, and a repository has multiple contributors. But if a developer deletes their account, the repository still exists. And if a repository is archived, the developer keeps working on other things.</p><pre><code>public class Developer {\n    private String username;\n    private List&lt;Repository&gt; repositories;\n\n    public Developer(String username) {\n        this.username = username;\n        this.repositories = new ArrayList&lt;&gt;();\n    }\n\n    public void contributeTo(Repository repo) {\n        repositories.add(repo);\n    }\n}\n\npublic class Repository {\n    private String name;\n    private List&lt;Developer&gt; contributors;\n\n    public Repository(String name) {\n        this.name = name;\n        this.contributors = new ArrayList&lt;&gt;();\n    }\n\n    public void addContributor(Developer dev) {\n        contributors.add(dev);\n    }\n}\n\n// Both objects are created independently\nDeveloper dev = new Developer(\"alice\");\nRepository repo = new Repository(\"payment-service\");\n\n// They reference each other, but neither owns the other\ndev.contributeTo(repo);\nrepo.addContributor(dev);</code></pre><p>The key here is independence. Both  and  are created outside of each other and just hold references. Deleting one doesn’t affect the other.</p><p>Association is the most general type of relationship. But sometimes, one object is part of another. That brings us to aggregation.</p><p> is a specialized form of association that represents a “has-a” relationship where the whole contains parts, but the parts can exist independently.</p><p>Think of a team and its microservices. A team owns multiple microservices, but if the team is reorganized, the services don’t disappear. They get reassigned to a different team.</p><pre><code>public class Team {\n    private String name;\n    private List&lt;Microservice&gt; services;\n\n    public Team(String name) {\n        this.name = name;\n        this.services = new ArrayList&lt;&gt;();\n    }\n\n    // Services are created outside and assigned to the team\n    public void addService(Microservice service) {\n        services.add(service);\n    }\n\n    public void removeService(Microservice service) {\n        services.remove(service);\n    }\n}\n\npublic class Microservice {\n    private String name;\n    private String repoUrl;\n\n    public Microservice(String name, String repoUrl) {\n        this.name = name;\n        this.repoUrl = repoUrl;\n    }\n}\n\n// Microservice exists independently\nMicroservice paymentService = new Microservice(\"payment-service\", \"github.com/org/payments\");\n\n// Team references the service but doesn't own it\nTeam platformTeam = new Team(\"Platform\");\nplatformTeam.addService(paymentService);\n\n// Service can be reassigned to another team\nTeam checkoutTeam = new Team(\"Checkout\");\ncheckoutTeam.addService(paymentService);</code></pre><p>The team has services, but services have their own lifecycle. They exist before being assigned to a team and continue to exist after being reassigned.</p><p>In aggregation, parts can survive without the whole. But what if the parts are so tightly coupled to the whole that they shouldn’t exist independently? </p><p> is a strong form of “has-a” where the whole owns the parts entirely. When the whole is destroyed, the parts are destroyed with it. The parts have no meaning outside of the whole.</p><p>Think of an order and its line items. Each line item (2x T-Shirt, 1x Laptop) only exists as part of that specific order. If the order is cancelled and deleted, the line items go with it. A line item floating around without an order makes no sense.</p><pre><code>public class Order {\n    private String orderId;\n    private List&lt;LineItem&gt; lineItems;  // Order creates and owns line items\n\n    public Order(String orderId) {\n        this.orderId = orderId;\n        this.lineItems = new ArrayList&lt;&gt;();\n    }\n\n    // Order creates the line item internally\n    public void addItem(String productId, String productName, int quantity, double price) {\n        lineItems.add(new LineItem(productId, productName, quantity, price));\n    }\n\n    public double getTotal() {\n        return lineItems.stream()\n            .mapToDouble(LineItem::getSubtotal)\n            .sum();\n    }\n\n    public void cancel() {\n        lineItems.clear();  // Line items destroyed with the order\n        System.out.println(\"Order \" + orderId + \" cancelled\");\n    }\n}\n\npublic class LineItem {\n    private String productId;\n    private String productName;\n    private int quantity;\n    private double unitPrice;\n\n    // Package-private: only Order should create line items\n    LineItem(String productId, String productName, int quantity, double unitPrice) {\n        this.productId = productId;\n        this.productName = productName;\n        this.quantity = quantity;\n        this.unitPrice = unitPrice;\n    }\n\n    double getSubtotal() {\n        return quantity * unitPrice;\n    }\n}\n\n// Order creates line items internally - they don't exist outside\nOrder order = new Order(\"ORD-001\");\norder.addItem(\"SKU-100\", \"Mechanical Keyboard\", 1, 149.99);\norder.addItem(\"SKU-200\", \"USB-C Hub\", 2, 39.99);\nSystem.out.println(order.getTotal());  // 229.97\norder.cancel();  // All line items destroyed</code></pre><p>Notice the difference from aggregation: in composition, the whole creates its parts internally ( inside ). In aggregation, parts are passed in from outside.</p><p>Composition is about ownership and lifecycle control. But not all relationships involve ownership. Sometimes one object just temporarily uses another. </p><p>Dependency is the weakest relationship between classes. It represents a temporary “uses-a” connection where one class uses another, typically as a method parameter, local variable, or return type, but doesn’t hold a long-term reference to it.</p><p>Think of a deployment pipeline. The pipeline uses a logger to record what’s happening, but it doesn’t own the logger or keep it around as part of its state. It just uses it during execution and moves on.</p><pre><code>public class DeploymentService {\n    // Dependency: uses HttpClient temporarily, doesn't store it\n    public DeploymentResult deploy(String serviceName, String version, HttpClient client) {\n        String url = \"https://deploy.internal/\" + serviceName;\n        HttpResponse response = client.post(url, Map.of(\"version\", version));\n\n        if (response.getStatusCode() == 200) {\n            return new DeploymentResult(true, \"Deployed \" + serviceName + \" v\" + version);\n        }\n        return new DeploymentResult(false, \"Deployment failed: \" + response.getBody());\n    }\n}\n\npublic class HttpClient {\n    public HttpResponse post(String url, Map&lt;String, String&gt; body) {\n        // HTTP connection setup, request serialization, TLS...\n        System.out.println(\"POST \" + url);\n        return new HttpResponse(200, \"OK\");\n    }\n}\n\n// DeploymentService uses HttpClient but doesn't own or store it\nDeploymentService deployer = new DeploymentService();\nHttpClient client = new HttpClient();\ndeployer.deploy(\"payment-service\", \"2.4.1\", client);</code></pre><p> depends on , but only during the  call. It doesn’t store the client as a field. Once the method returns, the relationship is gone.</p><p>Dependency is the weakest of the object relationships. The last concept in our list brings us full circle, connecting interfaces back to the classes that implement them.</p><p>Realization is the relationship between an interface and the class that implements it. The class “realizes” the contract defined by the interface by providing concrete implementations of all its methods.</p><p>We already saw this with  in the interfaces section. Let’s look at another example, a cache store:</p><pre><code>public interface CacheStore {\n    void put(String key, String value, int ttlSeconds);\n    String get(String key);\n    void evict(String key);\n}\n\npublic class RedisCache implements CacheStore {\n    private String connectionUrl;\n\n    public RedisCache(String connectionUrl) {\n        this.connectionUrl = connectionUrl;\n    }\n\n    @Override\n    public void put(String key, String value, int ttlSeconds) {\n        // Redis SETEX command with TTL\n        System.out.println(\"Redis SET \" + key + \" EX \" + ttlSeconds);\n    }\n\n    @Override\n    public String get(String key) {\n        // Redis GET command\n        System.out.println(\"Redis GET \" + key);\n        return null;  // Simplified\n    }\n\n    @Override\n    public void evict(String key) {\n        // Redis DEL command\n        System.out.println(\"Redis DEL \" + key);\n    }\n}</code></pre><p>Each class promises to fulfill the  contract. Your application code depends on , so you can use Redis in production, an in-memory map in tests, and Memcached in a different environment, all without changing a single line of business logic.</p><p>Realization is what makes polymorphism through interfaces possible. It’s the bridge between abstract contracts and concrete behavior.</p><p>Here’s how all 12 concepts relate to each other:</p><p>These 12 concepts form the foundation of object-oriented design. You don’t need to use all of them in every project, but understanding each one and knowing when to apply it will make you a better software engineer and help you tackle Low-Level Design interviews with confidence.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions/suggestions, feel free to leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/12-oop-concepts-every-developer-should-know?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div>","contentLength":22646,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!GcX3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069b5b1c-ea4e-4c1e-b77b-640b236b8d83_2638x1320.png","enclosureMime":"","commentsUrl":null},{"title":"I created a comprehensive resource to master Concurrency Interviews","url":"https://blog.algomaster.io/p/concurrency-interview-resource","date":1770300384,"author":"Ashish Pratap Singh","guid":741,"unread":true,"content":"<p>I’m excited to announce the launch of my , built to be the most complete, structured, and high-quality resources for concurrency interview prep available on the internet.</p><p>It covers <strong>concurrency and synchronization fundamentals</strong>,, and <strong>25 commonly asked interview problems</strong> organized by category, each with detailed explanations and implementations. The content supports 5 languages<strong>: Java, Python, C++, C#, and Go</strong>.</p><p>I’ve kept a meaningful portion of the course (around ~50%) as free. For full access, you can .</p><h3>25 Interview Problems (and growing)</h3><p>The course includes <strong>25 concurrency interview problems</strong>, with plans to add more over time.</p><p>For each problem, you get:</p><ul><li><p>The core concurrency challenges</p></li><li><p>Multiple synchronization approaches (so you learn trade-offs, not just one solution)</p></li><li><p>Clean implementations across supported languages</p></li></ul><h3>Language Specific Deep-Dives</h3><p>Concurrency is deeply language-dependent. Each language has its own primitives, libraries, and best practices.</p><p>That’s why the course includes dedicated deep-dive chapters for each language, covering the key concurrency primitives and standard libraries.</p><p>Concurrency is easier to learn when you can visualize what threads are doing.</p><p>This course includes  (flowcharts, sequence diagrams, state diagrams, and more) to make the behavior of threads, locks, and coordination mechanisms feel intuitive.</p><p>There are quizzes after chapters to test and reinforce your understanding.</p><p>There are interactive simulations to help you better understand the multi-threading concepts. I plan to add more simulations in coming weeks.</p><p>For any questions related to content or subscription, please reply to this email or reach out at </p>","contentLength":1652,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!QD7M!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c970fb5-a9f2-46c3-bd80-e8408439a6c6_2506x1736.png","enclosureMime":"","commentsUrl":null},{"title":"7 Graph Algorithms You Should Know for Coding Interviews in 2026","url":"https://blog.algomaster.io/p/7-graph-algorithms-you-should-know","date":1770177782,"author":"Ashish Pratap Singh","guid":740,"unread":true,"content":"<p>In this post Shayan will share  to know if you are preparing for coding interviews.</p><p>Graphs come up in about 35-40% of coding interviews at major tech companies because so many real systems are graphs: social networks, map routing, dependency chains, web crawlers. If you don’t know core graph patterns, you’ll struggle in the interview.</p><p>I’ve seen candidates get stuck on problems like “Number of Islands” simply because they hadn’t practiced basic grid traversal. These problems become straightforward once you know the patterns.</p><p>In this post, I’ll show you 7 graph algorithms that cover about 85% of graph-related interview questions. For each one, you’ll learn what it does, when to use it, how it works, and which LeetCode problems to practice.</p><blockquote><p> BFS explores a graph . It visits every node at distance 1 from the start, then distance 2, then distance 3, and so on.</p></blockquote><p>Use BFS when the problem is naturally about  or :</p><ul><li><p>Finding the shortest path in an unweighted graph</p></li><li><p>Finding all nodes within K distance</p></li><li><p>Any problem that asks for “minimum steps” or “shortest path” without weights</p></li></ul><p>BFS uses a . You start by adding the source node. Then you repeat: remove the front node, process it, and add all its unvisited neighbors to the back of the queue.</p><p>The queue enforces the level-by-level order. By the time you reach a node, you’ve visited all closer nodes first. This guarantees the first path you find is the shortest.</p><pre><code>queue = [start]\nvisited = {start}\n\nwhile queue is not empty:\n    node = queue.pop_front()\n    for neighbor in node.neighbors:\n        if neighbor not in visited:\n            visited.add(neighbor)\n            queue.push_back(neighbor)</code></pre><h4>Grid Problems (Flood Fill)</h4><p>BFS works well on 2D grids. </p><p>Think of grids like this:</p><ul><li><p>Its neighbors are usually  (sometimes diagonals too)</p></li></ul><p>Problems like “Number of Islands” and “Rotting Oranges” are grid-based BFS.</p><p>For flood fill, you start at a cell and spread to all connected cells matching a condition. You stop at boundaries or cells that don’t match. This is the algorithm behind the paint bucket tool in image editors.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> DFS explores a graph  before it backtracks. If a node has multiple neighbors, DFS fully explores the first neighbor’s branch, then returns and tries the next.</p></blockquote><p>DFS is the right tool when you care about <strong>reachability, structure, or exhaustive exploration</strong>, not minimum distance:</p><ul><li><p>Finding connected components</p></li><li><p>Path finding (when you don’t need the shortest path)</p></li></ul><p>A good mental model is a maze.</p><p>You choose a path, keep walking, and mark where you’ve been. When you hit a dead end, you backtrack to the last fork and try a different direction.</p><ul><li><p>Either an explicit stack you manage yourself</p></li><li><p>Or recursion, which uses the call stack implicitly</p></li></ul><p>Because the “most recent” node is processed next, DFS naturally pushes deeper into the graph.</p><pre><code>function dfs(node):\n    if node in visited:\n        return\n    visited.add(node)\n\n    for neighbor in node.neighbors:\n        dfs(neighbor)</code></pre><p>DFS is one of the most common ways to detect cycles, but the exact rule depends on the graph type (directed vs undirected graphs).</p><p>For undirected graphs: if you reach a visited node that isn’t your immediate parent, you’ve found a cycle.</p><p>For directed graphs: you use three states (unvisited, in-progress, completed). If you reach an in-progress node, you’ve found a back edge, which means a cycle.</p><pre><code>// Directed graph cycle detection\nstate = [UNVISITED] * n\n\nfunction hasCycle(node):\n    state[node] = IN_PROGRESS\n\n    for neighbor in node.neighbors:\n        if state[neighbor] == IN_PROGRESS:\n            return true  // cycle found\n        if state[neighbor] == UNVISITED:\n            if hasCycle(neighbor):\n                return true\n\n    state[node] = COMPLETED\n    return false</code></pre><p><strong>Practice these LeetCode problems:</strong></p><blockquote><p> Dijkstra’s Algorithm finds the <strong>shortest path in a weighted graph</strong> where all edge weights are . Unlike BFS, it works when edges have different costs.</p></blockquote><p>Reach for Dijkstra when you see  and the question is about :</p><ul><li><p>Shortest path with weighted edges</p></li><li><p>Navigation and routing (Google Maps)</p></li><li><p>Network routing with latency costs</p></li><li><p>Any problem mentioning “minimum cost path”</p></li></ul><p>BFS doesn’t work on weighted graphs because one step doesn’t equal one unit of distance. A direct path might cost 10 while a two-step path costs 2.</p><p>Dijkstra fixes this by always expanding the <strong>currently cheapest known node first</strong>:</p><ul><li><p>Maintain  = best known distance from the source</p></li><li><p>Use a <strong>min-heap / priority queue</strong> keyed by distance</p></li><li><p>Pop the node with the smallest distance, then relax its edges</p></li></ul><p>If the graph has only non-negative weights, the first time a node is popped with its best distance, that distance is final.</p><pre><code>dist = [infinity] * numNodes\ndist[source] = 0\npq = [(0, source)]  // (distance, node)\n\nwhile pq is not empty:\n    d, node = pq.pop_min()\n\n    if d &gt; dist[node]:\n        continue  // found a better path already\n\n    for (neighbor, weight) in node.edges:\n        newDist = d + weight\n        if newDist &lt; dist[neighbor]:\n            dist[neighbor] = newDist\n            pq.push((newDist, neighbor))</code></pre><h4>Sample Problem: Network Delay Time</h4><p>You have n network nodes. You’re given travel times as directed edges (u, v, w) where w is the time. You send a signal from node k. How long until all nodes receive it?</p><p>To solve this, you run Dijkstra from node k. Your answer is the maximum distance among all reachable nodes. If any node is unreachable, you return -1.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> Topological sort orders nodes in a <strong>Directed Acyclic Graph (DAG)</strong> so that for every edge , node  appears  node . In plain terms: it gives you an execution order that respects dependencies. That’s why it shows up so often in scheduling-style problems.</p></blockquote><p>Topological sort is the default pattern when you see dependency language:</p><ul><li><p>Course scheduling with prerequisites</p></li><li><p>Build systems and dependency resolution</p></li><li><p>Any problem that mentions “prerequisites” or “dependencies”</p></li></ul><h4>How it works (Kahn’s Algorithm):</h4><p>Kahn’s algorithm is the BFS-style way to do topological sorting.</p><p> A node’s in-degree is the number of edges pointing to it. If a node has in-degree 0, it has no dependencies and you can process it.</p><p>You start by adding all nodes with in-degree 0 to a queue. You process them one by one. When you process a node, you decrement the in-degree of its neighbors. If a neighbor’s in-degree drops to 0, you add it to the queue.</p><p>If you process all nodes, you have a valid topological order. If the queue empties before you’ve processed all nodes, you’ve found a cycle.</p><pre><code>inDegree = count incoming edges for each node\nqueue = all nodes with inDegree 0\nresult = []\n\nwhile queue is not empty:\n    node = queue.pop()\n    result.append(node)\n\n    for neighbor in node.outgoing:\n        inDegree[neighbor] -= 1\n        if inDegree[neighbor] == 0:\n            queue.push(neighbor)\n\nif len(result) &lt; numNodes:\n    return \"cycle detected\"</code></pre><h4>Sample Problem: Course Schedule II</h4><p>You have numCourses courses. Some have prerequisites: [0, 1] means you take course 1 before course 0. You need to return any valid order to finish all courses, or an empty array if impossible.</p><p>To solve this, you build a directed graph where edge b→a means “take b before a”. Then run Kahn’s algorithm. If you can’t process all courses, you’ve found a cyclic dependency.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> Union-Find is a data structure for tracking a collection of elements split into <strong>disjoint (non-overlapping) sets</strong>. It supports two core operations:</p><ul><li><p> which set does  belong to?</p></li><li><p> merge the sets containing  and </p></li></ul><p>Once you have these, you can answer connectivity questions like “are these two nodes connected?” efficiently.</p></blockquote><p>Union-Find shines when you’re adding connections over time and need fast connectivity checks:</p><ul><li><p>Dynamic connectivity queries</p></li><li><p>Detecting cycles in undirected graphs</p></li><li><p>Grouping elements as edges are added</p></li></ul><p>Each set has a leader (representative). Two elements are in the same set if they have the same leader. You store a parent array where parent[i] points to i’s parent. The root is the leader (where parent[i] = i).</p><p>To make operations extremely fast in practice, Union-Find uses two standard optimizations:</p><ol><li><p>: When you find the leader, you make each node on the path point directly to the leader.</p></li><li><p>: When merging two sets, attach the smaller tree under the larger one to keep trees shallow.</p></li></ol><pre><code>parent = [0, 1, 2, ..., n-1]  // each node is its own leader\nrank = [0] * n\n\nfunction find(x):\n    if parent[x] != x:\n        parent[x] = find(parent[x])  // path compression\n    return parent[x]\n\nfunction union(x, y):\n    px, py = find(x), find(y)\n    if px == py:\n        return false  // already connected\n\n    if rank[px] &lt; rank[py]:\n        parent[px] = py\n    else if rank[px] &gt; rank[py]:\n        parent[py] = px\n    else:\n        parent[py] = px\n        rank[px] += 1\n\n    return true</code></pre><h4>Sample Problem: Redundant Connection</h4><p>You have a graph that started as a tree (connected, no cycles), then one edge was added. You need to find the edge that can be removed to restore the tree.</p><p>To solve this using union find, process edges one by one. For each edge (u, v), you check if u and v are already connected using find(). If yes, this edge creates a cycle. If no, you union them. The first edge that connects already-connected nodes is your answer.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> A spanning tree connects all nodes in a graph using exactly n-1 edges with no cycles. The minimum spanning tree is the one with the smallest total edge weight.</p></blockquote><p>MST comes up whenever you need to connect a set of nodes with minimum total cost:</p><ul><li><p>Connecting all nodes with minimum cost</p></li><li><p>Network design (cables, roads, pipelines)</p></li><li><p>Approximation algorithms for NP-hard problems</p></li></ul><h4>How it works (Kruskal’s Algorithm):</h4><p>Kruskal’s is the most common MST approach in interviews because it’s simple and pairs naturally with Union-Find.</p><p>You sort all edges by weight. You process them in order. For each edge, you use Union-Find to check if it connects two different components. If yes, you add it to the MST. If no, you skip it to avoid a cycle.</p><pre><code>edges.sort(by weight)\nmst = []\nuf = UnionFind(n)\n\nfor edge in edges:\n    u, v, weight = edge\n    if uf.find(u) != uf.find(v):\n        uf.union(u, v)\n        mst.append(edge)\n\n    if len(mst) == n - 1:\n        break\n\nreturn mst</code></pre><h4>Sample Problem: Min Cost to Connect All Points</h4><p>You’re given points on a 2D plane. The cost to connect two points is their Manhattan distance. You return the minimum cost to connect all points.</p><p>To solve this, you treat each pair of points as a potential edge. Generate all edges, sort by weight, and run Kruskal’s. The MST gives you the minimum total cost.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> A graph is  if you can split its nodes into two groups such that <strong>every edge connects nodes from different groups</strong>. No edge is allowed within the same group.</p><p>A simple way to remember it: Can you  the graph so that no two adjacent nodes share the same color?</p></blockquote><p>Bipartite checks appear whenever the problem is about dividing things into two compatible sets:</p><ul><li><p>Team or group assignments</p></li><li><p>Checking for odd-length cycles (a graph is bipartite if and only if it has no odd-length cycles)</p></li><li><p>Scheduling with conflicts</p></li></ul><p>You can use  to try 2-coloring the graph.</p><p>Pick a start node and assign it color 0. Assign all its neighbors color 1. Assign their neighbors color 0. Continue alternating.</p><p>If you ever find an edge where both endpoints have the , the graph is  bipartite.</p><pre><code>color = [-1] * n  // uncolored\n\nfunction isBipartite(start):\n    queue = [start]\n    color[start] = 0\n\n    while queue is not empty:\n        node = queue.pop()\n        for neighbor in node.neighbors:\n            if color[neighbor] == -1:\n                color[neighbor] = 1 - color[node]\n                queue.push(neighbor)\n            else if color[neighbor] == color[node]:\n                return false\n\n    return true</code></pre><h4>Sample Problem: Is Graph Bipartite?</h4><p>You’re given an undirected graph. You return true if it’s bipartite.</p><p>Because the graph may be disconnected, run BFS/DFS from :</p><ul><li><p>Start a new traversal, try to 2-color that component</p></li><li><p>If any component fails, return </p></li><li><p>If all components succeed, the graph is bipartite</p></li></ul><h4>Practice these LeetCode problems:</h4><p>You now have 7 algorithms that cover most graph problems in coding interviews. Here’s how to retain them:</p><ol><li><p> Start with BFS and DFS. They’re the foundation the others build on.</p></li><li><p><strong>Match patterns to algorithms.</strong> “Shortest path” without weights → BFS. “Prerequisites” or “dependencies” → topological sort. “Minimum cost to connect” → MST.</p></li><li><p> Don’t just read the code. Write it yourself. Debug it. That’s how you learn.</p></li><li><p><strong>Practice with time limits.</strong> In interviews, you have 20-30 minutes per problem. Get comfortable working under that constraint.</p></li></ol><p>For a structured approach to learn, feel free to take a look at our roadmaps. It covers these algorithms plus topics like Strongly Connected Components, Lowest Common Ancestor, and Network Flow.</p>","contentLength":12897,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!1puy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd166816b-b57e-40e3-ac90-33b31795d92e_1280x926.jpeg","enclosureMime":"","commentsUrl":null},{"title":"Polling vs. Long Polling vs. SSE vs. WebSockets vs. Webhooks","url":"https://blog.algomaster.io/p/polling-vs-long-polling-vs-sse-vs-websockets-webhooks","date":1770091373,"author":"Ashish Pratap Singh","guid":739,"unread":true,"content":"<p>Whether you are chatting with a friend or playing an online game, updates show up in real time without hitting .</p><p>Behind these seamless experiences lies a key engineering decision: <strong>how does the server notify the client (or another system) when new data is available?</strong></p><p>The traditional HTTP was built around a simple request-response flow: <strong>the client asks, the server answers</strong>. But in real-time systems, the server often needs to push updates proactively, sometimes continuously.</p><p>That’s where communication models like <strong>Long Polling, Server-Sent Events (SSE), WebSockets, and Webhooks</strong> come in.</p><p>In this article, we’ll break down how each one works, it’s pros and cons, where it fits best, and how to choose the right approach for a  or a .</p><p>Let's start with the most straightforward approach.</p><p> is the simplest approach to getting updates from a server. The client sends requests to the server at regular intervals, checking if anything has changed.</p><p>Think of it like refreshing your email inbox every few minutes to check for new messages.</p><ol><li><p>Client sends an HTTP request to the server</p></li><li><p>Server responds immediately with current data (or empty response)</p></li><li><p>Client waits for a fixed interval (e.g., 5 seconds)</p></li><li><p>Client sends another request</p></li></ol><p>Notice something wasteful here? The client keeps asking even when nothing has changed. Three out of four requests in this diagram returned empty responses. </p><p>In real applications, this ratio is often much worse. You might make 100 requests before getting a single meaningful update.</p><h3>Example: Weather Dashboard</h3><p>Imagine you’re building a weather dashboard. Weather data doesn’t change that frequently, maybe every 15-30 minutes at most. </p><p>Polling makes sense here:</p><pre><code>setInterval(async () =&gt; {\n    const response = await fetch('/api/weather?city=london');\n    const weather = await response.json();\n    updateDashboard(weather);\n}, 60000); // Poll every minute</code></pre><p>Every minute, your client asks for the current weather. The server responds with temperature, humidity, conditions, and so on.</p><ul><li><p> Just a regular HTTP request in a loop. No special protocols or libraries needed.</p></li><li><p> Any HTTP client can do polling. No firewall or proxy issues.</p></li><li><p> Each request is independent. The server doesn’t need to maintain any connection state.</p></li><li><p> Standard HTTP requests that show up in network logs and dev tools.</p></li></ul><ul><li><p> Most requests return empty responses when nothing has changed. This wastes bandwidth and server resources.</p></li><li><p> Updates are delayed by the polling interval. If you poll every 10 seconds, updates can take up to 10 seconds to reach the client.</p></li><li><p> 10,000 clients polling every second means 10,000 requests per second, even when nothing is happening.</p></li><li><p><strong>Trade-off between latency and efficiency:</strong> Shorter intervals mean faster updates but more wasted requests. Longer intervals mean fewer requests but slower updates.</p></li></ul><ul><li><p> Weather data, daily reports, or anything that changes infrequently</p></li><li><p> MVPs, internal tools, or situations where simplicity matters more than efficiency</p></li><li><p> When you need to support older clients or environments that can’t use modern techniques</p></li><li><p> When delays of several seconds (or minutes) are acceptable</p></li></ul><p>Polling is a reasonable starting point, but you’ll quickly feel its limitations as your application grows. If you need faster updates without drowning your server in requests, that’s where long polling comes in.</p><p> improves on regular polling by having the server hold the request open until new data is available (or a timeout occurs). Instead of the client repeatedly asking “anything new?”, the server waits and responds only when there’s something to report.</p><p>This was the technique that powered early real-time applications like Facebook Messenger before WebSockets became widely supported.</p><ol><li><p>Client sends an HTTP request to the server</p></li><li><p>Server holds the connection open (doesn’t respond immediately)</p></li><li><p>When new data arrives, server sends the response</p></li><li><p>Client immediately sends another request</p></li><li><p>If no data arrives within the timeout period, server sends an empty response and client reconnects</p></li></ol><p>The key insight is that the server only responds when it has something meaningful to say. This eliminates the wasted “nothing new” responses of regular polling.</p><h3>Example: Chat Application</h3><p>Consider a chat app built with long polling. When you open a conversation, your browser sends a request like:</p><pre><code>GET /api/messages?conversation=123&amp;after=msg_999</code></pre><p>The server checks if there are any messages newer than . If not, instead of returning an empty response, it holds the connection and waits. </p><p>When someone sends a new message to that conversation, the server immediately responds with the new message. Your client receives it, renders it in the chat window, and immediately opens a new connection to wait for the next message.</p><p>There’s an important detail here: the . HTTP connections can’t stay open forever. Proxies, load balancers, and browsers all have limits. So the server needs to respond eventually, even if nothing happened.</p><p>A typical timeout is 30 seconds. If 30 seconds pass with no new data, the server sends an empty response, the client immediately reconnects, and the wait continues.</p><ul><li><p> Updates arrive almost instantly when they happen, without waiting for a polling interval.</p></li><li><p> Responses almost always contain useful data, not empty “nothing new” responses.</p></li><li><p><strong>Works through proxies and firewalls:</strong> Uses standard HTTP, so it works in restrictive network environments where WebSockets might be blocked.</p></li><li><p> No protocol upgrade, no special handling for connection state.</p></li></ul><ul><li><p> Each waiting client holds a connection open on the server. With 10,000 clients, you need 10,000 open connections.</p></li><li><p><strong>Timeout handling complexity:</strong> You need to handle timeouts, reconnection logic, and edge cases like the client receiving data just as the timeout expires.</p></li><li><p> If multiple events happen quickly, they may get batched together or delivered out of order.</p></li><li><p> Every response requires a new request, and each request carries HTTP headers. This overhead adds up.</p></li></ul><ul><li><p><strong>Chat applications (historically):</strong> Before WebSocket support was universal, long polling powered most chat systems</p></li><li><p> When WebSockets aren’t available due to proxy or firewall restrictions</p></li><li><p><strong>Server-initiated updates:</strong> When clients mostly receive data rather than send it</p></li><li><p> Works well for hundreds or thousands of concurrent connections, but gets expensive at massive scale</p></li></ul><p>Long polling feels like “almost real-time,” but it’s still request-response at heart. The client still initiates every exchange. What if the server could just push data to clients whenever it wants? That’s exactly what Server-Sent Events enable.</p>","contentLength":6525,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!DkxM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf447e18-ccb7-4f9c-8e09-2a4d51989ac8_2068x1366.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}