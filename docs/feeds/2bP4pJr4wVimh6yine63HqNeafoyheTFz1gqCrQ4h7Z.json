{"id":"2bP4pJr4wVimh6yine63HqNeafoyheTFz1gqCrQ4h7Z","title":"AlgoMaster Newsletter","displayTitle":"Dev - Algomaster","url":"https://blog.algomaster.io/feed","feedLink":"https://blog.algomaster.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":4,"items":[{"title":"Linked List: Explained in 4 minutes","url":"https://blog.algomaster.io/p/linked-list","date":1763267332,"author":"Ashish Pratap Singh","guid":699,"unread":true,"content":"<p> is one of the most fundamental data structures in computer science and a must-know topic for coding interviews.</p><p>Unlike arrays, linked lists don’t require you to predefine a size.</p><p>They allocate memory dynamically as elements are added. And because each element points to the next, you can  without shifting anything.</p><p>In this article, I’ll break down:</p><ul><li><p>What a linked list is and how it works</p></li><li><p>Different types of linked lists</p></li><li><p>Most common linked list operations and their time complexities</p></li><li><p>and the most common interview patterns built around linked lists</p></li></ul><p>I also made a  on Linked List. Check it out for more in-depth and visual explanation.</p><p><strong>Subscribe for more such videos!</strong></p><p>A  is a linear data structure made up of .</p><p>Each node contains two parts:</p><ol><li><p>A <strong>value: the actual data you want to store</strong></p></li><li><p>and a  or  to the  in the sequence</p></li></ol><p>The last node points to null, indicating the end of list.</p><p>Unlike arrays, where all elements are stored together in a single, continuous block of memory, the nodes in a linked list can be .</p><p>What keeps them connected are those pointers linking one node to the next.</p><p>This structure makes linked lists . You can grow or shrink them dynamically, and insertions or deletions don’t require shifting large chunks of memory like arrays do.</p><p>But this flexibility comes with a trade-off.</p><ul><li><p>Since the elements aren’t stored together, you  to a specific item like you can with arrays.</p></li></ul><p>Instead, to access the 5th or 10th element, you may have to <strong>traverse the list from the beginning</strong>, one node at a time, which takes .</p><p>Depending on how the nodes are connected, we can categorize them into three common types:</p><p>In a , each node contains a value and a pointer to the  and that’s it.</p><pre><code>class ListNode {\n    int val;\n    ListNode next;\n\n    ListNode(int val) {\n        this.val = val;\n        this.next = null;\n    }\n}</code></pre><p>You can only move in , from the head toward the tail.</p><ul><li><p>It’s simple to implement and memory-efficient.</p></li><li><p>But there’s a downside: if you need to go , there’s no easy way. You’d have to <strong>start from the head and traverse again</strong> until you reach the previous node.</p></li></ul><p>Singly linked lists are great for problems where you only need forward traversal.</p><p>A  takes things one step further.</p><p>Each node has :</p><ul><li><p>One pointing to the  node</p></li><li><p>One pointing to the  node</p></li></ul><pre><code>class DoublyListNode {\n    int val;\n    DoublyListNode next;\n    DoublyListNode prev;\n\n    DoublyListNode(int val) {\n        this.val = val;\n        this.next = null;\n        this.prev = null;\n    }\n}</code></pre><p>This means you can move , which makes certain operations much more efficient.</p><p>But the trade-off is that it uses  to store the additional pointer and it introduces a bit more complexity in updating links during insertions or deletions.</p><p>Doubly linked lists are widely used in real-world systems like: <strong>LRU caches, text editors for undo/redo functionality, and browser history navigation.</strong></p><p>A  is a variation where the <strong>last node points back to the first node</strong>, instead of .</p><p>This forms a . You can start at any node and keep going until you’re back where you started.</p><pre><code>class CircularListNode {\n    int val;\n    CircularListNode next;\n\n    CircularListNode(int val) {\n        this.val = val;\n        this.next = this; // initially points to itself\n    }\n}</code></pre><p>It’s great for problems where you need cyclical behavior like round-robin scheduling, or multiplayer games where turns cycle through players.</p><p>Now that we know what linked lists are and the different types, let’s go over the most common operations you’ll perform on them.</p><p>Traversal means moving through the list, visiting each node one by one. You  and move one node at a time.</p><pre><code>void traverse(Node head) {\n    Node current = head;\n\n    while (current != null) {\n        System.out.print(current.value + “ -&gt; “);\n        current = current.next;\n    }\n\n    System.out.println(”null”);\n}</code></pre><p>This operation takes  because, in the worst case, you might visit every node.</p><p>If you want to find whether a value exists in a linked list, you must:</p><ul><li><p>Check each node one by one until you find the value or reach the end</p></li></ul><pre><code>boolean search(int target) {\n    Node current = head;\n\n    while (current != null) {\n        if (current.value == target) {\n            return true;\n        }\n        current = current.next;\n    }\n\n    return false;\n}</code></pre><p>Since this is a linear search, it takes  in the worst case.</p><p>Linked lists really shine when it comes to insertion because you don’t have to shift elements like you would in an array.</p><p>There are different cases depending on where you are inserting a node:</p><p>This is the fastest case.</p><ul><li><p>Point its  to the current head</p></li><li><p>Update the head to the new node</p></li></ul><pre><code>void insertAtHead(int value) {\n    Node newNode = new Node(value);\n    newNode.next = head;\n    head = newNode;\n}</code></pre><p>It takes  since no traversal is needed.</p><p>To insert at the end, you have to:</p><ul><li><p>Traverse the list until you reach the last node</p></li><li><p>Update the last node’s  to the new node</p></li></ul><pre><code>void insertAtEnd(int value) {\n    Node newNode = new Node(value);\n\n    // If list is empty, new node becomes head\n    if (head == null) {\n        head = newNode;\n        return;\n    }\n\n    // Otherwise, go to the last node\n    Node current = head;\n    while (current.next != null) {\n        current = current.next;\n    }\n\n    current.next = newNode;\n}</code></pre><p>If you maintain a tail pointer, you can insert in constant time . Otherwise, you’ll need to traverse the entire list to find the last node, which takes .</p><h4>Insert at a Given Position</h4><p>To insert at a specific position (say after the 5th node), you must:</p><ul><li><p>Traverse to that position</p></li><li><p>Adjust the pointers to insert the new node</p></li></ul><pre><code>void insertAtPosition(int value, int position) {\n    if (position == 0) {\n        insertAtHead(value);\n        return;\n    }\n\n    Node current = head;\n    // move to node at position-1\n    for (int i = 0; i &lt; position - 1 &amp;&amp; current != null; i++) {\n        current = current.next;\n    }\n\n    Node newNode = new Node(value);\n    newNode.next = current.next;\n    current.next = newNode;\n}</code></pre><p>Again, this requires  time in the worst case.</p><p>Like insertion, deletion can also vary based on position.</p><h4>Delete from the Beginning</h4><p>This is fast. Just move the head pointer to the next node:</p><pre><code>void deleteHead() {\n    if (head == null) {\n        return; // nothing to delete\n    }\n    head = head.next;\n}</code></pre><p>This is constant time since no traversal is needed.</p><h4>Delete from the End or Middle</h4><p>To delete the last node or a node in the middle:</p><ul><li><p>You have to </p></li><li><p>Update its  pointer to skip the node you want to delete</p></li></ul><pre><code>void deleteAtPosition(int position) {\n    if (position == 0) {\n        deleteHead();\n        return;\n    }\n\n    Node current = head;\n    // move to node at position-1\n    for (int i = 0; i &lt; position - 1 &amp;&amp; current != null; i++) {\n        current = current.next;\n    }\n\n    // skip the node at `position`\n    current.next = current.next.next;\n}</code></pre><p>Since there’s <strong>no backward pointer in a singly linked list</strong>, this means  from the head, which takes  time.</p><p>It is also known as <strong>Floyd’s Tortoise and Hare</strong> algorithm.</p><ul><li><p>The  moves one step at a time.</p></li><li><p>The  moves two steps at a time.</p></li></ul><p>This simple trick allows you to solve problems like:</p><ul><li><p>: if there’s a loop, the fast and slow pointers will eventually meet.</p></li><li><p> of a linked list</p></li></ul><p>This is a  technique in interviews. The challenge is to reverse the direction of pointers so the last node becomes the new head.</p><p>We can do it in-place by maintaining three pointers at ,  and  node.</p><pre><code>void reverse() {\n    Node prev = null;\n    Node curr = head;\n\n    while (curr != null) {\n        Node nextNode = curr.next; // store next\n        curr.next = prev;          // reverse pointer\n        prev = curr;               // move prev forward\n        curr = nextNode;           // move curr forward\n    }\n\n    head = prev; // new head after full reversal\n}</code></pre><p>And then walking through the list and flipping the pointers one by one.</p><p>Another common trick is using dummy nodes. This one’s a  that makes your implementation simpler and less error-prone.</p><p>A  is just a placeholder node that you insert  the real head of the list.</p><p>It eliminates messy special cases like:</p><ul><li><p>“What if we delete the head?”</p></li><li><p>“What if insertion happens at the very beginning?”</p></li><li><p>“Do we need separate logic when head changes?”</p></li></ul><pre><code>Node deleteAll(Node head, int target) {\n    // 1. Create a dummy node before the real head\n    Node dummy = new Node(0);   // value doesn’t matter\n    dummy.next = head;\n\n    // 2. Use current pointer starting from dummy\n    Node current = dummy;\n\n    while (current.next != null) {\n        if (current.next.value == target) {\n            // Skip the node with target value\n            current.next = current.next.next;\n        } else {\n            // Move forward only if we didn’t delete\n            current = current.next;\n        }\n    }\n\n    // 3. New head might have changed\n    return dummy.next;\n}</code></pre><p>With a dummy node, you treat the head like any other node so your logic stays uniform and your code gets much cleaner.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/linked-list?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":9106,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/ec413e06-d5fc-406c-8401-55044c5c8bfd_745x534.png","enclosureMime":"","commentsUrl":null},{"title":"Latency vs. Throughput vs. Bandwidth","url":"https://blog.algomaster.io/p/latency-vs-throughput-vs-bandwidth","date":1763006898,"author":"Ashish Pratap Singh","guid":698,"unread":true,"content":"<p>Latency, throughput, and bandwidth are the core metrics that describe the performance of a network or distributed system.</p><p>Together they determine how fast the first byte arrives, how much data you can move per second, and the maximum capacity of the path.</p><p>Let’s understand them with the .</p><ul><li><p> The number of lanes on the highway (e.g., 5 lanes). This is the maximum physical capacity of the road.</p></li><li><p> The time it takes one car to drive from Exit 1 to Exit 10 at the speed limit with no traffic.</p></li><li><p> The total number of cars that pass Exit 10 per hour.</p></li></ul><p>Now, consider a traffic jam (congestion).</p><ul><li><p>The  stays the same. The road still has 5 lanes.</p></li><li><p>The  (travel time) rises for every car because travel takes longer.</p></li><li><p>The  (cars per hour) drops because fewer cars clear the exit per hour.</p></li></ul><p>Networks behave the same way. Congestion increases latency and reduces throughput even when raw bandwidth does not change.</p><p>Lets now explore them in more detail.</p><p> It is the time it takes for a single piece of data to travel from its source to its destination. It is commonly measured in .</p><p>In practice, we almost always measure , which is the time for a request to go out and an acknowledgment to come back.</p><p> When you make an API call, the latency is the time between your client sending the HTTP request and receiving the first byte of the response.</p><ul><li><p>The speed of light sets a hard limit. Even with a 1 Gbps internet connection, your latency to a server on another continent will be high.</p></li><li><p>Each router, DNS lookup, NAT, firewall, and load balancer adds queueing and processing time.</p></li><li><p>Establishing a connection (especially over HTTPS) requires multiple back-and-forth steps.</p></li><li><p>CPU contention, locks, context switches, garbage collection pauses, and cold code paths add processing time before a response is generated.</p></li><li><p>Cache misses, missing indexes, N+1 queries, synchronous disk I/O, and cross-region reads lengthen database query time.</p></li></ul><p>The goal is to cut distance, round trips, and processing time.</p><ul><li><p> Store frequently accessed data (e.g., in Redis or a browser cache) so you avoid a slow database call or network request.</p></li><li><p><strong>Content Delivery Networks (CDNs):</strong> This is a global network of caches. When a user in India requests a video, they get it from a server in Mumbai, not New York.</p></li><li><p> Run application logic (not just static content) closer to the user to reduce the number of round trips.</p></li><li><p> Instead of blocking the user while your system completes a time-consuming task,  to a background process and return a response immediately.</p></li><li><p>Use techniques like database indexes, partitioning, and sharding to reduce query time.</p></li><li><p>Keep a long-lived connection between a client and server that remains open across multiple requests and responses.</p></li></ul><p> It measures how much data actually gets transferred successfully over a network in a specific amount of time.</p><p>It is measured in <strong>Megabits per second (Mbps)</strong>, <strong>Gigabits per second (Gbps)</strong>, or often in <strong>Requests Per Second (RPS)</strong> or or <strong>Transactions Per Second (TPS).</strong></p><p> Your API server might have a low latency of 50ms. However, if that server can only process 100 requests concurrently before it runs out of CPU or database connections, its .</p><p>Throughput is almost always limited by a . This bottleneck could be anything:</p><ul><li><p>Database connection limits</p></li><li><p>Disk read/write speeds (I/O)</p></li></ul><h3>How to Increase Throughput</h3><p>The goal is to process more units of work in the same amount of time. You can do this by reducing the overhead of each task or by doing more tasks at once.</p><ul><li><p> Instead of making 1000 separate, tiny API calls to insert data, make one large API call with 1000 records. This reduces the per call overhead.</p></li><li><p> This means doing multiple things at the same time. Download managers use multiple connections to download parts of a file simultaneously.</p></li><li><p><strong>Increase Server Capacity:</strong> Scale horizontally (add more servers behind a load balancer) to handle more concurrent requests.</p></li></ul><p><strong>Bandwidth is the capacity.</strong> It represents the  amount of data your network link or system could handle.</p><p>Bandwidth is measured as a  per second, usually in . Common units: , , . In some systems you might also see  (KB/s, MB/s), where .</p><p> Your internet connection has . So a  download cannot finish faster than  in ideal conditions, no matter how low the latency is or how many threads you use.</p><p>Think of bandwidth as the width of the pipe. It belongs to the link, not to a single request or user.</p><h4>Two important clarifications:</h4><ol><li><p> Real applications rarely hit it because protocols, losses, CPU, and storage often limit the actual rate.</p></li><li><p><strong>Bandwidth is not throughput.</strong> Throughput is what you actually achieve at a moment in time. Bandwidth is what the link could deliver in ideal conditions.</p></li></ol><p>In real-world systems, we rarely talk about increasing bandwidth itself, that’s just buying a bigger pipe from your provider. Instead, we focus on <strong>using our available bandwidth more efficiently</strong>.</p><h3>How to Improve Bandwidth Utilization</h3><p>The goal is to get more work done with the capacity you already have.</p><ul><li><p> The most direct way to “improve” bandwidth is to make your data smaller. If you can make a 1 MB file 100 KB, you can send 10 times as many files over the same link in the same amount of time.</p><ul><li><p><strong>For Text (HTML, CSS, JSON):</strong> Use server-side compression like  or .</p></li><li><p> Serve modern formats like  or . They offer vastly better compression and quality compared to old JPEGs or PNGs.</p></li><li><p> Use modern codecs (like  or ) instead of older ones (like H.264) to cut the data size in half for the same visual quality.</p></li></ul></li><li><p> Use modern protocols like HTTP/2 or HTTP/3 (which uses QUIC). They are far more efficient at handling multiple requests over a single connection, which reduces the “waiting” caused by latency.</p></li></ul><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scalability?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQwMDc4MDk0LCJpYXQiOjE3Mzc1MzgxODMsImV4cCI6MTc0MDEzMDE4MywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.xroFXQDDEPvo2FWnnt-G2Ji9MzYIDtJ68NRQX6sT8x8&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":5900,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!4JMb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4178a8-233c-4a58-9455-78c56c6dc7a1_786x747.png","enclosureMime":"","commentsUrl":null},{"title":"Big-O Notation: Explained in 8 Minutes","url":"https://blog.algomaster.io/p/big-o-notation-explained-in-8-minutes","date":1762226492,"author":"Ashish Pratap Singh","guid":697,"unread":true,"content":"<p> a way to measure how efficiently your code performs as the input size grows.</p><p>You’ve probably seen code that works perfectly on small inputs but , , or  when the input becomes large.</p><p>Understanding Big O helps you avoid slow, inefficient solutions and write code that actually scales.</p><p>It’s also one of the <strong>most important topics in coding interviews</strong>.</p><p>You’ll almost always be asked to explain the <strong>time and space complexity</strong> of your solution and the more efficient your approach, the stronger your chances of passing the interview.</p><p>In this article, I’ll break down:</p><ul><li><p>What Big O Notation actually means</p></li><li><p>The most common time complexities you’ll come across</p></li><li><p>and rules to calculate Big O for any piece of code</p></li></ul><p>I also made a  on Big-O notation. Check it out for more in-depth and visual explanation.</p><p><strong>Subscribe for more such videos!</strong></p><p>So… what exactly is Big O Notation?</p><blockquote><p>Big O is a mathematical way to describe how the performance of an algorithm changes as the size of the input grows.</p></blockquote><p>It doesn’t tell you the exact time your code will take.</p><p>Instead, it gives you a , how fast the number of operations increases relative to the input size.</p><p>For example: if your input doubles, does your algorithm take twice as long? Ten times as long? Or does it barely change at all?</p><p>Big O helps you answer those questions without even running the code so you can choose the most efficient algorithm for large inputs.</p><p>One important thing to note is that <strong>Big O is machine-independent</strong>.</p><p>It doesn’t matter whether your code runs on a fast laptop or a slow server, the growth pattern stays the same.</p><p>The key variable that drives BIG O is the . As  increases, Big O helps you predict whether the algorithm will still be efficient or become impractical.</p><p>Now that you understand what Big O Notation is, let’s go over the <strong>most common time complexities</strong> you’ll encounter.</p><p>This is the fastest and most efficient time complexity.</p><p>An algorithm is  if it performs a fixed number of operations meaning the execution time does  depend on the size of the input.</p><p>A classic example is <strong>accessing an element in an array by index.</strong></p><pre><code>int[] a = {10, 20, 30, 40}\n\nint x = a[3];\n\nSystem.out.println(x);</code></pre><p>It doesn’t matter if the array has 10 elements…or 10 million, the time it takes to access an element </p><h3>Logarithmic Time – O(log n)</h3><p>An algorithm runs in  time when every step reduces the problem size by a constant factor (most often, by .</p><p>This means the amount of work grows , even when the input becomes massive.</p><p>The most common example is Binary Search.</p><pre><code>int binarySearch(int[] a, int target) {\n    int lo = 0, hi = a.length - 1;\n    \n    while (lo &lt;= hi) {\n        int mid = lo + (hi - lo) / 2;\n        if (a[mid] == target) \n          return mid;\n        if (a[mid] &lt; target) \n          lo = mid + 1;\n        else \n          hi = mid - 1;\n    }\n    return -1;\n}</code></pre><p>Each step discards <strong>50% of the remaining data</strong>.</p><p>To put that into perspective, here are the number of steps binary search takes for different input sizes.</p><ul><li><p>Input size = 8 → max 3 steps</p></li><li><p>Input size = 1,000 → max 10 steps</p></li><li><p>Input size = 1,000,000 → max 20 steps</p></li><li><p>Input size = 1,000,000,000 → still just 30 steps!</p></li></ul><p>An algorithm is  when its running time grows  to the size of the input.</p><p>If the input doubles, the number of operations also doubles.</p><p>A simple example is <strong>finding the maximum value in an array</strong>:</p><pre><code>int findMax(int[] array) {\n    int max = array[0];\n    for (int i = 1; i &lt; array.length; i++) {\n        if (array[i] &gt; max) {\n            max = array[i];\n        }\n    }\n    return max;\n}</code></pre><ul><li><p>You start with some initial max</p></li><li><p>Then you scan every element and compare it to the current max</p></li><li><p>Each comparison is O(1), but you do it  times so the overall time complexity becomes .</p></li></ul><p>So with 10 elements, you do 10 comparisons. With 1 million elements, you do 1 million comparisons.</p><p>Any algorithm that <strong>visits every element exactly once</strong> is linear time.</p><h3>Linearithmic Time – O(n log n)</h3><p>Algorithms with  time complexity combine two behaviors:</p><ul><li><p>A  factor from repeatedly splitting the input</p></li><li><p>An  factor from processing or merging the pieces</p></li></ul><p>It’s often described as <strong>logarithmic splitting with linear merging</strong>.</p><p>The classic example is :</p><ul><li><p>First, it  the array in half over and over. That’s the  part.</p></li><li><p>Then, it  the sorted halves back together and that takes  steps in total.</p></li></ul><p>Multiply those together and you get .</p><p>This complexity is slightly slower than linear time, but still very efficient and it’s the backbone of many fast sorting algorithms</p><p>In an O(n²) algorithm, the number of operations grows  to the square of the input size. So if you have n elements, you perform roughly n × n operations.</p><p>This typically happens when you have , where for  you iterate over .</p><pre><code>public class NSquare {\n    public int nsquare(int[] a) {\n        int n = a.length;\n\n        int pairSum = 0;\n\n        for (int i = 0; i &lt; n; i++) {\n            for (int j = i + 1; j &lt; n; j++) {\n                pairSum += a[i] * a[j];\n            }\n        }\n\n        return pairSum;\n    }\n}</code></pre><p>Classic examples include simple sorting algorithms like:</p><ul><li><p> (worst case)</p></li></ul><p>All of them compare or swap elements in nested loops, leading to  behavior.</p><p>These algorithms are fine for small inputs, but they become painfully slow as  grows:</p><p>For , you’re doing around . And if , you’re looking at  operations.</p><p>In coding interviews and wherever performance matters, you’ll often want to  look for ways to bring it down to O(n log n) or better.</p><p>Exponential time algorithms usually appear when we try to solve a problem by <strong>checking every possible combination</strong> often through brute force or backtracking.</p><p>Think of it as the opposite of binary search: Instead of eliminating half the work at each step, you are often  the work with each extra input element.</p><p>This happens in problems where each element can branch into multiple recursive calls.</p><p>A classic example is generating all  (also called the power set):</p><ul><li><p>If the set has  elements, there are  possible subsets.</p></li></ul><p>That means, just for 30 elements, you’re looking at over  possibilities. And for 40? Over .:</p><ul><li><p>n = 20 → ~1 million possibilities</p></li><li><p>n = 30 → ~1 billion possibilities</p></li><li><p>n = 40 → ~1 trillion possibilities</p></li></ul><p>This kind of growth becomes <strong>unmanageable very quickly.</strong> Even a small increase in  makes the runtime explode.</p><p>But the good news is that many exponential-time problems can be optimized using techniques like  or </p><p>These techniques prevent us from recomputing the same subproblems, often reducing the time from  down to a much more practical , which makes the solution usable in real-world scenarios.</p><p>In general, exponential algorithms are fine only for . For anything larger, you must either optimize or rethink the approach.</p><p>And finally… we’ve reached the most explosive time complexity of them all: , or </p><p>This is what you get when an algorithm tries <strong>every possible arrangement</strong> of a set of  elements.</p><p>The number of possibilities grows <strong>faster than any other complexity</strong> we’ve seen.</p><p>By definition,  (n factorial) means: n × (n - 1) × (n - 2) × ... × 1</p><ul></ul><p>Even at n = 15, the numbers are already in the trillions which makes it completely impractical to compute.</p><p>A classic example of O(n!) is <strong>generating all permutations of a string</strong>.</p><p>If you have a string of length 10 and try to print every permutation, that’s already several million operations and increasing  by just 1 doubles or triples the workload instantly.</p><p>These kinds of brute-force solutions are mostly used for very small inputs. Instead, we rely on smarter techniques like ,  , or  to reduce the problem space.</p><p>So far, we’ve focused on , how fast an algorithm runs as input size grows.</p><p>But Big O also applies to , and that is called .</p><p>In interviews, you’ll often be asked to analyze <strong>both time and space complexity</strong> —because in real systems, performance isn’t just about speed…<strong>It’s also about how much memory your solution consumes.</strong></p><p>Space complexity tells you how much  your algorithm uses <em>in addition to the input itself</em>.</p><p>This extra memory can come from:</p><ul><li><p>Temporary data structures like arrays, hash maps, stacks, queues, etc.</p></li><li><p>Recursion call stack frames</p></li><li><p>or Intermediate buffers used during computation</p></li></ul><p>Even if your algorithm is fast, it may still , which can be a serious problem in large-scale systems or environments with limited RAM.</p><p>Now, lets look at popular space complexities you will come across:</p><p>If you scan an array to find the maximum value, you only store  to track the current max.</p><pre><code>int maxValue(int[] a) {\n    int max = a[0];\n\n    for (int i = 1; i &lt; a.length; i++) {\n        if (a[i] &gt; max) {\n            max = a[i];\n        }\n    }\n\n    return max;\n}</code></pre><p>So while the input size may grow, your extra space stays constant.</p><p>Now suppose you collect all even numbers from the array into a new list.</p><pre><code>List&lt;Integer&gt; collectEvens(int[] a) {\n    List&lt;Integer&gt; evens = new ArrayList&lt;&gt;();\n\n    for (int x : a) {\n        if ((x &amp; 1) == 0)\n            evens.add(x);\n    }\n\n    return evens;\n}</code></pre><p>If half the numbers are even, that’s roughly  elements which still counts as  space.</p><h3>3) Quadratic Space - O(n²)</h3><p>Storing a full matrix (like an adjacency matrix or Dynamic Programming (DP) table) of size  takes  space.</p><p>Space complexity isn’t only about the extra data structures you create. Recursion also consumes memory through the , and this often goes unnoticed.</p><p>Every recursive call adds a new frame to the stack until the function returns. Depending on the algorithm, this stack usage can range from logarithmic to linear, or even worse.</p><p>For example, the <strong>naive recursive Fibonacci</strong> solution branches twice for each call. It takes  time but still uses  space because at most  calls are active on the stack at once.</p><p>For  on a tree, the stack depth is proportional to the .</p><ul><li><p>In a ,  → so space is </p></li><li><p>In the <strong>worst case (a skewed tree)</strong>,  → so space becomes </p></li></ul><p>Now, lets talk about how to  the complexity of a piece of code.</p><p>The simplest way is to <strong>break your code down into parts</strong> and analyze each part separately.</p><h3>Rule 1: Add Complexities of Sequential Operations</h3><p>If your algorithm performs one block of work  another, you  their time complexities.</p><p>Imagine your code has two separate, sequential loops:</p><pre><code>// Block A: O(n)\nfor (int i = 0; i &lt; m; i++) {\n  // ... do O(1) work\n}\n\n// Block B: O(n^2)\nfor (int i = 0; i &lt; n; i++) {\n  for (int j = 0; j &lt; n; j++) {\n    // ... do O(1) work\n  }\n}</code></pre><p>The total runtime is the time for Block A  the time for Block B which comes out to be O(m) + O(n^2).</p><h3>Rule 2: Multiply Complexities of Nested Operations</h3><p>If you algorithm has nested loop with the outer loop running n times and the inner loop running m times, the total complexity is O(n * m).</p><pre><code>// Block A: Outer loop runs n times -&gt; O(n)\nfor (int i = 0; i &lt; n; i++) {\n  \n  // Block B: Inner loop runs n times -&gt; O(n)\n  for (int j = 0; j &lt; n; j++) {\n    // ... do O(1) work\n  }\n}</code></pre><h3>Rule 3: Drop Constant Factors</h3><p>This rule states that we can ignore any constant multipliers in a Big O expression.</p><p>When you derive a time complexity expression, you may end up with something like:  or .</p><p>Big O is not about the , it’s about <strong>how fast your algorithm grows</strong> with input.</p><p>Whether your algorithm takes  steps or  steps, both grow  as  increases,  the constant multiplier doesn’t affect the growth trend. If you double the input, the runtime for both will (roughly) double.</p><p>So we drop constant factors:</p><ul><li><p> simplifies to </p></li><li><p> simplifies to  since 1/3 is also just a constant factor</p></li></ul><h3>Rule 4: Drop Lower-Order Terms</h3><p>If your final expression has multiple terms, you keep only the one that grows the fastest, the  and drop the rest.</p><p>Why? Because as  becomes very large, slower-growing terms become insignificant.</p><p>Lets use the example O(n^2 + n + 100). </p><p>Imaging n = 1,000,000 (one million).</p><ul><li><p> which is the dominant term becomes one trillion</p></li><li><p>while  is just one million and the constant term stays at 100</p></li></ul><p>At scale, the lower-order term  in the overall growth. So we only keep the term that dominates.</p><ul><li><p> simplifies to </p></li><li><p> simplifies to </p></li></ul><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scalability?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQwMDc4MDk0LCJpYXQiOjE3Mzc1MzgxODMsImV4cCI6MTc0MDEzMDE4MywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.xroFXQDDEPvo2FWnnt-G2Ji9MzYIDtJ68NRQX6sT8x8&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":12069,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!Vrvm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc9855bcf-994d-43b6-9fc4-f80574c1782e_1194x810.png","enclosureMime":"","commentsUrl":null},{"title":"Top 6 API Architecture Styles","url":"https://blog.algomaster.io/p/top-6-api-architecture-styles","date":1761796357,"author":"Ashish Pratap Singh","guid":696,"unread":true,"content":"<p>An <strong>API (Application Programming Interface)</strong> defines how two systems communicate, what data can be shared, and in what format.</p><p>But not all APIs are built the same. Over time, as applications evolved, so did the challenges they faced.</p><p>This led to the creation of new API styles, each designed to solve specific problems related to <strong>performance, flexibility, or real-time updates</strong>.</p><p>In this article, we’ll break down the  that power modern software.</p><p>In the beginning, there was <strong>SOAP (Simple Object Access Protocol)</strong>.</p><p>As the internet began to rise in the late 1990s, companies needed a standardized way for applications to communicate across different platforms and programming languages.</p><p>SOAP emerged as the first major standard to solve this.</p><p>SOAP demands that all messages be in  format, and it operates based on a very strict contract called a <strong>WSDL (Web Services Description Language)</strong>.</p><p>Think of WSDL as a detailed instruction manual that precisely defines every operation you can perform.</p><p>SOAP is very “verbose,” meaning it uses a lot of text to describe a simple action. All that text for one simple request makes messages large, which  network transmission and processing.</p><p>Furthermore, the strict WSDL contract creates ; if the server changes any part of the contract, the client will often break.</p><p>While this was acceptable for large, internal enterprise systems, SOAP was just too heavy and inflexible for the fast moving web and new mobile apps. Developers needed something simpler, lighter, and more flexible that used the web’s own language, HTTP.</p><p>In response to SOAP’s complexity, <strong>REST (Representational State Transfer)</strong> emerged and it quickly became the standard for the modern web.</p><p>REST represented a complete mindset shift. Instead of complex operations defined in a WSDL, REST treats data as “resources” (like ) that you interact with using the standard HTTP methods (GET, POST, PUT, DELETE) that power the entire web.</p><p>It is , meaning every request contains all the information needed to process it. It also embraced  over XML, which is far lighter and easier for both humans and machines (especially JavaScript) to read.</p><p>REST is amazing and runs the majority of the web. But as applications grew, two common problems emerged:</p><ol><li><p> Clients often receive more data than they need. You just need a user’s name, but  returns their name, address, entire post history, and a dozen other fields. This is wasted data that slows down apps, especially on mobile networks.</p></li><li><p> You need to show a user’s profile  their latest posts. This requires two separate requests:  and then . This “waterfall” of requests creates noticeable lag.</p></li></ol><p>As frontend applications grew richer especially with mobile and single-page apps, developers wanted more control over the data they fetched.</p><p>This led to the rise of .</p><p>What if you could ask for  what you need, all in one trip? </p><p>That’s what Facebook set out to solve when they created in 2012 and open-sourced in 2015.</p><p>GraphQL is a <strong>query language for your API</strong>. The most important shift is that the , not the server, defines the shape of the data it needs. </p><p>Instead of dozens of REST endpoints, you typically have just one (like ) that accepts a query. The client sends a query that precisely describes the data it wants, and the server returns a JSON object in that exact same shape.</p><p>This single query can pull from multiple sources (like a user database and a post database) and return it all in one response.</p><p>There is no  (you only get the  and , not the user’s email or post bodies) and no  (you get the user and their posts in one round trip).</p><p>However, GraphQL introduces its own set of challenges:</p><ul><li><p><strong>Complex Server Implementation:</strong> Building a GraphQL server (especially with nested data) can be more complex than a simple REST API.</p></li><li><p> The single endpoint and dynamic queries make traditional HTTP caching mechanisms less effective compared to REST.</p></li></ul><p>GraphQL shares a fundamental trait with REST: it’s text based (JSON) and works on a client “pull” model, where the client must make a request.</p><p>For high-performance internal communication between dozens of microservices, the overhead of parsing text and the HTTP request-response pattern is too slow.</p><p>This need for raw speed led to .</p><p>Developed by Google and open-sourced in 2015, gRPC is a modern <strong>Remote Procedure Call (RPC)</strong> framework designed for high-performance, language-agnostic communication  services.</p><p>It’s built for performance in two key ways:</p><ol><li><p> It replaces text-based JSON with <strong>Protocol Buffers (Protobufs)</strong>, a highly efficient binary format. This is much faster for computers to  (write) and  (read).</p></li><li><p> It runs on  by default. This modern protocol is far more efficient than HTTP/1.1, supporting features like , where many requests can fly back and forth on a single connection.</p></li></ol><p>You define your services and messages in a simple  file. This file acts as a language-agnostic contract, which gRPC uses to generate native code for any language you need (Java, Go, Python, etc.).</p><p>gRPC is an excellent choice for <strong>internal Service-to-Service Communication</strong>, especially over low-bandwidth networks.</p><p>However, its binary format is not human-readable, which can make debugging more challenging. It also isn’t directly supported by browsers, requiring a proxy like gRPC-Web for client-side use.</p><p>Although highly performant, gRPC is still a request-response pattern: the client asks, the server answers.</p><p>What if you need a persistent, two-way connection for a live chat app, a stock ticker, or a multiplayer game?</p><p>That’s where  comes in.</p><p>Traditional HTTP, which underpins REST, GraphQL, and gRPC, is a  protocol. The client must always initiate the conversation.</p><p>This becomes inefficient for real-time use cases, because the only way to constantly get updates is to repeatedly ask the server () or keep a request hanging until something happens ().</p><p> was built to solve exactly this. It creates a ,  communication channel over a single connection.</p><p>The connection begins as a normal HTTP request, then upgrades to a WebSocket. After that, both client and server can send data to each other whenever they want without new requests.</p><p><strong>Client-side code example:</strong></p><p>This model is perfect for live dashboards, multiplayer gaming, or chat applications where the server must push updates instantly.</p><p>But running WebSockets at scale is not trivial. You have to manage millions of long-lived connections, ensure state consistency across servers, and handle reconnects and failures.</p><p>Another limitation: the client must initiate the connection first. That works for browsers and apps, but fails for server-to-server events.</p><p>For example, what if Stripe’s server needs to notify  server that a payment just succeeded? Your server isn’t sitting there with an open connection to Stripe.</p><p>In such cases, we need a different mechanism:  a .</p><p>Webhooks are essentially “reverse APIs.” Instead of your application polling an API endpoint for new data, the  calls  (acting as the client) when a specific event occurs. It’s a user-defined HTTP callback.</p><p>A Webhook is a server to server push model. You provide a URL (an endpoint on your server) to a third party service like GitHub, Stripe, or Slack. When a specific event happens (like a  or a ), that service instantly sends an HTTP POST request with the event data (the payload) to your URL.</p><p>Webhooks eliminate the need for constant polling, allowing systems to react to events asynchronously and saving resources for both the client and the server.</p><p>Despite their power, Webhooks require careful implementation:</p><ul><li><p> Your endpoint is public. You  verify a signature (like ) to ensure the request is legitimate and not a forgery.</p></li><li><p> Webhooks can fail and be retried. Your endpoint must be , meaning processing the same notification multiple times has the same effect as processing it once.</p></li></ul><p>As you can see, there is no single “best” API. The “best” API is the one that fits your use case. In fact, a single, complex application will often use many of them together:</p><ul><li><p> or  for its public web and mobile apps.</p></li><li><p> for its internal, high speed microservice communication.</p></li><li><p> for its real time chat feature.</p></li><li><p> to receive events from its payment provider.</p></li></ul><p>The journey from SOAP to Webhooks shows an evolution towards more specific, efficient, and flexible tools. The real skill is not knowing just one, but knowing which one to pick for the job.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content.</p><p>If you have any questions or suggestions, leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/scalability?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQwMDc4MDk0LCJpYXQiOjE3Mzc1MzgxODMsImV4cCI6MTc0MDEzMDE4MywiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.xroFXQDDEPvo2FWnnt-G2Ji9MzYIDtJ68NRQX6sT8x8&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":8616,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!VOZJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F230bee2c-6957-499a-8bda-473bfb05b4b2_2332x1352.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}