{"id":"2bP4pJr4wVimh6yine63HqNeafoyheTFz1gqCrQ4h7Z","title":"AlgoMaster Newsletter","displayTitle":"Dev - Algomaster","url":"https://blog.algomaster.io/feed","feedLink":"https://blog.algomaster.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":7,"items":[{"title":"Designing a Scalable “Likes” Counting System for Social Media","url":"https://blog.algomaster.io/p/designing-a-scalable-likes-counting-system","date":1744868602,"author":"Ashish Pratap Singh","guid":723,"unread":true,"content":"<p>At first glance, counting  on a social media post seems simple, , right? </p><p>But at scale, this becomes a surprisingly complex <strong>distributed systems problem</strong>.</p><p>Imagine millions of users liking posts simultaneously, especially during . The system must remain , , and , even under heavy load.</p><ul><li><p>How do we handle  that receive massive, sudden traffic?</p></li><li><p>How do we maintain  in likes count across distributed nodes?</p></li><li><p>How do we ensure  and  as the number of like events scales into the millions?</p></li><li><p>How do we make sure each user’s like is counted ?</p></li></ul><p>In this article, we’ll walk through how to <strong>design a scalable like-counting system</strong> starting from a basic design and gradually evolving it into a robust, distributed architecture.</p><p>Along the way, we’ll explore important system design concepts and trade-offs, including:</p><ul><li><p><strong>Strong vs. Eventual Consistency</strong></p></li><li><p><strong>Deduplication (Idempotency)</strong></p></li></ul><p>We’ll also discuss how to support , and how to build features like  using the like stream for real-time analytics.</p><p>Before we jump into the design, let’s define what our “likes” system needs to support, both functionally and non-functionally.</p><h2>1. Functional Requirements</h2><ul><li><p>Users can  and  a post by toggling the like button.</p></li><li><p>A user can like a post . If they press “like” again, it should be treated as an .</p></li><li><p>Both the post owner and viewers should be able to see the  for a post.</p></li><li><p>The like count should update  after a like/unlike action.</p></li><li><p>Show a full list of all users who liked a post.</p></li></ul><h2>2. Non-Functional Requirements</h2><ul><li><p> The system should handle <strong>thousands of likes per second</strong>, including spikes from viral content (say a celebrity’s post).</p></li><li><p>The system must be horizontally scalable (able to add servers/nodes to handle growth)</p></li><li><p>: A failure in one component should not bring down the whole “likes” functionality.</p></li><li><p> Liking a post should feel . The displayed count can lag slightly but should stay reasonably up to date.</p></li><li><p> We aim for  — brief discrepancies in counts are acceptable, but they must reconcile quickly.  is nice-to-have, not a must.</p></li><li><p> No likes should be lost, even during failures. Every like must be recorded persistently.</p></li><li><p> A user’s like should count only once, even if the action is retried or duplicated.</p></li><li><p> The system should support analytics use cases like identifying  (e.g., most-liked posts in the last hour).</p></li></ul><p>With these in mind, let’s begin with a simple solution and then evolve it into a scalable architecture by addressing each limitation.</p><h2>1. Basic Single-Database Design</h2><p>Let’s start with the most straightforward solution — storing likes in a single relational database.</p><p>We create a  table with columns like:</p><ul><li><p>(optional)  (auto-incremented)</p></li></ul><p>Every time a user likes a post, we insert a new row into this table.</p><p>To get the total likes for a post, we simply run:</p><pre><code>SELECT COUNT(1) FROM Likes WHERE post_id = X;</code></pre><p>Alternatively, we could maintain a  field in a  table and increment it on every like.But for simplicity, let’s stick to querying the  table directly.</p><p>On unlike, we can simply delete the corresponding row from the  table.</p><pre><code>DELETE FROM Likes WHERE post_id = 123 AND user_id = 'user_456';</code></pre><p>This approach also allows us to easily retrieve the list of users who liked a specific post using a simple query:</p><pre><code>SELECT user_id FROM Likes WHERE post_id = X;</code></pre><blockquote><p>To optimize read queries, we can add an on, which improves lookup performance as the table grows.</p></blockquote><p>This setup is a typical monolith or simple web stack:</p><ul><li><p> Handles \"like\" requests from users.</p></li><li><p> Stores each like action in the  table</p></li></ul><p>When a user likes a post, the likes service inserts a record into the database. When someone views a post, it queries the total likes from the same database.</p><ul><li><p><strong>Simplicity and Strong Consistency: </strong>Once the DB insert is successful, any subsequent reads will reflect the updated count. Everyone, the liker and other users sees the updated value immediately.</p></li><li><p><strong>Enforces Uniqueness Easily: </strong>We can add a  constraint to ensure that each user can only like a post once. If a user tries to like the same post again, the DB will reject it.</p></li><li><p>Checking whether a user liked a post is a fast lookup on .</p><pre><code>SELECT 1 FROM Likes WHERE post_id = X AND user_id = Y;</code></pre></li></ul><ul><li><p>Every like triggers a write. With millions of users, the DB quickly becomes a bottleneck. A single SQL database can’t handle that level of write throughput.</p></li><li><p>Using  on large tables is slow. As posts gain millions of likes, counting them becomes a performance problem — even with indexing.</p></li><li><p>Each like inserts a row. Each post view might run a  query. Under load, these operations can slow down the system significantly.</p></li><li><p>If the database goes down, users can’t like posts or see updated counts.</p></li><li><p><strong>Limited Horizontal Scaling: </strong>You can add read replicas to spread reads, but writes still go to the primary database. Scaling vertically (more CPU/RAM) only goes so far.</p></li></ul><p>This design is , offering  and . </p><p>But it falls apart at scale.</p><p>To handle real-world traffic with millions of users and viral posts, we need to evolve this architecture to improve performance, scalability, and availability.</p><h2>2. Improving Read Efficiency – Caching and Aggregated Counts</h2><p>In our basic design, reading the like count with  is expensive and hits the database hard — especially for popular posts. </p><p>To fix this, we introduce two improvements:</p><ul><li><p><strong>Maintain precomputed like counts</strong></p></li><li><p><strong>Use caching for fast reads</strong></p></li></ul><p>Instead of computing the count on every read, we store it alongside the post. This can be done by:</p><ul><li><p>Adding a  column in the  table, or</p></li><li><p>Creating a separate  table with schema: </p></li></ul><p>Each time a user likes or unlikes a post, we  this count. This turns a costly  query into a fast primary-key lookup.</p><p>We add an in-memory cache (e.g.,  or ) to store frequently accessed like counts:</p><ul><li><p>The app checks the cache first.</p></li><li><p>If the data isn’t in the cache (cache miss), it queries the DB, then stores the result in cache.</p></li><li><p>Subsequent reads for the same post hit the cache, not the DB.</p></li></ul><ul><li><p>(a distributed in-memory cache cluster) Stores  pairs in memory for fast access.</p></li><li><p> Stores aggregated like counts in either the  table or a dedicated table.</p></li></ul><ul><li><p>Insert a row into the  table.</p></li><li><p>Increment the  in the  or  table.</p><pre><code><code>UPDATE Posts SET like_count = like_count + 1 WHERE post_id = X</code></code></pre></li><li><p>If both the like insertion and count update are in the same DB, we can wrap them in a  for strong consistency.</p><pre><code>BEGIN;\n\n-- 1. Insert into Likes table\nINSERT INTO Likes (post_id, user_id, timestamp)\nVALUES (123, 'user_456', NOW());\n\n-- 2. Increment like count\nUPDATE Posts\nSET like_count = like_count + 1\nWHERE post_id = 123;\n\nCOMMIT;</code></pre></li><li><p>Invalidate or update the cache entry for that post’s count.</p></li></ul><blockquote><p>We still need the  table to log all the like events and find users who liked a post.</p></blockquote><ul><li><p>On cache miss, query for  from  table, then populate the cache.</p><pre><code>SELECT like_count FROM WHERE post_id = X</code></pre></li></ul><ul><li><p> Fetching the like count is now  — either from cache or via a quick primary-key lookup. No more expensive aggregations over many rows.</p></li><li><p> Cache absorbs most read traffic, especially for viral posts that are viewed repeatedly.</p></li><li><p><strong>Strong Consistency (for a single server):</strong> A transactional write ensures that the  row and the  update stay in sync.</p></li><li><p> We’re caching a single number (the count), which is easy to update or invalidate on write.</p></li></ul><ul><li><p> When the like count changes in the DB, the cache needs to be updated or invalidated. Otherwise, clients may be shown stale data. A common pattern is: </p><pre><code>1. Update DB\n2. Invalidate cache</code></pre></li><li><p><strong>Write Contention on Hot Posts:</strong> Now every like involves two writes (insert in the  table + update in the  table). For popular posts, the  row can become a hotspot, causing lock contention in the DB. (We will address this later with sharding and other techniques.)</p></li><li><p>: All likes still go through the same DB instance. While reads are faster, we haven’t solved write scalability yet.</p></li><li><p><strong>Eventual Consistency in Caches:</strong> If using read replicas or multiple cache layers, some clients may see slightly stale counts due to replication lag or delayed cache updates. Usually acceptable within a short window (milliseconds to seconds).</p></li><li><p> Storing every post’s count in cache uses memory. Fortunately, the data size is small, and infrequently accessed entries (e.g. old posts) can be evicted.</p></li></ul><p>Despite the drawbacks, this step significantly <strong>improves read performance</strong>. Instead of recalculating likes for every view, we cache or store the count and serve it instantly.</p><p>But the <strong>write path is still centralized</strong>, and <strong>hot posts can create contention</strong>. To scale further, we need to distribute the write workload — which we’ll explore next.</p><h2>3. Scaling Out with Database Sharding</h2>","contentLength":8410,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec5f2e5e-f1a1-428f-ad79-f6494b061a5c_2748x2034.png","enclosureMime":"","commentsUrl":null},{"title":"What is Change Data Capture (CDC)?","url":"https://blog.algomaster.io/p/change-data-capture-cdc","date":1744695795,"author":"Ashish Pratap Singh","guid":722,"unread":true,"content":"<p>Modern applications often rely on multiple systems (e.g., search engines, caches, data lakes, microservices), all of which need . </p><p>Traditional  are slow, introduce latency, and often lead to stale data and inconsistencies.</p><p><strong>Change Data Capture (CDC)</strong> is a design pattern used to <strong>track and capture changes</strong> in a database (inserts, updates, deletes) and <strong>stream those changes in real time</strong> to downstream systems.</p><p>This ensures downstream systems remain in sync without needing expensive batch jobs.</p><p>In this article, we’ll dive into how CDC works, explore different implementation strategies, it’s real-world use cases, challenges and considerations.</p><p>At a high level, CDC works by continuously monitoring a database for data changes (insertions, updates, and deletions). </p><p>When a change occurs, CDC captures the change event and makes the information available for processing.</p><p>The process typically involves:</p><ul><li><p> Detecting changes from source systems. This can be achieved through database triggers, reading transaction logs, or using specialized CDC tools.</p></li><li><p> Extracting details about the change event (such as before and after values) along with metadata (e.g., timestamp, changed table).</p></li><li><p> Transmitting the change event to consumers, which might include message queues, data pipelines, or real-time analytics systems.</p></li></ul><p>This helps in achieving <strong>event-driven architectures</strong> where applications respond to data changes as they happen.</p><p>There are three main approaches to implementing CDC:</p><p>This approach relies on adding a or column to your database tables. </p><p>Every time a row is inserted or modified, this column is updated with the current timestamp. Applications then query the table for rows where the  time is later than the last sync time.</p><pre><code>SELECT * FROM orders WHERE last_updated &gt; '2024-02-15 12:00:00';</code></pre><ul><li><p> Easy to implement without major changes to the database architecture.</p></li><li><p><strong>No External Dependencies:</strong> Can be executed with basic SQL operations, making it accessible for smaller projects or legacy systems.</p></li></ul><ul><li><p> This method may not capture deleted records since the timestamp is typically updated on existing rows.</p></li><li><p> As your data grows, frequent querying based on timestamps can impact performance, especially if indexing is not optimally configured.</p></li><li><p> Misconfigured timestamp updates or clock skew issues can result in missed changes.</p></li></ul><p>Trigger-Based CDC involves setting up database triggers that automatically log changes to a separate audit table whenever an insert, update, or delete operation occurs. </p><p>This audit table then serves as a reliable source of change records, which can be pushed to other systems as needed.</p><pre><code>CREATE TRIGGER order_changes \nAFTER UPDATE ON orders \nFOR EACH ROW \nINSERT INTO order_audit (order_id, old_status, new_status, changed_at) \nVALUES (OLD.id, OLD.status, NEW.status, NOW());</code></pre><ul><li><p> Triggers capture changes immediately when they occur.</p></li><li><p> Offers a granular record of changes, including before-and-after values, which is useful for audit trails and debugging.</p></li><li><p> Can be tailored to capture specific columns or changes.</p></li></ul><ul><li><p> Triggers execute during the transaction, potentially slowing down database write operations.</p></li><li><p><strong>Complexity in Maintenance:</strong> Changes in database schema often require corresponding updates to trigger definitions.</p></li><li><p> In high-transaction environments, the additional load of maintaining an audit table can become significant.</p></li></ul><p>Log-Based CDC reads changes directly from the database’s  or . This method intercepts the low-level database operations, enabling it to capture every change made to the database without interfering with the application’s normal workflow.</p><ul><li><p> An open-source platform that streams database changes.</p></li><li><p> Often used in conjunction with Debezium to integrate with Apache Kafka.</p></li><li><p> A managed service for data migration and CDC in the cloud.</p></li></ul><ul><li><p> Minimizes the impact on the primary database since it leverages existing logs.</p></li><li><p> Well-suited for large, high-transaction environments.</p></li><li><p><strong>Comprehensive Change Capture:</strong> Accurately captures every change, including inserts, updates, and deletes.</p></li><li><p> Provides near real-time data movement, essential for dynamic data-driven systems.</p></li></ul><ul><li><p> May require additional configuration and understanding of the underlying database logs.</p></li><li><p><strong>Dependency on Database Features:</strong> Not all databases expose their logs in a manner that is easy to consume.</p></li><li><p> Often necessitates integration with additional tools or services, which can add to the overall system complexity.</p></li></ul><blockquote><p>In modern applications, Log-based CDC is generally preferred because it efficiently captures all types of changes (inserts, updates, and deletes) directly from transaction logs, minimizes impact on the primary database, and scales well with high data volumes.</p></blockquote><h2>3.1. Microservices Communication</h2><p>In a microservices architecture, individual services often need to communicate and share state changes without being tightly coupled. </p><p>With CDC in place, the change is captured and propagated via a messaging system (such as Kafka) so that each microservice can stay updated on the relevant changes in other services' databases without needing direct service-to-service calls.</p><p>Event sourcing involves recording every change to an application state as a sequence of events. CDC can be leveraged to capture these changes in real time, building a complete log of all modifications.</p><p>Consider a financial application that logs every transaction. Instead of simply updating an account’s balance, every deposit, withdrawal, or transfer is recorded as an event. CDC captures these events and builds a detailed log of all state changes. This audit trail can later be used to reconstruct any account’s history or to debug issues.</p><p>Data warehousing involves consolidating large volumes of transactional data for analysis and reporting. CDC can capture database changes as they happen and push them into a data warehouse in near real-time.</p><p>Analysts and decision-makers then use up-to-date data for reporting, analytics and dashboards.</p><p>Caches are used to improve application performance by storing frequently accessed data. However, stale cache data can cause issues, leading to outdated or incorrect information being displayed.</p><p>CDC can trigger cache updates automatically whenever the underlying data changes.</p><p>An online news platform uses caching to speed up page loads for popular articles. However, when an article is updated (e.g., a correction is issued or new content is added), the cache must be invalidated to prevent serving stale content. </p><p>With CDC, changes in the content database are captured and automatically trigger cache updates, ensuring readers always see the most current information.</p><p> is a popular open-source tool that provides log-based CDC for various databases like MySQL, PostgreSQL, and MongoDB.</p><p>When integrated with Apache Kafka, Debezium can capture and stream database changes in near real time.</p><h3><strong>Step 1: Set Up Kafka and Debezium</strong></h3><p>Before configuring Debezium, you need to have a running Kafka cluster.</p><pre><code><code># Start Zookeeper\nbin/zookeeper-server-start.sh config/zookeeper.properties &amp;\n\n# Start Kafka Broker\nbin/kafka-server-start.sh config/server.properties &amp;</code></code></pre><h3><strong>Step 2: Configure Debezium Connector for MySQL</strong></h3><p>Next, create a Debezium connector configuration to capture changes from your MySQL database. This configuration tells Debezium which database and tables to monitor, along with necessary connection details.</p><pre><code><code>{\n  \"name\": \"inventory-connector\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n    \"database.hostname\": \"localhost\",\n    \"database.port\": \"3306\",\n    \"database.user\": \"cdc_user\",\n    \"database.password\": \"password\",\n    \"database.server.id\": \"184054\",\n    \"database.include.list\": \"ecommerce\",\n    \"table.include.list\": \"ecommerce.orders\",\n    \"database.history.kafka.bootstrap.servers\": \"localhost:9092\",\n    \"database.history.kafka.topic\": \"dbhistory.inventory\"\n  }\n}</code></code></pre><h3><strong>Step 3: Listen for Changes</strong></h3><p>Once the Debezium connector is properly configured and running, it starts capturing change events from the MySQL database. </p><p>These events are then published to Kafka topics. You can consume these events using Kafka command-line tools or any Kafka consumer application.</p><pre><code><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic dbhistory.inventory --from-beginning</code></code></pre><p>While CDC is a powerful tool for real-time data integration, its implementation comes with several challenges that must be carefully managed:</p><ol><li><p>: Databases evolve over time, with changes such as adding or modifying columns. A robust CDC pipeline must gracefully adapt to schema alterations, ensuring that no data changes are lost or misinterpreted.</p></li><li><p>: In high-transaction environments, large volumes of data changes can occur rapidly. It’s essential to design a CDC system that can process and relay these events efficiently to avoid overwhelming downstream systems.</p></li><li><p>: Maintaining the correct sequence of events is critical, especially in distributed architectures. The CDC solution should ensure that events are processed in the exact order they occurred, preserving data integrity across all consuming services.</p></li><li><p>: Because CDC involves capturing detailed data changes, it can expose sensitive information if not properly managed. Implementing robust security measures such as encryption, data masking, and strict access controls is vital to protect data and comply with regulatory requirements.</p></li></ol><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/how-i-mastered-data-structures-and-algorithms?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQ1NjU1MjUyLCJpYXQiOjE3MjE1MjE3MzEsImV4cCI6MTcyNDExMzczMSwiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.2cNY811YEugd5iH9XJQhakBzyahGqF7PcATBlFj5J2w&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p>","contentLength":9501,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff728d711-09f5-40da-9d77-6969a8b45ace_2262x1552.png","enclosureMime":"","commentsUrl":null},{"title":"Designing a Distributed Key-Value Store (Step-by-Step)","url":"https://blog.algomaster.io/p/designing-a-distributed-key-value-store","date":1744262181,"author":"Ashish Pratap Singh","guid":721,"unread":true,"content":"<p>A  is a simple type of database where data is stored as  pairs. Clients can retrieve or update values by providing the corresponding key, similar to how a  works.</p><p>A <strong>distributed key-value store</strong> takes this concept a step further. Instead of storing all the data on a single machine, it spreads the data across multiple servers (or nodes).</p><p>This distribution enables the system to , handling large volumes of data and user requests, while maintaining  and .</p><p>But this also introduces a new set of challenges like:</p><ul><li><p>How do we efficiently store and retrieve data?</p></li><li><p>How do we persist data to prevent loss during node crashes?</p></li><li><p>How do we evenly distribute data across multiple nodes?</p></li><li><p>How do we keep the data consistent across nodes?</p></li><li><p>How do we detect and recover from node failures?</p></li></ul><p>In this article, we’ll walk through how to design a distributed key-value store from the ground up, one that is capable of handling large-scale workloads with , , and .</p><p>We’ll begin by identifying the  (what the system should do) and <strong>non-functional requirements</strong> (how the system should behave).</p><p>Then, we’ll start with a  and progressively evolve it into a fully distributed system, one building block at a time. </p><p>Along the way, we’ll explore key system design concepts such as:</p><ul><li><p>Consistency models (strong, eventual, causal)</p></li></ul><p>By the end of this post, you’ll have a solid understanding of what it takes to build a production-grade distributed key-value store and the trade-offs involved at each stage of the design.</p><h2>1. Functional Requirements</h2><p>At its core, our key-value store must support basic  operations focused on keys:</p><ul><li><p> – Insert a new key-value pair or update the value if the key already exists. This is the primary write operation.</p></li><li><p> – Retrieve the value associated with a given key. If the key doesn’t exist, return an appropriate result (e.g. null or error).</p></li><li><p> – Remove the key and its associated value from the store. After deletion, a subsequent GET on that key should indicate that it no longer exists.</p></li></ul><h2>2. Non-Functional Requirements</h2><p>To be usable at scale, our distributed store must meet several critical non-functional goals:</p><ul><li><p>The system should scale horizontally to handle massive amounts of data and traffic as demand grows.</p></li><li><p>The store must remain accessible with minimal downtime, even in the face of server failures. There should be no single point of failure.</p></li><li><p>Operations like GET and PUT should return results quickly—ideally within a few milliseconds.</p></li><li><p>The system should support a large number of operations per second and serve many concurrent clients without significant performance degradation.</p></li></ul><h2>1. Start Simple – A Single Node Key-Value Store</h2><p>Before jumping into a fully distributed setup, it’s important to understand the basics. Let’s start with the simplest version of a key-value store: one that runs on a .</p><p>At this stage, the entire system consists of just one server responsible for handling all client requests.</p><p>The design is straightforward:</p><ul><li><p>The server stores data in memory, using a hash map or dictionary-like structure.</p></li><li><p>It exposes basic operations:</p><ul><li><p> – Add a new entry or update an existing one</p></li><li><p> – Retrieve the value for a key</p></li><li><p> – Remove a key and its value</p></li></ul></li></ul><p>Here’s a basic Java-like pseudocode for the core logic:</p><pre><code>Map&lt;String, String&gt; store = new HashMap&lt;&gt;();\n\nvoid put(String key, String value) {\n    store.put(key, value);\n}\n\nString get(String key) {\n    return store.getOrDefault(key, null);\n}\n\nvoid delete(String key) {\n    store.remove(key);\n}</code></pre><p>Clients send HTTP requests (or use some lightweight protocol), and the server responds accordingly. </p><ul><li><p><code>PUT /set?key=user1&amp;value=John</code></p></li></ul><p>This setup works perfectly for simple use cases, small-scale prototypes, or local development environments but this architecture breaks down quickly in real-world scenarios:</p><ol><li><p>If the server crashes, the data is lost, and the system becomes unavailable.</p></li><li><p>You're constrained by the memory, CPU, and storage of a single machine.</p></li><li><p>The system can’t handle growing traffic or data volume. There’s no way to scale out by adding more machines.</p></li><li><p>One bug, hardware issue, or network failure can bring the entire system down.</p></li></ol><h2>2. Add Persistence – Don’t Lose Data on Restart</h2><p>Our single-node key-value store works, but only until the server crashes or restarts. Once that happens, everything in memory is wiped out. All the data is gone.</p><p>To make our store useful in the real world, we need , a way to ensure data survives restarts, crashes, or power failures.</p><p>Let’s walk through how we can add persistence without complicating the system too much.</p><p>The most common and reliable approach is to use a . Before making any change to the in-memory data, we <strong>append the operation to a file on disk</strong>.</p><p>Every time a  or  is issued, it’s logged to disk before applying it to the in-memory store.</p><ul><li><p>If the server crashes, we can  during startup to rebuild the state.</p></li><li><p>It ensures durability without relying entirely on memory.</p></li></ul><h3>Snapshotting – Speeding Up Recovery</h3><p>Over time, the log file grows. Replaying thousands (or millions) of operations on startup can be slow.</p><p>To fix this, we periodically create , a full dump of the current in-memory key-value store to disk.</p><ol><li><p>Load the latest snapshot.</p></li><li><p>Replay only the log entries after the snapshot.</p></li></ol><p>This reduces startup time while keeping the system durable.</p><p>By adding persistence, we’ve made our system:</p><ul><li><p>: Data won’t vanish after a restart.</p></li><li><p>: We can rebuild state even after a crash.</p></li><li><p>: Still basic, but safer.</p></li></ul><p>At this point, we’ve built a durable single-node key-value store. It can survive crashes and restarts. But there’s one major problem we haven’t solved yet:</p><ul><li><p>The amount of data exceeds the capacity of a single machine?</p></li><li><p>Traffic spikes and one server can’t keep up?</p></li><li><p>We want to support millions of users?</p></li></ul><p>A single server also creates a —any bug, hardware failure, or crash can bring the entire system down.</p><p>To overcome these limitations, we’ll move toward  by <strong>partitioning the data across multiple nodes</strong>.</p><h2>3. Data Partition – Split Data Across Nodes</h2>","contentLength":5928,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/236689d9-941f-4947-a58e-3e38359c61ae_1542x962.png","enclosureMime":"","commentsUrl":null},{"title":"How PostgreSQL Works: Internal Architecture Explained","url":"https://blog.algomaster.io/p/postgresql-internal-architecture","date":1744084855,"author":"Ashish Pratap Singh","guid":720,"unread":true,"content":"<p>In this post, we’ll explore how PostgreSQL works under the hood and dive into the architecture that makes it a powerful choice for a wide range of use cases.</p><p> has emerged as one of the most powerful and versatile open-source relational databases, trusted by software engineers to handle everything from small applications to large-scale enterprise systems.</p><p>Its <strong>robustness, flexibility, and rich feature set</strong> make it a go-to choice for developers worldwide. But to truly harness its potential, understanding its internal architecture and advanced features is essential.</p><p>In this blog post, we’ll take a deep dive into PostgreSQL’s core components and capabilities, with insights that will help you optimize performance, scalability, and reliability in your applications.</p><ol><li><p><strong>Process-Based Architecture</strong>: How PostgreSQL manages connections for stability and isolation.</p></li><li><p><strong>Write Ahead Logging (WAL)</strong>: Ensuring data durability, crash recovery, and replication.</p></li><li><p><strong>Multi Version Concurrency Control (MVCC)</strong>: Allowing concurrent reads and writes without blocking.</p></li><li><p>: From parsing and planning to execution and result delivery.</p></li><li><p>: Choosing the right index for your data.</p></li><li><p>: Managing large tables efficiently with range, list, or hash-based partitioning.</p></li><li><p>: Streaming changes for replication and change data capture.</p></li><li><p>: Extending PostgreSQL’s capabilities with custom features.</p></li><li><p>: Real-time insights for monitoring and optimizing database performance.</p></li></ol><h2>1. Process-Based Architecture</h2><p>PostgreSQL follows a <strong>process-per-connection architecture</strong>, meaning each client connection is handled by a dedicated operating system process.</p><p>When the PostgreSQL server (postmaster) starts, it listens for incoming connections on the configured port. For each new client connection, it <strong>forks a new backend process</strong> to handle that session​. This backend process handles all communication and query execution for that client.</p><p>Once the session ends (i.e., the client disconnects), the associated process terminates.</p><p>This architecture differs from  used in some other databases (e.g., MySQL or SQL Server), where a single process spawns threads for multiple connections.</p><h4>Why PostgreSQL Chooses Processes?</h4><ul><li><p>: Each connection runs in its own process, ensuring that if one session crashes, it doesn’t affect others. This design reduces the risk of memory corruption, race conditions, or resource conflicts between clients.</p></li><li><p>: The process model provides a higher level of stability, as issues in one connection are contained within its process.</p></li><li><p>Since each connection is isolated, PostgreSQL doesn’t need fine-grained locking on every internal data structure (as thread-based systems do). This makes the system easier to maintain, debug, and extend.</p></li></ul><p>While robust, this architecture does come with certain trade-offs:</p><ul><li><p>: Each backend process maintains its own stack and local memory for query execution. This can consume significant RAM, especially when many idle or parallel connections are open.</p></li><li><p>: Handling thousands of concurrent client connections means creating thousands of OS processes. This can lead to high context-switching costs and pressure on kernel-level resources.</p></li></ul><p>To maximize performance while retaining the benefits of PostgreSQL’s architecture:</p><ul><li><p>Use a  like PgBouncer</p><ul><li><p>PgBouncer sits between clients and PostgreSQL, reusing a small pool of persistent connections.</p></li><li><p>This drastically reduces process overhead while supporting thousands of clients.</p></li></ul></li><li><p>Tune  parameter in </p><ul><li><p>Don't allow too many concurrent processes unless your hardware can handle them.</p></li><li><p>A common pattern: set  to ~200–500, and let PgBouncer manage the rest.</p></li></ul></li></ul><p>In addition to client backends, PostgreSQL runs <strong>persistent background processes</strong>, also forked by the postmaster at startup. Each has a distinct job and helps with system maintenance and performance:</p><ul><li><p>Periodically flushes dirty pages from shared buffers to disk (to bound recovery time)</p></li><li><p>Writes WAL (Write-Ahead Log) changes from memory to disk</p></li><li><p>Automatically cleans up dead tuples and refreshes statistics</p></li><li><p>Continuously writes dirty buffers in the background to smooth out I/O spikes</p></li><li><p>Handles streaming WAL to replicas for physical/logical replication</p></li></ul><h2>2. Write Ahead Logging (WAL)</h2><p><strong>Write-Ahead Logging (WAL)</strong> is one of PostgreSQL’s most critical mechanisms for ensuring <strong>data durability, consistency, crash recovery, and replication</strong>.</p><p>WAL is a <strong>sequential log of all database changes</strong>, written  those changes are applied to the actual data files (the heap or index pages). This ensures that, in the event of a crash or power failure, PostgreSQL can  and restore the database to a <strong>safe and consistent state</strong>.</p><ul><li><p>Client executes a write operation (INSERT, UPDATE, DELETE, DDL).</p></li><li><p>PostgreSQL generates a WAL record describing the change.</p></li><li><p>The WAL record is written to memory (WAL buffer).</p></li><li><p>On commit, the WAL is flushed to disk (fsync)—this makes the transaction durable.</p></li><li><p>The actual data files (heap pages) may be updated later, asynchronously by background processes (like the checkpointer or background writer).</p></li><li><p>In case of a crash, PostgreSQL replays the WAL from the last checkpoint to recover the lost changes.</p></li></ul><ul><li><p>: WAL replay brings the database to a consistent state after a crash. This ensures that no committed data is lost, even after a system failure.</p></li><li><p>: Enables both synchronous and asynchronous replication for disaster recovery and read scaling. A replica essentially replays WAL just like crash recovery, but continuously, to stay in sync with the primary.</p></li><li><p><strong>Point-in-Time Recovery (PITR)</strong>: By archiving WAL, you can restore a backup and replay WAL to a specific point in time, essentially “time-traveling” the database to a desired state.</p></li></ul><h4>Managing WAL: Best Practices</h4><p>If unmanaged, WAL can consume significant disk space. Here are ways to handle it:</p><ul><li><pre><code><code>archive_mode = on\narchive_command = 'cp %p /mnt/wal_archive/%f'</code></code></pre></li><li><p> to delete old WAL files.</p></li><li><p>Set appropriate (e.g., , ) to balance recovery time vs. runtime I/O.</p></li><li><p> to avoid prematurely removing WAL needed by standby or logical consumers.</p></li></ul><h2>3. Multi Version Concurrency Control (MVCC)</h2><p>PostgreSQL uses  to handle simultaneous transactions without requiring heavy locking. MVCC is a powerful mechanism that allows <strong>reads and writes to occur concurrently</strong> by maintaining multiple versions of a row. </p><p>This ensures that every transaction sees a  of the database as it existed at the time the transaction began—regardless of ongoing changes by other users.</p><p>Traditional databases often use locks to prevent conflicts between readers and writers. However, this leads to blocking and contention:</p><ul><li><p>Writers lock rows to prevent readers from seeing half-written data</p></li><li><p>Readers may block writers while reading data</p></li></ul><p>PostgreSQL avoids this by using MVCC to isolate transactions without locking—resulting in <strong>high concurrency, better performance, and smoother scalability</strong>.</p><p>When a transaction starts, PostgreSQL assigns it a unique .</p><p>PostgreSQL tracks versions of rows using hidden system columns:</p><ul><li><p>: The transaction ID that inserted the row</p></li><li><p>: The transaction ID that deleted (or updated) the row</p></li></ul><p>Let’s say we have a table called :</p><pre><code>id | balance\n---+---------\n1  | 1000</code></pre><ul><li><p> → inserted by transaction ID 100</p></li><li><p> → it hasn’t been deleted/updated yet</p></li></ul><h4>What Happens During Reads?</h4><p>When a transaction reads a row, it sees the version that was current at the start of the transaction:</p><ul><li><p>It checks  and  to determine if the row  at the time the transaction started.</p></li><li><p>If another transaction later updates the row, it  with a new , leaving the original intact for other readers.</p></li></ul><pre><code>BEGIN;  -- Transaction ID = 200\nSELECT * FROM accounts WHERE id = 1;</code></pre><ul><li><p>T1 sees the row with </p></li><li><p>Since  and , the row is visible to T1</p></li><li><p>T1 keeps using this  of the database until it commits, even if the data is updated later</p></li></ul><h4>What Happens During Writes?</h4><p>Writers create new versions of rows without blocking readers, ensuring high concurrency.</p><p>PostgreSQL . Instead:</p><ul><li><p>: Adds a new row with the current transaction’s XID in </p></li><li><p>: Marks the old row’s  and inserts a new row with a fresh </p></li><li><p>: Only sets ; the row remains until cleanup</p></li></ul><p>This design ensures —older transactions can still see the old versions of rows, even after they're updated. Readers , and writers . Each sees the world as it existed at their transaction start.</p><p><strong>Transaction 2 (T2) – A concurrent update</strong></p><pre><code>BEGIN;  -- Transaction ID = 201\nUPDATE accounts SET balance = 1500 WHERE id = 1;</code></pre><p>Here’s what happens internally:</p><ul><li><ul><li><p>Its  → marking it as deleted by T2</p></li></ul></li><li><ul><li><p>Its  and </p></li></ul></li></ul><p>So now there are  of the row:</p><pre><code>Old version: balance = 1000, xmin = 100, xmax = 201\nNew version: balance = 1500, xmin = 201, xmax = NULL</code></pre><p><strong>What Each Transaction Sees</strong></p><ul><li><p> is still active and keeps using its snapshot from the beginning:</p><ul><li><p>It sees the old row: </p></li><li><p>It ignores the new row, because its  (which is after T1 started)</p></li></ul></li><li><ul><li><p>The new row it just inserted: </p></li></ul></li></ul><p>Once both transactions are done, PostgreSQL will still keep both row versions until the old one is no longer needed.</p><h4>Cleanup: The Role of VACUUM</h4><p>PostgreSQL stores all row versions in the table (heap) until it's safe to remove them. But old versions <strong>don’t disappear automatically</strong>.</p><p>That’s where process comes in:</p><ul><li><p>Removes old row versions that are no longer visible to any transaction</p></li><li><p>Prevents  by freezing old transaction IDs</p></li></ul><p>PostgreSQL runs  in the background by default, but tuning its frequency is critical in high-write systems.</p><h2>4. Query Execution Pipeline</h2><p>When you run a query in PostgreSQL—whether it's a simple  or a complex join across multiple tables, it goes through a <strong>well-defined five-stage pipeline</strong>.</p><p>Each stage transforms the query from raw SQL into actual database operations that return results or modify data.</p><h4>The Five Stages of Query Execution</h4><ol><li><ul><li><p>PostgreSQL takes the raw SQL string and checks it for .</p></li><li><p>It then converts the query into a  — a structured, internal representation of what the query is trying to do.</p></li></ul></li><li><ul><li><p>Think of this step as preprocessing or query transformation—reshaping the original request before planning.</p></li><li><p>PostgreSQL applies <strong>rules and view expansions</strong> to the parse tree.</p></li><li><p>For example, if the query targets a , the system <strong>rewrites the view reference</strong> into its underlying query.</p></li></ul></li><li><ul><li><p>The planner generates <strong>multiple candidate execution plans</strong> for the query.</p></li><li><p>It estimates the  of each plan using table statistics and selects the  one.</p></li><li><p>Cost is measured in terms of CPU, I/O, and memory usage—not time directly, but an abstract \"execution cost\" unit.</p></li></ul></li><li><ul><li><p>PostgreSQL  step by step.</p></li><li><p>Execution is , meaning each node requests rows from its child, processes them, and passes results upward.</p></li></ul></li><li><ul><li><p>Once the executor produces result tuples, they are  (for SELECT), or  (for INSERT/UPDATE).</p></li></ul></li></ol><h4><strong>Key Features in PostgreSQL’s Execution Engine</strong></h4><ul><li><p>: For large datasets, queries can be split across multiple CPU cores.</p></li><li><p>: Complex queries can be compiled at runtime for faster execution.</p></li></ul><p>Use the  command to analyze query plans and identify performance bottlenecks. For example:</p><pre><code>EXPLAIN ANALYZE\nSELECT * FROM orders WHERE customer_id = 123 AND order_date &gt; now() - interval '30 days';</code></pre><p>Indexes are essential to database performance. PostgreSQL offers a variety of index types to optimize query performance for different data types:</p><ol><li><p><strong>B-tree (Default Index Type)</strong></p><ul><li><p> Equality and range queries (, , , , )</p></li><li><p> Numbers, strings, dates—anything with a natural sort order</p><pre><code>CREATE INDEX idx_price ON products(price);</code></pre></li></ul></li><li><ul><li><p> Simple equality comparisons ( only)</p></li><li><p>Slightly faster than B-tree for some equality lookups, but limited in capabilities.</p><pre><code>CREATE INDEX idx_hash_email ON users USING HASH(email);</code></pre></li></ul></li><li><p><strong>GIN (Generalized Inverted Index)</strong></p><ul><li><p> Documents or composite values (arrays, JSON, full-text search) when you need to check if a document contains a value or perform membership checks.</p></li><li><p> Indexes every element inside a value (like words in a text field or keys in a JSON)</p><pre><code>-- For full-text search\nCREATE INDEX idx_gin_content ON articles USING GIN(to_tsvector('english', content));\n\n-- For JSONB\nCREATE INDEX idx_jsonb_data ON items USING GIN(data jsonb_path_ops);\n\n-- For array values\nCREATE INDEX idx_tags_gin ON posts USING GIN(tags);</code></pre></li></ul></li><li><p><strong>GiST (Generalized Search Tree)</strong></p><ul><li><p> Complex, non-scalar data types (geospatial, ranges, fuzzy text search)</p></li><li><p> Stores  or intervals and supports overlaps, proximity, containment. Underpins the PostGIS extension, range queries, and more.</p><pre><code>-- For geospatial data (via PostGIS)\nCREATE INDEX idx_location_gist ON places USING GiST(geom);\n\n-- For range types\nCREATE INDEX idx_price_range ON items USING GiST(price_range);</code></pre></li></ul></li><li><p><strong>SP-GiST (Space-Partitioned GiST)</strong></p><ul><li><p> Data with natural space partitioning (e.g., tries, quadtrees)</p></li><li><p> Prefix searches, IP subnet matching, k-d trees</p><pre><code>-- For text prefix matching\nCREATE INDEX idx_prefix ON entries USING SPGIST(title);</code></pre></li></ul></li><li><ul><li><p> Huge, append-only tables where data is naturally sorted (e.g., time-series)</p><pre><code>CREATE INDEX idx_log_time_brin ON logs USING BRIN(log_timestamp);</code></pre></li></ul></li></ol><p> allows large tables to be divided into smaller, more manageable pieces based on range, list, or hash criteria. This improves query performance by enabling , where only relevant partitions are scanned.</p><p>PostgreSQL supports , which means you define partitions using standard SQL with the  clause.</p><p>To create a partitioned table by date range:</p><pre><code>-- Create the partitioned parent table\nCREATE TABLE measurements (\n  city_id    INT NOT NULL,\n  logdate    DATE NOT NULL,\n  peaktemp   INT,\n  unitsales  INT\n) PARTITION BY RANGE (logdate);\n\n-- Create child partitions for each year\nCREATE TABLE measurements_2024 PARTITION OF measurements\n  FOR VALUES FROM ('2024-01-01') TO ('2024-12-31');\n\nCREATE TABLE measurements_2025 PARTITION OF measurements\n  FOR VALUES FROM ('2025-01-01') TO ('2025-12-31');</code></pre><p>When you insert into , PostgreSQL automatically routes the row to the correct partition based on the  value.</p><ul><li><p>Splits data by ranges of values (e.g., dates, numbers).</p></li><li><p>Divides data by discrete values (e.g., region codes {“APAC”, “EMEA”, ...}).</p></li><li><p>Distributes rows evenly using a hash function (when range/list isn’t practical or balanced).</p></li></ul><blockquote><p>Partitioning reduces query times by avoiding full table scans. Ideal for time-series data or datasets that can be logically grouped (e.g., by region or category).</p></blockquote><p> is the process of transforming PostgreSQL’s  into  like:</p><pre><code>INSERT INTO customers (id, name) VALUES (1, 'Alice');\n\nUPDATE orders SET status = 'shipped' WHERE id = 42;\n\nDELETE FROM payments WHERE id = 7;</code></pre><p>It allows changes from the WAL to be streamed in a logical format, making it useful for replication and <strong>Change Data Capture (CDC)</strong>. </p><h4>How Logical Decoding Works?</h4><ol><li><p>PostgreSQL WAL contains all changes, but in a binary, low-level format</p></li><li><p>Logical decoding interprets WAL into high-level row changes</p></li><li><p>The output is emitted using an , such as:</p><ul><li><p> (used for PostgreSQL logical replication)</p></li><li><p> (outputs changes as JSON)</p></li><li><p> (outputs Protocol Buffers)</p></li><li><p> (for debugging/logging)</p></li></ul></li><li><p>A  is created to:</p><ul><li><p>Track how much of the WAL has been consumed</p></li><li><p>Prevent PostgreSQL from deleting WAL segments that are still needed by a consumer</p></li></ul></li></ol><ul><li><p><strong>Change Data Capture (CDC):</strong> Stream inserts/updates/deletes to message brokers (e.g., Kafka, RabbitMQ)</p></li><li><p> Sync changes into a data warehouse (like BigQuery or Snowflake)</p></li><li><p> Trigger downstream services on data changes</p></li><li><p> Sync tables across PostgreSQL instances (or even to MongoDB/MySQL with external tools)</p></li></ul><blockquote><p> Logical decoding requires extensions like wal2json for specific output formats, as it’s not built-in by default.</p></blockquote><p>One of PostgreSQL’s greatest strengths is its . </p><p>From its inception, PostgreSQL was designed to be , enabling users to extend its functionality without modifying core source code.</p><p>This has turned PostgreSQL from a traditional relational database into a true —capable of powering everything from analytics to full-text search, machine learning, and distributed systems.</p><p>An  in PostgreSQL is a package of additional functionality that can include:</p><ul><li><p>SQL objects (functions, types, tables, operators)</p></li><li><p>Procedural language support</p></li><li><p>Native C code for performance</p></li><li><p>Background workers or hooks into the core engine</p></li></ul><p>Once installed, extensions behave like built-in features, seamlessly integrated into the database engine.</p><p>Extensions can be created by the community or bundled with PostgreSQL. To install one:</p><pre><code>CREATE EXTENSION pgcrypto;</code></pre><p>Extensions live in the  directory and are managed through , , and .</p><ul><li><p>: Adds  for hashing, encryption, and password handling</p></li><li><p>: Enables vector , useful for machine learning and AI applications</p></li><li><p>: Turns PostgreSQL into a , supporting geometry, GIS queries, spatial indexing</p></li><li><p>: Enables , allowing you to shard and scale out across multiple nodes</p></li></ul><blockquote><p>This flexibility makes PostgreSQL adaptable to a wide range of applications, from traditional relational databases to specialized data processing systems.</p></blockquote><p>PostgreSQL’s  gathers real-time data on database activity, helping you monitor and optimize performance. </p><p>There are two main types of statistics PostgreSQL collects:</p><ol><li><p><strong>Cumulative Activity Statistics</strong>: Used for , , and </p></li><li><p>: Used by the  to optimize execution plans</p></li></ol><h4>Cumulative Statistics System (pg_stat views)</h4><p>These statistics are stored in * and reflect what’s happened since the server started or since stats were last reset.</p><ul><li><p>: Shows live queries and their status.</p></li><li><p>Shows table-level read/write/VACUUM stats.</p></li><li><p>Tracks index usage (helps detect unused indexes).</p></li><li><p>: Tracks execution metrics for SQL statements, helping identify slow or resource-intensive queries.</p></li></ul><p><strong>Example: Identify Top Slow Queries</strong></p><pre><code><code>SELECT query, calls, total_exec_time, mean_exec_time\nFROM pg_stat_statements\nORDER BY total_exec_time DESC\nLIMIT 5;</code></code></pre><h4>Planner Statistics (ANALYZE statistics)</h4><p>This refers to the data collected by the  command (automatically run by autovacuum or manually by DBAs) about the contents of tables.</p><p>These stats are stored in the  system catalog and are critical for query planning.</p><p>What PostgreSQL collects per column:</p><ul><li><p>List of <strong>most common values (MCVs)</strong> and their frequencies</p></li><li><p>A  of value distribution</p></li></ul><p>With  (using ), PostgreSQL can also store:</p><ul><li><p> between columns</p></li><li><p> (e.g., if )</p></li><li><p> estimates for combinations of columns</p></li></ul><p>PostgreSQL’s query planner uses these stats to:</p><ul><li><p>Estimate  (how many rows a WHERE clause will match)</p></li><li><p>Choose between , , , or </p></li><li><p>Allocate memory for  or </p></li></ul><p>If  finds that  occurs in 90% of rows, the planner may avoid using an index because the query isn’t selective.</p><p>PostgreSQL’s robust architecture and feature set make it a powerful choice for developers. From its process-based model and MVCC for concurrency to advanced features like logical decoding and extensions, it offers the flexibility and reliability needed for modern applications.</p><p>To take your understanding further, explore:</p><p>By mastering these concepts, you’ll be well-equipped to design, optimize, and troubleshoot PostgreSQL databases, ensuring your applications run efficiently and reliably.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/how-i-mastered-data-structures-and-algorithms?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQ1NjU1MjUyLCJpYXQiOjE3MjE1MjE3MzEsImV4cCI6MTcyNDExMzczMSwiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.2cNY811YEugd5iH9XJQhakBzyahGqF7PcATBlFj5J2w&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p>","contentLength":18633,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/f36a1d93-5b88-409a-b435-ea1f031e5ca0_1498x1114.png","enclosureMime":"","commentsUrl":null},{"title":"How to Handle Failures in Distributed Systems","url":"https://blog.algomaster.io/p/handling-failures-in-distributed-systems","date":1743652725,"author":"Ashish Pratap Singh","guid":719,"unread":true,"content":"<p>In a distributed system, <strong>failures aren’t a possibility—they’re a certainty.</strong></p><p>Your database might go down. A service might become unresponsive. A network call might time out. The question is not  these things will happen—but .</p><p>As engineers, our job is to design systems that embrace this reality and <strong>gracefully handle failures</strong>.</p><p>In this article, we’ll cover:</p><ul><li><p> in distributed systems</p></li><li><p> for handling failures</p></li></ul><p>Distributed systems involve multiple independent components communicating over a network.</p><p>And each of these introduces potential failure points:</p><p>The network is the most unreliable component in any distributed architecture.</p><ul></ul><blockquote><p>Even if two services are running in the same data center, network glitches can still occur.</p></blockquote><p>A single machine (or container) can go down due to:</p><ul></ul><blockquote><p>In distributed systems, <strong>every node is potentially a single point of failure</strong> unless redundancy is built in.</p></blockquote><p>A service may fail even if the machine it's running on is healthy.</p><ul><li><p>Code bugs (null pointers, unhandled exceptions)</p></li><li><p>Deadlocks or resource exhaustion</p></li><li><p>Memory leaks causing the service to slow down or crash</p></li><li><p>Misconfigurations (e.g., bad environment variables)</p></li></ul><ul><li><p>Caches (like Redis or Memcached)</p></li><li><p>External APIs (payment gateways, 3rd-party auth providers)</p></li><li><p>Message queues (like Kafka, RabbitMQ)</p></li></ul><p>If any of these are unavailable, misbehaving, or inconsistent, it can cause cascading failures across the system.</p><blockquote><p> Your checkout service calls the payment API, which calls a bank API, which calls a fraud-detection microservice. Each hop is a potential point of failure.</p></blockquote><p>Data replication across systems (like DB sharding, caching layers, or eventual consistency models) can introduce:</p><ul><li><p>Lost updates due to race conditions</p></li></ul><blockquote><p> A user updates their address, but due to replication lag, the shipping system fetches the old address and sends the package to the wrong place.</p></blockquote><h2>6. <strong>Configuration &amp; Deployment Errors</strong></h2><p>Failures aren't always caused by bugs—they’re often caused by mis-configurations and human errors:</p><ul><li><p>Misconfigured load balancers</p></li><li><p>Missing secrets in the environment</p></li><li><p>Incompatible library updates</p></li><li><p>Deleting the wrong database</p></li><li><p>Rolling out a new version without backward compatibility</p></li></ul><blockquote><p>According to multiple incident postmortems (e.g., AWS, Google), a large number of production outages are triggered by —not code.</p></blockquote><h2>7. <strong>Time-Related Issues (Clock Skew, Timeouts)</strong></h2><p>Distributed systems often rely on time for:</p><ul></ul><p>But system clocks on different machines can drift out of sync (called ), which can wreak havoc.</p><pre><code>Machine A: 12:00:01\nMachine B: 11:59:59</code></pre><p>A token generated on Machine B might be considered “expired” when validated by Machine A, even if it was just created.</p><p>Let’s look at the  that make your system resilient when parts of it inevitably fail.</p><h2>1. Set<strong> Timeouts for Remote Calls</strong></h2><p>A timeout is the <strong>maximum time you’re willing to wait</strong> for a response from another service. If a service doesn’t respond in that time window, you abort the operation and handle it as a failure.</p><p>Every network call whether it’s to a REST API, database, message queue, or third-party service should have a timeout.</p><p>Waiting too long can hog threads, pile up requests, and cause cascading failures. It’s better to fail fast and try again (smartly).</p><p>To be effective, timeouts should be:</p><ul><li><p>Short enough to fail fast</p></li><li><p>Long enough for the request to realistically complete</p></li><li><p>Vary depending on the operation (e.g., reads vs writes, internal vs external calls)</p></li></ul><p>A good practice is to base timeouts on the <strong>service’s typical latency</strong> (e.g., use the 99th percentile response time or service <a href=\"https://en.wikipedia.org/wiki/Service-level_objective\">SLO</a>, plus a safety margin)​. </p><p>If your downstream service has a p99 latency of 450ms:</p><pre><code>Recommended Timeout = 450ms + 50ms buffer = 500ms</code></pre><p>This ensures most successful responses arrive before the timeout, while truly slow or hung requests get aborted.</p><ul><li><p>Never use infinite or unbounded timeouts</p></li><li><p>Don’t assume the caller will enforce a timeout for you</p></li></ul><h2>2. Intelligently, Not Blindly</h2>","contentLength":3857,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8a8f6a06-24d2-424a-8cb9-b20f7801fbf6_1866x954.png","enclosureMime":"","commentsUrl":null},{"title":"System Design was HARD until I Learned these 30 Concepts","url":"https://blog.algomaster.io/p/30-system-design-concepts","date":1743305560,"author":"Ashish Pratap Singh","guid":718,"unread":true,"content":"<p> can feel overwhelming especially when you're just starting out and don’t know where to begin.</p><p>But once you understand the  and , it becomes much less intimidating—whether you're preparing for  or <strong>designing scalable systems</strong> at work.</p><p>In this article, I’ll walk you through the <strong>30 most important System Design concepts</strong> every developer should know.</p><p>Learning these concepts helped me land <strong>offers from multiple big tech companies. </strong>And over the past 8 years as a Software Engineer, I’ve seen them used repeatedly when building and scaling large systems.</p><p>I’ve also included <strong>links to detailed articles</strong> I’ve written on several of these topics, so you can dive deeper whenever you’d like.</p><p>I also recently created a  that quickly walks through all 30 concepts—<strong>packed with visuals and animations</strong> to make everything easier to understand.</p><p><strong>Subscribe for more such videos!</strong></p><h2><strong>1. Client-Server Architecture</strong></h2><p>Almost every web application that you use is built on this simple yet powerful concept called <strong>client-server architecture</strong>.</p><p>On one side, you have a —this could be a web browser, a mobile app, or any other frontend application.</p><p>and on the other side, you have a —a machine that runs continuously, waiting to handle incoming requests.</p><p>The client sends a request to <strong>store, retrieve, or modify data</strong>.</p><p>The server receives the request, processes it, performs the necessary operations, and sends back a response.</p><p>This sounds simple, but there’s a big question: <em><strong>How does the client even know where to find the server?</strong></em></p><p>A client doesn’t magically know where a server is, it needs an  to locate and communicate with it.</p><p>On the internet, computers identify each other using , which work like phone numbers for servers.</p><p>Every publicly deployed server has a . When a client wants to interact with a service, it must send requests to the correct IP address.</p><ul><li><p>When we visit a website, we don’t type its IP address—we just enter the website name.</p></li><li><p>We can’t expect users (or even systems) to memorize a string of random numbers for every service they connect to.</p></li><li><p>And if we migrate our service to another server, its IP address may change—breaking all direct connections.</p></li></ul><p>Instead of relying on hard-to-remember IP addresses, we use something much more human-friendly: .</p><p>But, we need a way to map a domain name to it’s corresponding IP address.</p><p>This is where <strong>DNS (or Domain Name System)</strong> comes in. It maps easy to remember domain names (like <a href=\"http://algomaster.io\">algomaster.io</a>) to their corresponding IP addresses.</p><p>Here’s what happens behind the scenes:</p><ol><li><p>When you type  into your browser, your computer asks a DNS server for the corresponding IP address.</p></li><li><p>Once the DNS server responds with the IP, your browser uses it to establish a connection with the server and make a request.</p></li></ol><p>You can find the IP address of any domain using the  command. Just open your terminal and type ping followed by the domain name. And it’ll return the IP address currently assigned to that domain.</p><p>When you visit a website, your request doesn’t always go directly to the server—sometimes, it passes through a  or  first.</p><p>A  acts as a  between your device and the internet.</p><p>When you request a webpage, the proxy forwards your request to the target server, retrieves the response, and sends it back to you.</p><p>Proxy hides your IP address, keeping your location and identity private.</p><p>A  works the other way around. It intercepts client requests and forwards them to backend servers based on predefined rules.</p><p>Allowing direct access to servers can pose , exposing them to threats like  and .</p><p>A reverse proxy mitigates these risks by acting as a controlled entry point that regulates incoming traffic and hides server IPs.</p><p>It can also act as a load balancer, distributing traffic across multiple servers.</p><p>If you want to learn about Proxy vs Reverse Proxy in more detail, checkout this article:</p><p>Whenever a client communicates with a server, there’s always some delay. One of the biggest causes of this delay is .</p><p>For example, if our server is in , but a user in  sends a request, the data has to travel halfway across the world—and then the response has to make the same long trip back.</p><p>This round-trip delay is called —the total time it takes for data to travel between the client and the server. High latency can make applications feel slow and unresponsive.</p><p>One way to  is by deploying our service across <strong>multiple data centers worldwide</strong>.</p><p>This way, users can connect to the  server instead of waiting for data to travel across the globe.</p><p><em><strong>Once a connection is made, how do clients and servers actually communicate?</strong></em></p><p>Every time you visit a website, your browser and the server communicate using a set of rules called <strong>HTTP (Hypertext Transfer Protocol)</strong>.</p><p>That’s why most URLs start with  or its secure version, .</p><ul><li><p>The  sends a request to the server. This request includes a  (containing details like the request type, browser type, and cookies) and sometimes a  (which carries additional data, like form inputs).</p></li><li><p>The  processes the request and responds with an —either returning the requested data or an error message if something goes wrong.</p></li></ul><p>HTTP has a major security flaw, it . This is a serious problem, especially for sensitive information like passwords, credit card details, and personal data.</p><p>That’s why modern websites use <strong>HTTPS (Hypertext Transfer Protocol Secure)</strong> instead. HTTPS encrypts all data using  ensuring that even if someone intercepts the request, they can’t read or alter it.</p><p>But clients and servers don’t directly exchange raw HTTP requests and response.</p><p>HTTP is just a  for transferring data but it doesn’t define:</p><ul><li><p>How requests should be structured</p></li><li><p>What format responses should be in</p></li><li><p>or how different clients should interact with the server.</p></li></ul><p>This is where <strong>APIs (or Application Programming Interfaces)</strong> come in.</p><p>Think of an API as a  that allows clients (like web and mobile apps) to communicate with servers without worrying about low-level details.</p><p>Almost every digital service you use—social media, e-commerce, online banking, ride-hailing apps—is built on APIs working together behind the scenes.</p><p>Here’s how it typically works:</p><ol><li><p>A  sends a request to an API.</p></li><li><p>The , hosted on a server, processes the request, interacts with databases or other services, and prepares a response.</p></li><li><p>The <strong>API sends back the response</strong> in a structured format, usually  or , which the client understands and can display.</p></li></ol><p>APIs provide a —the client doesn’t need to know  the server processes the request, only that it <strong>returns the expected data</strong>.</p><p>If you want to learn more about APIs, checkout this article:</p><p>But, not all APIs are built the same. Different API styles exist to serve different needs. Two of the most popular ones are  and .</p><p>Among the different API styles, <strong>REST (Representational State Transfer) </strong>is the most widely used.</p><p>A  follows a set of rules that define how clients and servers communicate over HTTP in a structured way.</p><ul><li><p> Every request is independent; the server doesn’t store client state.</p></li><li><p> Everything is treated as a resource (e.g., /users, /orders, /products).</p></li><li><p><strong>Uses Standard HTTP Methods:</strong> Clients interact with resources using  like:</p><ul><li><p> → Retrieves data (e.g., fetching a user profile).</p></li><li><p> → Creates new data (e.g., adding a new user).</p></li><li><p> → Updates existing data (e.g., changing user settings).</p></li><li><p> → Removes data (e.g., deleting an account).</p></li></ul></li></ul><p>REST APIs are great because they’re <strong>simple, scalable, and easy to cache</strong>, but they have , especially when dealing with complex data retrieval.</p><p>REST endpoints often return , leading to inefficient network usage. If an API doesn’t return related data, the client may need to  to retrieve all required information.</p><p>To address these challenges, GraphQL was introduced in 2015 by Facebook.</p><p>Unlike REST, which forces clients to retrieve ,  lets clients <strong>ask for exactly what they need—nothing more, nothing less</strong>.</p><p>With a REST API, if you need a user details, user profile details along with their recent posts, you might have to  to different endpoints:</p><ol><li><p> → fetch user details</p></li><li><p><code>GET /api/users/123/profile</code> → fetch user profile</p></li><li><p> → fetch user’s posts</p></li></ol><p>With GraphQL, you can <strong>combine those requests into one</strong> and fetch exactly the data you need in a single query:</p><p>The <strong>server responds with only the requested fields</strong>, reducing unnecessary data transfer and improving efficiency.</p><p>However, GraphQL also comes with trade-offs—it <strong>requires more processing on the server side</strong> and isn’t as easy to cache as REST.</p><p>Learn more about REST vs GraphQL here:</p><p>When a client makes a request, they usually want to .</p><p>But this brings up another question—<em><strong>where is the actual data stored?</strong></em></p><p>If our application deals with , we could store it .</p><p>But modern applications handle —far more than what memory can efficiently handle.</p><p>That’s why we need a <strong>dedicated server for storing and managing data</strong>—a .</p><p>A database is the backbone of any application.It ensures that data is stored, retrieved, and managed efficiently while keeping it secure, consistent, and durable.</p><p>When a client requests to  or  data, the <strong>server communicates with the database</strong>, fetches the required information, and returns it to the client.</p><p>But not all databases are the same. Different applications have different <strong>scalability, performance, and consistency</strong> requirements, which is choosing the right type of database is important.</p><p>If you want to learn about different types of databases, checkout this article:</p><p>In system design, we typically choose between .</p><p>SQL databases store data in tables with a  and follow the .</p><ul><li><p> - A transaction is  (it either completes fully or not at all).</p></li><li><p> – Data always remains  and follows defined rules.</p></li><li><p> – Transactions  with each other.</p></li><li><p> – Once data is saved, it , even if the system crashes.</p></li></ul><p>Because of these guarantees, SQL databases are ideal for applications that require <strong>strong consistency and structured relationships</strong>, such as .</p><blockquote><p>Examples of popular SQL databases include: MySQL and PostgreSQL</p></blockquote><p>NoSQL databases on the other hand are designed for <strong>high scalability and performance</strong>.</p><p>They <strong>don’t require a fixed schema</strong> and use different data models, including:</p><ul><li><p> – Fast lookups for simple key-value pairs (e.g., Redis).</p></li><li><p> – Store flexible, JSON-like documents (e.g., MongoDB).</p></li><li><p> – Best for highly connected data (e.g., Neo4j).</p></li><li><p> – Optimized for large-scale, distributed data (e.g., Cassandra).</p></li></ul><p>So, <strong>which one should you use?</strong> It depends on the system requirements.</p><ul><li><p>If you need <strong>structured, relational data with strong consistency</strong> → </p></li><li><p>If you need <strong>high scalability, flexible schemas, or fast reads/writes at scale</strong> → <strong>NoSQL is a better choice.</strong></p></li></ul><p>Many modern applications <strong>use both SQL and NoSQL together.</strong></p><p>For example, an e-commerce platform might:</p><ul><li><p>Store customer orders in SQL (because they require strict consistency).</p></li><li><p>and store Product recommendations in NoSQL (because they need flexible and fast lookups).</p></li></ul><p>If you want to learn more about SQL vs NoSQL, checkout this article:</p><p>As our , so does the number of <strong>requests hitting our application servers</strong>.</p><p>Initially, a  might be enough to handle the load. But, as traffic increases, that single server can become a bottleneck, slowing everything down.</p><p>One of the quickest solutions is to <strong>upgrade the existing server</strong> by adding more CPU, RAM or storage.</p><p>This approach is called <strong>Vertical Scaling (Scaling Up)</strong>—making a single machine more powerful.</p><p>But there are some major limitations with this approach:</p><ol><li><p> You can’t keep upgrading a server forever. Every machine has a maximum capacity.</p></li><li><p> More powerful servers become exponentially more expensive.</p></li><li><p><strong>Single Point of Failure (SPOF)</strong> if this one server crashes, the entire system .</p></li></ol><p>So, while vertical scaling is a quick fix, it’s not a long-term solution for handling high traffic and ensuring system reliability.</p><p>Lets look at a better approach—one that makes our system more scalable and fault tolerant.</p><p>Instead of upgrading a single server, what if we  to share the load?</p><p>This approach is called <strong>Horizontal Scaling (Scaling Out)</strong>—where we <strong>distribute the workload across multiple machines</strong>.</p><p>This approach is better because:</p><ul><li><p><strong>More servers = More capacity →</strong> The system can handle increasing traffic more effectively.</p></li><li><p><strong>No Single Point of Failure →</strong> If one server goes down, others can , improving reliability.</p></li><li><p>Instead of investing in a single, super-expensive machine, we can use multiple affordable ones.</p></li></ul><p>But horizontal scaling introduces a new challenge: <em><strong>how do clients know which server to connect to?</strong></em></p><p>This is where a  comes in.</p><p>A  sits between  and , acting as a  that distributes requests across multiple servers.</p><p>If one server crashes, the Load Balancer automatically redirects traffic to another healthy server.</p><p><em><strong>But how does a Load Balancer decide which server should handle the next request?</strong></em></p><p>It uses <strong>Load Balancing algorithms</strong>, such as:</p><ol><li><p> Requests are sent to servers sequentially, one after another in a loop.</p></li><li><p> Requests are sent to the server with the fewest active connections.</p></li><li><p> Requests from the same IP address always go to the , which helps with </p></li></ol><p>Learn more about load balancing algorithms here:</p><p>So far, we’ve talked about scaling our application servers, but as traffic grows, the volume of data also increases.</p><p>At first, we can scale a database  by adding more CPU, RAM, and storage, but there’s a limit to how much a single machine can handle.</p><p>So, let’s explore other <strong>database scaling techniques</strong> that help manage large volumes of data efficiently.</p><p>One of the quickest and most effective ways to speed up database  is .</p><p>Think of it like the index page at the back of a book—instead of flipping through every page, you jump directly to the relevant section.</p><p>A  works the same way. It’s is a super-efficient lookup table that helps the database quickly locate the required data without scanning the entire table.</p><p>An index stores column values along with pointers to the actual data rows in the table.</p><p>Indexes are typically created on <strong>columns that are frequently queried</strong>, such as:</p><ul><li><p>Columns used in WHERE conditions</p></li></ul><p>But be careful—while indexes <strong>speed up reads, they slow down writes</strong> (, , ) since the index needs to be updated whenever data changes.</p><p>That’s why we should <strong>only index the most frequently accessed columns</strong>.</p><p>Learn more about Database Indexes here:</p><p>Indexing significantly improves , but <em><strong>what if even indexing isn’t enough, and our database can’t handle the growing number of read requests?</strong></em></p><p>That’s where our next database scaling technique  comes in.</p><p>Just like we added more application servers to handle traffic, we can scale our database by creating of it across multiple servers.</p><ul><li><p>We have  (also called the ) that handles all  (, , ).</p></li><li><p>We have  that handle .</p></li><li><p>Whenever data is written to the primary database, it gets copied to the read replicas so that they stay in sync.</p></li></ul><p>Replication improves the read performance since read requests are spread across multiple replicas, reducing the load on each one.</p><p>This also improves availability since if the primary replica fails, a read replica can take over as the new primary.</p><p>Replication is great for scaling read heavy applications, <em><strong>but what if we need to scale writes or store huge amounts of data?</strong></em></p><p>Let’s say our service now has and our database has grown to.</p><p>A  will eventually struggle to handle all this data efficiently.</p><p>Instead of keeping everything in one place, we <strong>split the database into smaller, more manageable pieces</strong> and distribute them across .</p><p>This technique is called .</p><ul><li><p>We divide the database into smaller parts called .</p></li><li><p>Each  of the total data.</p></li><li><p>Data is distributed based on a  (e.g., ).</p></li></ul><p>By distributing data this way, we:</p><ul><li><p> → Each shard handles  of queries.</p></li><li><p><strong>Speed up read and write performance</strong> → Queries are distributed across multiple shards instead of hitting a single database.</p></li></ul><p>Sharding is also referred to as horizontal partitioning since it splits data by rows.</p><p>If you want to learn more about Sharding, checkout this article:</p><p><em><strong>But what if the issue isn’t the number of rows, but rather the number of columns?</strong></em></p><p>In such cases, we use , where we <strong>split the database by columns</strong>. Let’s explore that next.</p><h2><strong>18. Vertical Partitioning</strong></h2><p>Imagine we have a  that stores:</p><ul><li><p>profile details (name, email, profile picture)</p></li><li><p>login history (last_login, IP addresses)</p></li><li><p>and billing information (billing address, payment details)</p></li></ul><p>As this table , queries become  because the database must scan  even when a request only needs a </p><p>To optimize this, we use <strong>Vertical Partitioning where we split user table into smaller, more focused tables</strong> based on usage patterns.</p><ul><li><p> → Stores name, email, profile picture.</p></li><li><p> → Stores login timestamps.</p></li><li><p> → Stores billing address, payment details.</p></li></ul><p>This improves  since each request only scans  instead of the entire .</p><p>It reduces , making data retrieval quicker.</p><p>However, no matter how much we optimize the database, <strong>retrieving data from disk is always slower than retrieving from memory</strong>.</p><p><em><strong>What if we could store frequently accessed data in memory for lightning-fast access?</strong></em></p><p> is used to optimize the performance of a system by <strong>storing frequently accessed data in memory</strong> instead of repeatedly fetching it from the database.</p><p>One of the most common caching strategies is the </p><ol><li><p>When a user , the application first check the .</p></li><li><p>If the data is , it’s returned <strong>instantly, avoiding a database call</strong>.</p></li><li><p>If the data , the application <strong>retrieves it from the database</strong>, stores it in the cache for future requests, and returns it to the user.</p></li><li><p>Next time, the same data is requested, it’s served , making the request much faster.</p></li></ol><p>To prevent outdated data from being served, we use —an expiration time set on cached data so it gets  after a certain period.</p><blockquote><p>Popular caching tools include </p></blockquote><p>If you want to learn more about caching strategies, check out this article:</p><p>Lets look at the next database scaling technique.</p><p>Most relational databases use  to store data efficiently by breaking it into separate tables.</p><p>For example, in an e-commerce system:</p><ul><li><p>The  stores user details.</p></li><li><p>The  stores their orders.</p></li><li><p>The  stores product details.</p></li></ul><p>While this , it also When retrieving data from multiple tables, the database must <strong>combine them using JOIN operations</strong>, which can slow down queries as the dataset grows.</p><pre><code>SELECT o.order_id, u.name, u.email, o.product, o.amount\nFROM orders o\nJOIN users u ON o.user_id = u.user_id;</code></pre><p> reduces the number of joins by combining related data into a single table, even if it means some data gets duplicated.</p><p> Instead of keeping and in separate tables, we create  table that stores user details along with their latest orders.</p><p>Now, when retrieving a user’s order history, we don’t need a —the data is already stored together leading to faster queries and better read performance.</p><pre><code>SELECT order_id, user_name AS name, user_email AS email, product, amount\nFROM orders;</code></pre><p>Denormalization is often used in read-heavy applications where speed is more critical but the downside is it leads to  and more .</p><p>As we scale our system across multiple <strong>servers, databases, and data centers</strong>, we enter the world of .</p><p>One of the fundamental principles of distributed systems is the , which states that: No distributed system can achieve all three of the following at the same time:</p><ul><li><p> Every node always returns the .</p></li><li><p> The system  to requests, even if some nodes are down (but the data may not be the latest).</p></li><li><p> The system continues operating even if there’s a  between nodes.</p></li></ul><p>Since <strong>network failures (P) are inevitable</strong>, we must choose between:</p><ul><li><p><strong>Consistency + Partition Tolerance (CP)</strong> → Ensures every request gets the latest data but may reject requests during failures. Example: .</p></li><li><p><strong>Availability + Partition Tolerance (AP)</strong> → Ensures the system always responds, even if some data is stale. Example: <strong>NoSQL databases like Cassandra and DynamoDB</strong>.</p></li></ul><p>To learn more about CAP theorem, check out this article:</p><p>In distributed NoSQL databases, achieving  across all servers .</p><p>Instead, we use —which means:</p><ul><li><p><strong>Not all nodes are updated instantly</strong>, but given enough time, they  sync and return the same data.</p></li><li><p>This allows the system to remain <strong>highly available and fast</strong>, even under extreme loads.</p></li></ul><p><strong>How Eventual Consistency Works:</strong></p><ol><li><p>A user updates data in  of the database.</p></li><li><p>The system  acknowledges the update, ensuring .</p></li><li><p>The update is then <strong>propagated asynchronously to other replicas</strong>.</p></li><li><p>After a short delay, all replicas have the latest data, ensuring .</p></li></ol><p>Most modern applications don’t just store text records, they also need to handle , ,  and other .</p><p>But here’s the problem: <strong>Traditional databases are not designed to store large, unstructured files efficiently</strong>.</p><p>We use <strong>Blob Storage like Amazon S3</strong>—a <strong>highly scalable and cost-effective</strong> way to store <strong>large, unstructured files</strong> in the cloud.</p><p>Blobs are the individual files like images, videos or documents.</p><p>These blobs are stored inside <strong>logical containers or buckets</strong> in the cloud.</p><p>Each file gets a , making it easy to retrieve and serve over the web.</p><blockquote><p> https://my-bucket-name.s3.amazonaws.com/videos/tutorial.mp4</p></blockquote><p>There are several advantages with using blob storage like:</p><ul><li><p> → It can store petabytes of data effortlessly.</p></li><li><p> → You only pay for storage and retrieval that you actually use.</p></li><li><p> → Data is copied across multiple data centers and availability zones for durability.</p></li><li><p> → Files can be retrieved using REST APIs or direct URLs.</p></li></ul><p>A common use case is to stream audio or video files to user application in real-time.</p><p>But streaming directly from blob-storage can be slow, especially if the data is stored in a .</p><p>For example, imagine you’re in  trying to watch a YouTube video that’s hosted on a server in .</p><p>Since the video data has to , this could lead to <strong>buffering and slow load times</strong>.</p><p>A <strong>Content Delivery Network (or CDN)</strong> solves this problem by <strong>delivering content faster</strong> to users based on their location.</p><p>A CDN is a global network of distributed servers that work together to deliver  (like HTML pages, JavaScript files, stylesheets, images, and videos) to users based on their .</p><p>Instead of serving content from a , a CDN  on multiple <strong>edge servers located worldwide</strong>.</p><p>When a user requests content, the  delivers it instead of reaching all the way to the .</p><p>Since content is served from the , users experience  with minimal buffering.</p><p>To learn more about CDN, check out this article:</p><p>Most web applications use , which follows a  model.</p><ol><li><p>The client sends a request.</p></li><li><p>The  processes the request and sends a response.</p></li><li><p>If the client needs new data, it must .</p></li></ol><p>This works fine for  but it’s <strong>too slow and inefficient for real-time applications</strong> like: live chat apps, stock market dashboards and online multiplayer games.</p><p>With HTTP, the only way to get real-time updates is through —sending repeated requests every few seconds.</p><p>But polling is inefficient because it increases server load and wastes bandwidth, as  (when there’s no new data).</p><p>WebSockets solve this problem by allowing <strong>continuous, two-way communication</strong> between the  over a <strong>single persistent connection</strong>.</p><p>Here is how WebSockets work:</p><ol><li><p>The <strong>client initiates a WebSocket connection</strong> with the server.</p></li><li><p>Once established, the connection .</p></li><li><p><strong>The server can push updates</strong> to the client , without waiting for a request.</p></li><li><p>The client can also send messages </p></li></ol><p>This enables real-time interactions and eliminates the need for polling.</p><p>To learn more about WebSockets, check out this article:</p><p>WebSockets enable real-time communication between a client and a server, <em><strong>but what if a server needs to notify another server when an event occurs?</strong></em></p><ul><li><p>When a user makes a payment, Stripe needs to notify your application .</p></li><li><p>If someone pushes code to GitHub, a CI/CD system (e.g., Jenkins) should be triggered automatically.</p></li></ul><p>Instead of constantly  to check if an event has occured,  allow a server to  to another server as soon as the event occurs.</p><ul><li><p>The  registers a webhook URL with the <strong>provider (e.g., Stripe, GitHub, Twilio)</strong>.</p></li><li><p>When an event occurs (e.g., user makes a payment), the <strong>provider sends an HTTP POST request to the webhook URL</strong> with event details.</p></li><li><p>Your app <strong>processes the incoming request</strong> and updates data accordingly.</p></li></ul><p>This saves server resources and reduces unnecessary API calls.</p><p>Traditionally, applications were built using a , where:</p><ul><li><p>All features (e.g., authentication, payments, orders, shipping) are inside .</p></li><li><p>If one part of the system , the  is affected.</p></li><li><p>—one bad update can take down the entire app.</p></li></ul><p> Imagine an e-commerce app where the , , and modules are all tightly connected in a .</p><p>If the inventory system crashes, the entire app could go down.</p><p>Monoliths work <strong>fine for small applications</strong>, but for , they become <strong>hard to manage, scale, and deploy</strong>.</p><p>The solution is to break down your application into smaller, independent services called  that work together.</p><ol><li><p>Handles </p></li><li><p>Has its , so it can scale .</p></li><li><p>Communicates with other microservices using .</p></li></ol><p>This way services can be scaled and deployed individually without affecting the entire system.</p><p>However, when multiple microservices need to communicate, direct  aren’t always efficient—this is where  come in.</p><p>In a , functions call each other  and wait for a response.</p><p>But in a <strong>microservices-based system</strong>, this approach is inefficient because:</p><ul><li><p>If one service is  or , everything waits.</p></li><li><p>High traffic can  a single service.</p></li><li><p><strong>Synchronous communication</strong> (waiting for immediate responses) doesn’t scale well.</p></li></ul><p>A  enables services to <strong>communicate asynchronously</strong>, allowing requests to be processed  other operations.</p><ol><li><p>A  (e.g., checkout service) <strong>places a message in the queue</strong> (e.g., \"Process Payment\").</p></li><li><p>The <strong>queue temporarily holds the message</strong> until a  (e.g., payment service) is ready to process it.</p></li><li><p>The <strong>consumer retrieves the message</strong> and processes it.</p></li></ol><p>Using message queues, we can decouple services and improve the scalability and fault tolerance.</p><blockquote><p>Common message queue systems include: Apache Kafka, Amazon SQS and RabbitMQ.</p></blockquote><p>To learn more about Message Queues, check out this article:</p><p>Using message queues, we can prevent overload on internal services within our system. </p><p><em><strong>But, how do we prevent overload for the public APIs and services we deploy.</strong></em></p><p>Imagine a <strong>bot starts making thousands of requests per second</strong> to your website.</p><p>Without restrictions, this could:</p><ul><li><p> by consuming all available resources.</p></li><li><p> due to excessive API usage.</p></li><li><p>and  for legitimate users.</p></li></ul><p>Rate Limiting <strong>restricts the number of requests</strong> a client can send within a specific time frame.</p><ol><li><p>Every user or IP address is assigned a  (e.g., ).</p></li><li><p>If they , the server <strong>blocks additional requests temporarily</strong> and <strong>returns an error (HTTP 429 – Too Many Requests)</strong>.</p></li></ol><p>There are various rate limiting algorithms. Some of the popular ones are:</p><ul><li><p> → Limits requests based on a fixed time window (e.g., 100 requests per minute).</p></li><li><p> → More flexible version that dynamically adjusts limits to smooth out request bursts.</p></li><li><p> → Users get  for requests, which  at a fixed rate.</p></li></ul><p>To learn more about rate limiting algorithms, checkout this article:</p><p>We don’t need to implement our own rate limiting system - this can be handled by something called an .</p><p>An API Gateway is a  that handles <strong>authentication, rate limiting, logging and monitoring, and request routing</strong>.</p><p>Imagine a <strong>microservices-based application</strong> with multiple services.</p><p>Instead of exposing each service , an  acts as a  for all client requests.</p><ol><li><p>The  to the API Gateway.</p></li><li><p>The Gateway  (e.g., authentication, rate limits).</p></li><li><p>It  to the appropriate micro-service.</p></li><li><p>The response is sent back  to the client.</p></li></ol><p>API gateway simplifies API management and improves scalability and security.</p><blockquote><p>Popular API Gateway solutions include <strong>NGINX, Kong and AWS API Gateway.</strong></p></blockquote><p>If you want to learn more about API gateways, checkout this article:</p><p>In distributed systems,  and  are common. If a user <strong>accidentally refreshes a payment page</strong>, the system might receive  instead of one.</p><p>Idempotency ensures that  produce the  as if the request was made .</p><ol><li><p>Each request is assigned a  (e.g., ).</p></li><li><p>Before processing, the system checks if the request .</p></li><li><p>If yes → It <strong>ignores the duplicate request</strong>.</p></li><li><p>If no → It <strong>processes the request normally</strong>.</p></li></ol><p>Idempotency prevents duplicate transactions and ensures data consistency in distributed systems.</p><p>If you want to learn more about idempotency, checkout this article:</p><p>Thank you so much for reading!</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/how-i-mastered-data-structures-and-algorithms?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&amp;token=eyJ1c2VyX2lkIjo4MzYwMjc0MywicG9zdF9pZCI6MTQ1NjU1MjUyLCJpYXQiOjE3MjE1MjE3MzEsImV4cCI6MTcyNDExMzczMSwiaXNzIjoicHViLTIyMDIyNjgiLCJzdWIiOiJwb3N0LXJlYWN0aW9uIn0.2cNY811YEugd5iH9XJQhakBzyahGqF7PcATBlFj5J2w&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p><p>I hope you have a lovely day!</p>","contentLength":27936,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/8cc36e60-9120-4c61-b465-bfd43ef9a705_1350x871.png","enclosureMime":"","commentsUrl":null},{"title":"Top 10 Kafka Use Cases","url":"https://blog.algomaster.io/p/top-10-kafka-use-cases","date":1743053561,"author":"Ashish Pratap Singh","guid":717,"unread":true,"content":"<p> began its journey at  as an internal tool designed to collect and process massive amounts of log data efficiently. But over the years, Kafka has evolved far beyond that initial use case.</p><p>Today, Kafka is a <strong>powerful, distributed event streaming platform</strong> used by companies across every industry—from tech giants like Netflix and Uber to banks, retailers, and IoT platforms.</p><p>Its core architecture, based on <strong>immutable append-only logs</strong>, , and , makes it incredibly scalable, fault-tolerant, and versatile.</p><p>In this article, we’ll explore the <strong>10 powerful use cases of Kafka </strong>with real-world examples.</p><p>In modern distributed applications, logs and metrics are generated across hundreds or even thousands of servers, containers, and applications. These logs need to be collected for monitoring, debugging, and security auditing.</p><p>Traditionally, logs were stored locally on servers, making it difficult to search, correlate, and analyze system-wide events.</p><p>Kafka solves this by acting as a , enabling fault-tolerant, scalable, and high-throughput pipeline for log collection processing.</p><p>Instead of sending logs directly to a storage system, applications and logging agents stream log events to Kafka topics. Kafka provides a durable buffer that absorbs spikes in log volumes while decoupling producers and consumers.</p><h3>Kafka Log Aggregation Pipeline</h3><h4><strong>Step 1: Applications Send Logs to Kafka (Producers)</strong></h4><p>Each microservice, web server, or application container generates logs in real time and sends them to Kafka via lightweight log forwarders (log agents) like: <strong>Fluentd, Logstash or Filebeat.</strong></p><p>These tools publish logs to specific Kafka topics (e.g., , ).</p><h4><strong>Step 2: Kafka Brokers Store Logs</strong></h4><p>Kafka acts as the central, durable and distributed log store, providing:</p><ul><li><p> - logs are replicated across multiple brokers</p></li><li><p>  - logs are stored on disk for configurable retention periods</p></li><li><p> Kafka can handle logs from thousands of sources</p></li></ul><h4><strong>Step 3: Consumers Process Logs</strong></h4><p>Log consumers (like Elasticsearch, Hadoop, or cloud storage systems) read data from Kafka and process it for:</p><ul><li><p>for searching and filtering logs</p></li><li><p>long-term archiving in S3, HDFS, or object storage</p></li><li><p>trigger alerts based on log patterns</p></li></ul><h4><strong>Step 4: Visualization and Alerting</strong></h4><p>Processed logs are visualized and monitored using tools like:</p><ul><li><p>  - for dashboards and visualization</p></li><li><p> - for real-time alerting</p></li><li><p>  - for advanced log analysis and security insights</p></li></ul><p>Change Data Capture (CDC) is a technique used to <strong>track changes in a database (inserts, updates, deletes)</strong> and <strong>stream those changes in real time</strong> to downstream systems.</p><p>Modern architectures rely on multiple systems—search engines, caches, data lakes, microservices—all of which need . Traditional  are slow, introduce latency, and often lead to:</p><ul><li><p> in search indexes and analytics dashboards</p></li><li><p> when syncing multiple systems</p></li><li><p> due to frequent polling</p></li></ul><p>Kafka provides a <strong>high-throughput, real-time event pipeline</strong> that captures and distributes changes from a source database to multiple consumers—ensuring low latency and consistency across systems.</p><h4><strong>Step 1: Capture Changes from the Database</strong></h4><p>Tools like , , or  read the <strong>database transaction logs (binlogs, WALs)</strong> to detect: <code>INSERTs, UPDATEs, DELETEs</code></p><p>Each change is transformed into a structured event and published to a Kafka topic.</p><h4>Step 2: Stream Events via Kafka</h4><p>Kafka topics act as an , providing:</p><ul><li><p> — All changes are stored reliably</p></li><li><p> — Events for a given key (e.g., primary key) are strictly ordered</p></li><li><p> — Thousands of events per second per partition</p></li></ul><h4>Step 3: Distribute to Real-Time Consumers</h4><p>Multiple consumers can subscribe to the change events for various use cases:</p><ul><li><p> → Sync changes to <strong>Elasticsearch / OpenSearch</strong></p></li><li><p> → Update  for fast reads</p></li><li><p> → Stream into <strong>BigQuery / Snowflake / Redshift</strong></p></li></ul>","contentLength":3679,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/410d1420-15c2-49d0-be29-a0e7e13db06b_1596x1082.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}