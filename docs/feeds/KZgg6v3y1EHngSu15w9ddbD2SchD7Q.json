{"id":"KZgg6v3y1EHngSu15w9ddbD2SchD7Q","title":"Dev News - Last 2 days","displayTitle":"Dev News - Last 2 days","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":72,"items":[{"title":"How are operators used with CRDs, CRs?","url":"https://www.reddit.com/r/kubernetes/comments/1i9dzce/how_are_operators_used_with_crds_crs/","date":1737775653,"author":"/u/SeeTheUntruth_Ad7178","guid":570,"unread":true,"content":"<p>I’m relatively new to Kubernetes world. I followed instructions on installing an open source app via operator. Steps are simple - install operator with helm, then apply CRs with kubectl.</p><p>The problem is when I install the operator it also creates the resource. when I apply the CR file, the changes are applied only once. Every other modification in that file, does not get applied. I can’t figure out if this is a bug with the operator or I just don’t know how to use them operators. </p><p>Does an operator “magically” look for a CR file and uses it as part of its install? </p><p>What is the proper way of applying modifications to a CR file? </p><p>When I run k apply and none of the changes are actually applied, I start deleting pods, then deployments and at the end up deleting everything and starting over. </p><p>Any k8s wisdom or simple example would be greatly appreciated. (There aren’t many resource on this specifically. There are many tutorials on how to write your own operator and crd, but I’m not looking for that. )</p>","contentLength":1018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Several Linux DRM Drivers Orphaned Due To Developer Health","url":"https://www.phoronix.com/news/Several-Linux-DRM-Orphaned","date":1737775080,"author":"/u/GL4389","guid":576,"unread":true,"content":"<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1i9dt3e/several_linux_drm_drivers_orphaned_due_to/"},{"title":"Caltrain's electric fleet more efficient than expected","url":"https://www.caltrain.com/news/caltrains-electric-fleet-more-efficient-expected","date":1737769991,"author":"ssuds","guid":202,"unread":true,"content":"<p><em></em></p><p><em></em></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42818692"},{"title":"beekeeper: an ergonomic workerpool crate","url":"https://www.reddit.com/r/rust/comments/1i9bkjc/beekeeper_an_ergonomic_workerpool_crate/","date":1737767980,"author":"/u/jdidion","guid":588,"unread":true,"content":"<p>Hi <a href=\"https://www.reddit.com/r/rust\">r/rust</a>! I've just published <a href=\"https://docs.rs/beekeeper/latest/beekeeper/\">beekeeper</a>, a workerpool library written in Rust.</p><p>This crate started as a fork of <a href=\"https://github.com/lorepozo/workerpool\">workerpool</a>, but at this point it's pretty much a re-write and has lots of additional features, including: * A  trait that is not required to extend  * The ability to write workers with mutable state * Support for custom worker factories * Stock worker implementations, including support for parallelizing callables (functions or closures) * Methods for submitting batches of tasks * The ability to retrieve results as either ordered or unordered iterators, send them to a channel (with support for several alternative channel crates), or store them in the workerpool data structure (called a ) for later retrieval * The ability to pause and resume processing * (optional) support for automatically retrying failures * (optional) support for pinning worker threads to CPU cores</p><p>I'd love for you to try it out and give feedback about what could be improved.</p>","contentLength":965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring prefect for scheduling data workloads","url":"https://www.youtube.com/watch?v=hkm5OfiiorE","date":1737765055,"author":"probabl","guid":382,"unread":true,"content":"<article>It is easy to train a model in a notebook, but if we want to do anything with it in production we probably want to be able to train it regularly. There's good old cron, but it's 2025 now so maybe it's time to explore something a bit more modern. That's why in this livestream we will be exploring Prefect.\n\nWebsite: https://probabl.ai/\nDiscord: https://discord.probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\nIf you're keen to see more videos like this, you can follow us over at  @probabl_ai.\n\n#probabl</article>","contentLength":723,"flags":null,"enclosureUrl":"https://www.youtube.com/v/hkm5OfiiorE?version=3","enclosureMime":"","commentsUrl":null},{"title":"File Explorer is merged to Helix editor","url":"https://github.com/helix-editor/helix/pull/11285","date":1737764900,"author":"manusachi","guid":201,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42818278"},{"title":"You could have invented Fenwick trees","url":"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D","date":1737764614,"author":"matt_d","guid":200,"unread":true,"content":"<div data-magellan-destination=\"s1\"><p> Suppose we have a sequence of  integers  and want to be able to perform arbitrary interleavings of the following two operations, as illustrated in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f1\">1</a> :</p><section><div data-magellan-destination=\"f1\"><div><p> Update and range query operations.</p></div></div></section><ul><li><p> the value at any given index<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#fn1\"></a> by adding some value .</p></li></ul><p> Note that update is phrased in terms of  some value  to the existing value; we can also  a given index to a new value  by adding , where  is the old value.</p><p> If we simply store the integers in a mutable array, then we can update in constant time, but range queries require time linear in the size of the range, since we must iterate through the entire range  to add up the values.</p><p> In order to improve the running time of range queries, we could try to cache (at least some of) the range sums. However, this must be done with care, since the cached sums must be kept up to date when updating the value at an index. For example, a straightforward approach would be to use an array  where  stores the prefix sum ;  can be precomputed in linear time via a scan. Now range queries are fast: we can obtain  in constant time by computing  (for convenience we set  so this works even when ). Unfortunately, it is update that now takes linear time, since changing  requires updating  for every .</p><p> Is it possible to design a data structure that allows  operations to run in sublinear time? (You may wish to pause and think about it before reading the next paragraph!) This is not just academic: the problem was originally considered in the context of  (Rissanen &amp; Langdon, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref11\">1979</a>; Bird &amp; Gibbons, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref2\">2002</a>), a family of techniques for turning messages into sequences of bits for storage or transmission. In order to minimize the bits required, one generally wants to assign shorter bit sequences to more frequent characters, and vice versa; this leads to the need to maintain a dynamic table of character frequencies. We  the table every time a new character is processed and  the table for cumulative frequencies in order to subdivide a unit interval into consecutive segments proportional to the frequency of each character (Ryabko, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref12\">1989</a>; Fenwick, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref5\">1994</a>).</p><p> So, can we get both operations to run in sublinear time? The answer, of course, is yes. One simple technique is to divide the sequence into  buckets, each of size , and create an additional array of size  to cache the sum of each bucket. Updates still run in (1), since we simply have to update the value at the given index and the corresponding bucket sum. Range queries now run in  time: to find the sum , we manually add the values from  to the end of its bucket, and from  to the beginning of its bucket; for all the buckets in between we can just look up their sum.</p><p> We can make range queries even faster, at the cost of making updates slightly slower, by introducing additional levels of caching. For example, we can divide the sequence into  “big buckets” and then further subdivide each big bucket into  “small buckets”, with each small bucket holding  values. The sum of each bucket is cached; now each update requires modifying three values, and range queries run in  time.</p><p> In the limit, we end up with a binary divide-and-conquer approach to caching range sums, with both update and range query taking  time. In particular, we can make a balanced binary tree where the leaves store the sequence itself, and every internal node stores the sum of its children. (This will be a familiar idea to many functional programmers; for example, finger trees (Hinze &amp; Paterson, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref7\">2006</a>; Apfelmus, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref1\">2009</a>) use a similar sort of caching scheme.) The resulting data structure is popularly known as a ,<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#fn2\"></a> presumably because each internal node ultimately caches the sum of a (contiguous)  of the underlying sequence. Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f2\">2</a> shows a segment tree built on a sample array of length  (for simplicity, we will assume that  is a power of two, although it is easy to generalize to situations where it is not). Each leaf of the tree corresponds to an array entry; each internal node is drawn with a grey bar showing the segment of the underlying array of which it is the sum.</p><section></section><p> Let’s see how we can use a segment tree to implement the two required operations so that they run in logarithmic time.</p><ul><li><p> To update the value at index , we also need to update any cached range sums which include it. These are exactly the nodes along the path from the leaf at index  to the root of the tree; there are  such nodes. Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f3\">3</a> illustrates this update process for the example segment tree from Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f2\">2</a>; updating the entry at index 5 requires modifying only the shaded nodes along the path from the root to the updated entry.</p></li><li><p> To perform a range query, we descend through the tree while keeping track of the range covered by the current node.</p><ul><li><p> If the range of the current node is wholly contained within the query range, return the value of the current node.</p></li><li><p> If the range of the current node is disjoint from the query range, return 0.</p></li><li><p> Otherwise, recursively query both children and return the sum of the results.</p></li></ul></li></ul><p> Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f4\">4</a> illustrates the process of computing the sum of the range . Blue nodes are the ones we recurse through; green nodes are those whose range is wholly contained in the query range and are returned without recursing further; grey nodes are disjoint from the query range and return zero. The final result in this example is the sum of values at the green nodes,  (it is easily verified that this is in fact the sum of values in the range ).</p><section><div data-magellan-destination=\"f3\"><div><p> Updating a segment tree.</p></div></div></section><section><div data-magellan-destination=\"f4\"><div><p> Performing a range query on a segment tree.</p></div></div></section><p> On this small example tree, it may seem that we visit a significant fraction of the total nodes, but in general, we visit no more than about . Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f5\">5</a> makes this more clear. Only one blue node in the entire tree can have two blue children, and hence, each level of the tree can contain at most two blue nodes and two non-blue nodes. We essentially perform two binary searches, one to find each endpoint of the query range.</p><section><div data-magellan-destination=\"f5\"><div><p> Performing a range query on a larger segment tree.</p></div></div></section><p> Segment trees are a very nice solution to the problem: as we will see in Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s2\">2</a>, they fit well in a functional language; they also lend themselves to powerful generalizations such as lazily propagated range updates and persistent update history via shared immutable structure (<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref9\">Ivanov, 2011</a>).</p><p>, or  (Fenwick, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref5\">1994</a>; <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref8\">Ivanov, 2011</a>), are an alternative solution to the problem. What they lack in generality, they make up for with an extremely small memory footprint—they require literally nothing more than an array storing the values in the tree—and a blazing fast implementation. In other words, they are perfect for applications such as low-level coding/decoding routines where we don’t need any of the advanced features that segment trees offer, and want to squeeze out every last bit of performance.</p><p> Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f6\">6</a> shows a typical implementation of Fenwick trees in Java. As you can see, the implementation is incredibly concise and consists mostly of some small loops doing just a few arithmetic and bit operations per iteration. It is not at all clear what this code is doing, or how it works! Upon closer inspection, the , , and  functions are straightforward, but the other functions are a puzzle. We can see that both the  and  functions call another function , which for some reason performs a bitwise logical AND of an integer and its negation. In fact,  computes the  of , that is, it returns the smallest  such that the th bit of  is a one. However, it is not obvious how the implementation of  works, nor how and why least significant bits are being used to compute updates and prefix sums.</p><section><div data-magellan-destination=\"f6\"><div><p> Implementing Fenwick trees with bit tricks.</p></div></div></section><p> Our goal is  to write elegant functional code for this—already solved!—problem. Rather, our goal will be to use a functional domain-specific language for bit strings, along with equational reasoning, to  and  this baffling imperative code from first principles—a demonstration of the power of functional thinking and equational reasoning to understand code written even in other, non-functional languages. After developing more intuition for segment trees (Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s2\">2</a>), we will see how Fenwick trees can be viewed as a variant on segment trees (Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s3\">3</a>). We will then take a detour into two’s complement binary encoding, develop a suitable DSL for bit manipulations, and explain the implementation of the  function (Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s4\">4</a>). Armed with the DSL, we will then derive functions for converting back and forth between Fenwick trees and standard binary trees (Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s5\">5</a>). Finally, we will be able to derive functions for moving within a Fenwick tree by converting to binary tree indices, doing the obvious operations to effect the desired motion within the binary tree, and then converting back. Fusing away the conversions via equational reasoning will finally reveal the hidden LSB function, as expected (Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s6\">6</a>).</p></div><div data-magellan-destination=\"s2\"><p> Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f7\">7</a> exhibits a simple implementation of a segment tree in Haskell, using some utilities for working with index ranges shown in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f8\">8</a>. We store a segment tree as a recursive algebraic data type and implement  and  using code that directly corresponds to the recursive descriptions given in the previous section;  and  can then also be implemented in terms of them. It is not hard to generalize this code to work for segment trees storing values from either an arbitrary commutative monoid if we don’t need the  operation—or from an arbitrary Abelian group (i.e. commutative monoid with inverses) if we do need —but we keep things simple since the generalization doesn’t add anything to our story.</p><section><div data-magellan-destination=\"f7\"><div><p> Simple segment tree implementation in Haskell.</p></div></div></section><section></section><p> Although this implementation is simple and relatively straightforward to understand, compared to simply storing the sequence of values in an array, it incurs a good deal of overhead. We can be more clever in our use of space by storing all the nodes of a segment tree in an array, using the standard left-to-right breadth-first indexing scheme illustrated in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f9\">9</a> (for example, this scheme, or something like it, is commonly used to implement binary heaps). The root has label 1; every time we descend one level we append an extra bit: 0 when we descend to the left child and 1 when we descend to the right. Thus, the index of each node expressed in binary records the sequence of left-right choices along the path to that node from the root. Going from a node to its children is as simple as doing a left bit shift and optionally adding 1; going from a node to its parent is a right bit shift. This defines a bijection from the positive natural numbers to the nodes of an infinite binary tree. If we label the segment tree array with , then  stores the sum of all the ,  stores the sum of the first half of the ,  stores the sum of the second half, and so on.  themselves are stored as .</p><section><div data-magellan-destination=\"f9\"><div><p> Indexing a binary tree.</p></div></div></section><p> The important point is that since descending recursively through the tree corresponds to simple operations on indices, all the algorithms we have discussed can be straightforwardly transformed into code that works with a (mutable) array: for example, instead of storing a reference to the current subtree, we store an integer index; every time we want to descend to the left or right, we simply double the current index or double and add one, and so on. Working with tree nodes stored in an array presents an additional opportunity: rather than being forced to start at the root and recurse downwards, we can start at a particular index of interest and move  the tree instead.</p><p> So how do we get from segment trees to Fenwick trees? We start with an innocuous-seeming observation: <em>not all the values stored in a segment tree are necessary</em>. Of course, all the non-leaf nodes are “unnecessary” in the sense that they represent cached range sums which could easily be recomputed from the original sequence. That’s the whole point: caching these “redundant” sums trades off space for time, allowing us to perform arbitrary updates and range queries quickly, at the cost of doubling the required storage space.</p><p> But that’s not what I mean! In fact, there is a different set of values we can forget about, but in such a way that we still retain the logarithmic running time for updates and range queries. Which values, you ask? Simple: just forget the data stored in <em>every node which is a right child</em>. Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f10\">10</a> shows the same example tree we have been using, but with the data deleted from every right child. Note that “every right child” includes both leaves and internal nodes: we forget the data associated to  node which is the right child of its parent. We will refer to the nodes with discarded data as  and the remaining nodes (that is, left children and the root) as . We also say that a tree with all its right children inactivated in this way has been .</p><section><div data-magellan-destination=\"f10\"><div><p> Inactivating all right children in a segment tree.</p></div></div></section><p> Updating a thinned segment tree is easy: just update the same nodes as before, ignoring any updates to inactive nodes. But how do we answer range queries? It’s not too hard to see that there is enough information remaining to reconstruct the information that was discarded (you might like to try convincing yourself of this: can you deduce what values must go in the greyed-out nodes in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f10\">10</a>, without peeking at any previous figures?). However, in and of itself, this observation does not give us a nice algorithm for computing range sums.</p><p> It turns out the key is to think about . As we saw in the introduction and the implementation of  in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f6\">6</a>, if we can compute the prefix sum  for any , then we can compute the range sum  as .</p><div data-magellan-destination=\"the1\"><p> Given a thinned segment tree, the sum of any prefix of the original array (and hence also any range sum) can be computed, in logarithmic time, using only the values of active nodes.</p></div><div data-magellan-destination=\"prf1\"><p> Surprisingly, in the special case of prefix queries, the original range query algorithm described in Section <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#s1\">1</a> and implemented in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f7\">7</a> works unchanged! That is to say, the base case in which the range of the current node is wholly contained within the query range—and we thus return the value of the current node—will only ever happen at active nodes.</p></div><p> First, the root itself is active, and hence, querying the full range will work. Next, consider the case where we are at a node and recurse on both children. The left child is always active, so we only need to consider the case where we recurse to the right. It is impossible that the range of the right child will be wholly contained in the query range: since the query range is always a prefix of the form , if the right child’s range is wholly contained in  then the left child’s range must be as well—which means that the parent node’s range (which is the union of its children’s ranges) would also be wholly contained in the query range. But in that case we would simply return the parent’s value without recursing into the right child. Thus, when we do recurse into a right child, we might end up returning 0, or we might recurse further into both grandchildren, but in any case we will never try to look at the value of the right child itself.</p><p> Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f11\">11</a> illustrates performing a prefix query on a segment tree. Notice that visited right children are only ever blue or grey; the only green nodes are left children.</p><section><div data-magellan-destination=\"f11\"><div><p> Performing a prefix query on a segment tree.</p></div></div></section></div><div data-magellan-destination=\"s3\"><p> How should we actually store a thinned segment tree in memory? If we stare at Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f10\">10</a> again, one strategy suggests itself: simply take every active node and “slide” it down and to the right until it lands in an empty slot in the underlying array, as illustrated in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f12\">12</a>. This sets up a one-to-one correspondence between active nodes and indices in the range . Another way to understand this indexing scheme is to use a postorder traversal of the tree, skipping over inactive nodes and giving consecutive indices to active nodes encountered during the traversal. We can also visualize the result by drawing the tree in a “right-leaning” style (Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f13\">13</a>), vertically aligning each active node with the array slot where it is stored.</p><section><div data-magellan-destination=\"f12\"><div><p> Sliding active values down a thinned segment tree.</p></div></div></section><section><div data-magellan-destination=\"f13\"><div><p> Right-leaning drawing of a thinned segment tree, vertically aligning nodes with their storage location.</p></div></div></section><p> This method of storing the active nodes from a thinned segment tree in an array is precisely a . I will also sometimes refer to it as a , when I want to particularly emphasize the underlying array data structure. Although it is certainly a clever use of space, the big question is how to implement the update and range query operations. Our implementations of these operations for segment trees worked by recursively descending through the tree, either directly if the tree is stored as a recursive data structure, or using simple operations on indices if the tree is stored in an array. However, when storing the active nodes of a thinned tree in a Fenwick array, it is not  obvious what operations on array indices will correspond to moving around the tree. In order to attack this problem, we first take a detour through a domain-specific language for two’s complement binary values.</p></div><div data-magellan-destination=\"s4\"><p> The bit tricks usually employed to implement Fenwick trees rely on a  representation of binary numbers, which allow positive and negative numbers to be represented in a uniform way; for example, a value consisting of all 1 bits represents . We therefore turn now to developing a domain-specific language, embedded in Haskell, for manipulating two’s complement binary representations.</p><p> First, we define a type of bits, with functions for inversion, logical conjunction, and logical disjunction:</p><p> Next, we must define bit strings, i.e. sequences of bits. Rather than fix a specific bit width, it will be much more elegant to work with  bit strings.<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#fn3\"></a> It is tempting to use standard Haskell lists to represent potentially infinite bit strings, but this leads to a number of problems. For example, equality of infinite lists is not decidable, and there is no way in general to convert from an infinite list of bits back to an —how would we know when to stop? In fact, these practical problems stem from a more fundamental one: infinite lists of bits are actually a bad representation for two’s complement bit strings, because of “junk”, that is, infinite lists of bits which do not correspond to values in our intended semantic domain. For example,  [,] is an infinite list which alternates between  and  forever, but it does not represent a valid two’s complement encoding of an integer. Even worse are non-periodic lists, such as the one with  at every prime index and  everywhere else.</p><p> In fact, the bit strings we want are the  ones, that is, strings which eventually settle down to an infinite tail of all zeros (which represent nonnegative integers) or all ones (which represent negative integers). Every such string has a finite representation, so directly encoding eventually constant bit strings in Haskell not only gets rid of the junk but also leads to elegant, terminating algorithms for working with them.</p><p> represents an infinite sequence of bit , whereas  represents the bit string  followed by a final bit . We use , rather than , to match the way we usually write bit strings, with the least significant bit last. Note also the use of a  on the  field of ; this is to rule out infinite lists of bits using only , such as = () . In other words, the only way to make a non-bottom value of type  is to have a finite sequence of  finally terminated by .</p><p> Although we have eliminated junk values, one remaining problem is that there can be multiple distinct representations of the same value. For example,  ()  and  both represent the infinite bit string containing all zeros. However, we can solve this with a carefully constructed <em>bidirectional pattern synonym</em> (Pickering ., <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref10\">2016</a>).</p><p> Matching with the pattern  uses a  (Erwig &amp; Jones, <a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#ref4\">2001</a>) to potentially expand a  one step into a , so that we can pretend  values are always constructed with . Conversely, constructing a  with  will do nothing if we happen to snoc an identical bit  onto an existing . This ensures that as long as we stick to using  and never directly use ,  values will always be  so that the terminal  is immediately followed by a different bit. Finally, we mark the pattern  as  on its own, since matching on  is indeed sufficient to handle every possible input of type . However, in order to obtain terminating algorithms we will often include one or more special cases for .</p><p> Let’s begin with some functions for converting  to and from  and for displaying  (intended only for testing).</p><p> We can now begin implementing some basic operations on . First, incrementing and decrementing can be implemented recursively as follows:</p><p> The , or LSB, of a sequence of bits can be defined as follows:</p><p> Note that we add a special case for  to ensure that  is total. Technically,  does not have a least significant bit, so defining  seems sensible.</p><p> Bitwise logical conjunction can be defined straightforwardly. Note that we only need two cases; if the finite parts of the inputs have different lengths, matching with  will automatically expand the shorter one to match the longer one.</p><p> Bitwise inversion is likewise straightforward.</p><p> The above functions follow familiar patterns. We could easily generalize to eventually constant streams over an arbitrary element type and then implement <img src=\"data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20250116174732089-0720:S0956796824000169:S0956796824000169_inline9001.png?pub-status=live\" width=\"26\" height=\"16\" data-original-image=\"/binary/version/id/urn:cambridge.org:id:binary:20250116174732089-0720:S0956796824000169:S0956796824000169_inline9001.png\" data-zoomable=\"false\"> in terms of a generic  and  in terms of . However, for the present purpose we do not need the extra generality.</p><p> We implement addition with the usual carry-propagation algorithm, along with some special cases for .</p><p> It is not too hard to convince ourselves that this definition of addition is terminating and yields correct results; but we can also be fairly confident by just trying it with QuickCheck:</p><p> Finally, the following definition of negation is probably familiar to anyone who has studied two’s complement arithmetic; I leave it as an exercise for the interested reader to prove that  for all .</p><p> We now have the tools to resolve the first mystery of the Fenwick tree implementation.</p><p> For the last equality, we need a lemma that <img src=\"data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20250116174732089-0720:S0956796824000169:S0956796824000169_inline9004.png?pub-status=live\" width=\"123\" height=\"16\" data-original-image=\"/binary/version/id/urn:cambridge.org:id:binary:20250116174732089-0720:S0956796824000169:S0956796824000169_inline9004.png\" data-zoomable=\"false\">, which should be intuitively clear and can easily be proved by induction as well.</p><p> Finally, in order to express the index conversion functions we will develop in the next section, we need a few more things in our DSL. First, some functions to set and clear individual bits and to test whether particular bits are set:</p><p> The only other things we will need are left and right shift, and a generic  combinator that iterates a given function, returning the first iterate for which a predicate is false.</p></div><div data-magellan-destination=\"s6\"><h2> Deriving Fenwick operations</h2><p> We can now finally derive the required operations on Fenwick array indices for moving through the tree, by starting with operations on a binary indexed tree and conjugating by conversion to and from Fenwick indices. First, in order to fuse away the resulting conversion, we will need a few lemmas.</p><div data-magellan-destination=\"lem6_1\"><p> (shr-inc-dec). For all  which are odd (that is, end with I),</p><ul></ul></div><div data-magellan-destination=\"prf2\"><p> Both are immediate by definition.</p></div><div data-magellan-destination=\"lem6_2\"><p> (while-inc-dec). <em>The following both hold for all Bits values:</em></p><ul></ul></div><p> Easy proof by induction on . For example, for the  case, the functions on both sides discard consecutive 1 bits and then flip the first 0 bit to a 1.</p><p> Finally, we will need a lemma about shifting zero bits in and out of the right side of a value.</p><div data-magellan-destination=\"prf3\"><p> Intuitively, this says that if we first shift out all the zero bits and then left shift until bit  is set, we could get the same result by forgetting about the right shifts entirely; shifting out zero bits and then shifting them back in should be the identity.</p></div><p> Formally, the proof is by induction on . If  is odd, the equality is immediate since . Otherwise, if , on the left-hand side the  is immediately discarded by , whereas on the right-hand side , and the extra  can be absorbed into the  since . What remains is simply the induction hypothesis.</p><p> With these lemmas under our belt, let’s see how to move around a Fenwick array in order to implement  and ; we’ll begin with . When implementing the  operation, we need to start at a leaf and follow the path up to the root, updating all the active nodes along the way. In fact, for any given leaf, its closest active parent is precisely the node stored in the slot that used to correspond to that leaf (see Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f13\">13</a>). So to update index , we just need to start at index  in the Fenwick array, and then repeatedly find the closest active parent, updating as we go. Recall that the imperative code for  works this way, apparently finding the closest active parent at each step by adding the LSB of the current index:</p><p> Let’s see how to derive this behavior.</p><p> To find the closest active parent of a node under a binary indexing scheme, we first move up to the immediate parent (by dividing the index by two, i.e. performing a right bit shift); then continue moving up to the next immediate parent as long as the current node is a right child (i.e. has an odd index). This yields the definition:</p><p> This is why we used the slightly strange indexing scheme with the root having index 2—otherwise this definition would not work for any node whose active parent is the root!</p><p> Now, to derive the corresponding operation on Fenwick indices, we conjugate by conversion to and from Fenwick indices and compute as follows. To make the computation easier to read, the portion being rewritten is underlined at each step.</p><p> In the final step, since the input  satisfies , we have , so Lemma 6.3 applies.</p><p> Reading from right to left, the pipeline we have just computed performs the following steps:</p><p> 2. Shift out consecutive zeros until finding the least significant 1 bit</p><p> 4. Shift zeros back in to bring the most significant bit back to position , then clear it.</p><p> Intuitively, this does look a lot like adding the LSB! In general, to find the LSB, one must shift through consecutive 0 bits until finding the first 1; the question is how to keep track of how many 0 bits were shifted on the way. The  function itself keeps track via the recursion stack; after finding the first 1 bit, the recursion stack unwinds and re-snocs all the 0 bits recursed through on the way. The above pipeline represents an alternative approach: set bit  as a “sentinel” to keep track of how much we have shifted; right shift until the first 1 is literally in the ones place, at which point we increment; and then shift all the 0 bits back in by doing left shifts until the sentinel bit gets back to the  place. One example of this process is illustrated in Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f19\">19</a>. Of course, this only works for values that are sufficiently small that the sentinel bit will not be disturbed throughout the operation.</p><section><div data-magellan-destination=\"f19\"><div><p> Adding LSB with a sentinel bit + shifts.</p></div></div></section><p> To make this more formal, we begin by defining a helper function , which does an operation “at the LSB”, that is, it shifts out 0 bits until finding a 1, applies the given function, then restores the 0 bits.</p><div data-magellan-destination=\"prf4\"><p> Straightforward induction on .</p></div><p> We can formally relate the “shifting with a sentinel” scheme to the use of , with the following (admittedly rather technical) lemma:</p><p> The proof is rather tedious and not all that illuminating, so we omit it</p><p> (an extended version including a full proof may be found on the author’s website, at <a href=\"http://ozark.hendrix.edu/\">http://ozark.hendrix.edu/</a> yorgey/pub/Fenwick-ext.pdf). However, we do note that both  and  fit the criteria for : incrementing or decrementing some  cannot affect the st bit as long as , and the result of incrementing or decrementing a number less than  will be a number less than . We can now put all the pieces together show that adding the LSB at each step is the correct way to implement .</p><p> We can carry out a similar process to derive an implementation for prefix query (which supposedly involves  the LSB). Again, if we want to compute the sum of [1, ], we can start at index  in the Fenwick array, which stores the sum of the unique segment ending at . If the node at index  stores the segment [,], we next need to find the unique node storing a segment that ends at . We can do this repeatedly, adding up segments as we go.</p><p> Staring at Figure&nbsp;<a href=\"https://www.cambridge.org/core/journals/journal-of-functional-programming/article/you-could-have-invented-fenwick-trees/B4628279D4E54229CED97249E96F721D#f20\">20</a> for inspiration, we can see that what we want to do is find the  of our , that is, we go up until finding the first ancestor which is a right child, then go to its left sibling. Under a binary indexing scheme, this can be implemented simply as:</p><section><div data-magellan-destination=\"f20\"><div><p> Moving up a segment tree to find successive prefix segments.</p></div></div></section><div data-magellan-destination=\"the6_7\"><p> Subtracting the LSB is the correct way to move up a Fenwick-indexed tree to the active node covering the segment previous to the current one, that is, </p><p> everywhere on the range .</p></div></div><div data-magellan-destination=\"s7\"><p> Historically, to my knowledge, Fenwick trees were not actually developed as an optimization of segment trees as presented here. This has merely been a fictional—but hopefully illuminating—alternate history of ideas, highlighting the power of functional thinking, domain-specific languages, and equational reasoning to explore relationships between different structures and algorithms. As future work, it would be interesting to explore some of the mentioned generalizations of segment trees, to see whether one can derive Fenwick-like structures that support additional operations.</p></div>","contentLength":29142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42818248"},{"title":"Any way to have live reload and bebugger work together inside a docker container ?","url":"https://www.reddit.com/r/golang/comments/1i99g1o/any_way_to_have_live_reload_and_bebugger_work/","date":1737761881,"author":"/u/TryallAllombria","guid":564,"unread":true,"content":"<p>Hey, I've been trying to make Delve and Air working together. I'm from the NodeJS world and i'm used to hit save to have my server reloading, but also keep my debugger alive during the process. It's a nice dev workflow and I was wondering how I could find the same workflow using golang ?</p><p>I tried numerous configuration, either delve is stopping my server from starting until I attach my debug process. Or it won't reload either my server or the delve debugger properly if I pass the --continue flag.</p><p>How are you working with live reload and debugger with golang ? Do you use a more manual approach by reloading your app yourself ? Or start a debug server when required ?</p>","contentLength":669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nothing but NATS - Going Beyond Cloud Native - Byron Ruth & Kevin Hoffman, Synadia","url":"https://www.youtube.com/watch?v=ypsAus_OBmo","date":1737761567,"author":"CNCF [Cloud Native Computing Foundation]","guid":368,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNothing but NATS - Going Beyond Cloud Native - Byron Ruth &amp; Kevin Hoffman, Synadia\n\nThese days building so-called cloud-native apps involves assembling a custom stack of tools 10x bigger than the app we're building. Additionally, applications increasingly need to expand out to the edge and cloud-native stacks simply don't work in those environments. Fortunately with NATS, we don't need a stack. In this session you'll see how we can leverage compute, storage, and connectivity to build cloud-to-edge native apps more powerful than ever, with less code, effort, and frustration.</article>","contentLength":899,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ypsAus_OBmo?version=3","enclosureMime":"","commentsUrl":null},{"title":"AI Coding Is Based on a Faulty Premise","url":"https://articles.pragdave.me/p/ai-coding-is-based-on-a-faulty-premise?r=2rvraz","date":1737758338,"author":"/u/ThatArrowsmith","guid":584,"unread":true,"content":"<p>I am glad to  be living in a time where AI is becoming mainstream. When I write code, I often use AI to complete code and to lookup the way libraries and tools can be used.</p><p>At the same time, I am increasingly distressed by the race to replace human developers, particularly the more junior ones, with AI assistants.</p><p>There is good reason to believe that this is a regressive trend, even in the short-to-medium term. That reason is linked to the history of software development, the software crisis of 1990s, and the movement towards a more agile style of development.</p><p><a href=\"https://en.wikipedia.org/wiki/NATO_Software_Engineering_Conferences\" rel=\"\"> two conferences</a></p><p>During the 1970s, academics (and some larger companies) tried to refine what this phrase meant. </p><p>There was clearly a large chasm between what a customer wanted and the delivery of software to satisfy them. So software engineering came to mean a process of narrowing the gap by dividing it into lots of phases, where the conceptual gap between each phase was smaller (and therefore more tractable) than a single large gap.</p><p><a href=\"https://www.praxisframework.org/files/royce1970.pdf\" rel=\"\">Managing the Development of Large Software Systems</a></p><p>It shows the way many people felt that software should be developed; a set of steps, where the output of each is a refinement of its input. Each step would produce an increasing amount of specification, until finally we reached coding, testing, and delivery.</p><p>This diagram defined a generation of software development practices. Unfortunately, its advocates didn’t bother to read the paper past this nice simple picture. Royce points out that each step is likely to find errors in the preceding step, and so the diagram should look like this:</p><p>Then he points out that the reality is likely to be more anarchic. Errors found towards the end of the process may well have been caused by problems many steps back.</p><p>His solution to this was regrettable: increase the level of detail of the specifications passed between steps. By a lot. Fortunately, because no one bothered to read the whole paper, we were spared that additional process insult.</p><p><a href=\"https://www.researchgate.net/publication/263849222_The_Chaos_Report\" rel=\"\">Chaos Report</a></p><p>In retrospect, the reason was pretty damn obvious:</p><p><a href=\"https://en.wikipedia.org/wiki/Telephone_game\" rel=\"\">telephone</a></p><p>Teams in the 1990s knew intuitively that this kind of snowballing cascade of small errors was unsustainable, so they subverted the project development structures imposed on them. They chose to ignore things that were patently incorrect. They talked to each other, and adjusted what they did in order to produce bits of code that would actually work together. And they got better and better at padding estimates.</p><p>The Manifesto for Agile Software Development, created in 2001, explicitly acknowledged that specifications at all levels were suspect, and that the only true measure of a project is the value it produces. It insisted that people should be in the loop, applying their intuition and skill to take small steps, and then using feedback to asses how successful they’d been. Each step was like a mini-waterfall, tacitly running from requirements to tests in a matter on minutes, often publishing code to the end users many times a day.</p><p>(I’m resisting the temptation here to rant about the current state of “agile.” That’ll be another article.)</p><p>But that’s not what I’m seeing. Companies are jumping on AI as a way of removing those messy (and expensive) humans from the process of developing software.</p><p>Right now (early 2025), we’re starting see a move from AI as coding assistant to AI as program writer. Nonprogrammers are working with Claude and ChatGPT to create simple programs. Companies are experimenting more and more with content produced largely by AI. </p><p>And the problem with that is the same as the problem with the poorly-applied waterfall approach: people don’t know what they want. They’ll ask AI for a solution to their perceived need, and then run what they are given, often without understanding what it does. This applies equally to end users and (amazingly) to developers.</p><p>So the AI produces code based on at best a partial and at worst an inaccurate description, and then that code becomes part of a larger system, integrating with other chunks of imaginary code.</p><p>That’s OK, people say. We’ll have the AI write tests, too.</p><p>Don’t get me wrong. AI is a world-changing tool.</p><p>But good software developers have already changed the world beyond recognition. And they’ve done that by taking uncertain, inaccurate ideas and using their experience, intuition, and communications skills to hone them into something that changes people’s lives.</p><p>Remove these people from the equation, and will be back in the 1990s, in a world full of poor software and unmet needs.</p><p>Or am I just being a Luddite? Let’s discuss below.</p>","contentLength":4605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i984yn/ai_coding_is_based_on_a_faulty_premise/"},{"title":"Show HN: Lightpanda, an open-source headless browser in Zig","url":"https://github.com/lightpanda-io/browser","date":1737756932,"author":"fbouvier","guid":186,"unread":true,"content":"<p>We’re Francis and Pierre, and we're excited to share Lightpanda (<a href=\"https://lightpanda.io\" rel=\"nofollow\">https://lightpanda.io</a>), an open-source headless browser we’ve been building for the past 2 years from scratch in Zig (not dependent on Chromium or Firefox). It’s a faster and lighter alternative for headless operations without any graphical rendering.</p><p>Why start over? We’ve worked a lot with Chrome headless at our previous company, scraping millions of web pages per day. While it’s powerful, it’s also heavy on CPU and memory usage. For scraping at scale, building AI agents, or automating websites, the overheads are high. So we asked ourselves: what if we built a browser that only did what’s absolutely necessary for headless automation?</p><p>Our browser is made of the following main components:</p><p>- an HTML parser and DOM tree (based on Netsurf libs)</p><p>- a Javascript runtime (v8)</p><p>- partial web APIs support (currently DOM and XHR/Fetch)</p><p>- and a CDP (Chrome Debug Protocol) server to allow plug &amp; play connection with existing scripts (Puppeteer, Playwright, etc).</p><p>The main idea is to avoid any graphical rendering and just work with data manipulation, which in our experience covers a wide range of headless use cases (excluding some, like screenshot generation).</p><p>In our current test case Lightpanda is roughly 10x faster than Chrome headless while using 10x less memory.</p><p>It's a work in progress, there are hundreds of Web APIs, and for now we just support some of them. It's a beta version, so expect most websites to fail or crash. The plan is to increase coverage over time.</p><p>We chose Zig for its seamless integration with C libs and its  feature that allow us to generate bi-directional Native to JS APIs (see our zig-js-runtime lib <a href=\"https://github.com/lightpanda-io/zig-js-runtime\">https://github.com/lightpanda-io/zig-js-runtime</a>). And of course for its performance :)</p><p>As a company, our business model is based on a Managed Cloud, browser as a service. Currently, this is primarily powered by Chrome, but as we integrate more web APIs it will gradually transition to Lightpanda.</p><p>We would love to hear your thoughts and feedback. Where should we focus our efforts next to support your use cases?</p>","contentLength":2114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42817439"},{"title":"Show HN: Onit – open-source ChatGPT Desktop with local mode, Claude, Gemini","url":"https://github.com/synth-inc/onit","date":1737756916,"author":"telenardo","guid":185,"unread":true,"content":"<p>Hey Hackernews- it’s Tim Lenardo and I’m launching v1 of Onit today!</p><p>Onit is ChatGPT Desktop, but with local mode and support for other model providers (Anthropic, GoogleAI, etc). It's also like Cursor Chat, but everywhere on your computer - not just in your IDE!</p><p>Onit is open-source! You can download a pre-built version from our website: \nwww.getonit.ai</p><p>We built this because we believe:\nUniversal Access: AI assistants should be accessible from anywhere on my computer, not just in the browser or in specific apps\nProvider Freedom: Consumers should have the choice between providers (anthropic, openAI, etc.) not be locked into a single one (ChatGPT desktop only has OpenAI models)\nLocal first: AI is more useful with access to your data. But that doesn't count for much if you have to upload personal files to an untrusted server. Onit will always provide options for local processing. No personal data leaves your computer without approval\nCustomizability: Onit is your assistant. You should be able to configure it to your liking\nExtensibility: Onit should allow the community to build and share extensions, making it more useful for everyone.</p><p>The features for V1 include:\n Local mode - chat with any model running locally on Ollama! No internet connection required\n Multi-provider support - Top models for OpenAI, Anthropic, xAI, and GoogleAI\n File upload - add images or files for context (bonus: Drag &amp; drop works too!)\n History - revisit prior chats through the history view or with a simple up/down arrow shortcut\n Customizable Shortcut - you pick your hotkey to launch the chat window. (Command+zero by default)</p><p>What data are you collecting?\nOnit V1 does not have a server. Local requests are handled locally, and remote requests are sent to model providers directly from the client. We collect crash reports through Firebase and a single \"chat sent\" event through PostHog analytics. We don't store your prompts or responses.</p><p>How to does Onit support local mode? \nFor use local mode, run Ollama. You can get Ollama here: <a href=\"https://ollama.com/\">https://ollama.com/</a> \nOnit gets a list of your local models through Ollama’s API.</p><p>Which models do you support? \nFor remote models, Onit V1 supports Anthropic, OpenAI, xAI and GoogleAI. Default models include (o1, o1-mini, GPT-4o, Claude3.5 Sonnet, Claude3.5 Haiku, Gemini 2.0, Grok 2, Grok 2 Vision).\nFor local mode, Onit supports any models you can run locally on Ollama!</p><p>What license is Onit under? \nWe’re releasing V1 available on a Creative Commons Non-Commercial license. We believe the transparency of open-source is critical. We also want to make sure individuals can customize Onit to their needs (please submit PRs!). However, we don’t want people to sell the code as their own.</p><p>Where is the monetization?\nWe’re not monetizing V1. In the future we may add paid premium features. Local chat will- of course- always remain free. If you disagree with a monetized feature, you can always build from source!</p><p>Why not Linux or Windows? \nGotta start somewhere! If the reception is positive, we’ll work hard to add further support.</p><p>Who are we? \nWe are Synth, Inc, a small team of developers in San Francisco building at the frontier of AI progress. Other projects include Checkbin (www.checkbin.dev) and Alias (deprecated - www.alias.inc).</p><p>We’d love to hear from you! Feel free to reach out at contact@getonit dot ai.</p><p>Future roadmap includes:\n Autocontext - automatically pull context from computer, rather than having to repeatedly upload. \n Local-RAG - let users index and create context from their files without uploading anything. \n Local-typeahead - i.e. Cursor Tab but for everywhere\n Additional support - add Linux/Windows, Mistral/Deepseek etc etc.\n (maybe) Bundle Ollama to avoid double-download\n And lot’s more!</p>","contentLength":3758,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42817438"},{"title":"Trying out Zed after more than a decade of Vim/Neovim","url":"https://sgoel.dev/posts/trying-out-zed-after-more-than-a-decade-of-vim-neovim/","date":1737755580,"author":"siddhant","guid":199,"unread":true,"content":"<p>I'm currently drafting this blog post in <a rel=\"noopener\" target=\"_blank\" href=\"https://zed.dev\">Zed</a>. After using Vim/Neovim for more than 15\nyears, I recently decided to try out something new. I don't know if this little\nexperiment will work, or if I'll run back to my trusty Neovim, but hey, what I do know\nis that I'd like to find out.</p><p>Why though? If you've used a specific tool for that long, why (try to) switch to\nsomething else anyway? Everyone has their own reasons. I have two.</p><h3>1. The desire to use something that just works</h3><p>Lately, I've been drifting towards things that \"just work\".</p><p>I love (Neo)Vim. Like I wrote earlier, it's been my primary editor for the past 15\nyears. What I don't love is all the configuration that goes into it before I can use it\nto start writing code.</p><p>When I first started using Vim (in 2009!), my configuration was a tiny  file\nthat was handed over to me by my internship supervisor at that time. I had no idea what\nit did, but it worked. Over time, as I felt the need to customize things, my \nstarted accumulating increasingly more code copied from StackOverflow that I didn't\nunderstand.</p><p>At some point Neovim came out, and with it, the ability to configure things using Lua.\nThis was a huge step up, because I could at least understand what I was configuring.</p><p>Either way, it's still configuration. What changed was just  that configuration was\ndone. The combination of configuring the base editor and installing a set of plugins\n(and making sure that they play nice with each other) isn't really something that I\nwould like to spend my time on, going forward.</p><p>One recent example that highlights this problem: my workflow consists of switching\nback and forth between a terminal window and an editor window. At any time, I have\nmultiple projects open in both. On macOS, it's been difficult to find a solution that\n\"just works\". The most popular Neovim GUI clients on macOS include <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/qvacua/vimr\">Vimr</a> and <a rel=\"noopener\" target=\"_blank\" href=\"https://neovide.dev/\">Neovide</a>,\nwhich are both excellent projects. Neovide though, <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/neovide/neovide/issues/1332\">does not support multiple windows</a>,\nwhich is integral to my workflow and hence requires me to implement <a href=\"https://sgoel.dev/posts/switching-between-projects-in-neovim/\">workarounds</a>, which\nactually broke last week after I ran . And while Vimr does support\nmultiple windows, it is <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/qvacua/vimr/issues/887\">not able to render icons in nvim-tree</a>, which is less than\nideal. 🤷🏻‍♂️</p><h3>2. Deeper and native LLM integration</h3><p>LLMs are happening, whether we like it or not. And no, the fact that they aren't 100%\ncorrect all the time is not a reason to discard them entirely. I understand why some\npeople don't want to use them. And I respect that. I personally find them useful and\nwould like to integrate them more into my daily workflow.</p><p>The Neovim ecosystem has a bunch of plugins for using LLMs. But as I wrote in the\nprevious section, I'd like to avoid plugins when possible. Installing plugins means\nupdating them, which inevitably breaks things.</p><p>One example where I find LLMs useful when coding is handling boilerplate stuff. In my\ncurrent Neovim setup, I have the the <a rel=\"noopener\" target=\"_blank\" href=\"https://github.com/Exafunction/codeium.nvim\">Codeium extension</a> installed, which offers code\nsuggestions that are like an autocomplete on steroids. Often, I can write a function\nname and what parameters it's expecting, and the LLM writes out the function for me.\nThis is particularly helpful when writing small functions or test cases.</p><p>I'd like to be able to do more of such things in my editor. I'm not entirely sold on the\nidea of agentic editors, as I'd like to keep at least  agency over the code that's\ngoing in, but that's a different topic.</p><p>Another example of where I find LLMs useful: I'm not a native English speaker and often\nstruggle to find the right sentence framing that is both concise and catchy. LLMs are\nexcellent at this! I can scribble my initial thoughts, throw it at an LLM and ask it to\nreframe the whole thing, which is great!</p><p>Generally, it feels like the way we're writing code is changing with the introduction of\nthis extremely powerful tool, and I'd like to move with the times.</p><p>So yeah, those are the two reasons why I've been looking to try out something else\nlately. What made me go for Zed?</p><p>Yep, that's literally the first reason. After using Vim for 15 years, my fingers have\nbuilt up enough muscle memory that not using Vim keybindings when writing code is\ndownright impossible.</p><p>Zed's <a rel=\"noopener\" target=\"_blank\" href=\"https://zed.dev/docs/vim\">Vim mode</a> is surprisingly solid! So far, I feel right at home. Almost all Vim\nkeybindings that I'm used to work just as expected. The one or two bindings that don't\nwork, are something that I can make my peace with and retrain my fingers on. Everything\nelse works excellent. It also looks like they're <a rel=\"noopener\" target=\"_blank\" href=\"https://zed.dev/blog/vim-2025\">doubling down on their Vim mode</a>\nsupport in 2025, which is a great sign!</p><p>When you fire up Zed, the editor is fully functional without having to write a single\nline of configuration. That's awesome. The editor may prompt you every now and then to\ninstall support for specific languages. But that's usually just one button click.</p><p>Lua is great, but one JSON file is even better. Zed uses JSON as the configuration\nsyntax and so far it feels a lot simpler than what I've worked with so far.</p><p>I had no idea I needed this until I had it!</p><p>While editing the Zed configuration file, I found it really helpful how the editor\nsuggests configuration keys and their potential values. This feature felt really\nthoughtful when I first saw it and I could imagine that it goes a long way in getting\nnew users up to speed quicker.</p><h3>5. Native LLM integration</h3><p>I know some people have been turned off by this, but I actually like this feature.</p><p>Zed has a feature called Assistant, which is a tool to, well, assist you, using a large\nlanguage model of your choice. A few popular LLM providers are supported. Once set up,\nyou can open up the Assistant panel to work together with the language model you've\nchosen. For instance, it's pretty easy to type in questions in the panel window and then\npaste text from your open file buffers into it to give the LLM more context. There's a\nlot more to this feature that I haven't used yet, but I'll probably get to it the more I\nuse it.</p><p>Overall, the integration feels very native and very useful.</p><p>Last but definitely not the least is speed! Zed is  fast. Everything feels very\nsnappy, and it's clear that the team has put a lot of effort into making everything\nfast.</p><p>It's only been a day since I really started using Zed instead of Neovim as my daily\ndriver. And so far the experience has been quite nice.</p><p>Like I wrote earlier, this is an experiment. We'll see how things turn out. That being\nsaid though, the first impression looks  good.</p>","contentLength":6413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42817277"},{"title":"Success with errors in Go: stack traces and metadata","url":"https://blog.gregweber.info/blog/go-errors-library/","date":1737752357,"author":"/u/gregwebs","guid":565,"unread":true,"content":"<p>There are several patterns for dealing with errors I encounter almost universally on Go projects:</p><ul><li>adding metadata to existing errors</li></ul><p>The first two can be handled with the help of a light-weight error library. There are a lot of error libraries available that do mostly the same things.\nI maintain <a href=\"https://github.com/gregwebs/errors\">github.com/gregwebs/errors</a>.</p><p>A Panic in Go produces a stack trace, but an error does not.</p><p>Adding a stack trace to your go code happens automatically with an error library that supports stack traces. If you use error creation functions:</p><pre data-lang=\"go\"><code data-lang=\"go\"></code></pre><p>These will create an error with a stack trace.\nWrapping functions (see next sections) will automatically add a stack trace as well.</p><p>I am told errors don't have traces because that this would have a negative performance impact.\nHowever, in almost all of my usage of Go, when an error is returned performance is no longer a concern.\nSometimes when errors can alter performance that indicates an error is being returned for what is a normal condition rather than an error condition.\nAn example of this is read APIs returning EOF.\nCertainly there are some cases where performance needs to be optimized on an error path. It's worth noting that the Zig language has figured out how to further <a href=\"https://ziglang.org/documentation/master/#Error-Return-Traces\">minimize the impact of collecting error traces</a> and allows them to be turned off in release builds.\nBelow we also see an example where a stack trace would not be helpful.\nIn these cases, one can still use standard APIs that don't add stack traces:</p><pre data-lang=\"go\"><code data-lang=\"go\"></code></pre><p>Additionally, the <a href=\"https://github.com/gregwebs/errors\">github.com/gregwebs/errors</a> library provides a 'WrapNoStack' function that can be used to wrap an error without adding a stack trace.</p><p>The standard way of adding metadata to Go errors is to wrap them in format strings:</p><pre data-lang=\"go\"><code data-lang=\"go\"></code></pre><p>I think that adding a new formatting verb for errors was a pragmatic choice to avoid dependency issues and match what users were currently doing, but I think the API is cryptic and limited compared to the library approach of:</p><pre data-lang=\"go\"><code data-lang=\"go\"></code></pre><p>There is also a  function for using formatting strings. However, as I have shifted towards structured logging I want all my metadata structured, including for errors.\nThe errors library features an slog compatible API for adding metadata called  where \"s\" stands for \"structured\".</p><pre data-lang=\"go\"><code data-lang=\"go\"></code></pre><p>A common pattern supported by the library is to accumulate attributes that can be used both with slog and for annotating errors:</p><pre data-lang=\"go\"><code data-lang=\"go\"></code></pre><p>With a stack trace and relevant metadata annotating an error, I can frequently open up a bug report just from seeing the error report without having to dig into logs. When I do need to dig into the logs, having metadata on the error helps greatly with tracking things down. If you have a request id/trace id you can attach that to the error to help find it in the logs.</p><p>Future maintainers of your code base will thank you when they can quickly track down errors.</p><p>Future posts will discuss approaches to:</p>","contentLength":2826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1i95udo/success_with_errors_in_go_stack_traces_and/"},{"title":"Best rootless kubernetes distribution for production or production-scale demo?","url":"https://www.reddit.com/r/kubernetes/comments/1i959yv/best_rootless_kubernetes_distribution_for/","date":1737750898,"author":"/u/BosonCollider","guid":571,"unread":true,"content":"<p>I'm in an environment where machines not earmarked for production may be extremely locked down with no ability to install packages globally, and rootless podman as the only preinstalled container runtime.</p><p>What's the way to go here? I normally like k3s and Talos. The options I see are:</p><p>* rootless k3d (doubly experimental) * kind * usernetes v2</p><p>Does anyone have experience with these? My main requirement is to easily be able to helm install operators and use hostpath volumes for proof of concept deployments with minimal friction.</p>","contentLength":529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MediaTek improvements in Linux 6.13","url":"https://www.collabora.com/news-and-blog/news-and-events/mediatek-improvements-in-linux-613.html","date":1737749806,"author":"/u/mfilion","guid":575,"unread":true,"content":"<p>Made available earlier this week, Linux 6.13 brought numerous contributions from our engineering team. Collabora's deep involvement with the MediaTek community also continued to shine, with multiple improvements for MediaTek SoCs landing with this release.</p><p>AngeloGioacchino Del Regno introduced support to communicate with the Dynamic Voltage and Frequency Scaling Resource Collector (DVFSRC) hardware in MediaTek SoCs. As a result, the previously upstreamed DVFSRC Regulator and External Memory Interface (EMI) Interconnect drivers can now be enabled. This allows the System Companion Processor (SCP) Core Voltage and the EMI Bus Bandwidth to be scaled, improving the power efficiency of MediaTek-powered machines.</p><p>Moreover, during a round of testing, Angelo also found that the MMC/SD Controller of MediaTek SoCs doesn't always embed Command Queue Hardware (CQHCI) on all SoCs or even on all instances of the said controller on the same SoCs. For example, on MT8186, MT8192, MT8195 (and others), the eMMC instance of this controller does support CQE by hardware but the (Micro)SD one does not: this brings numerous performance hits, sometimes also shown as micro-stuttering in UX, because of a necessary spinlock at every read, or write, operation.</p><p>In order to overcome this, he implemented support in the MediaTek MMC/SD Controller driver (mtk-sd) for a software-based alternative for controller instances that don't support CQHCI, called Host Software Queue (HSQ): by doing so, MediaTek's driver acquires spinlocks less frequently, reducing expensive context switching, since it joins the sending of multiple commands into one single operation.</p><p>His tests with this patch, which he ran on a MT8195 Tomato Chromebook with a SanDisk Extreme Pro (A2 rated) MicroSD card, show decreased average access times (with a reduction of up to 150 milliseconds during intensive read/write cycles). This is an improvement of +50.5% IOPS and bandwidth during random 4k reads/writes, +24.28% during random 4k reads, and +3.14% in sequential reads. This has greatly reduced UX micro-stuttering, making them mostly unnoticeable during read/write-intensive workloads, such as opening the Chromium or Firefox browsers on GNOME.</p><p>Another notable contribution from Angelo includes the introduction of the support for OF Graphs in the MediaTek Display Controller (mediatek-drm) driver. The display IP cores in MediaTek SoCs allow a good amount of flexibility, supporting being interconnected with various different IP cores (for example, the Read DMA IP can be connected to either the Color Management, DisplayPort, DSI, Merge, or another IP) to form a full Display Data Path (DDP) that ends with an actual display. As the Display Data Path for a machine (Chromebook/Laptop, Smartphone, SBCs, etc) depends on the physically available display output(s), this cannot be hardcoded as a SoC-global parameter and is machine-specific.</p><p>His change removes the need to hardcode display paths into the driver for each machine, allowing them to be specified in the Device Tree of the target machine with an OF Graph. This avoids unnecessary growth of the mediatek-drm driver and reflects the configuration flexibility of the MediaTek Display Controller into devicetree files.</p><p>More contributions from Angelo include various other cleanups and improvements. He modernized the MediaTek RTC driver to use the RTC-subsystem provided APIs, reducing both binary and code size. He also added support for restricting Link Speed (Gen) and Link Width (Lanes) in the MediaTek PCI-Express Gen3 Controller driver.</p><p>Stay tuned for more MediaTek updates, as well as a complete recap of our contributions to Linux 6.13.</p>","contentLength":3661,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1i94uul/mediatek_improvements_in_linux_613/"},{"title":"Iroh: p2p chat, in rust, from scratch","url":"https://youtu.be/ogN_mBkWu7o","date":1737749386,"author":"/u/diogocsvalerio","guid":592,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i94oxv/iroh_p2p_chat_in_rust_from_scratch/"},{"title":"A very Chicago gamble","url":"https://www.bitsaboutmoney.com/archive/chicago-casino-investment-offering/","date":1737748591,"author":"gregorymichael","guid":198,"unread":true,"content":"<p>This column doesn’t offer investment advice, as I am not a registered investment advisor. This is not merely a mandatory disclaimer; this is a warning. We will discuss some specific securities below that I am not  of recommending.</p><p>Finance performs a strange alchemy, teleporting value through time and space. Ordinarily, Bits about Money focuses more on the plumbing of it than the deals. But a deal enthusiast who goes by <a href=\"https://www.patreon.com/theconservativeincomeinvestor\">The Conservative Income Investor</a> recently flagged a capital raise to me. It has everything: echoes of the culture that is the American PMC 2020-2024, complex financial structuring, a novel web application to move money, a crypto company in the background, and municipal politics. So it seems squarely within this column’s beat.</p><p>The municipality happens to be Chicago, my hometown and (after a 20 year stint in Japan) current residence. And so I feel some sense of civic duty, as a Chicagoan, taxpayer, and reasonably financially sophisticated person, to say the following publicly: </p><p>But before we get to present-day shenanigans, we need to go back several decades, because municipal politics is inextricable from the shenanigans.</p><h2>Chicago has wanted a casino for a long time</h2><p>Chicago and the state of Illinois more broadly have a deeply unserious polity. It has mortgaged its future through consistently overpaying public sector employees (principally, in Chicago, police/fire/teachers) and undertaxing. Neither decreasing total compensation of public sector employees nor reneging on previously-negotiated deferred compensation (pensions and healthcare for retirees) nor raising taxes to appropriate levels is considered politically palatable. One reason is that the <a href=\"https://www.ilga.gov/commission/lrb/conent.htm\">Illinois state constitution</a> (Article 13 Section 5) makes public employee pensions sacrosanct. The constitution is, of course, not a fact of nature; it is a political compromise by, again, a deeply unserious polity.</p><p>And so Illinois and Chicago specifically are constantly on the make for new revenue streams. One which was mooted <a href=\"https://repryanspain.com/2024/02/07/looking-back-riverboat-gambling-enacted-in-illinois-february-7-1990/\">since my childhood in the 1980s</a> was an expansion of gambling. So-called sin taxes (on gambling, liquor, tobacco, and similar) are politically attractive because they do not cause as much opposition as raising consumption or property taxes.</p><p>And so Chicago has had a decades-long campaign to build a casino within city limits. Why couldn’t Chicago actually get this done in several decades? One reason is the usual incompetence. The other reason is that the political economy of casinos is controversial. Many policies create winners and losers, but casinos inescapably create losers than most policies up for vote. Local political elites often band together against them, worried about siphoning money from local consumers. They also worry that they tend to create spillover effects, such as crime and moral collapse among a portion of patrons.</p><p>And so, as I once mentioned in a <a href=\"https://www.thinkingpoker.net/2020/12/episode-341-patrick-mckenzie/\">podcast with Thinking Poker</a>, pro-casino political coalitions try to pick off anti-casino political elites by assuaging their concerns and/or bribing them. (In Japan, the de facto concession was “We’ll limit the amount Japanese people can lose here and maximize for soaking Chinese tourists. Now, let’s write that down in a way which doesn’t say exactly that, because it sounds bad if you put it that way.”)</p><p>In Chicago, much of the opposition came from African American political elites. They had the usual set of concerns for casinos, plus one other which is slightly more idiosyncratic. A belief with wide currency in that community is that the community would be much more wealthy than it currently is, but for vice entrepreneurs siphoning that community’s resources out of the community. This belief has lead to e.g. <a href=\"https://www.latimes.com/california/story/2020-02-11/south-los-angeles-korean-liquor-protest-leimert-park-riots\">pogroms</a> against Korean liquor store owners. I direct interested readers to histories of the Rodney King riots or the Asian American experience in 20th century America. (This was covered extensively in an elective I took more than 20 years ago, and so I have since forgotten the academic citations for this true but parenthetical point.)</p><p>Bally’s <a href=\"https://seminoletribune.org/ballys-wins-bid-for-chicago-casino/\">won the bid</a> for the newly licensed Chicago casino in 2022, in part due to offering the right mix of concessions and inducements in its <a href=\"https://www.chicago.gov/city/en/sites/chicago-casino/home/hca.html\">Host Community Agreement</a>. One of those was promising Chicago that the new casino would be at least 25% owned by women and Minorities. The M is capital in the Chicago municipal code, and I will preserve this stylistic choice, because the word does not mean what most educated Americans assume it means. We shall return to that meaning later.</p><p>In fulfillment of its obligations under the HCA, Bally’s Chicago, Inc., an entity in the corporate web which will build and operate the casino, has <a href=\"http://ballyschicagoinvest.com\">conducted a stock offering</a> since December. It runs through January 2025.</p><p>The stock offering has a <a href=\"https://www.sec.gov/Archives/edgar/data/1935799/000110465924132193/tm2310971-13_s1a.htm\">prospectus</a> associated with it. BCI does not appear to be relying on an exemption from registration, in the fashion that e.g. most startups would, restricting them to raising money from accredited investors.</p><p>While reading the prospectus, I read a <a href=\"https://wirepoints.org/no-white-men-allowed-in-ballys-chicago-share-offering-promoted-by-city-officials-wirepoints/\">much-remarked-upon</a> statement, and assumed it was a misprint.</p><p><em>This offering is only being made to individuals and entities that satisfy the Class A Qualification Criteria (as defined herein). Our Host Community Agreement with the City of Chicago requires that 25% of Bally’s Chicago OpCo’s equity must be owned by persons that have satisfied the Class A Qualification Criteria. The Class A Qualification Criteria include, among other criteria, that the person:</em></p><ul><li><em>if an individual, must be a woman;</em></li><li><em>if an individual, must be a Minority, as defined by MCC 2-92-670(n) (see below); or</em></li><li><em>if an entity, must be controlled by women or Minorities.</em></li></ul><p>Why did I assume this was a mistake? Well, for one thing, on the face of it Bally’s has told the SEC that this offering is only available to Minorities who are  women, which does not match the intent expressed elsewhere or during their roadshow. I have immense sympathy for drafting errors. Bally’s, feel free to let the lawyers know they forgot a significant “or” on the first bullet point. [: An actual lawyer, not an Internet lawyer, informs me that the first bullet point has an implied \"or\" in this construction. Mea maxima culpa, associate who drafted this.] </p><p>The other reason I thought this was likely a mistake is that the American social, legal, and constitutional order is profoundly opposed to discrimination by race, and considers that action . Even when individual actors  to do it, they usually feel embarrassed enough about it to dissemble.&nbsp;</p><p>For example, the last few years tech companies <a href=\"https://x.com/patio11/status/1678235882481127427\">absolutely</a>, <a href=\"https://www.wsj.com/articles/youtube-hiring-for-some-positions-excluded-white-and-asian-males-lawsuit-says-1519948013\">notoriously</a> engaged in legally prohibited discrimination in hiring, sometimes as an intentionally directed and explicitly written down policy. This is often assumed to be a conspiracy theory by disaffected white males. Perhaps that is an understandable belief, since people who read the project plans either a) supported them or b) value their future careers and are therefore mostly not leaking them, and thus we only have public evidence of those project plans which end up screenshotted in litigation. Similarly, when I say that the state of California <a href=\"https://worksinprogress.co/issue/the-story-of-vaccinateca/\">proudly engaged in redlining in the provision of lifesaving medical care</a> in 2021, many people of good-will assume that I simply  be mistaken.&nbsp;<em>I get it, but I was there.</em></p><p>Returning from the ancient history of 2021 to this very week: Chicago has directed a private entity to segregate, and that entity is segregating, principally via web application. If you attempt to engage Bally’s for an investment here, you will see the following blocking question during qualification stages for the investment opportunity. (The web application will also ask for your name, address, social security number, and accredited investor status.)</p><p>There is a right answer to this question. If you give the wrong answer, Bally’s will decline you the opportunity to invest. You get entirely stopped by the web application.</p><p>I express no opinion on whether this is legal, by Bally’s or Chicago. After all, I am not a lawyer, and this has certainly been seen by many lawyers at this point, in e.g. preparing the submission to the SEC. Presumably all of them went through 1L courses which introduced concepts like the <a href=\"https://constitution.congress.gov/constitution/amendment-14/\">Fourteenth Amendment</a>, case law which says government actions discriminating by race are <a href=\"https://crsreports.congress.gov/product/pdf/IF/IF12391\">subject to strict scrutiny</a>, and case law which says that the government cannot proxy through a private entity to do things it is prohibited to do itself. And clearly no one admitted to the bar in Illinois thinks that Chicago can waive the U.S. Constitution if it considers that politically advantageous to get a gridlocked casino through municipal politics.</p><p>So I will charitably assume the existence of a memo where competent professionals have laid out a case for the legality of this course of action. They must have concluded that no future Department of Justice Civil Rights Division, not even in an administration elected after the Host Community Agreement had been inked, would descend upon this official act like the hammer of an avenging god.</p><h2>Chicago’s peculiar definition of Minority</h2><p>Long-time observers of Chicago politics might opine that the city very rarely does anything without creating a carveout for politically connected individuals. The local phrase for this sort of social connection is having “clout” or, sometimes, “<a href=\"https://www.google.com/search?q=site%3Achicagotribune.com+%22is+clouted%22\">is clouted</a>.” You can find examples of the sort of carveouts Chicago reserves for the clouted in the professional histories of the board members of Bally’s Chicago, Inc, for example, which are included in the prospectus.</p><p>So what’s the carveout here? The definition of a racial or ethnic minority is a legendarily contentious one in U.S. politics, largely because inclusion or exclusion from it makes one eligible (or ineligible) for concrete benefits. Sites of contention often include e.g. are Asian Americans a minority, or are e.g. Cuban Americans Hispanic, etc.</p><p>Chicago leaves itself an out for its definition of Minority, which <em>lets it designate any individual or group as a Minority</em>, on an ad hoc, unreported, unaccountable basis. That sounds like I must be strawmanning Chicago. See the below screenshot and explanation in the prospectus</p><p><em>Qualification under [the final] clause is determined on a case-by-case basis and there is no exhaustive or definitive list of groups or individuals that the City of Chicago has determined to qualify as Minority under this clause. However, in the event the City of Chicago identifies any additional groups or individuals as falling under this clause in the future, members of such groups would satisfy the Class A Qualification Criteria.</em></p><p>Now, fairminded people reading “groups… found by the City of Chicago to be socially disadvantaged by having suffered racial or ethnic prejudice or cultural bias within American society” would note “Well, OK, on the face of it, that definitely includes e.g. Jewish Americans or Irish Americans. We have some lamentable history as a nation and city, sure. But no intellectually serious person in the United States considers Irish Americans ‘a racial or ethnic minority’ <em>in the common usage of the term</em>.” And thus, the capitalization of Minority.</p><p>You’ll have to ask the city for their list of ad hoc exceptions made under this bullet point. Long-time watchers of Chicago municipal politics, however, might say that asking is of limited utility.</p><p>I will note that, as a matter of engineering fact, the web application will blithely accept self-certification under this bullet point . You are welcome to your guess as to whether Bally’s or any city employee will review the 1,000 investors individually and, if they review them, what the process is for determining whether e.g. a particular Patrick counts as a Minority or not.&nbsp;</p><p>I’d wager there is no process at all here. It seems like a better bet than most offered in the casino.</p><p>Bally’s Chicago is a product of Bally’s, a publicly traded company. You can read <a href=\"https://www.ballys.com/investor-relations/financials/default.aspx\">their 10-Ks</a>. According to their <a href=\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001747079/e72243a7-c47d-4ca7-a9fc-e91f8f95fb18.pdf\">most recent quarterly report</a>, they operate 15 casinos across the U.S., and have substantial online gambling operations. Like many casinos, they are  diversified, insofar as a casino resort also functions as a hotel and restaurant/bar/etc venue.</p><p>Bally’s Chicago has a complex capital stack, which one would probably need to understand to evaluate the opportunity to invest in it. I am not saying “complex” as a criticism: this is fairly ho hum by the standards of large commercial real estate developments, a subject I am not an expert on but <a href=\"https://www.complexsystemspodcast.com/episodes/the-hundred-year-old-telegram-worth-5-million-with-jim-mckenzie-2/\">grew up hearing about at the dinner table</a>. I  heavily implying that I would not expect a Chicagoan picked at random, or for that matter an alderman, to be able to look at the following diagram and correctly describe what it means. Prospectus, ibid, pg 145.</p><p>The entity which Chicago is stumping for is Bally’s Chicago, Inc. (BCI), the central square on that diagram.  investors are receiving ownership in , not in , which will be operated by Bally’s Chicago Operating Company, LLC (BCOC). That entity gets 25% economic interest in the future profits (insert  asterisk here) of the casino; the other 75% flows to Bally’s Chicago Holding Company, LLC (BCHC). BCHC is a wholly-owned subsidiary of Bally’s, Inc, the publicly traded company.</p><p>When one offers someone the opportunity to invest in something, one has to decide upon a valuation for the something. The price of a slice of the pie is set in notional reference to the price of the whole pie.</p><p>Bally’s says that its good faith guesstimate on the whole pie is the economic interest in future profits of the Chicago casino is… <em>a billion dollars exactly</em>. The prospectus, as is wont for these situations, disclaims floridly that that price might not be accurate. One example of many: “We made a number of assumptions to determine the price of our Class A Interests. If any of our assumptions are incorrect, including our assumptions regarding the total enterprise value of the Company, then the Class A Interests will be worth less than the price stated in this prospectus. In such case, the return on investment or rate of return on an investment in our Class A Interests could be significantly below an investor’s expectation.”</p><p>Bally’s will, as is standard and customary for this sort of thing, pretend that investors have read and understood the ~200 page prospectus, and civil society will pretend to believe them.</p><p>It isn’t  to pick a billion dollars out of one’s hindquarters as an investment valuation. That particular number exerts a sort of memetic quality in e.g. Silicon Valley, and there are legendary amounts of negotiation between sophisticated parties to accept just a bit more structure to get a e.g. $920 million valuation to a $1 billion valuation, because so-called unicorn status is good for PR, for attracting prospective employees, and (a real factor) for founder ego.</p><p>But if you invest at a valuation not justified by the fundamentals of the investment, you will tend to underperform. This is an inescapable fact of investing. (And that is why the sophisticated investors, accepting a “worse” valuation, want “better” structure to compensate for it.)</p><p>And this partially explains why Chicago is holding a roadshow in African American churches attempting to convince participants to invest in a mezzanine-y equity slice of a casino at a $1 billion valuation, perhaps at 100X leverage. (I tip my cap to <a href=\"https://thetriibe.com/2025/01/chicagos-black-residents-can-invest-in-ballys-casino/\">publicly available reporting</a> of the roadshow.) And not, for example, attempting to convince Goldman Sachs to put together some sophisticated investors and take down the $250 million allocation.</p><h2>Is this valuation a gift to investors?</h2><p>Chicago’s pitch to investors, delivered (per above reporting by Triibe) by “City Treasurer Melissa Conyears-Ervin and members of the Chicago Aldermanic Black Caucus”, emphasizes the potential of creating “generational wealth” (direct quote) with this casino investment. This point of view aligns with the above described political economy of attempting to buy off influential communities and/or community elites with an equity carveout, which successfully got this particular casino through decades of political gridlock.</p><p>And so the investment case  that Bally’s is intentionally giving takers something for nothing. That is, they must be sandbagging the valuation they assigned to this bundle of rights: it’s not really worth $1 billion, it is worth e.g. $5 billion. Only you favored Chicagoans well-loved by your alderman are able to buy at the non-market price, leading to essentially free money. Not merely small amounts of it, either. Generational. Wealth.</p><p>The pitch very likely explicitly said the requisite words about this being a risky investment, wink wink, and very definitely described an opportunity for extreme levels of leverage and a lengthy expected road to ROI, which we’ll return to in a moment.</p><p>Do I think sophisticated investors would agree with Bally’s that this bundle of rights is worth $1 billion? </p><p>One reason is the perception of an absence: why is this pitch being given to individual savers in a church at a minimum investment of $250, and not in a swank office to an entity capable of committing $25 million? But perhaps I’m just suspicious.</p><p>No, let’s go to more direct evidence: if 25% of this bundle of rights is worth $250 million, then 75% must be worth $750 million, right? And if an entity owning 75% of the bundle, Bally’s, also owns 14 other casinos, online gambling properties, and similar, then that entity must be worth a lot more than $750 million, right?</p><p><strong>The market does not agree with this assessment</strong>. The entire market capitalization of Bally’s (NYSE: BALY) is, as of this writing, ~$1.5 billion. What’s the difference between the $50 million average imputed value of the other casinos and the $750 million imputed value of the Chicago casino?<strong> The $750 million is , that’s what.</strong></p><p>And, again, the  of this pitch is that the bundle of rights is getting sold on the cheap, and that it is actually worth much more than $1 billion. It very clearly is not, or sophisticated investors would be swooping in and buying BALY’s common stock. Crack it open like an oyster and dig into that sweet sweet Chicago gambling revenue if you need to!</p><p>This is somewhat elementary and handwavy napkin analysis of a complicated business which, like most casinos and hotels, is heavily levered with a complex capital stack. But the investment case gets smothered by a napkin.</p><h2>Capital stack arbitrage, or, giving retail 100:1 leverage on single stock issuances</h2><p>The Host Community Agreement, as above, obligates Bally’s to find a way to sell preferred Chicagoans $250 million of stock. This was likely complicated by rich Chicagoans not being suckers and less-well-off Chicagoans not having $250 million lying around.</p><p>And so Bally’s has introduced a novel structure.</p><p>In brief, that structure sells stock to investors on credit, with the credit being extended by Bally’s, and paid down by future dividend distributions of the stock. If you’re very interested in the mechanics, you can find them at length in the prospectus, but the complex legal code is an excuse for this screenshot:</p><p>What is the “Attributable Subordinated Loan?” I’m glad you asked. Bally’s staked (ba dum bum) BCI with a few hundred million dollars to fund development. Where did it find the money to do that? A mix of equity and debt financing, as is common for virtually all complex commercial real estate transactions. Bally’s, per their <a href=\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001747079/0a22a582-5179-4a27-8a24-cbfc84043b0f.pdf\">most recent 10-K</a>, has long-term debt from sophisticated investors which costs them 5.x% per year. (It would be more expensive if they wanted to lock that down today.)</p><p>In return for Bally’s advancing BCI money through BCHC, BCI owed BCHC money, on an intercompany IOU. This capital offering cancels that intercompany IOU and replaces it with the Subordinated Loans. The prospectus does not quote the rate that the left pocket of Bally’s charged the right pocket of Bally’s. It does quote the rate for the Subordinated Loans: 11% annually compounding quarterly.</p><p>The road show makes much of the fact that this leverage is non-recourse. Quoting the Triibe reporting again:</p><p><em>The loan is non-recourse, explained Sidney Dillard of Loop Capital Markets, who is the underwriter of the offering, during the information session. “That loan is not recourse, meaning that you are not responsible for it,” she said.</em></p><p>I am not someone who has ever offered SEC registered securities for sale, but I am aware that when one does that, one has to adopt a  with respect to how one simultaneously a) sells a product that one has to offer and b) describes the operations of that product without wandering into lying.</p><p>And so  would not describe “non-recourse” as a loan one is not responsible for. I have a non-recourse mortgage. I am very, very much responsible for paying the mortgage. If I do not pay the mortgage, I expect to swiftly not own the property securing the mortgage. The “non-recourse” bit means that the lender cannot come after your  assets or income, for example by suing you for a judgement, then forcing you to disgorge your savings account or e.g. interests in a wholly owned LLC.</p><p>The Subordinated Loans are, per the prospectus (probably a bit more reliable than the understanding of e.g. Chicago employees on the finer details), not between the owners of the Class-A{1,2,3} equity and any Bally’s entities. They are strictly loans between Bally’s entities themselves. Those loans are senior to Class-A{1,2,3} equity in the payments waterfall of future profits (we need that asterisk again!) from the casino to equity holders. The expectation is that Bally’s will individually book repayments against records which are kept on a per-shareholder basis without actually obligating the shareholder, <em>while keeping the actual cash thrown off by the casino</em>, prior to eventually releasing a shareholder from the indebtedness that Bally’s will say that, technically speaking, they have not actually incurred.</p><p>At that point, the shareholder will own the slice of equity that an unsophisticated listener of that roadshow might think they own free-and-clear.</p><p>Now, Bally’s forecasts that many shareholders will be very underwater on these investments. (Wow, that’s a  sentence.) Prospectus, ibid, pg 23:</p><p><em>Given the capital intensity of developing, constructing, opening and operating a casino resort project of this scale, we currently expect that Bally’s Chicago OpCo will not have any OpCo cash available for distribution until approximately three to five years after our permanent resort and casino begins operations.</em></p><p>Assuming the most charitable estimate from that range, a Class A-3 shareholder will have $250 of equity securing a notional $25,000 investment and future obligations of approximately $34,000 (three years of compound interest on $24,750). This suggests that the holder’s equity value is  and that no sophisticated investor would purchase that investment for the $25,000 which the unsophisticated shareholder might believe it to be worth in 3 years. They might be willing to pay something more similar to, hmm, negative nine thousand dollars.</p><p>Seen in that light <em>this offer of investment sounds predatory</em>. But don’t worry, Chicagoans, Bally’s has your back. You do not have to worry about not being able to sell your stock due to its lack of intrinsic value, because you are not able to sell your stock. Prospectus, ibid, pg 179 under heading Shares Eligible For Future Sale, and elsewhere in the document.</p><p>Class A-4 holders, the ones with no notional debt, who purchased their shares for $25,000 cash-on-the-barrel, are not eligible to sell their stock at any time except as allowed by Bally’s to people approved by Bally’s. (I’ll flag that this is not an unusual term in private equities. Bally’s pre-commitment to discriminating racially against future prospective buyers?  unusual.)</p><p>Buyers of Class A-{1,2,3} stock are unable to sell until the associated Subordinated Loan is paid off in full.</p><p>One wonders whether senior Chicago officials will be doing a roadshow in 2030 explaining what happened.</p><h2>The casino will not distribute profits, per se</h2><p>While the natural expectation is that one is participating in the profits of the casino, the prospectus helpfully clarifies that one is not. The \"cash available for distribution\" does not necessarily correspond 1:1 with profits. It... well. See the discussion on page 22 and 23 of the prospectus, including the excerpt below.</p><p><em>While we and Bally’s Chicago OpCo intend to make distributions equal to 100% of the cash available for distribution and OpCo cash available for distribution, respectively, on a quarterly basis, the actual amount of any distributions may fluctuate depending on our and Bally’s Chicago OpCo’s ability to generate cash from operations and our and Bally’s Chicago OpCo’s cash flow needs, which, among other things, may be impacted by debt service payments on our or Bally’s Chicago OpCo’s senior indebtedness, capital expenditures, potential expansion opportunities and the availability of financing alternatives, the need to service any future indebtedness or other liquidity needs and general industry and business conditions, including the pace of the construction and development of our permanent resort and casino in Chicago. Our Board will have full discretion on how to deploy cash available for distribution, including the payment of dividends. Any debt we or Bally’s Chicago OpCo may incur in the future is likely to restrict our and Bally’s Chicago OpCo ability to pay dividends or distributions, and such restriction may prohibit us and Bally’s Chicago OpCo from making distributions, or reduce the amount of cash available for distribution and OpCo cash available for distribution.</em></p><p>Now, as someone who grew up with a father constantly complaining about sharp operating in Chicago commercial real estate, I can quickly outline about two dozen different ways for one to cause the operating company here to a) transfer money to other corporate entities and b) therefore have less cash available for distribution.</p><p>As a representative but not limiting example, you can probably choose your own marks for technology services from a parent to a great-grandchild subsidiary. Sure, there is some notional expectation that the marks be at arms-length price, but what is the arms-length price for e.g. casino loyalty accounting software and a particular chain's database of existing users? What low-resourced investor could possibly mount a court challenge against <em>the entity with all the data necessary to value that asset</em>. In Las Vegas, a casino has to calculate and diligently communicate the house edge before raking punters. Here... not so much.</p><p>That would require sharp operating... of a sort which is  in Chicago commercial real estate. This is a constant risk of being the junior partner in a structure, particularly without an aligned senior partner who would be as adversely impacted by sharp operating as you would be. Of course, here the senior partner owns e.g. the database they are renting to the entity that they also control, and so funds available for distribution from that entity might not match the expectations of junior partners.</p><p>Pick your sponsors carefully, folks.</p><h2>Tax consequences of this offering</h2><p>Suppose, and this is very unlikely because it is illegal (<a href=\"https://www.finra.org/rules-guidance/key-topics/margin-accounts\">Reg T</a>) but run with it, that one has a typical brokerage account in the United States and, with $250, purchases $25,000 of marketable securities. Those securities periodically throw off dividend payments. One periodically pays one’s brokerage interest, because one has borrowed money from the brokerage to buy those securities on margin.</p><p>In the typical case, one would be taxed upon those interest payments, which are income. One does not simply  one’s margin interest against that income before paying taxes. One instead must itemize, and then one <a href=\"https://www.irs.gov/forms-pubs/about-schedule-a-form-1040\">will be able to</a> (on Schedule A) deduct investment expenses, as described in <a href=\"https://www.irs.gov/publications/p550\">Publication 550</a>. Feel free to run this by your accountant; the details get complicated and wonky.</p><p>If one does not itemize, as many lower-income taxpayers do not, one must of course simply pay the tax on the entirety of one’s interest income. If one protests that one does not actually  any interest income, because it has been taken by one’s brokerage to pay margin interest, the IRS will not be maximally sympathetic.</p><p>Bally’s has very creative professionals involved in the structuring of this offering, and realizing the above issue would compromise fitness for purpose, they have… adopted a theory. I will quote that theory, from the prospectus, verbatim. I have taken the liberty of bolding an important bit in the middle of this.</p><p><em>Section 305 of the Internal Revenue Code provides that if a corporation distributes property to some shareholders and other shareholders have an increase in their proportionate interests in the assets or earnings and profits of the corporation, such other shareholders may be deemed to receive a distribution that could be a taxable dividend. In this case, because we and Bally’s expect to treat the Subordinated Loans as “stock” for U.S. federal income tax purposes, “property” distributions will likely be considered to be made to “some shareholders” of Bally’s Chicago, Inc. as payments are made on the Subordinated Loans, and equivalent cash (“property”) distributions will be made with respect to the Class A-4 Interests. In addition, as payments are made on the Subordinated Loans, particularly those that repay the original principal amount of such Subordinated Loans, the proportionate interests of holders of our Class A-1 Interests, Class A-2 Interests and Class A-3 Interests in the assets or earnings and profits of Bally’s Chicago, Inc. may be viewed as increasing. Accordingly, it is possible that such increase could be treated as a deemed distribution under Section 305 of the Code or otherwise as taxable income to such holders under other theories. However, under the Treasury Regulations relating to Section 305 of the Code and other IRS administrative guidance, certain financing arrangements in the form of preferred stock investments that fund a corporation and then are systematically eliminated through property distributions until they are fully retired, and are designed to facilitate the ownership of a business with an effect of increasing another stockholder’s proportionate interests in the assets or earnings and profits of a corporation over such period, do not result in a deemed distribution to such other stockholder. The applicability of these authorities to the holders of our Class A-1 Interests, Class A-2 Interests and Class A-3 Interests in this situation is uncertain. <strong>Although the matter is not free from doubt, we intend to take the position, and this discussion assumes, that U.S. Holders of applicable series of Class A Interests would not be treated as receiving a deemed distribution from us or otherwise realizing income as a result of repayment of the Subordinated Loans corresponding to such shares. However, there can be no assurance that the IRS will not take a contrary position</strong>, for example, treating the proportionate interest in our earnings and profits owned by U.S. Holders of the applicable series of Class A Interests as having increased upon repayment of the Subordinated Loans corresponding to such shares, and treating such U.S. Holders as having received a distribution. In that case, such deemed distribution will be taxable as a dividend, return of capital or capital gain as described above under “— Distributions,” and U.S. Holders may be subject to U.S. federal income tax without the receipt of any cash. U.S. Holders should consult their own tax advisors about the application of Code Section 305 and any other potential deemed receipt of income risk with respect to our Class A Interests .</em></p><p>Now, I’m neither a lawyer, tax accountant, nor am I someone who listened carefully to the roadshow when it doubtlessly stepped through this for the benefit of the audience. But here’s what it means:</p><p>Bally’s is taking the position, though they acknowledge that the IRS might disagree, that owners of the Class A-{2,3,4} interests aren’t  getting any income until the Subordinated Loans have been paid in full. This means that they don’t have to pay income taxes in years where they are not actually receiving cash distributions.</p><p>No, they wait until the Subordinated Loan is paid in full, and then immediately owe income taxes in one whack, at the difference between their basis in the stock (say, $250) and the then-FMV of the stock (say, $25,000). Resulting in Bally’s diligently filing a document with the IRS saying that e.g. a lower-income Chicagoan has just received a bit less than $25,000 in income from them, and should probably pay taxes on it. You can, of course, receive income without receiving ; it happens all the time in tech, and is the cause of much structuring to avoid the consequences of it, which can be painful for e.g. early career employees.&nbsp;</p><p>Those taxes will be paid substantially out-of-pocket, because there is almost no conceivable universe where a stock of an actual healthy operating enterprise worth e.g. $25,000 pays an ordinary dividend of e.g. $5,000. The market would adjust the value of the stock upwards to account for the extraordinarily rich stream of dividends, which would adjust the tax bill upwards.</p><p>Financially sophisticated investors might prepare for a tax bomb like this by e.g. borrowing against the value of the stock. That’s basically impossible for this issuance, due to the stock not being publicly listed, the restrictions on transfer, small dollar amounts, etc. The other option is, of course, selling the stock, to whomever Bally’s deigns to approve.</p><p>Tax-motivated transactions are, of course, , and the lucky buyer will probably be able to extract a bit of a deal, doubly so because they are likely much more sophisticated than the initial buyer of the stock, and they have less risk to account for (because of e.g. several years of operating history of the casino before the tax bomb explodes).</p><p>I am not an investment advisor, and not your investment advisor. I am, however, a recreational poker player who lives in Chicago. I intend to periodically donate money to the Chicago economy by making poor decisions on the river at Bally’s Chicago. </p><p>I do not, however, presently intend to participate in Bally’s stock offering, nor do I presently intend to buy their common stock.</p><p>I will note, out of an overabundance of scrupulousness, that I own a tiny amount of MGM stock, which is a direct competitor to Bally’s. I caught the poker bug at a conference in Las Vegas (hosted at the Tropicana, since acquired by Bally’s and then <a href=\"https://www.ballys.com/news/news-details/2024/Tropicana-Las-Vegas-Implosion-to-Make-Way-for-the-As-Ballpark-and-Ballys-Entertainment-Resort-Destination-2024-9NZuey6eTG/default.aspx\">brought down in a controlled implosion</a>).</p><p>MGM, across the street, actually had poker tables. I have had many enjoyable post-conference excursions staying at their hotel to (in several but not all years) lose money at those tables. I bought the stock for the same reason I buy stock in every hotel, airline, bank, and similar I use: in the unlikely event a not-particularly-high-stakes poker player has a routine customer service complaint, Investor Relations is available as an escalation strategy, over e.g. hotel staff who might be long-since inured to listening to complaints from people who lost money in a casino.</p><p>Oh yeah, I mentioned that there is a crypto angle to this. The registrar and transfer agent for offering 100:1 leverage to retail investors on a casino stock is, see prospectus pg 41, <a href=\"https://www.bitgo.com/\">BitGo Trust</a>. If I had made up that detail, as a crypto skeptic, you might have accused me of being a bit on the nose.</p><div><div><a href=\"https://www.bitsaboutmoney.com/archive/bam-2024-retrospective/\">Bits about Money yearly recap and plans →</a></div></div>","contentLength":35871,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42816418"},{"title":"The hidden complexity of scaling WebSockets","url":"https://composehq.com/blog/scaling-websockets-1-23-25","date":1737748131,"author":"atul-jalan","guid":197,"unread":true,"content":"<p>With the rising demand for sync engines and real-time feature, WebSockets have become a critical component for modern applications. At Compose, WebSockets form the backbone of our service, powering our backend SDKs that enable developers to deliver low-latency interactive applications with just backend code.</p><p>But, scaling WebSockets has proven to be far more complex than we expected. Below are some of the most important lessons we've learned along the way.</p><h3>Handle deployments gracefully</h3><div>Users should never notice when deployments happen, so WebSocket connections need to persist across deployments. This is a delicate process, and requires robust reconnection logic to deal with unexpected issues. At Compose, we achieve near-zero downtime by following these steps:<ol><li><p>Once the new servers are healthy, old servers begin returning responses to health checks.</p></li><li><p>After 4 consecutive  responses, the load balancer declares the server unhealthy and removes the old servers from the pool. The load balancer health checks every 5 seconds, so this process takes up to 25 seconds.</p></li><li><div>Old servers send a custom WebSocket close message instructing clients to delay reconnection by a random interval to avoid a reconnection surge.<ul><li><p>The custom close message lets clients show users a more accurate message during the ~10 second period where the client is disconnected.</p></li><li><p>The random delay helps prevent thundering herd issues where all clients reconnect at once. Clients also double the exponential backoff for deployment-related reconnections to account for unforseen issues.</p></li><li><p>The close message is delayed by 20 seconds to account for the time it takes for the load balancer to shift traffic.</p></li></ul></div></li><li><p>Once all clients disconnect, the old servers shut down completely.</p></li></ol></div><p>If you're using a managed service like Render or Railway, you should be especially cognizant that client connections are transferred gracefully during deployments.</p><p>Many managed services that tout zero-downtime deployments will wait until all outstanding requests are processed before shutting down a server. Since WebSocket connections are persistent, this can lead to situations in which old servers are active for minutes or even hours after a deploy until the managed service forcibly terminates the process.</p><h3>Establish a consistent message schema</h3><div>While HTTP comes with built-in routing conventions (,,), WebSockets require developers to define their own schema for organizing messages.</div><div>At Compose, every WebSocket message starts with a fixed 2-byte prefix for categorizing messages.<ul><li><p>It's space-efficient (only 2 bytes), while still scaling to 65,536 different types.</p></li><li><p>It enables clients to reliably slice the prefix from the message without affecting the rest of the data, since the prefix is always 2 bytes.</p></li><li><p>It gives us a simple method for upgrading our APIs by versioning message types.</p></li></ul></div><div><div><pre><code> = {\n  : ,\n  : ,\n  : ,\n  : ,\n  \n}</code></pre></div></div><p>Additionally, we use delimiters to separate different fields inside the message, which is both faster to encode/decode and more memory-efficient than JSON.</p><div><div><pre><code> = ;\n\n() {\n   [[], ...args].();\n}\n\n() {\n   [, ...args] = message.();\n   { , args };\n}</code></pre></div></div><p>We're lucky that our backend and frontend are written in TypeScript, allowing us to share message schemas between the two and ensure that neither falls out of sync.</p><h3>Detect silent disconnects with heartbeats</h3><div>Connections can drop unexpectedly without triggering a<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebSocket/close_event\">close event</a>, leading to a situation in which the client thinks they're connected, but actually aren't. To prevent stale connections, implementing a robust heartbeat mechanism is essential.</div><div>We send periodic<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers#pings_and_pongs_the_heartbeat_of_websockets\">ping/pong messages</a>between client and server and reconnect in cases where the heartbeat isn't received within some interval.</div><div>Our server sends a  message every 30 seconds, and expects a  response. In cases where the client doesn't receive a every 45 seconds, it immediately drops the connection and tries to reconnect. Similarly, the server closes connections that miss responses within 45 seconds.</div><p>By monitoring heartbeats on both ends, we detect and handle rare cases where the client side network appears functional but the server never receives responses.</p><div>WebSocket connections can be unexpectedly blocked, especially on restrictive public networks. To mitigate such issues, Compose uses<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events\">server-sent events (SSE)</a>as a fallback for receiving updates, while HTTP requests handle client-to-server communication.</div><img src=\"https://composehq.com/blog-scaling-websockets-fallback.png\" alt=\"SSE fallback\"><p>Since SSE is HTTP-based, it's much less likely to be blocked, providing a reliable alternative in restricted environments. Plus it still achieves decently low latency, especially compared to short-polling solutions.</p><div>There's a whole lot more to scaling WebSockets that we didn't cover here. For example:<ul><li><p>: While most frameworks include built-in tools for rate limiting, data validation, and error handling, you'll generally have to implement these features on your own for WebSockets.</p></li><li><p>: Edge networks make it easy to cache HTTP responses close to users, but there's no standard way to accomplish this with WebSockets.</p></li><li><p>: Guarding against abuse by ensuring that each message is valid for that user before processing it.</p></li></ul></div><p>But regardless of the complexity, users expect modern applications to be fast, realtime, and collaborative. And, as of now, there's no better way to achieve that than WebSockets.</p><div>At Compose, WebSockets power the entire platform - from the database all the way to the main UI thread. Via our SDKs, developers can generate full web apps from their backend logic. Making sure those apps are fast and performant at scale requires WebSockets. If you're interested in learning more,<a href=\"https://docs.composehq.com\" target=\"_blank\" rel=\"noopener noreferrer\">check out our docs</a>. It takes less than 5 minutes to install the SDK and build your first app.</div>","contentLength":5632,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42816359"},{"title":"What technology was used to create the core of rust?","url":"https://www.reddit.com/r/rust/comments/1i93tma/what_technology_was_used_to_create_the_core_of/","date":1737747155,"author":"/u/BeastBoyMike","guid":587,"unread":true,"content":"<p>Like what language did they write the rust compiler in..? It's core libraries, are they adopted from c/c++?</p>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Upgraded chat app","url":"https://www.reddit.com/r/golang/comments/1i93ao3/upgraded_chat_app/","date":1737745822,"author":"/u/DixGee","guid":563,"unread":true,"content":"<p>Hi everyone, I created a web chat app for anonymous chatting last month and posted here. Since then I made lots of changes and completely transformed the app using React, Go and Redis. Now users will have to be authenticated to start chatting. I want to add more features like detecting whether users are online, sharing files, etc. A big reason why I made this app was the idea of getting a job in go tbf. So far I've seen only roles for experienced candidates. If anyone can guide me on that, it would be really helpful.</p>","contentLength":522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Would you ever pay/donate to a Photoshop competitor that is Linux-first?","url":"https://www.reddit.com/r/linux/comments/1i92s5c/would_you_ever_paydonate_to_a_photoshop/","date":1737744521,"author":"/u/True-Direction5217","guid":579,"unread":true,"content":"<p>I've seen many posts throughout the years that share my sentiment on not having Photoshop on Linux, and that the alternatives aren't in one way or another filling that need. I've realized that even if Photoshop came to Linux today, I don't think I'd use it, because of Adobe. I left Windows because software these days are plagued by enshitifcation, invading your privacy, poor performance and security. Top it off with Adobe's recent nonsense around training on your data and exorbitant cancellation fees, was the final nail in the coffin for me.</p><p>So I decided to work on my own PS-like editor that have been such a joy to write so far, but still very far off from a releasable state. I would love to work on full-time and thinking about raising some form of funding instead of working at another morally-bankrupt company.</p><p>I know some will ask why not help out on other existing projects like Gimp. The reason is quite simply these projects are built on years of legacy code with a vision &amp; programming language that isn't completely compatible with mine. None of my MRs would ever get accepted because what I have in mind for my editor would simply not be possible without major rewrites. It would be much easier to start from scratch.</p><p>From what I can gather, Adobe does not see it as worth investing in adding support for Linux, whether that is just due to the size of the user base or maybe they feel people in the Linux community are less inclined to pay for software in general. </p><p>So my question to the community: Would anyone be interested in buying or donating to such a thing or am I completely delusional here?</p>","contentLength":1614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learned from mistakes","url":"https://www.reddit.com/r/linux/comments/1i92kkw/learned_from_mistakes/","date":1737743999,"author":"/u/Lillian_La_Elara_","guid":578,"unread":true,"content":"<p>So yesterday was deffenetly a day... i had a good working KDE Setup but of course the kind of person i am i had to fiddle around, i see all these beautifull Hyprland setups, thought i try to install it, make it look pretty, that didn't work out. Watching YouTube guides rarely work out, there are some that work but that's usually not the case. This time when i tried to instatll Hyprland, i wiped my KDE and started a new Archinstall, which installed a chunk of the stuff, as i followed the instructions the person opend the terminal and guided into the config to edit stuff, that's when everything became problematic as i was unable to interact with the config...i couldn't delete or write into it i was so utterly confused, so i re installed KDE tried to pretty it up, i eventually fucked up and rigged my own system, probably with the Sudo Chown command i lost all root acess with the following error that UID 1000 has acess and it should be 0, i spent an hour trouble shooting, i was able to acess root function from the bios, starting Arch in single mode but it refused to change the Writing,Reading conditions... after an hour i decided to okay reinstall KDE... for the 3rd time that day...i eventually finnished at 1AM. As you can see i didn't do any major fuckery with my system this time, i kept it simple, i still had to use Sudo Chown to give my user account read and write acess to my mounted drives but this time the system integrity is good, nothing is bricked so far. I still have to eventually make a backup file with all the programs,dependencys downloaded. Will i try this again? Im not sure...im kinda scared to...i probably messed it up with Chown command or something along that line...i tried to install eww, i tried to install an image burner both failed...maybe keeping it simple as is...ain't that bad...it works, heck after this install my RAM usage somehow gotten a lot better too. If it were my secoundary PC i would keep fiddle around but this is my one and only main PC and i need it to be stable and functional.</p>","contentLength":2043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anatomy of a Formal Proof","url":"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html","date":1737742775,"author":"bikenaga","guid":196,"unread":true,"content":"<section data-ams-doc=\"frontmatter\"></section><section role=\"doc-introduction\" data-ams-doc=\"section\" data-ams-doc-level=\"1\"><p>It has been a long day and you are making your way through a paper related to your work. You suddenly come across the following remark: “…since  and  are eigenvectors of  with distinct eigenvalues, they are linearly independent.” Wait—how does the proof go? You should really know this. Here  and  are nonzero elements of a vector space  and  is a linear map. You force yourself to pick up a pen and write down the following argument:</p><blockquote data-ams-style=\"null\"><p>Let  and  with  Suppose  Applying  and using linearity, we have  Multiplying the original equation by  we have  Subtracting the two yields  and since  and  are nonzero, we have  The corresponding argument with  and  swapped yields  so the only linear combination of  and  that yields  is the trivial one.</p></blockquote><p>Your colleagues have all gone home and there is nobody around to discuss this with. So, instead, you turn to your computer and start up Lean, the proof assistant you happen to use. Can you prove the claim formally? As you type, the information window in your editor complains about every misstep—the syntax is fiddly, and you have to get the notation and the instructions just right—but that’s okay, because working through the proof is relaxing and kind of fun. Lean often makes you spell out arguments that are painfully obvious, but you have found that if you set things up just right, it will cheerfully fill in some of the details. After a short while, you have success! Lean signs off on the proof, confirming you have managed to construct a formal derivation in the system’s axiomatic foundation.</p><pre data-ams-specific-use=\"numbers=right\">import Mathlib.LinearAlgebra.LinearIndependent\n\nvariable [Field K] [AddCommGroup V] [Module K V]\n\nexample (f : V →ₗ[K] V)\n    (μ ν : K) (hμν : μ ≠ ν)\n    (x y : V) (hx₀ : x ≠ 0) (hy₀ : y ≠ 0)\n    (hx : f x = μ • x) (hy : f y = ν • y) :\n    ∀ a b : K,\n      a • x + b • y = 0 → a = 0 ∧ b = 0 := by\n  intro a b hab\n  have :=\n  calc (μ - ν) • a • x\n      = (a • μ • x + b • ν • y) -\n        ν • (a • x + b • y) := by module\n    _ = f (a • x + b • y) -\n        ν • (a • x + b • y) := by simp [hx, hy]\n    _ = 0 := by simp [hab]\n  simp_all [sub_eq_zero]</pre><p>What a lovely proof! It took some effort to work it out, but it was a pleasure seeing the steps play out formally, and you are proud of the result.</p></section><section data-ams-doc=\"section\" data-ams-doc-level=\"1\"><p>Let’s take a look at what you have done. You are using Lean’s mathematical library, Mathlib, a communally developed and maintained repository of formally verified mathematics. At the time of writing, Mathlib contains more than 80,000 definitions and more than 160,000 theorems, starting from axiomatic primitives and developing mathematics from the bottom up. It also contains dozens of user-contributed automated reasoning procedures that help with the formalization process, as well as notation declarations and configuration information, all of which codify different aspects of our mathematical understanding.</p><p>To avoid having to load the entire library into memory at once, Lean asks you to tell the system what parts of the library you want to use. So the first line of the proof, the one that starts with the keyword , tells Lean you want to use the file with the name shown. That pulls in all the files in the library that those notions depend on, which is quite a lot, including basic algebra, properties of scalars and vectors, and so on.</p><p>The next line, the one that begins with the word , declares some of the objects you want to work with: a field,  and a vector space,  You could have, alternatively, put these declarations after the  keyword on line 5; using the  command to declare them separately is especially convenient when multiple definitions and theorems share the same data and hypotheses. It’s a quirk of the library’s design that you declare that  is an additive, commutative, group, coupled with a scalar action that turns  into a  (Remember, a vector space is nothing more than a module over a ring in which the ring in question is actually a field.) Mathlib includes thousands of axiomatically declared structures, and the contributors to the library take great care to maximize reusability by breaking structural hypotheses into reusable pieces that can easily be configured and composed.</p><p>Next comes the keyword  (line&nbsp;5), which indicates that the result you are proving is not intended for future use. Using  is good for experimentation. To prove a lemma or theorem that you intend to use later, you would instead use  or  and provide a name. In that case, once Lean has processed the proof, it is stored in the , which means that any file that imports this one can see it. If you think you have proved a theorem that ought to go into Mathlib, you can issue a  to add it to the library. Mathlib is overseen by teams of  and  who moderate and update the contents, ensuring the quality and stability of the library.</p><p>You begin stating the claim by introducing the key players:</p><dl data-ams-content-type=\"itemize\"><div><dd><p>a linear map  from  to itself. Lean’s notation for such linear maps is ,</p></dd></div><div><dd><p>two elements of  denoted  and  which will end up being your eigenvalues, and a hypothesis called  that says  and  are distinct,</p></dd></div><div><dd><p>two vectors called  and  as well as hypotheses  and  that say that they are nonzero,</p></dd></div><div><dd><p>and, finally, your two key assumptions,  and , which say that  and </p></dd></div></dl><p>In the notation , the  is a fixed symbol signifying linear maps, whereas the parameter in brackets indicates that we mean linearity over .</p><p>At this point, you have set up your , that is, the data and hypotheses you need to state your claim. You have your  space  a linear map  two scalars  and  and eigenvectors  and  for  and  respectively. At that point you are ready to state the conclusion (lines&nbsp;9 and&nbsp;10), namely, that  and  are linearly independent: . You have decided to use the definition of linear independence for a pair of vectors: Whenever  and  are scalars, if  then  and </p><p>Notice that you have used the symbol  in expressions like  (line&nbsp;7) to express that  and  are elements of , rather than writing  and . In Lean’s axiomatic foundation, a version of , every object has a fundamental data type. In this example, we would say that that  and  are  of the  Interestingly, we use the same notation in an expression like  (also line&nbsp;7) to express that  is a label for the assumption . In dependent type theory, assertions like  (also known as ) are analogous to data types, and they are handled by the same fundamental mechanisms. Lean checks that an expression like  is a well-formed expression of type , given information about the variables and symbols involved, and, in the same way, it checks that a formal expression is a well-formed proof of the proposition , given the data and assumptions it depends on. That is why the latter appears after the colon in the example’s conclusion: the statement of the example announces your intent to construct a formal proof of the conclusion, given the data and assumptions that come before.</p><p>What comes after the symbol  (line&nbsp;10) is the proof itself, or, more precisely, instructions that tell Lean how to construct the proof. If you are successful, the corresponding expression is stored in memory and checked for correctness by Lean’s trusted . The keyword  after the  instructs Lean to enter , which means that the text that follows should be interpreted as a list of instructions that tell Lean how to build the required proof. A  is a procedure that automatically fills in a chain of formal inferences that is needed to justify a reasoning step. Such a chain can be as short as a single logical axiom or rule, but it can also be quite long and involved. Tactics thus play an important role in bridging the gap between the kind of reasoning that is intuitively clear and natural to mathematicians and the stringent axiomatic rules embodied by a formal system.</p><p>The first tactic you use is  (line&nbsp;11). This introduces the two variables ,  which your statement quantifies over, as well as the antecedent of the implication, that is, the assumption . As you type or move your cursor around a proof, Lean displays the , i.e., information that is relevant at that point in the proof, in its  window. After the  tactic, the tactic state looks something like this:</p><pre>K : Type\nV : Type\nf : V →ₗ[K] V\nμ ν : K\nhμν : μ ≠ ν\nx y : V\nhx₀ : x ≠ 0\nhy₀ : y ≠ 0\nhx : f x = μ • x\nhy : f y = ν • y\na b : K\nhab : a • x + b • y = 0\n⊢ a = 0 ∧ b = 0</pre><p>This is nice a summary of where you are in the proof, including the objects and assumptions you started with as well as the objects ,  and the assumption  that you introduced in the first step. The line that starts with  indicates that your current  is to prove the conjunction .</p><p>While writing the proof, you notice that Lean complains with warnings and error messages. This is expected, since the proof is incomplete. You can appease Lean by apologizing for the incomplete proof: If you use the  tactic on the final line of the proof, Lean will not raise an error over the fact that the proof is incomplete, but it will still raise a gentle warning about the use of .</p><p>The next step in the proof starts with  (line&nbsp;12). This introduces a calculation, similar to a  block in LaTeX. The calculation itself is very similar to the one in the proof sketch above, but notice that the individual steps in the calculation are justified by short subproofs like  (line&nbsp;15) or  (line&nbsp;17). The  tactic proves equalities of universal linear expressions, in other words, linear equalities that are true in all modules, and do not use specific facts about the module at hand. The  tactic is a powerful tool that uses a database of equalities and equivalences from Mathlib, together with user-specified equations (like ) to rewrite its goal into a simpler form. Happily, in this case, the goal becomes a trivial equality, and the goal is therefore closed. In a moment, we will consider the steps that  has taken in greater detail.</p><p>After this calculation, you completed your proof with the  tactic (line&nbsp;19), which is a variant of  that recursively uses all the hypotheses to simplify all hypotheses and the goal. Now if you leave a  after the , Lean will complain that there is nothing to be sorry about and insist that remove the apology. Lean checks the correctness of the proof and confirms it with its silence: the absence of errors is the proclamation that you have succeeded.</p></section><section data-ams-doc=\"section\" data-ams-doc-level=\"1\"><p>So what exactly does the  tactic do? The following provides a manual, more detailed proof of one step of the calculation in your proof, with the direction of the equality reversed.</p><pre>variable (f : V →ₗ[K] V)\n  (μ ν : K) (x y : V) (a b : K)\n\nexample (hx : f x = μ • x) (hy : f y = ν • y) :\n  f (a • x + b • y) = (a • μ • x + b • ν • y) := by\ncalc f (a • x + b • y)\n    = f (a • x) + f (b • y) := by\n        rw [map_add]\n  _ = a • f x + b • f y := by\n        rw [map_smul, map_smul]\n  _ = (a • μ • x + b • ν • y) := by\n        rw [hx, hy]</pre><p>This example demonstrates the  tactic, denoted . In the calculation, we first rewrite with the lemma , which states that for any linear map , the equation  holds. The next step in the calculation is justified by rewriting twice with the lemma , which states that for any linear map , scalar , and vector , the equation  holds. (Here  stands for “scalar multiplication.”) The underscores before each subsequent step are part of the  syntax, indicating to the parser that the calculation continues. The proof concludes by rewriting with the hypotheses  and , which assert that  and  are eigenvectors of  with eigenvalues  and  respectively.</p><p>The lemmas  and  are part of the Mathlib library, and they are labeled with the . This attribute tells Lean that the lemma should be added to the database of lemmas that  uses to simplify expressions. And that is the reason why the  tactic could prove the goal in one go: it chained together the  steps that we spelled out step by step in the calculation above.</p><p>One of the challenges of formalization is that we often need to spell out, in painful detail, inferences that seem obvious or straightforward to us. The more we can get the computer to fill in, the better. The simplifier is an important example of  that can help in this respect. The  tactic is another.</p><p>Broadly speaking, there are two classes of automation. Firstly, there is  automation. One example that we have seen is the  tactic, the simplifier. Another example is , a tactic that provides “Automated Extensible Search for Obvious Proofs.” In the proof assistant Isabelle, there is a tool called , which can search for proofs using a large database of lemmas. And recently AI copilots have demonstrated the ability to suggest tactics and fill in parts of proofs in Lean. The wide applicability of these general-purpose tools is balanced by the fact that, at least for now, they can only assist with proofs that are relatively straightforward.</p><p>The other class of automation is  automation. Here we have also seen an example: the  tactic. Other examples include the  tactic, which solves equations in commutative rings, the  tactic, which solves linear arithmetic problems, and the  tactic, which proves that functions satisfy a given property such as “continuity” or “measurability.” These tactics are less general; they typically have a well-defined and narrow scope, but they can be very powerful in their domain of applicability. A prime example of this approach comes from the use of software tools known as SAT solvers and SMT solvers: if a claim can be encoded as a boolean formula or a formula in some decidable theory, then these can be used to justify it automatically. This has been fruitfully applied in non-trivial ways, see for example&nbsp;.</p></section><section data-ams-doc=\"section\" data-ams-doc-level=\"1\"><h2></h2><p>Often, one of the first things we do when we have proved a lemma or a theorem is check whether the hypotheses can be weakened, in order to increase its applicability. Proof assistants are especially helpful in this respect, because they enable us to tinker with hypotheses interactively and see what breaks. Returning to our example, upon verifying the initial result, you might wonder: what are the minimal assumptions that we need to make this proof go through? Let’s experiment. The proof never mentions inverses of scalars, so it should work for a larger class of rings. In a first attempt, we might try to replace the vector space  by an arbitrary module  as follows:</p><pre>variable {R M : Type}\n  [Ring R] [AddCommGroup M] [Module R M]</pre><p>But Lean complains at the first step in the calculation, which requires . We can address this by assuming that the ring of scalars is commutative. However, after making that change, the final step of the proof is still broken. A bit of reflection shows that this step uses the result of the calculation steps together with the assumptions  and  to prove that . In other words, we need the additional property that  implies that  or  for every  in  and  in . Mathlib expresses this as the property , and we can add that assumption as follows:</p><pre>variable {R M : Type}\n  [CommRing R] [AddCommGroup M] [Module R M]\n  [NoZeroSMulDivisors R M]\n\nexample (f : M →ₗ[R] M)\n  (μ ν : R) (hμν : μ ≠ ν)\n  (x y : M) (hx₀ : x ≠ 0) (hy₀ : y ≠ 0)\n  (hx : f x = μ • x) (hy : f y = ν • y) :\n  ∀ a b : R, a • x + b • y = 0 → a = 0 ∧ b = 0</pre><p>Lean accepts this statement with the same proof as before! We have therefore obtained a more general theorem without changing a single character of the proof.</p></section><section data-ams-doc=\"section\" data-ams-doc-level=\"1\"><h2></h2><p>Let’s see if we can strengthen your result even further. You started with a basic version involving vector spaces over fields and only two eigenvectors, and we just generalized it to suitable modules over commutative rings. Where can we go from here?</p><p>Let’s try to generalize the theorem to arbitrary families of eigenvectors. To do this, we will need to think carefully about how to express, in Lean, the fact that an arbitrary family of vectors is linearly independent. We could write down a definition from scratch, but it makes sense to see if we can take advantage of things that are already in the library. If we navigate to the Mathlib documentation webpage ⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid12\"></a> and start typing  in the search box, one of the first few results that comes up is the following:</p><pre>def LinearIndependent {ι : Type}\n  (R : Type) {M : Type}\n  (v : ι → M) [Semiring R]\n  [AddCommMonoid M] [Module R M] :\n  Prop</pre><p>Here  and  are the relevant ring and module, as before,  is an indexing type, and  is a family of elements of  indexed by . It is standard, and convenient, to represent a family  of elements of  as a function , in which case the  element,  is simply written . All the arguments in curly and square brackets are generally left implicit, which is to say, we expect to write  and have Lean figure out the rest. The annotation  means that the expression  is a proposition, namely, the proposition that the family  is linearly independent over . At the moment, we don’t need to know the body of the definition; we can use it as a black box.</p><p>To do so, we will add an indexing type  to the statement of our theorem, and since we also want to say that the vectors are all , we will also use  to index a family  of scalars which will act as the eigenvalues. We can now formulate the statement we are after as follows:</p><pre>example {R M : Type} [CommRing R]\n    [AddCommGroup M] [Module R M]\n    [NoZeroSMulDivisors R M]\n    (f : M →ₗ[R] M)\n    (μ : ι → R) (hμ : Function.Injective μ)\n    (v : ι → M) (hv : ∀ i, v i ≠ 0)\n    (h : ∀ i, f (v i) = μ i • v i) :\n    LinearIndependent R v</pre><p>Note that we have formulated the fact that we are considering  eigenvalues by assuming  is injective.</p><p>We should check to see whether something like this theorem is already in the library. If we go back to the documentation webpage and search for , only two results come up:</p><pre>theorem\n  Module.End.eigenvectors_linearIndependent\n  {R : Type} {M : Type} [CommRing R]\n  [AddCommGroup M] [Module R M]\n  [NoZeroSMulDivisors R M]\n  (f : Module.End R M)\n  (μs : Set R) (xs : ↑μs → M)\n  (h_eigenvec : ∀ (μ : ↑μs),\n    f.HasEigenvector (↑μ) (xs μ)) :\n  LinearIndependent (ι := ↑μs) R xs</pre><pre>theorem\n  Module.End.eigenvectors_linearIndependent'\n  {R : Type} {M : Type} [CommRing R]\n  [AddCommGroup M] [Module R M] {ι : Type}\n  [NoZeroSMulDivisors R M]\n  (f : Module.End R M)\n  (μ : ι → R) (hμ : Function.Injective μ)\n  (v : ι → M)\n  (h_eigenvec : ∀ (i : ι),\n    f.HasEigenvector (μ i) (v i)) :\n  LinearIndependent R v</pre><p>The difference between the two is that the first is about a set of scalars and a function assigning an eigenvector to each scalar, whereas the second one is about indexed families of scalars and eigenvectors. The second one is more promising for our application, since we also chose to use indexed families of vectors and scalars. To use this theorem to prove our version, we invoke the  tactic.</p><pre>example {R M : Type} [CommRing R]\n    [AddCommGroup M] [Module R M]\n    [NoZeroSMulDivisors R M]\n    (f : M →ₗ[R] M)\n    (μ : ι → R) (hμ : Function.Injective μ)\n    (v : ι → M) (hv : ∀ i, v i ≠ 0)\n    (h : ∀ i, f (v i) = μ i • v i) :\n    LinearIndependent R v := by\n  apply Module.End.eigenvectors_linearIndependent'</pre><p>This leaves us with a number of goals:</p><dl data-ams-content-type=\"enumerate\"><div><dd><p>, asking us to prove that something is injective.</p></dd></div><div><dd><p>, which looks like it should have something to do with our assumptions  and .</p></dd></div><div><dd><p>, asking us to provide an endomorphism of  this should just be our linear map .</p></dd></div><div><dd><p>And finally, , which will be our </p></dd></div></dl><p>The  in the first two goals and the  in the second goal mean that Lean does not yet know how to instantiate the variables  and  in the theorem we have invoked. It left those tasks as the third and fourth goals; Lean expects that it will be more convenient for us to provide that information implicitly when we solve the other goals.</p><p>We have made progress, but we still need to provide the information requested. We should clearly use  to solve the first goal, which we do by adding the next line to our proof:</p><pre>apply Module.End.eigenvectors_linearIndependent'\n  exact hμ</pre><p>As you might guess, the  tactic tells Lean to use the assumption  to close the goal. As a side effect, that also closes goal 4: Lean is now able to infer that the family in question is  We then have two goals left:</p><dl data-ams-content-type=\"enumerate\"><div><dd><p>, and</p></dd></div></dl><p>We know that the endomorphism of  should be  so we swap the order of the goals and give Lean this information:</p><pre>apply Module.End.eigenvectors_linearIndependent'\n  exact hμ\n  swap ; exact f</pre><p>This leaves us with one last goal, namely, . We need to use our assumptions  and , but it’s unclear how to package them together to satisfy the definition of . Instead of going back to the documentation page and looking up the definition, we can ask Lean what we have to do.</p><pre>apply Module.End.eigenvectors_linearIndependent'\n  exact hμ\n  swap ; exact f\n  intro i ; constructor</pre><p>As before, the  tactic introduces an arbitrary , and then the  tactic tells Lean that we are ready to provide the information needed to show that  is an eigenvector of  with eigenvalue . This makes progress, but we still have the following goals to fulfill:</p><dl data-ams-content-type=\"enumerate\"><div><dd><p>, which is obviously an application of .</p></dd></div></dl><p>For 1, the hypothesis  should do the trick, but applying it directly doesn’t work. After digging into the details, we see that this is because the eigenspace of  with respect to a scalar  is defined as the kernel of  so we will need to convert  to this form. We could do this manually, but we can first check whether we can use a preexisting lemma from the library. The fastest way to do this, in the middle of a Lean proof, is to try the  tactic. This will do a search for ways of applying existing lemmas to close the goal . This tactic doesn’t always work, but it can’t hurt to try. Aha! In this case it tells us right away that we can close the first goal by writing . The second goal is also easy to close by applying , so now we have a complete proof:</p><pre>apply Module.End.eigenvectors_linearIndependent'\n  exact hμ\n  swap ; exact f\n  intro i\n  constructor\n  exact Module.End.mem_eigenspace_iff.mpr (h i)\n  apply hv</pre><p>The beginning of our proof was a bit messy, but we can clean things up to obtain a nice final result:</p><pre>example {R M : Type} [CommRing R]\n    [AddCommGroup M] [Module R M]\n    [NoZeroSMulDivisors R M]\n    (f : M →ₗ[R] M)\n    (μ : ι → R) (hμ : Function.Injective μ)\n    (v : ι → M) (hv : ∀ i, v i ≠ 0)\n    (h : ∀ i, f (v i) = μ i • v i) :\n    LinearIndependent R v := by\n  apply\n    Module.End.eigenvectors_linearIndependent' f μ hμ\n  intro i\n  constructor\n  · exact Module.End.mem_eigenspace_iff.mpr (h i)\n  · apply hv</pre><p>The  tactic now supplies , , and  right away, leaving only one remaining goal.</p><p>We have thus achieved the level of generality we were after. Should this theorem be added to Mathlib? Upon consideration, we should conclude that our theorem is not substantially different from . In our proof, we only added some plumbing and changed the way we talk about eigenvectors to match the theorem in the library. Now that we know about , it seems that this is the standard way to talk about eigenvectors in Mathlib. If we were to modify our statement to use this instead of our bespoke  and , our theorem would be a direct application of . In other words, we have come to realize that the theorem in the library is what we really wanted all along. But the good news is that we have learned a lot in the process, and we are now much more comfortable reasoning about linear independence and eigenvectors with Mathlib.</p></section><section data-ams-doc=\"section\" data-ams-doc-level=\"1\"><p>Programmers and computer scientists have long made use of  platforms like GitHub to work on large, collaborative software projects. Formalization has brought the same tools and methodologies to mathematics. At the time of writing, Mathlib comprises approximately 5,000 files and 1.5 million lines, written by over 300 contributors. These contributions go through an open review process on GitHub, before they are merged into the main repository. In total there have been a bit more than 30,000 contributions since Mathlib’s inception in 2017.</p><p>A nice thing about version control tools is that they maintain the entire history of the project, allowing us to see what has changed and when. For example, we can trace the history of the concept of “linear independence” in Mathlib. It all started on December 7, 2017, when Johannes Hölzl committed⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid22\"></a> the file  to the repository. The file was 708 lines long, and on line 186 it contained a definition of linear independence.</p><pre>def linear_independent (s : set β) : Prop :=\n  ∀ l : lc α β, (∀x∉s, l x = 0) →\n    l.sum (λv c, c • v) = 0 → l = 0</pre><p>The terms  quantify over , the type of all linear combinations of elements of  with coefficients in .</p><p>On March 10, 2018, an administrative operation⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid23\"></a> moved  out of  so that it became a top-level folder. Another such move occurred on January 15, 2019, when Simon Hudon moved⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid24\"></a> all the mathematical content into  to separate it from the tests and other auxiliary files.</p><p>On July 3, 2019, Alexander Bentkamp morphed⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid25\"></a> the definition into</p><pre>def linear_independent : Prop :=\n  (finsupp.total ι M R v).ker = ⊥</pre><p>In other words, a collection of vectors  in  is linearly independent if the natural map from the free module generated by the vectors  to  has trivial kernel.</p><p>Then, on October 5, 2020, Anne Baanen split⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid26\"></a> linear independence and the accumulated supporting theory into a separate file: . The file was 918 lines long. The story continued on February 23, 2023, when Pol’tta / Miyahara Kō ported⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid27\"></a> the file to Lean 4 as part of a massive collaborative effort to move all of Mathlib to the new version of Lean. The filename is now , and it lives in the new  repository on GitHub. The definition has not changed substantially since the change by Bentkamp. At the time of writing, it reads as follows:</p><pre>def LinearIndependent : Prop :=\n  LinearMap.ker (Finsupp.total ι M R v) = ⊥</pre></section><section data-ams-doc=\"section\" data-ams-doc-level=\"1\"><p>The earliest programs for checking mathematical proofs include Nicolaas de Bruijn’s Automath system, launched in 1967, and Andrzej Trybulec’s Mizar system, launched in 1973. Since then, dozens of proof assistants have been developed; Coq, Isabelle, and HOL Light are among the more prominent ones still in use today. The Lean project, launched by Leonardo de Moura in 2013, is a relative newcomer. A special issue of the , with articles by John Harrison&nbsp;, Thomas Hales&nbsp;, and Freek Wiedijk&nbsp;, surveyed the state of the field in 2008. We have come a long way since then.</p><p>Proof assistants are now commonly used in industry to verify hardware, software, network protocols, cryptographic protocols, cyberphysical systems, and more. Mathematicians have only recently begun to embrace the technology, and it is becoming clear that there are several benefits to representing mathematics in digital form. Just as the word processor opened up new opportunities for written expression and communication, the digitization of mathematics opens up new opportunities for mathematical research and teaching.⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid29\"></a> The practical benefits are not the only motivation; to many of us, formalizing mathematics feels like the right thing to do. Mathematical definitions and theorems  to be rendered digitally.</p><p>It is also becoming clear that the technology is here to stay. Mathlib currently has roughly 1.5 million lines of code and continues to grow. Important results, including foundations for Clausen and Scholze’s condensed mathematics,⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid30\"></a> the polynomial Freiman–Ruzsa conjecture,⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid31\"></a> and an exponentially improved upper bound to Ramsey’s theorem,⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid32\"></a> have been formally verified before journal referees signed off on them. A number of collaborative verification projects have been launched, including a proof of the sphere eversion theorem⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid33\"></a> and a proof of a strengthened version of Carlson’s theorem on pointwise almost everywhere convergence of Fourier series.⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid34\"></a></p><p>We expect that, in the years to come, AI copilots that combine neural and symbolic methods will significantly ease the burden of formalization. More dramatically, we expect that the technology we have discussed here will play a significant role in the discovery of new mathematics. Note that DeepMind’s AlphaProof, which was deemed to have performed at the level of a silver medalist at the most recent International Mathematical Olympiad,⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid35\"></a> was trained to find formal proofs in Lean. It is exciting to think about what a synergetic combination of machine learning, symbolic methods, and user interaction will bring to mathematics in the years ahead.</p><p>You can find online documentation and tutorials for all the proof assistants we have just mentioned. Lean’s lively social media channel on the Zulip platform is welcoming to newcomers, and the Lean community web pages contain links⁠<a role=\"doc-noteref\" data-ams-ref=\"fn\" href=\"https://www.ams.org/journals/notices/202502/noti3114/noti3114.html#ltxid36\"></a> to learning resources, like the Natural Number Game, to help you get started. Proof assistants are not easy to use, and learning to formalize mathematics requires significant time and effort. Interaction with proof assistants like Lean comes naturally, however, to those who have grown up immersed in computational technology. We have therefore found that one of the best ways to take advantage of proof assistants is to have our students help us out. Formal mathematics is a language, and only they can claim to be among the first generation of native speakers, while the rest of us struggle to master the grammar and intonation.</p><p>With all the changes looming, we ought to be concerned about the ways that proof assistants and AI will change the mathematics that we know and love. It is therefore all the more important for those of us who are more settled in our careers to play an active role in the adoption of the new technologies, using our mathematical values and expertise to guide our students as they negotiate the changing landscape. The new developments offer us a wonderful opportunity to lead from behind, and it falls on all of us to support the next generation of mathematicians as they forge a path into the digital future.</p></section><section role=\"doc-acknowledgments\" data-ams-doc=\"section\" data-ams-doc-level=\"1\"><p>We are grateful to the Hausdorff Research Institute for Mathematics for hosting three of us for the trimester program, “Prospects of Formal Mathematics,” in the summer of 2024, during which most of this article was written. We are also grateful to Paul Buckingham and three anonymous referees for helpful comments, corrections, and suggestions.</p></section><section role=\"doc-bibliography\" data-ams-content-type=\"biblist\" data-ams-doc-level=\"1\"><dl><dd><div data-ams-doc=\"biblioentry\"> Jeremy Avigad, <i>Mathematics and the formal turn</i>, Bull. Amer. Math. Soc. (N.S.)  (2024), no.&nbsp;2, 225–240, DOI <a href=\"https://doi.org/10.1090/bull/1832\">10.1090/bull/1832</a>. MR<a href=\"https://mathscinet.ams.org/mathscinet-getitem?mr=4726989\">4726989</a>, </div></dd><dd><div data-ams-doc=\"biblioentry\"> Joshua Brakensiek, Marijn Heule, John Mackey, and David Narváez, <i>The resolution of Keller’s conjecture</i>, J. Automat. Reason.  (2022), no.&nbsp;3, 277–300, DOI <a href=\"https://doi.org/10.1007/s10817-022-09623-5\">10.1007/s10817-022-09623-5</a>. MR<a href=\"https://mathscinet.ams.org/mathscinet-getitem?mr=4449705\">4449705</a>, </div></dd><dd><div data-ams-doc=\"biblioentry\"> Kevin Buzzard, <i>What is the point of computers? A question for pure mathematicians</i>, ICM—International Congress of Mathematicians. Vol. 2. Plenary lectures, EMS Press, Berlin, 2023, pp.&nbsp;578–608. MR<a href=\"https://mathscinet.ams.org/mathscinet-getitem?mr=4680264\">4680264</a>, </div></dd><dd><div data-ams-doc=\"biblioentry\"> Thomas C. Hales, , Notices Amer. Math. Soc.  (2008), no.&nbsp;11, 1370–1380. MR<a href=\"https://mathscinet.ams.org/mathscinet-getitem?mr=2463990\">2463990</a>, </div></dd><dd><div data-ams-doc=\"biblioentry\"> John Harrison, <i>Formal proof—theory and practice</i>, Notices Amer. Math. Soc.  (2008), no.&nbsp;11, 1395–1406. MR<a href=\"https://mathscinet.ams.org/mathscinet-getitem?mr=2463992\">2463992</a>, </div></dd><dd><div data-ams-doc=\"biblioentry\"> Patrick Massot, <i>Why formalize mathematics</i> (2021)., </div></dd><dd><div data-ams-doc=\"biblioentry\"> Freek Wiedijk, <i>Formal proof—getting started</i>, Notices Amer. Math. Soc.  (2008), no.&nbsp;11, 1408–1417. MR<a href=\"https://mathscinet.ams.org/mathscinet-getitem?mr=2463993\">2463993</a>, </div></dd></dl></section><section data-ams-doc=\"refhead\" data-ams-doc-level=\"1\"><p>Photo of Jeremy Avigad is courtesy of Carnegie Mellon University.</p><p>Photo of Johan Commelin is courtesy of Johan Commelin.</p><p>Photo of Heather Macbeth is courtesy of Peter Insley.</p><p>Photo of Adam Topaz is courtesy of Adam Topaz.</p></section>","contentLength":31464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42815755"},{"title":"Podcast with Luca Rossi","url":"https://refactoring.fm/p/growing-the-development-forest-with","date":1737741660,"author":"Martin Fowler","guid":301,"unread":true,"content":"<p>Luca Rossi hosts a podcast (and newsletter) called Refactoring, so it's\n      obvious that we have some interests in common. The tile comes from me\n      leaning heavily on Beth Anders-Beck and Kent Beck's metaphor of <a href=\"https://refactoring.fm/bliki/ForestAndDesert.html\">The Forest and The Desert</a>. We talk\n      about the impact of AI on software development, the metaphor of technical\n      debt, and the current state of agile software development.</p>","contentLength":397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Backup - Tooling and recommendations","url":"https://www.reddit.com/r/kubernetes/comments/1i919r0/kubernetes_backup_tooling_and_recommendations/","date":1737740794,"author":"/u/flxptrs","guid":569,"unread":true,"content":"<p>I would love to hear your input on kubernetes backups. We run a multi tenant cluster. Most of the services are based on operators, so the tenants deploy and operate whatever they need. Pretty nice in terms of platform operations. </p><p>The only weak spot is our backup strategy. We use velero, but we are not happy. There are multiple issues and shortcomings for multi tenancy, but also other bugs which make it a ongoing pain. </p><p>So my question is: what do you use for backups and what's your strategy? Any recommendations especially for multi tenant scenarios? </p>","contentLength":554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Subpixel Snake [video]","url":"https://www.youtube.com/watch?v=iDwganLjpW0","date":1737739304,"author":"codetrotter","guid":195,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42815288"},{"title":"A WebAssembly compiler that fits in a tweet","url":"https://wasmgroundup.com/blog/wasm-compiler-in-a-tweet/","date":1737737476,"author":"todsacerdoti","guid":194,"unread":true,"content":"<p>One of the initial explorations that started this book was how small and simple a compile-to-WebAssembly language implemented in JavaScript could be. Our first “WebAssembly compiler in a tweet” was 269 bytes; since then, we’ve managed to whittle it down to a measly 192 bytes.</p><p>The final result is a compiler that takes an arithmetic expression — written in reverse polish notation — and compiles it down to a valid WebAssembly module. That module exports a single function which returns the result of the original arithmetic expression. Here it is:</p><p>And here’s an example of how you can use it:</p><p>But this is not just a clever trick — if you take the time to understand what this code does, you’ll learn a surprising amount about WebAssembly! In the rest of the post, we’ll explain how it all works by de-obfuscating the code one step at a time.</p><p>The first thing we can do to make it more readable is to format it:</p><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div><p>While it’s still pretty unreadable, now we can at least identify different parts of the code.</p><p>At a high level, what we’re doing is ‘parsing’ the expression in a very simple way, turning it into the appropriate Wasm bytecode,\nand then hand-crafting the bytes for a single-function module.</p><p>In a more complex compiler you would probably use a library to generate the WebAssembly module and compile the expressions but our main metric\nhere is code size so we write the bytes directly in an array.</p><p>In JavaScript the assignment operator is an expression. This means that it generates a result after evaluating, as you can see in the following examples:</p><p>The code above will output:</p><p>This is because  assigns  to  and the whole assignment expression evaluates to the value being assigned.</p><p>In , we assign the result of evaluating  to . This equivalent expression may be easier to understand: .</p><p>In our code, we use this trick to reuse variables and update their value in places where\nwe can also use the value being assigned. It also allows us to have our compiler in a single expression, avoiding the need for curly braces, semicolons and return statements.</p><p>To undo it, we turn the body of our function into a block and do each assignment on its own line:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>Now the assignments are easier to identify but the meaning of variables and function\narguments are still hard to understand. Let’s fix that by undoing a couple of variable tricks.</p><p>The first step is to stop using single letter variables, and to use more descriptive names instead. The next step is to stop reusing variables: for example,  initially holds the code to compile, but once we don’t need that any more we reuse it to hold the bytecode instructions.</p><p>To undo this we are going to introduce a new  variable and rename  to . We’ll also rename  to . This variable contains a value that is close to the number of bytecodes.</p><p>By declaring  in the body we can remove it from the function argument’s list. We did this\nas a trick to avoid the need to declare it with  or , saving some bytes and the need for a function body.</p><p>The trick works by adding unused arguments at the end of the function argument list and using them as local variables. Our compiler function expects a single argument with the code;  is there for us to use since we don’t expect the caller to provide any value for it.</p><p>Here’s the code without this trick:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>If you look at the array in our code, you may notice that there are many commas followed by another comma instead of a value. This syntax defines “sparse arrays”. Here’s an example:</p><p>We use this syntactic trick to save one byte each time we need a  to appear in the array. This works because <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Typed_arrays\" target=\"_blank\" rel=\"noopener noreferrer\">Typed Arrays</a> coerce all array items to numbers, and an “empty item” will be converted to 0:</p><p>Let’s undo this trick by adding all the zeroes back:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>In our code, we have a variable  that contains a number that is close to the number\nof bytecodes in the compiled expression, but not exactly the same:</p><p>In the WebAssembly module we need to use the number of bytes in the function body (the expression to evaluate) in two places:</p><ul><li>To define the code section’s length</li><li>To define the function body’s length</li></ul><p>Since there’s only one function in the code section both values are similar:</p><ul><li>The section takes two extra bytes (section identifier and number of code entries)</li><li>The function body takes another two bytes (number of locals and  instruction)</li></ul><p>To avoid writing  twice we assign to  the value of  in the place where we need the code section byte count\nand then calculate  () where we need the function body byte count.</p><p>This is all a trick to avoid having to write  twice.</p><p>let’s assign the length to  and calculate the right value in each place:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>Let’s see how it works by creating a simple tagged template that turns the string to uppercase:</p><p>As you can see, the first argument to the tagged template function is an array. Luckily for us, the first argument of <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/split#separator\" target=\"_blank\" rel=\"noopener noreferrer\">String.prototype.split</a> is handled in the following way:</p><blockquote><p>All values that are not undefined or objects with a  method are coerced to strings.</p></blockquote><p>And coercing an array with one string in it is the same as the string itself:</p><p>Since the function we want to call takes a single string argument, we can use it as a tagged template and save the parentheses in the function call.</p><p>Let’s write it as a function call instead:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>The ternary operator has expressions on each branch saving us the  statements. Here’s what the code looks like when we use an  statement instead:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>The next trick to undo is the one present twice in the following code:</p><p>First we use coercion in  to check if the token  is a string representing a\npositive number. Then we use coercion again in  to let JavaScript turn  into a  in the :</p><p>The code above evaluates to:</p><p>Let’s write the parsing and checking explicitly:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>The semantics of our compiler change a little bit here. The original version will only accept\npositive integers as input; if you want a negative number you have to subtract from zero:  to get . The new version allows negative numbers since it checks with  instead of .</p><p>The next trick is in the  branch:</p><p>Our calculator compiler only accepts four arithmetic operations: , , , and . But\nin the code above you can only see three:  and a magical number: . Here’s how it works — these are the bytecode numbers for arithmetic operations in WebAssembly:</p><p>We only enter this branch if the token  is not a number, which means it can only be\none of the arithmetic operators above. So, given a single character which is one of those four operators, we want to produce the appropriate opcode.</p><p>We  have written . That is, we find the symbol’s index in the string:</p><p>…and add  to it to get the bytecode number. But when  is not in the string,  returns . We can use that to our advantage, and treat  to mean “plus or any other token”:</p><ul><li>:  (any other token will be  too)</li></ul><p>And that’s why we add  instead of . Let’s undo the  trick:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>Here again the semantics change a little bit. Before, if the token  wasn’t found, the expression would evaluate to  which would map to an addition. Now it will evaluate to  which will map to bytecode  which is the <a href=\"https://developer.mozilla.org/en-US/docs/WebAssembly/Reference/Numeric/Population_count\" target=\"_blank\" rel=\"noopener noreferrer\"></a> instruction.</p><p>But don’t worry, we’ll fix it in the next step.</p><p>After explaining how the  trick works and removing the  part, let’s\ngo ahead and remove the trick completely. To do it we are going to create an object that maps from an arithmetic operation token to its bytecode:</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>To keep the initial semantics, if the token is not a valid operation we return the bytecode for : in .</p><p>From the usage example at the beginning of the post, you may have noticed that the exported\nfunction’s name is the empty string:</p><p>We did this to save us the bytes needed to specify the export name,\nbut also to save an extra byte/character in the code because with the length of the export name being \nwe can use the sparse array syntax to leave an empty spot in the WebAssembly module array.</p><p>To revert this trick we are going to name the exported function as , which in UTF-8 is the byte :</p><section><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div></section><p>We can now call it with a nicer name:</p><p>Our initial implementation only supported positive numbers, but that’s not the only number restriction in our compiler.</p><p>To keep WebAssembly modules as small as possible, numbers are encoded using a variable-length encoding algorithm called <a href=\"https://en.wikipedia.org/wiki/LEB128\" target=\"_blank\" rel=\"noopener noreferrer\">LEB128</a>. You can tell we are not implementing the whole algorithm by looking at the part of the code that encodes numbers: . We’re assuming the number being encoded fits in 7 bits, the shortest possible LEB128 representation.</p><p>Let’s try the limits of our implementation:</p><p>This means the only numbers that will be parsed correctly are from  to .</p><blockquote><p>Uncaught CompileError: WebAssembly.instantiate(): Compiling function #0 failed: function body must end with “end” opcode @+33</p></blockquote><p>In the last one we went over the 7 bits and the module was rejected during validation.</p><p>Explaining and implementing LEB128 takes a lot of text and code. If you want to read more\nabout it we have a whole deep dive on LEB128 in <a href=\"https://wasmgroundup.com/\" target=\"_blank\" rel=\"noopener noreferrer\">our book</a>.</p><p>During the code golfing phase I had a literal shower thought but sadly it didn’t work.</p><p>The idea was to simplify  by using the UTF-8 character code plus an offset like this:  and saving 3 bytes in the process. The reason it didn’t work is that the characters  don’t appear in the same order in UTF-8 and WebAssembly bytecode.</p><p>The last part to expand/explain is the array of numbers used to build the WebAssembly module.</p><p>It takes a big part of a <a href=\"https://www.w3.org/TR/2019/REC-wasm-core-1-20191205/\" target=\"_blank\" rel=\"noopener noreferrer\">specification</a> to explain every byte in the array, but here’s a commented version that should give you a high level idea of what each part does:</p><div data-ch-theme=\"nord\"><div data-ch-measured=\"false\"><code></code></div></div><p>There you go! We’ve turned a rather opaque 192-byte snippet into something that’s almost readable. And in the process, you hopefully learned a little bit about WebAssembly.</p><p>If we dropped the size restrictions, there are lots of things we might want to improve in this compiler: handle numbers greater than 127, add nicer syntax, add support for conditionals, loops, etc. If you’re interested in what that might look like, I encourage you to check out our book <a href=\"https://wasmgroundup.com/\" target=\"_blank\" rel=\"noopener noreferrer\">WebAssembly from the Ground Up</a>. You’ll learn the ins and outs of WebAssembly by writing a real compiler for a simple programming language. It’s a lot of fun!</p><p>Special thanks to <a href=\"https://bsky.app/profile/orthoplex.bsky.social\" target=\"_blank\" rel=\"noopener noreferrer\">lexi</a> for contributing some of the tricks used above.</p>","contentLength":10187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42814948"},{"title":"Snowdrop OS – a homebrew operating system from scratch, in assembly language","url":"http://sebastianmihai.com/snowdrop/","date":1737736811,"author":"ksec","guid":193,"unread":true,"content":"\n\nWelcome to the pages of Snowdrop OS, my homebrew operating system project.\nSnowdrop OS was born of my childhood curiosity around what happens when a PC is turned on, the mysteries of bootable disks, and the hidden aspects of operating systems. It is a 16-bit real mode operating system for the IBM PC architecture. I designed and developed this homebrew OS from scratch, using only x86 assembly language. \n<p>I have created and included a number of utilities, including a file manager, text editor, graphical applications, BASIC interpreter, x86 assembler and debugger. I also ported one of my DOS games to it. After all, what kind of an operating system doesn't have games?\n</p><p>The Snowdrop OS and the apps are distributed as both a floppy disk (1.44Mb) image, as well as a CD-ROM image. The images contain the following, all programmed from scratch:\n</p><ul><li>a boot loader which loads the kernel into memory</li><li>a kernel which sets up interrupt vectors to be used by user apps, and then loads the startup app</li><li>user apps, including a shell (command line interface), utilities, test apps, and aSMtris, my Tetris clone</li></ul>\nSnowdrop OS can also be installed to a hard disk - prompting the user to do so during boot - if it detects one. \n<p>I hope that Snowdrop can serve other programmers who are looking to get a basic understanding of operating system functions. Like my other projects, the source code is fully available, without any restrictions on its usage and modification.\n</p>\nSome interesting areas in the <a href=\"http://sebastianmihai.com/snowdrop/src\">source code</a> are: <div>v1 - initial version, single tasking, shell, aSMtris\nv2 - PS/2 mouse driver and mouse test apps\n<p>v3 - basic multi-tasking support and virtual display support\n</p>v4 - FAT12 driver write/delete, file manager, text editor\n<p>v5 - serial port driver, formatting utilities, file copy support\n</p>v6 - multiplayer snake game (over serial port)\n<p>v7 - slide show presentation app\n</p>v8 - \"keep memory\" task lifetime mode, for custom services\n<p>v9 - parallel port driver, BMP image support, sprites\n</p>v10 - system timer frequency change\n<p>v11 - animated sprites, sound driver (internal speaker)\n</p>v12 - keyboard driver\n<p>v13 - more sprites functionality, Storks game\n</p>v14 - kernel config, program arguments, file utilities\n<p>v15 - 16x2 LCD controller app, text editor fixes\n</p>v16 - GUI framework\n<p>v17 - Snowmine (Minesweeper-like game)\n</p>v18 - BASIC interpreter and linker\n<p>v19 - install to hard disk\n</p>v20 - BASIC and text editor improvements\n<p>v21 - integration of BASIC and GUI framework\n</p>v22 - x86 assembler, multi-disk support, file view utilities\nv24 - service loading\n<p>v25 - dynamic memory and data structures\n</p>v26 - installer improvements, pseudo-mouse driver\n<p>v27 - kernel and inter-task messaging\n</p>v28 - GUI higher resolution, draw application, desktop application\n<p>v29 - data compression, Hangman game\n</p>v30 - pseudo-mouse driver improvements\n<p>v31 - runtime libraries (RTL), BASIC interpreter RTL\n</p></div>","contentLength":2851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42814820"},{"title":"AI is Creating a Generation of Illiterate Programmers","url":"https://nmn.gl/blog/ai-illiterate-programmers","date":1737736437,"author":"/u/namanyayg","guid":586,"unread":true,"content":"<p>A couple of days ago, Cursor went down during the ChatGPT outage.</p><p>I stared at my terminal facing those red error messages that I hate to see. An AWS error glared back at me. I didn’t want to figure it out without AI’s help.</p><p>After 12 years of coding, I’d somehow become worse at my own craft. And this isn’t hyperbole—this is the new reality for software developers.</p><p>First, I stopped reading documentation. Why bother when AI could explain things instantly?</p><p>Then, my debugging skills took the hit. Stack traces now feel unapproachable without AI. I don’t even read error messages anymore, I just copy and paste them.</p><p>I’ve become a human clipboard, a mere intermediary between my code and an LLM.</p><p>Previously, every error message used to teach me something. Now? The solution appears magically, and I learn nothing. The dopamine hit of instant answers has replaced the satisfaction of genuine understanding.</p><p>Deep comprehension is the next thing that was affected. Remember spending hours understanding why a solution works? Now, I simply implement AI suggestions. If they don’t work, I <a href=\"https://nmn.gl/blog/ai-senior-developer\">improve the context</a>, and just ask the AI again. It’s a cycle of increasing dependency.</p><p>Then come the emotional changes. Previously, it was a part of the  of programming to solve new problems. Now, I get frustrated if AI doesn’t give me a solution in 5 minutes.</p><p>The scariest part? I’m building an <a href=\"https://nmn.gl/blog/giga\">AI-powered development tool</a>, but I can’t shake the feeling I’m contributing to the very problem that’s eroding our collective skills.</p><p>I’m not suggesting anything radical like going AI-free completely—that’s unrealistic. Instead, I’m starting with “No-AI Days.” One day a week where:</p><ul><li>Read every error message completely</li><li>Use actual debuggers again</li><li>Read source code instead of asking AI</li></ul><p>I won’t lie, it sucks. I feel slower, dumber, and more frustrated.</p><p>But I can also see the difference. I feel a stronger connection with my code and a sense of ownership, which had slowly disappeared with AI. Plus, I learn a lot more.</p><p>We’re not becoming 10x developers with AI.</p><p>We’re becoming 10x  on AI. </p><p>Every time we let AI solve a problem we could’ve solved ourselves, we’re trading long-term understanding for short-term productivity. We’re optimizing for today’s commit at the cost of tomorrow’s ability.</p><p>I’m not suggesting we abandon AI tools—that ship has sailed. But we need rules of engagement. Here’s some ideas that I have:</p><ul><li>No AI for problems that you haven’t tried to understand first</li><li>Read and understand all AI-suggested solutions</li><li>Regular periods of coding without AI assistance</li><li>Focus on learning patterns, not just fixing immediate issues</li></ul><p>I won’t lie, I don’t think I’ll be able to follow these rules all the time. But it’s a start, and I strongly believe anyone who’s new to programming should  follow all of these rules.</p><p>Right now, somewhere, a new programmer is learning to code. They’ll never know the satisfaction of solving problems truly on their own. They’ll never experience the deep understanding that comes from wrestling with a bug for hours.</p><p>We’re creating a generation of developers who can ask AI the right questions but can’t understand the answers. Every time AI goes down, they’re exposed as increasingly helpless. As of now, <a href=\"https://nmn.gl/blog/ai-midlevel-engineer\">AI isn’t capable enough</a> to replace programmers fully, but this will only get worse as it improves. The real question isn’t whether AI will replace programmers. It’s whether we’re replacing ourselves.</p><p>Try coding without AI for just one day. The results might surprise you.</p>","contentLength":3557,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i8zi9v/ai_is_creating_a_generation_of_illiterate/"},{"title":"Wild – A fast linker for Linux","url":"https://github.com/davidlattimore/wild","date":1737735953,"author":"hkalbasi","guid":192,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42814683"},{"title":"How do people deploy a prometheus stack?","url":"https://www.reddit.com/r/kubernetes/comments/1i8z5xa/how_do_people_deploy_a_prometheus_stack/","date":1737735580,"author":"/u/HardChalice","guid":573,"unread":true,"content":"<p>I'm running a homelab on microk8s just to get experience with kubernetes. Currently have Traefik setup as my ingress with their IngressRoutes with a gitea and argocd instance for my CI/CD.</p><p>I've been looking into deploying a prometheus/loki/grafana stack and I'm torn on the best way to deploy it. I know there is the kube-peometheus operator but that would circumvent my argoCD. There is a helm chart for it but that's community maintained and not official. Or do I implement them all from scratch for the experience? </p><p>So I wanted to see how others have implemented in both production and homelab-like environments. </p>","contentLength":614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New book-sorting algorithm almost reaches perfection","url":"https://www.quantamagazine.org/new-book-sorting-algorithm-almost-reaches-perfection-20250124/","date":1737733823,"author":"isaacfrond","guid":191,"unread":true,"content":"<p>Computer scientists often deal with abstract problems that are hard to comprehend, but an exciting new algorithm matters to anyone who owns books and at least one shelf. The algorithm addresses something called the library sorting problem (more formally, the “list labeling” problem). The challenge is to devise a strategy for organizing books in some kind of sorted order — alphabetically, for instance — that minimizes how long it takes to place a new book on the shelf.</p><p>Imagine, for example, that you keep your books clumped together, leaving empty space on the far right of the shelf. Then, if you add a book by Isabel Allende to your collection, you might have to move every book on the shelf to make room for it. That would be a time-consuming operation. And if you then get a book by Douglas Adams, you’ll have to do it all over again. A better arrangement would leave unoccupied spaces distributed throughout the shelf — but how, exactly, should they be distributed?</p><p>This problem was introduced in a <a href=\"https://link.springer.com/chapter/10.1007/3-540-10843-2_34\">1981 paper</a>, and it goes beyond simply providing librarians with organizational guidance. That’s because the problem also applies to the arrangement of files on hard drives and in databases, where the items to be arranged could number in the billions. An inefficient system means significant wait times and major computational expense. Researchers have invented some efficient methods for storing items, but they’ve long wanted to determine the best possible way.</p><p>Last year, in <a href=\"https://arxiv.org/abs/2405.00807\">a study</a> that was presented at the Foundations of Computer Science conference in Chicago, a team of seven researchers described a way to organize items that comes tantalizingly close to the theoretical ideal. The new approach combines a little knowledge of the bookshelf’s past contents with the surprising power of randomness.</p><p>“It’s a very important problem,” said <a href=\"https://web.eecs.umich.edu/~pettie/\">Seth Pettie</a>, a computer scientist at the University of Michigan, because many of the data structures we rely upon today store information sequentially. He called the new work “extremely inspired [and] easily one of my top three favorite papers of the year.”</p><p>So how does one measure a well-sorted bookshelf? A common way is to see how long it takes to insert an individual item. Naturally, that depends on how many items there are in the first place, a value typically denoted by . In the Isabel Allende example, when all the books have to move to accommodate a new one, the time it takes is proportional to . The bigger the , the longer it takes. That makes this an “upper bound” to the problem: It will never take longer than a time proportional to  to add one book to the shelf.</p><p>The authors of the 1981 paper that ushered in this problem wanted to know if it was possible to design an algorithm with an average insertion time much less than. And indeed, they proved that one could do better. They created an algorithm that was guaranteed to achieve an average insertion time proportional to (log ). This algorithm had two properties: It was “deterministic,” meaning that its decisions did not depend on any randomness, and it was also “smooth,” meaning that the books must be spread evenly within subsections of the shelf where insertions (or deletions) are made. The authors left open the question of whether the upper bound could be improved even further. For over four decades, no one managed to do so.</p><p>However, the intervening years did see improvements to the lower bound. While the upper bound specifies the maximum possible time needed to insert a book, the lower bound gives the fastest possible insertion time. To find a definitive solution to a problem, researchers strive to narrow the gap between the upper and lower bounds, ideally until they coincide. When that happens, the algorithm is deemed optimal — inexorably bounded from above and below, leaving no room for further refinement.</p><p>In 2004, a team of researchers found that the <a href=\"https://epubs.siam.org/doi/abs/10.1137/S0895480100315808?journalCode=sjdmec\">best any algorithm could do</a> for the library sorting problem — in other words, the ultimate lower bound — was log . This result pertained to the most general version of the problem, applying to any algorithm of any type. Two of the same authors had already secured a result for a more specific version of the problem in 1990, showing that for any smooth algorithm, <a href=\"https://link.springer.com/chapter/10.1007/3-540-52846-6_87\">the lower bound is significantly higher</a>: (log ). And in 2012, another team <a href=\"https://dl.acm.org/doi/abs/10.1145/2213977.2214083\">proved the same lower bound</a>, (log ), for any deterministic algorithm that does not use randomness at all.</p><p>These results showed that for any smooth or deterministic algorithm, you could not achieve an average insertion time better than (log ), which was the same as the upper bound established in the 1981 paper. In other words, to improve that upper bound, researchers would need to devise a different kind of algorithm. “If you’re going to do better, you have to be randomized and non-smooth,” said <a href=\"https://www.cs.stonybrook.edu/people/faculty/michaelbender\">Michael Bender</a>, a computer scientist at Stony Brook University.</p><figure></figure><p>But getting rid of smoothness, which requires items to be spread apart more or less evenly, seemed like a mistake. (Remember the problems that arose from our initial example — the non-smooth configuration where all the books were clumped together on the left-hand side of the shelf.) And it also was not obvious how leaving things to random chance — essentially a coin toss — would help matters. “Intuitively, it wasn’t clear that was a direction that made sense,” Bender said.</p><p>Nevertheless, in 2022, Bender and five colleagues decided to try out a randomized, non-smooth algorithm anyway, just to see whether it might offer any advantages.</p><p>Ironically, progress came from another restriction. There are sound privacy or security reasons why you may want to use an algorithm that’s blind to the history of the bookshelf. “If I had on my bookshelf and took it off,” said <a href=\"https://csd.cmu.edu/people/faculty/william-kuszmaul\">William Kuszmaul</a> of Carnegie Mellon University, nobody would be able to tell.</p><p>In a 2022 paper, Bender, Kuszmaul and four co-authors created just such an algorithm — one that was “history independent,” non-smooth and randomized — which finally <a href=\"https://arxiv.org/abs/2203.02763\">reduced the 1981 upper bound</a>, bringing the average insertion time down to (log ).</p><p>Kuszmaul remembers being surprised that a tool normally used to ensure privacy could confer other benefits. “It’s as if you used cryptography to make your algorithm faster,” he said. “Which just seems kind of strange.”</p><p><a href=\"https://www.cc.gatech.edu/people/helen-xu\">Helen Xu</a> of the Georgia Institute of Technology, who was not part of this research team, was also impressed.&nbsp; She said that the idea of using history independence for reasons other than security may have implications for many other types of problems.</p><p>Bender, Kuszmaul and others made an even bigger improvement with last year’s paper. They again broke the record, lowering the upper bound to (log ) times (log log ) — equivalent to (log ). In other words, they came exceedingly close to the theoretical limit, the ultimate lower bound of log .</p><p>Once again, their approach was non-smooth and randomized, but this time their algorithm relied on a limited degree of history dependence. It looked at past trends to plan for future events, but only up to a point. Suppose, for instance, you’ve been getting a lot of books by authors whose last name starts with N — Nabokov, Neruda, Ng. The algorithm extrapolates from that and assumes more are probably coming, so it’ll leave a little extra space in the N section. But reserving too much space could lead to trouble if a bunch of A-name authors start pouring in. “The way we made it a good thing was by being strategically random about how much history to look at when we make our decisions,” Bender said.</p><p>The result built on and transformed their previous work. It “uses randomness in a completely different way than the 2022 paper,” Pettie said.</p><p>These papers collectively represent “a significant improvement” on the theory side, said <a href=\"https://computerscience.uchicago.edu/people/brian-wheatman/\">Brian Wheatman</a>, a computer scientist at the University of Chicago. “And on the applied side, I think they have the potential for a big improvement as well.”</p><p>Xu agrees. “In the past few years, there’s been interest in using data structures based on list labeling for storing and processing dynamic graphs,” she said. These advances would almost certainly make things faster.</p><p>Meanwhile, there’s more for theorists to contemplate. “We know that we can almost do log ,” Bender said, “[but] there’s still this tiny gap” — the diminutive log log term that stands in the way of a complete solution. “We don’t know if the right thing to do is to lower the upper bound or raise the lower bound.”</p><p>Pettie, for one, doesn’t expect the lower bound to change. “Usually in these situations, when you see a gap this close, and one of the bounds looks quite natural and the other looks unnatural, then the natural one is the right answer,” he said. It’s much more likely that any future improvements will affect the upper bound, bringing it all the way down to log  “But the world’s full of weird surprises.”</p>","contentLength":9019,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42814275"},{"title":"Show HN: Cs16.css – CSS library based on Counter Strike 1.6 UI","url":"https://cs16.samke.me/","date":1737733027,"author":"samke-","guid":184,"unread":true,"content":"<div>\n                Lorem ipsum dolor sit amet consectetur adipisicing elit.\n                Distinctio ad suscipit aut asperiores laudantium error amet\n                sapiente et tempora numquam voluptates, velit sint quos\n                exercitationem unde obcaecati deleniti maiores officia natus\n                ipsa rem fuga commodi esse. Sunt repellendus ipsa illo a\n                accusantium consequuntur nihil dicta necessitatibus porro,\n                saepe, sed repudiandae!\n              </div>","contentLength":501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42814110"},{"title":"Vim in Zed: Improving the experience (The 2025 Roadmap)","url":"https://zed.dev/blog/vim-2025","date":1737732080,"author":"/u/marcospb19","guid":581,"unread":true,"content":"<p>Zed’s Vim mode continues to improve! My newest favorite feature is the  operator, which was the \"last holdout\" for common Vim operators:</p><p>But we’ve also been making progress on fleshing out the command palette with things like  and  to execute a command on matching (or non-matching) lines.</p><p>The broader community has also been working on pulling in top Vim plugins. We have versions of vim-surround, sneak, and as of recently, tree-sitter text objects:</p><p>All in all, if you use Vim today, the chances are that most of what you want to do on a daily basis in the editor will work in Zed exactly as it does in Vim.</p><p>There are three areas of focus I’d like to push on in 2025 for Vim in Zed:</p><ol><li>Building out the non-editor user experience</li><li>Raising the bar on matching Vim edge-case-for-edge-case.</li><li>Creating a new, better, multi-cursor Vim integration</li></ol><p>Although Vim seems to have “no UI” this is a sleight of hand. After  you’re in ex mode, which is full of features that make it work well. In particular, I want to bring to Zed:</p><ul><li>Filename completion in the command palette , or  for example</li><li>Command history, so you can fix a  without typing it out again</li><li> to run a sequence of keystrokes (and a few other missing commands)</li></ul><p>There are also other Vim UI’s to bring into Zed:</p><ul><li> to see your registers.</li></ul><p>Interestingly both registers and marks are more powerful in Vim than Zed today because they persist state to disk across a restart. We need to build that too.</p><p>Finally Zed has significant surface area that Vim does not have, and I'd like them to feel Vim-native:</p><ul><li>Keyboard shortcuts for collaboration features like ‘follow next person’, and a keyboard driven way to join and leave calls, mute/share etc.</li><li>Better shortcuts for the AI edit predictor, and handling in normal mode. Similarly for the inline assistant, which also tends to bug out in Vim mode today.</li><li>Bringing more netrw like bindings to the project panel</li><li>Tighter integration with the upcoming Git panel</li></ul><p>People who switch to Zed from Vim are attracted by the “just works by default”\naspect of Zed: language servers just work, and the advanced features (AI,\ncollaboration). But, nothing is more frustrating than a Vim mode that doesn’t\nwork   like Vim. We get a lot of VS Code Vim extension refugees, and the number\none complaint is that “it just didn’t feel right”.</p><p>Zed is already much closer to Vim. We have extensive \"side-by-side\" testing\nwhere we run headless Neovim to ensure our keyboard shortcuts do exactly the\nsame thing. That said, there's always more to do, both to add the remaining\nminor motions / come to mind, and fix edge cases in things like .</p><p>Zed has multi-cursor, and Zed’s Vim mode works “as you’d expect” with multiple\ncursors for the most part. The problem I'd like to solve is two-fold:</p><ul><li>Tidying up the bindings so there's a coherent feature set around Vim with multi-cursor.</li><li>In particular making sure you can easily rotate into multi-cursor as though it were another mode.</li><li>Adding to Zed support for moving just one of your selections in case you make a mistake selecting.</li></ul><p>There are a few editors that have prior art here, particularly Kakoune and\nHelix; but also Vim plugins that I want to get inspiration from.</p><p>If you'd like to help work on any of this, you can <a href=\"https://cal.com/conradirwin/pairing\">book time to pair with\nme</a>. Also feel free to peruse the list in\nthe <a href=\"https://zed.dev/channel/vim-393/notes\">Vim channel notes</a> and pick up\nthings that sound interesting to you. While I am happy to accept PRs out of the\nblue, I always prefer talking things through first.</p><p>If there's something I'm missing, or not prioritizing, please add it to the\n<a href=\"https://github.com/zed-industries/zed\">issue tracker</a>, or upvote things that\nare already there.</p>","contentLength":3595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i8xsw3/vim_in_zed_improving_the_experience_the_2025/"},{"title":"How I Use Home Assistant in 2025","url":"https://vpetersson.com/2025/01/22/how-i-use-home-assistant-in-2025.html","date":1737730293,"author":"ingve","guid":190,"unread":true,"content":"<p>I’ve been using Home Assistant for about seven years now, starting back when I was living in a small apartment. At the time, my setup was modest: I used the  (when it first launched) to tie together all my apartment’s lights. As I got more comfortable with automations, I also began building <a href=\"https://vpetersson.com/2019/11/16/home-assistant-and-esphome.html\">custom hardware like temperature and humidity sensors</a>.</p><p>However, once I started adding more complexity (more devices, more automations), I realized that running Home Assistant on a Raspberry Pi just wasn’t viable anymore. This was before Home Assistant offered their own hardware (which I haven’t tried, so I can’t say much about it). But for me, the main issue was the database. By default, Home Assistant uses SQLite, and when you have a ton of sensor data flowing in, SQLite can start choking.</p><p>My solution was to move everything to a VM on my home server. I also migrated Home Assistant’s main database to MySQL, and for longer-term metrics and historical data, I set up an InfluxDB server. (I’ve documented the details of my <a href=\"https://vpetersson.com/2024/05/04/home-server-journey.html\">home server build in another blog post</a>.)</p><h2>Scaling Up in a New House</h2><p>When I moved into a house, my Home Assistant installation grew significantly: more rooms, more lights, and more devices overall. Right now, I have over 100 devices connected to Home Assistant, including a large number of smart lights (all IKEA), plus an assortment of other smart devices. Practically every bulb in my home is now integrated into Home Assistant.</p><h3>Adaptive Lighting: Moving Beyond Flux</h3><p>One of the crucial features for me is . Initially, I used <a href=\"https://vpetersson.com/2020/05/25/homeassistant-ikea-tradfri-flux-sensors.html\">Flux</a> (an older solution for synchronizing lights with the time of day), but I’ve recently migrated to the new Adaptive Lighting integration available through HACS (Home Assistant Community Store). This newer system is much more sophisticated and has better capabilities for adjusting color temperature and brightness throughout the day.</p><p>Managing this setup comes with two main challenges. First, neither Flux nor Adaptive Lighting can target light groups. Instead, you need to explicitly list every single light entity in your configuration. This becomes particularly tedious when you have dozens of lights that you want to manage together. It would have been much more convenient to just point the integration to a group and have it handle all the lights within that group automatically.</p><p>The second challenge is that even though all my bulbs are from IKEA, they don’t have all the same features. This means I need separate configurations for each category to get Adaptive Lighting working correctly. But the effort is worth it: circadian rhythms are important to me, and I really want that smooth, automatic shift in color temperature from warm yellows in the morning and evenings to cooler whites and blues during midday.</p><h2>Using Cursor to Speed Up Configuration</h2><p>One big leap for me this year has been leveraging <a href=\"https://www.cursor.com\">Cursor</a>, an AI coding assistant, to handle the more tedious parts of Home Assistant’s YAML configurations. I’ll admit, I’ve never had the time to master every detail of Home Assistant’s DSL or its configuration files.</p><p>The first major task I tackled with Cursor was writing a custom script to parse all my lights, figure out exactly what kind of bulb each one is, and spit out debugging information. This is the foundation of building the correct adaptive lighting setup. Once the script categorizes the bulbs, I can then create or update the YAML configuration for each bulb type.</p><p>Here’s the script I use to analyze my Home Assistant lights. It connects to the Home Assistant API, categorizes all lights by their capabilities, and provides detailed debugging information about their current state and supported features:</p><div><div><pre><code></code></pre></div></div><ol><li><strong>Run the custom parsing script</strong> on my Home Assistant setup to produce a detailed list of bulbs and their capabilities.</li><li> into Cursor (in “agent mode” or similar), along with my old configuration.</li><li> the updated YAML for the new Adaptive Lighting system.</li></ol><p>It’s been a huge time-saver. Sure, I still do some manual debugging, but I also use Cursor to assist with the troubleshooting. For instance, if something breaks in Home Assistant, I feed the logs into Cursor and ask it to help me fix the error. It’s surprisingly effective.</p><p>After extensive testing, I’ve optimized my adaptive lighting configurations for different IKEA bulb types. Here are my recommended settings that provide smooth transitions while maintaining good visibility throughout the day.</p><p>For IKEA’s <em>LED bulb GU10 345 lumen, smart/wireless dimmable white spectrum</em> bulbs.</p><div><div><pre><code></code></pre></div></div><h3>Dimmable color and white spectrum</h3><p>For the <em>LED bulb E27 806 lumen, wireless dimmable color and white spectrum/globe opal white</em> bulbs.</p><div><div><pre><code></code></pre></div></div><p>For the basic <em>LED bulb GU10 345 lumen, smart/wireless dimmable warm white</em> bulbs.</p><div><div><pre><code></code></pre></div></div><p>Now that the lighting is running smoothly, my next big smart home project is upgrading all my radiators with Zigbee-based smart TRVs (thermostatic radiator valves). The goal is to have each room in my home maintain an optimal temperature by reading from the central Nest thermostat. In older British homes like mine, temperature control isn’t very granular, so having each radiator adjust itself is a major comfort and efficiency boost.</p><p>I’ve already purchased <a href=\"https://s.click.aliexpress.com/e/_EzwaYAM\">these TRVs</a> but haven’t had time to configure them yet. My plan is:</p><ol><li> to my Zigbee network.</li><li> from my Nest thermostat (the main sensor).</li><li> in Home Assistant so that each room’s radiator valve opens or closes based on its own target temperature.</li></ol><p>I’m hoping this will help solve the typical British house problem: some rooms end up too warm, while others are never warm enough. With per-room heating control, it should be far more balanced and efficient.</p><p>That’s where my Home Assistant journey sits at the moment. I’m thrilled with how the adaptive lighting is working, especially now that I’ve harnessed an AI coding assistant to manage the complexity of my YAML files. The next challenge, smart radiator valves, will hopefully bring my home’s temperature control on par with my lighting automation.</p><i>Found an error or typo? File PR against <a href=\"https://github.com/vpetersson/vpetersson.com/tree/master/_posts/2025-01-22-how-i-use-home-assistant-in-2025.md\" rel=\"nofollow\">this file</a>.</i>","contentLength":6057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42813513"},{"title":"Build It Yourself","url":"https://lucumr.pocoo.org/2025/1/24/build-it-yourself/","date":1737729756,"author":"/u/burntsushi","guid":591,"unread":true,"content":"<p>written on Friday, January 24, 2025</p><p>Another day, another <a href=\"https://lucumr.pocoo.org/2016/3/24/open-source-trust-scaling/\">rant</a><a href=\"https://lucumr.pocoo.org/2022/1/10/dependency-risk-and-funding/\">about</a><a href=\"https://lucumr.pocoo.org/2024/3/26/rust-cdo/\">dependencies</a>. from me.  This time I will ask you that we\nstart and support a vibe shift when it comes to dependencies.</p><p>You're probably familiar with the concept of “dependency churn.”  It's that\nnever-ending treadmill of updates, patches, audits, and transitive\ndependencies that we as developers love to casually install in the name of\nproductivity.  Who doesn't enjoy waiting for yet another \njust so you can get that fix for a bug you don't even have?</p><p>It's a plague in most ecosystems with good packaging solutions.\nJavaScript and Rust are particularly badly affected by that.  A brand new\nTokio project drags in 28 crates, a new Rocket project balloons that to\n172, and a little template engine like MiniJinja can exist with just a\nsingle dependency — while its CLI variant slurps up 142.</p><p>If that doesn't sound like a big deal, let's consider <a href=\"https://crates.io/crates/terminal_size\">terminal_size</a>.  It is a crate that does\nexactly what its name suggests: it figures out your terminal dimensions.\nThe underlying APIs it uses have effectively been stable since the earliest days of computing\nterminals—what, 50 years or so? And yet, for one function, terminal-size\nmanages to introduce three or four additional crates, depending on your\noperating system.  That triggers a whole chain reaction, so you end up\ncompiling thousands of other functions just to figure out if your terminal\nis 80x25 or 120x40.  That crate had 26 releases.  My own version of that\nthat I have stuck away in a project from 10 years ago still works without\na single update.  Because shocker: nothing about figuring out terminal\nsizes has changed.  </p><p>So why does  have so many updates if it's so stable?\nBecause it's build on top of platform abstraction libraries that\nconstantly churn, so it needs to update to avoid code duplication and\nblowing up compile times even more.</p><p>But “big supply chain” will tell you that you must do it this way.  Don't\nyou dare to copy paste that function into your library.  Or don't you date\nto use “unsafe” yourself.  You're not qualified enough to write unsafe\ncode, let the platform abstraction architects do that.  Otherwise someone\n<a href=\"https://github.com/geiger-rs/cargo-geiger\">will slap you</a>.  There are\nentire companies who are making a living of supplying you with the tools\nneeded to deal with your dependency mess.  In the name of security, we're\npushed to having dependencies and keeping them up to date, despite most of\nthose dependencies being the primary source of security problems.</p><p>The goal of code in many ways should be to be written in a way that it\ndoes not need updates.  It should eventually achieve some level of\nstability.  In the Rust ecosystem stable code is punished.  If you have a\nperfectly working dependency but you have a somewhat inactive bug tracker,\nRUSTSEC will come by and <a href=\"https://lucumr.pocoo.org/2024/3/26/rust-cdo/\">give you a chunk rating</a>.</p><p>But there  a simpler path.  You write code yourself.  Sure, it's more\nwork up front, but once it's written, it's done. No new crates, no waiting\nfor upsteam authors to fix that edge case.  If it's broken for you, you\nfix it yourself.  Code that works doesn't necessarily need the\nmaintenance treadmill.  Your code has a corner case?  Who cares.  This is\nthat vibe shift we need in the Rust world: celebrating fewer dependencies\nrather than more.</p><p>We're at a point in the most ecosystems where pulling in libraries is not\njust the default action, it's seen positively: “Look how modular and\ncomposable my code is!”  Actually, it might just be a symptom of never\nwanting to type out more than a few lines.</p><p>Now one will make the argument that it takes so much time to write all of\nthis.  It's 2025 and it's faster for me to have ChatGPT or Cursor whip up\na dependency free implementation of these common functions, than it is for\nme to start figuring out a dependency.  And it makes sense as for many\nsuch small functions the maintenance overhead is tiny and much lower than\nactually dealing with constant upgrading of dependencies.  The code is just\na few lines and you also get the benefit of no longer need to compile\nthousands of lines of other people's code for a single function.</p><p>But let's face it: corporate code review culture has also has infected\nOpen Source software.  Companies are more likely to reward engineers than\nscold them for pulling in that new “shiny library” that solves the problem\nthey never actually had.  That creates problems, so dependabot and friends\nwere born.  Today I just dread getting dependabot pull requests but on\nprojects but I have to accept it.  I'm part of an ecosystem with my stuff\nand that ecosystem is all about churn, churn, churn.  In companies you can\nalso keep entire internal engineering teams busy with vendoring\ndependencies, internal audits and upgrading things throughout the company.</p><p>Fighting this fight is incredibly hard!  Every new hire has been trained\non the idea that dependencies are great, that code reuse is great.  That\nhaving old code sitting around is a sign of bad engineering culture.</p><p>It's also hard to fight this in Open Source.  Years ago I wrote <a href=\"https://crates.io/crates/sha1_smol\">sha1-smol</a> which originally was just called\n.  It became the standard crate to calculate SHA1 hashes.\nEventually I was pressured to donate that package name to rust-crypto and\nto depend on the rest of the crypto ecosystem as it was so established.\nIf you want to use the new sha1 crate, you get to enjoy 10 dependencies.\nBut there was just no way around it, because that name in the registry is\nprecious and people also wanted to have trait compatibility.  It feels\ntiring to be the only person in a conversation pushing to keep the churn\ndown and dependencies low.</p><p>It's time to have a new perspective: we should give kudos to engineers who\nwrite a small function themselves instead of hooking in a transitive web\nof crates.  We should be suspicious of big crate graphs.  Celebrated are\nthe minimal dependencies, the humble function that just quietly does the\njob, the code that doesn't need to be touched for years because it was\ndone right once.</p><p>And sure, it's not black and white.  There are the important libraries\nthat solve hard problems.  Graphics libraries that abstract over complex\ndrivers, implementations of protocols like HTTP and QUIC.  I won't be able\nto get rid of tokio and I have no desire to.  But when you end up using\none function, but you compile hundreds, some alarm bell should go off.</p><p>We need that vibe shift.  To celebrate building it yourself when it's\nappropriate to do so.  To give credit to library authors who build low to\nno-dependency Open Source libraries.</p><p>For instance minijinja celebrates it in the readme:</p><pre>$ cargo tree\nminimal v0.1.0 (examples/minimal)\n└── minijinja v2.6.0 (minijinja)\n    └── serde v1.0.144\n</pre><p>And it has a PR to eventually <a href=\"https://github.com/mitsuhiko/minijinja/pull/539\">get rid of the last dependency</a>.  And sometime this\nyear I will make it my goal to go ahead proudly and trim down all that fat\nin my projects.</p>","contentLength":6868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i8wwy0/build_it_yourself/"},{"title":"How I open-sourced my secret access tokens from GitHub, Slack and NPM and who of them cares about it | Vue & Node admin panel framework","url":"https://adminforth.dev/blog/how-i-opensourced-my-secret-tokens/","date":1737729595,"author":"/u/vanbrosh","guid":585,"unread":true,"content":"<p>Our framework has a CI pipeline that runs , publishes the package to NPM (), and creates a new release on GitHub. It also sends a notification about the release to a Slack webhook for our team.</p><p>Secrets for these services were stored in our CI’s built-in Vault (we are running a self-hosted Woodpecker CI).</p><p>Recently, while moving plugins to separate repositories, I decided to try <a href=\"https://infisical.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Infisical</a> for centralized secrets management instead of the internal CI Vault. Infisical provides a self-hosted open-source solution, has a well-organized UI, and offers better access control than our CI Vault. It was important to me that I could reuse secrets across different repositories without copying them every time I created a new plugin.</p><p>Pretty dumb method to export secrets to the  file, but it was late evening, and I didn’t want to spend much time on it at the start.</p><p>I made the first push, and everything worked fine on the first attempt. I was happy.</p><p>Then I started adding the same code to the first plugin, and the plugin build failed with a very unexpected error.</p><p>It said that my NPM token was invalid. I was surprised and started printing the environment variables to see what was wrong (printing environment variables to the build log is a pretty bad practice and is the last thing you want to do, but I knew it was an internal CI, and the project was private).</p><p>I saw that my NPM token was still in the environment variables and was the same.</p><p>I went back to the first repository and retried the build. It failed with the same error.</p><p>I went to NPM and found out that the token had disappeared entirely from the list. I was shocked and recreated it.</p><p>On the next build, I discovered that the Slack webhook was also not working. However, GitHub releases were created without issues in both repositories.</p><p>Then I noticed an email push notification from Slack titled \"Notification about invalidated webhook URLs.\"</p><p>This was the moment I realized that  had simply taken my  file and published it to NPM.</p><p>Shortly after, I noticed a recent email from NPM titled \"Granular access token deleted.\"</p><p>The next thing I did was revoke all tokens, including the GitHub token, which still worked, and unpublish all packages from NPM (though they might still be cloned by some caches/aggregators/archivers).</p><p>GitHub was not able to recognize that the token had been leaked to an NPM package and revoke it. Although they do a pretty good job when you push other vendors’ secrets to a GitHub repository, it seems they don’t check NPM sources.</p><p>NPM detected that the NPM token was published to their registry and revoked it. However, it was hard to understand why—it was simply deleted. They sent an email, but it did not explain why the token was deleted or specify the source of the leak. Showing an error in the tokens list on the NPM website would have been the best option.</p><p>I was surprised, but Slack did a great job. They monitor NPM (I don’t think they monitor the whole NPM registry; there’s probably some interesting technology behind it). They detected that the NPM token was published to the registry and invalidated it. They sent an email with a clear explanation of why it was invalidated and what steps to take next.</p><p>We can talk a lot about bad programming practices, but the main takeaway is that we are human. And humans still make mistakes.It makes a lot of sense to monitor for such human errors.</p><p>In my case, NPM and Slack saved me from a potential security breach. Without their intervention, I would have learned about the issue only when someone used my tokens for malicious purposes.GitHub didn’t detect or revoke the token, and many other services wouldn’t have done so either.</p><p>Here are some common recommendations I learned from this experience:</p><ul><li>Try to limit token access as much as possible to only the required granularity. Even if something is leaked, it won’t cause much harm. Don’t grant access to all resources/packages/repos unless it’s necessary.</li><li>Check what you publish, especially when making changes to your build pipeline. I missed the fact that the  file was being published.</li><li>Appreciate services that monitor for leaks and respond to them. They can save you from potential security breaches.</li></ul>","contentLength":4199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i8wurp/how_i_opensourced_my_secret_access_tokens_from/"},{"title":"What platforms should I be considering?","url":"https://www.reddit.com/r/kubernetes/comments/1i8wi4q/what_platforms_should_i_be_considering/","date":1737728611,"author":"/u/jaymef","guid":572,"unread":true,"content":"<p>Bit of context. Old school sysadmin with number of years experience. I'm fairly comfortable with containers, Linux administration, networking/security etc. but have never ventured into Kubernetes. </p><p>I'm looking to run some form of container platform onprem, mostly to be used to support our companies web development/staging environments. The majority of our production workloads are cloud based.</p><p>I want to do containers onprem but I'd like to avoid deploying an overly complex system that nobody understands. It does not have to be mission critical, but some high availability for system patches/reboots etc. would be preferred.</p><p>I would like to start with maybe three bare metal servers and go from there.</p><p>I've been doing some research and it looks like K3s might be an option. I've also come across Nomad, OpenShift and its upstream OKD, Rancher, MicroK8s, Talos, K0S and a bunch of other products.</p><p>For Openshift/OKD, I'm a bit weary because I don't want vendor lock in and Red Hat screwed us with killing RHEV/oVirt platform. Nomad I feel somewhat similar, not sure about getting in bed with Hashicorp.</p><p>I'm not looking for someone to make a decision for me, but would appreciate some help with being pointed in the right direction at what solutions might be a good fit so I can start setting up POC's. I'd like a platform with a lot of community support.</p>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Every System is a Log: Avoiding coordination in distributed applications","url":"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/","date":1737727030,"author":"sewen","guid":189,"unread":true,"content":"<p><strong>Building resilient distributed applications remains a tough challenge.</strong></p><p>It should be possible to focus almost entirely on the business logic and the complexity inherent to the domain. Instead, you need to review line-by-line and check: <em>“what if the service crashes here?”</em>, <em>“what if the API we call here is temporarily unavailable”</em>, <em>“what if a concurrent invocation overtakes this one here”</em>, or <em>“what if this process becomes a zombie while executing this function, how do I prevent it from corrupting the state?”</em>.</p><p>As a result, you spend a huge amount of time worrying about failover strategies, retries, race conditions, locking/fencing, ordering of operations, order visibility of changes, decoupling availability, etc. They add queues, key-value stores, locking services, schedulers, workflow orchestrators and they try to get them all to play nice together. And the hard truth is, many applications don’t get it right and are not correct under failures or even under load.</p><img alt=\"Problems in distributed applications and services\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/dist_app_problems.png\"><p>How can we radically simplify this? In this article, we walk through a core idea that addresses many of these issues, by avoiding distributed coordination. Much of this goes back to learnings from when we built <a href=\"https://flink.apache.org/\">Apache Flink</a>.</p><p>Let’s start with an observation about distributed applications and infrastructure: </p><ul><li><p> are logs: <a href=\"https://kafka.apache.org/\">Apache Kafka</a>, <a href=\"https://pulsar.apache.org/\">Pulsar</a>, <a href=\"https://engineering.fb.com/2019/10/07/core-infra/scribe/\">Meta’s Scribe</a> are distributed implementations of the log abstraction. Message brokers (e.g., RabbitMQ, SQS) internally replicate messages through logs.</p></li><li><p> (and K/V stores) are logs: changes go to the write-ahead-log first, then get materialized into the tables. The database community has the famous saying <em>“The log is the database; everything else is cache (or materialized views)”</em> - often attributed to <a href=\"https://www.linkedin.com/in/pathelland\">Pat Helland</a>. The idea of <a href=\"https://martin.kleppmann.com/2015/11/05/database-inside-out-at-oredev.html\">“Turning the Database Inside Out”</a> starts with a log.</p></li><li><p>Distributed <strong>locking- and leader election services</strong> (like <a href=\"https://zookeeper.apache.org/\">ZooKeeper</a>, <a href=\"https://etcd.io/\">Etcd</a>, …) are consensus logs at their core. Consensus algorithms, like Raft, inherently model log replication.</p></li><li><p>Persistent  materialize logs of their state transitions.</p></li></ul><p>When you build an application or microservice that interacts, for example, with a database, a message queue, and a service API (backed by another database), you are orchestrating a handful of different logs in your business logic.</p><p>In this example, we want to implement a  handler. The payment has an ID that identifies it. The handler is triggered from a queue (which also re-delivers the event if the handler fails or times out) and the processing involves checking a fraud detection model, updating account balance, storing the status, and sending a notification. There are other handlers that may handle the same payment ID, for example to cancel the payment, block it, unblock it, revert it.</p><img alt=\"An example, \u0016naïve implementation\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/code_simple.png\"><p>You can probably spot some issues:</p><ol><li>Concurrent invocations <em>(other handlers like “cancel”, or retries of the same event)</em> can produce arbitrary results.</li><li>A failure after line 15 means the next retry does nothing and we don’t send a notification.</li><li>If the fraud model is not completely deterministic (or if it is updated between retries), we might assume a payment is valid, crash after line 9, the retry declares the payment not valid, and we are setting the status to BLOCKED despite the fact that we withdrew the money.</li></ol><p>Nasty stuff! Let’s try to improve that.</p><img alt=\"An example, \u0016more elaborate implementation\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/code_complex.png\"><p>This second version of the code does some things better, but still has issues. One of them is around line 20, where we need to ensure that we are still the owner of the lock at the point in time where the database persists the update. That is really hard to do, because distributed lock release or re-entrance is never 100% correct, due to the impossibility of precise failure detection <em>(is a process failed or just slow or is our network partitioned?)</em>, so locking generally requires an additional fencing mechanism. Martin Kleppmann has <a href=\"https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html\">a great blog post</a> about the rabbit hole of getting distributed locking right.</p><p>Why is it so hard to make this seemingly simple handler work reliably? Because our goal is to make consistent changes depending on the status of disparate systems, where each has its own view of the world, maintained in its separate log.</p><div>Distributed applications often need to , carefully  that help them ensure correctness. This is the heart of much of the complexity in modern distributed applications.</div><p>Now let’s assume that all these systems (queues, DBs, locks, …) operate off the same log - for the sake of this thought experiment - the log of the message queue that delivers the  to the  handler (the ).</p><p>Every time our  handler wants to change some state of another system, it writes a record to the upstream log. That new record is linked to the original . Whenever the queue decides to re-deliver the  again (e.g., under a timeout or an assumed failure), it also attaches all linked log entries (the ).</p><img alt=\"Implementing a step journal into the log\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/one_log_journal.png\"><p>Now we can adjust lines 9,10,11 (the call to the fraud-detector API and storing the result) to write the result of the API call to the upstream log. When the handler is retried after a potential failure, it automatically sees whether the result was written before. This is not just efficient, but we no longer store completed steps in a shared DB where it is easy to have it accidentally picked up in unexpected ways (see <a href=\"https://portswigger.net/research/smashing-the-state-machine\">this article</a> for how this can be a severe security and integrity loophole).</p><p>This becomes particularly useful, if we require a  to add an entry to the log: We can only append that entry, if no newer retry was triggered. That is easy for a queue to track (it knows whether it sent the  out again) and our  handler would quit if the conditional append failed, knowing that another retry attempt has taken over.</p><p>Now, concurrently executing retries (if the queue incorrectly assumes a handler failed and re-sent the event) can no longer corrupt the step history. This implementation gives us pretty strong workflow-style execution guarantees for our code!</p><img alt=\"Safety through conditional appends to the log\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/one_log_conditional_append.png\"><p>To make locking (line 2) and state update (line 20) work reliably, the  handler writes the relevant events (, , ) to the upstream log as well. After writing the  event, the handler waits until the lock service grants the lock.The lock service and the database now follow the upstream log as if it was their own write-ahead log. The database can simply apply the update when it reads the event, the locking service of course only grants the lock when available (might have to wait to encounter the previous holder’s  event).</p><img alt=\"Locking and state management through the log\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/one_log_lock_state.png\"><p>Somewhat surprisingly, this pretty much eliminates all problems and corner cases we had with locks and state before: Lock acquisition and release through the upstream log and handler’s journal means we reliably keep the lock across retries. Having the update event conditionally appended to the same event journal as the lock event replaces the need for the lock’s fencing token - plus, we can be sure that we apply the update once and only once.</p><p>So, once we implement our logic like this, EVERYTHING JUST WORKS.</p><p>We can inject all sorts of failures, stalls, network partitions. As long as the log is correctly implemented, the program will always remain correct. And all that the  handler needs to do is (1) trigger actions as conditional-log-appends and (2) skip over actions whose log entries are already attached to the . This is super easy to implement in a library, because it doesn’t require any form of distributed coordination.</p><p>We haven’t added a new distributed system primitive; in fact we’ve removed several. The benefits come from avoiding the need for coordination.</p><p>Before we started using the same log, the state was spread across systems: The status of the operation, whether a lock is held, who held it, what value a branch was based on. Because each system maintains their state as if it was independent, the different parts of the state can get out-of-sync and be altered in unexpected ways (e.g., through a race condition or zombie process). It’s hard to implement robust logic and guarantee strong invariants that way.</p><div>Having a single place (the one log) that forces a linear history of events as the ground truth and owns the decision of who can add to that ground truth, means we don’t have to coordinate much any more.</div><p>Coordination avoidance is one of the few silver bullets in distributed systems - a way to reduce complexity, rather than shift it. For example, guarding our second code snippet with a ZooKeeper lock only shifted complexity. It reduced the code’s need to worry about concurrency, but introduced issues around lost locks and cleanup of persistent locks. In contrast, the approach to unify the different states in one log actually reduced work, which resulted in higher efficiency, fewer corner cases, and easier operations.</p><p>I know what you’re thinking: That’s a nice thought experiment, but my queue/log doesn’t work like that, my database doesn’t follow some other log, and isn’t this breaking all rules for separation of concerns?</p><p>This idea can serve as a conceptual blue-print for an architecture based on a log (e.g., Kafka) - a bit like <a href=\"https://martin.kleppmann.com/2015/11/05/database-inside-out-at-oredev.html\">“Turning the Database Inside Out”</a><em>(maybe we should call this “Turning the Microservice Inside Out?”)</em>. In practice, today’s log implementations miss efficient built-in ways to track retries, make conditional-appends, link events into a journal, and would leave that to the application developer to implement.</p><p><a href=\"https://restate.dev/\"></a> is an implementation of this idea. Restate Server is the broker that owns the upstream log and push-invokes the handlers with events (e.g., similar to AWS SNS and Event Bridge), ensuring reliable retries after crashes. Every event gets the latest  (set of linked events) attached, just as described in the thought experiment above. Restate uses bi-directional streaming protocols (e.g., HTTP/2) to invoke the event handlers and send journal events and acknowledgements back and forth.</p><img alt=\"Restate in an application stack\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/restate_in_the_stack.png\"><p>The server issues a <em>unique epoch to every invocation and retry</em>, which the SDK attaches to every journal event that it sends, allowing the server to reject events from subsumed handler executions .</p><p>The code snippet shows the example in Restate’s API <em>(here TypeScript, but Java, Kotlin, Python, Go, and Rust are supported as well)</em>. The code does not explicitly append log events, but rather uses an SDK for actions, and the SDK interacts with the log.</p><img alt=\"Example code with the Restate SDK\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/code_restate.png\"><p>To persist intermediate steps (line 8), handlers use the SDK (), which sends the event to the log and awaits the ack of the conditional append to the event’s . On retries, the SDK checks the  whether the step’s event already exists and restores the result from there directly.</p><p>Messages to other handlers are transported with exactly-once semantics (line 16). Message and RPC events are both added to the journal and routed to the destination handler. Similar to , the journal deduplicates the message-sending steps. Because messages result in a single durable invocation (sequence of retries that share a journal), you can easily build end-to-end exactly-once semantics on top of this.</p><p>Restate supports handlers that lock a key when executing (and hold the lock across retries). Those handlers can read and update state that is scoped to that key. They are implemented similarly to the thought experiment: The lock and state update events are added to the journal and additionally processed by an embedded lock service and K/V store, making locks and state virtually incorruptible through partial failures, race conditions, zombie processes, etc.These stateful handlers can be grouped together to share state. Restate calls that a , because the handlers are like methods with access to the object instance’s state. The state is infinitely retained in the K/V store, even when the log events eventually get garbage collected.</p><img alt=\"Virtual Objects in Restate\" src=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/virtual_object.png\"><p>There are more building blocks in Restate, including <em>persistent Futures/Promises</em>, , or  . They all build on the same concept: Events routed through the same log, stored in the journal, and processed into a database or scheduler.</p><div>Applications often aim to create the behavior of  for their critical functions. The single-log approach provides that with a single dependency and without coordination across queues, DBs, locks, and schedulers. Restate implements that pattern.</div><p>It would not make sense to use a single log for every operation in a distributed multi-service architecture. While it could give interesting properties, this would couple services too tightly, create a single giant blast radius, and void many benefits of service-oriented designs.</p><p>The sweet-spot we target with Restate’s implementation of this idea, is to drive all state that is strictly scoped to a handler or service through the log, plus transport of messages between services. The result is a coupling and blast radius similar to any event-driven service: If the upstream queue/log is down, the service cannot be invoked.</p><h3>State in a database or in the log? <a href=\"https://restate.dev/blog/every-system-is-a-log-avoiding-coordination-in-distributed-applications/#state-in-a-database-or-in-the-log\" aria-hidden=\"true\">#</a></h3><p>We assume that Restate is not going to replace general purpose databases. Shared databases should and will remain a part of the infrastructure, and continue to do what they are great at.</p><p>The K/V state built on the log is a great fit for state machines <em>(like the status of a payment)</em>, temporary state when joining/aggregating events and signals, or really any state that is purely updated through the event-driven handlers and scoped around a key (though a key may be something broader, like an aggregate root in Domain Driven Design).</p><p>It also gives you the building blocks for a highly robust and consistent core state. You can even use that to build overlays over other stores, track metadata like versions for entries in databases, or build data structures like semaphores. <a href=\"https://github.com/restatedev/examples/blob/main/typescript/patterns-use-cases/src/database/main.ts\">Here is an example</a> of how to use this to make exactly-once updates to databases from handlers.</p><p>If you want to try this pattern out for yourself and see and feel this idea in action, Restate is open source and you can download it at <a href=\"https://restate.dev/get-restate/\">https://restate.dev/get-restate/</a></p><p>Today, Restate runs on a single node - similar to a Postgres database server. , supporting replication, scale out deployments, working with object store snapshots - stay tuned for more exciting updates during that release.</p><p>With the release, we will publish Part 2 of this article, which is looking at the design of the broker that maintains that log, drives the execution, retries, and implements the extensible logic to use the log for communication, locking, journaling, state, signals, scheduling, etc. As you might expect, if the core abstraction is a log, that system is a specific type of event-driven architecture.</p><p>In Part 3 of this series, we look at the implementation of the log that backs everything. Why not just use Kafka? Or just use Postgres?\nIn this case, we opted to develop a new type of log - something that generally one shouldn’t do, but once in a while, there is actually a good case for it. We believe that this is one of those cases, and will discuss the details of the log design, what makes it unique, and what it can do that’s hard to do with any existing implementation.</p>","contentLength":14978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42813049"},{"title":"Profiling and Optimising Go Code","url":"https://www.reddit.com/r/golang/comments/1i8vsrl/profiling_and_optimising_go_code/","date":1737726589,"author":"/u/greatdharmatma","guid":568,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/greatdharmatma\"> /u/greatdharmatma </a>","contentLength":37,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TIL: large capacity slices/maps in sync.Pool can waste memory","url":"https://www.reddit.com/r/golang/comments/1i8vbyh/til_large_capacity_slicesmaps_in_syncpool_can/","date":1737725174,"author":"/u/lzap","guid":567,"unread":true,"content":"<p>When I was browsing Go standard library I found this:</p><pre><code>var bufferPool = sync.Pool{New: func() any { return new([]byte) }} func getBuffer() *[]byte { p := bufferPool.Get().(*[]byte) *p = (*p)[:0] return p } func putBuffer(p *[]byte) { // Proper usage of a sync.Pool requires each entry to have approximately // the same memory cost. To obtain this property when the stored type // contains a variably-sized buffer, we add a hard limit on the maximum buffer // to place back in the pool. // // See if cap(*p) &gt; 64&lt;&lt;10 { *p = nil } bufferPool.Put(p) }https://go.dev/issue/23199 </code></pre><p>When a large object, a byte slice in this code, is created and put to the pool it essentially wastes (\"leaks\") that memory if <strong>not used to its full capacity</strong>. There is a massive discussion around this at <a href=\"https://go.dev/issue/23199\">https://go.dev/issue/23199</a> and it is interesting read.</p><p>The code (from the  package) essentially prevents putting slices with capacity higher than  so it mitigates a situation when a very large log is created and then for the rest of the whole application lifetime it could not be used at all.</p><p>Hopefully,  which are being introduced in Go 1.24 this spring will help to solve this easily and the sync.Pool from the standard library can take advantage of this. Though I would share it, found it intersting. Cheers.</p>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask HN: Why buy domains and 301 redirect them to me?","url":"https://news.ycombinator.com/item?id=42812779","date":1737724856,"author":"HughParry","guid":188,"unread":true,"content":"Say I'm running a SaaS product, example.com.<p>Somebody has bought several domains like getexample.com, buyexample.io, joinexample.net, and is 301 redirecting them to example.com.</p><p>What's their play here? Is this setup for a phishing attack in the future? Are they just going to try and sell the domains to me in the future? Not encountered behaviour like this before (or at least, I don't know if this is the beginning phase of a common scam)</p>","contentLength":438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42812779"},{"title":"Richard Stallman to Visit India's Birla Institute of Technology and Science, Hyderabad","url":"https://www.reddit.com/r/linux/comments/1i8v1ka/richard_stallman_to_visit_indias_birla_institute/","date":1737724287,"author":"/u/fury999io","guid":577,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transforms: Reference frames tracking through time, is now no_std!","url":"https://www.reddit.com/r/rust/comments/1i8uxuy/transforms_reference_frames_tracking_through_time/","date":1737723967,"author":"/u/Strange-Guidance7654","guid":590,"unread":true,"content":"<p>I made a post a few months ago with the initial release and I am happy to see some usage on my work. I have continued developing the library.</p><p>: The Transforms crate aims to solve the isue in robotics where every sensor and limb in a robot has its own point of reference. Transforms provides a tracking system that links all these reference frames together and tracks them through time, such that their relative positions can always be queried.</p><p>This crate is heavily inspired by ROS2 / TF2, but is its own implementation and built fully from the ground up.</p><p>: In an attempt to increase the conciseness and minimalism in the crate, I removed any nested async implementations and made the crate no_std compatible.</p><p>Thank you very much for your time reading this and I welcome any constructive criticism.</p>","contentLength":794,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Richard Stallman in BITS Pilani, India","url":"https://www.reddit.com/r/linux/comments/1i8uxl6/richard_stallman_in_bits_pilani_india/","date":1737723941,"author":"/u/SpyCracker21","guid":580,"unread":true,"content":"<div><p>Richard Stallman has come to my college today to give a talk and said chatGPT is Bullshit and is an example of Artificial Stupidness 😂</p></div>   submitted by   <a href=\"https://www.reddit.com/user/SpyCracker21\"> /u/SpyCracker21 </a>","contentLength":172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two features Typescript will never include","url":"https://www.danielfullstack.com/article/two-features-typescript-will-never-include","date":1737722335,"author":"/u/craciun_07","guid":583,"unread":true,"content":"<p>TypeScript is an amazing tool, but no language is perfect. Some features never make it to TypeScript no matter how useful they might be.</p><p>Today we explore two of them:  and . We’ll look at why they’re not supported and show you some clever workarounds.</p><p>Typescript is closely aligned with <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://en.wikipedia.org/wiki/Set_theory\"></a> from mathematics. We know this because Typescript supports set operations with types, such as Union, Intersection, Concatenation…</p><p>But there is no support for Type subtraction, or in other words .</p><p>For example, you can’t say something like . But there is a cool trick you can use as a workaround.</p><p>This trick works best in generic function arguments. Take a look at this handler function that accepts all strings except :</p><p>It’s not perfect, and you can see this workaround only works in specific contexts, but given that there is rarely a need for type subtraction, it does make sense why Typescript wouldn’t support it natively.</p><p>TypeScript uses a structural type system. That means it only cares about the structure of data rather than their names. This is why  don’t exist in TypeScript.</p><p>But let's say you want to distinguish between absolute and relative file path strings. Treating them as interchangeable could lead to bugs later down the line.</p><p>Conveniently, you can simulate nominal types by creating “branded types”:</p><p>This is a neat trick that works for all use cases. I would love to hear situations where you would use it in the comments.</p><p>TypeScript will never officially support negated types or nominal types, but their workarounds still promise good results.</p><p>Do you know other features Typescript is avoiding to add? Please share them in the comments below.</p><blockquote><em>Need a cofounder for your SaaS project? Check out <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"http://www.devmarket.pro\"></a> today!</em></blockquote>","contentLength":1715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i8ufr8/two_features_typescript_will_never_include/"},{"title":"Logging in Golang Libraries","url":"https://www.reddit.com/r/golang/comments/1i8timw/logging_in_golang_libraries/","date":1737718968,"author":"/u/roma-glushko","guid":566,"unread":true,"content":"<p>Hey folks, I want to implement logging in my library without imposing any specific library implementation on my end users. I would like to support:</p><p>Basically, I want to be able to log my errors that happen in a background goroutines and potentially some other useful info in that library.</p>","contentLength":287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do you use helmfile? Why or why not?","url":"https://www.reddit.com/r/kubernetes/comments/1i8r8t9/do_you_use_helmfile_why_or_why_not/","date":1737709021,"author":"/u/singhalkarun","guid":574,"unread":true,"content":"<p>How do you structure your helm packages installation? How do you manage upgrades? Do you have CI/CD for upgrades?</p>","contentLength":113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two different URLs in one QR code","url":"https://dualqrcode.com/","date":1737707354,"author":"/u/fixedBaq2jd85","guid":582,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i8qw17/two_different_urls_in_one_qr_code/"},{"title":"Windows Kernel Programming with Rust - Matthias Heiden | EuroRust 2024","url":"https://www.youtube.com/watch?v=NfBXDEgm6VY","date":1737702609,"author":"/u/small_kimono","guid":589,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i8pumg/windows_kernel_programming_with_rust_matthias/"},{"title":"ChatLoopBackOff - Episode 43 (KubeVela)","url":"https://www.youtube.com/watch?v=ClbJB-m0Kc4","date":1737698511,"author":"CNCF [Cloud Native Computing Foundation]","guid":367,"unread":true,"content":"<article>KubeVela, an incubating CNCF project, is a modern application delivery platform that simplifies the deployment and management of cloud-native applications. It uses the Open Application Model (OAM) to create an application centric approach via \"deployment as code\".\n\nJoin CNCF Ambassador, Andy Suderman as he explores KubeVela’s value for teams by highlighting its integration with CI processes, built-in observability, multi-tenancy and security support.</article>","contentLength":456,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ClbJB-m0Kc4?version=3","enclosureMime":"","commentsUrl":null},{"title":"A phishing attack involving g.co, Google's URL shortener","url":"https://gist.github.com/zachlatta/f86317493654b550c689dc6509973aa4","date":1737689926,"author":"zachlatta","guid":187,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42810252"},{"title":"Multi-Zone Clusters Inside and Out - Tom Dean & Phil Henderson, Buoyant","url":"https://www.youtube.com/watch?v=WhFsYVHmg6E","date":1737683394,"author":"CNCF [Cloud Native Computing Foundation]","guid":366,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\nMulti-Zone Clusters Inside and Out - Tom Dean &amp; Phil Henderson, Buoyant\n\nMulti-zone clusters are a great tool for improving application reliability — and also a great way to spend a ton of cash. Why? What really happens when you set these things up? How do you use them effectively without bankrupting your whole organization? In this session, we'll dig into the nuts and bolts of what goes on under the hood of a multi-zone cluster, including what a zone is, what Kubernetes understands about zones, how zones affect routing, and why multi-zone clusters can drive costs up. We'll spend some time on Kubernetes' Topology Aware Routing, covering its advantages as well as its very real limitations. Finally, we'll dive into how you can influence Kubernetes' choices to take advantage of multi-zone clusters' reliability while containing costs. Join us for learning and live demos!</article>","contentLength":1199,"flags":null,"enclosureUrl":"https://www.youtube.com/v/WhFsYVHmg6E?version=3","enclosureMime":"","commentsUrl":null},{"title":"Goodbye etcd! Running Kubernetes on Distributed PostgreSQL - Denis Magda, Yugabyte","url":"https://www.youtube.com/watch?v=VdF1tKfDnQ0","date":1737682409,"author":"CNCF [Cloud Native Computing Foundation]","guid":365,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nGoodbye etcd! Running Kubernetes on Distributed PostgreSQL - Denis Magda, Yugabyte\n\nKubernetes once favored Etcd as a database for all cluster data. Back then, relational databases lacked the availability and scalability characteristics required by Kubernetes. However, as Etcd encountered challenges with various Kubernetes workloads, relational databases continued to evolve. This session is a practical guide for deploying fault-tolerant and scalable Kubernetes clusters on distributed PostgreSQL. We’ll begin with Kine, which integrates into the Kubernetes architecture, enabling relational databases for cluster metadata management. Then, we’ll use Kine to deploy Kubernetes on a single-server PostgreSQL instance. After that, we’ll migrate to a multi-node PostgreSQL instance, allowing Kubernetes to tolerate zone and region outages and scale to thousands of nodes on demand.</article>","contentLength":1206,"flags":null,"enclosureUrl":"https://www.youtube.com/v/VdF1tKfDnQ0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Epic Games To Cover Developer iOS Fees","url":"https://games.slashdot.org/story/25/01/23/2350253/epic-games-to-cover-developer-ios-fees?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1737680400,"author":"BeauHD","guid":268,"unread":true,"content":"Epic Games is expanding its mobile app store to include nearly 20 third-party games on Android and EU iOS, launching a free games program, and temporarily covering Apple's Core Technology Fee for participating developers to counter platform restrictions. \"Our aim here isn't just to launch a bunch of different stores in different places, but to build a single, cross-platform store in which, within the era of multi-platform games, if you buy a game or digital items in one place, you have the ability to own them everywhere,\" Epic CEO Tim Sweeney told reporters during a press briefing. The Verge reports: Under the program, Epic will offer new free games in the store each month before eventually switching to a weekly schedule. However, the games aren't actually in the store yet -- Epic said on Thursday that it \"ran into a few bugs that we're working through now\" and \"we'll provide an update once the games are live and ready to play!\"\n \nTo sweeten the deal for developers that participate in the free games program on iOS, Epic will help defray the cost of using third-party marketplaces. For one year, it will pay these developers' Core Technology Fee (CTF): a 50 euro cent fee levied on every install of an iOS app that uses third-party stores after it exceeds 1 million annual downloads. (Apple gives developers with less than 10 million euros in global revenue a three-year on-ramp.) [...] Epic writes in its blog post that covering the fee \"is not financially viable for every third party app store or for Epic long term, but we'll do it while the European Commission investigates Apple's non-compliance with the law.\"","contentLength":1631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient LLM Deployment: A Unified Approach with Ray, VLLM, and Kubernetes - Lily (Xiaoxuan) Liu","url":"https://www.youtube.com/watch?v=K3NW-gV1OtA","date":1737680375,"author":"CNCF [Cloud Native Computing Foundation]","guid":364,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nEfficient LLM Deployment: A Unified Approach with Ray, VLLM, and Kubernetes - Lily (Xiaoxuan) Liu, UC Berkeley, Anyscale\n\nWith the groundbreaking release of ChatGPT, large language models (LLMs) have taken the world by storm: they have enabled new applications, have exacerbated GPU shortage, and raised new questions about their answers’ veracity. This talk delves into an AI stack, encompassing cloud-native orchestration, distributed computing, and advanced LLMOps. Key topics include: - Kubernetes: The foundational technology that seamlessly manages AI workloads across diverse cloud environments. - Ray: The versatile, open-source framework that streamlines the development and scaling of distributed applications. - vLLM: The cutting-edge, high-performance, and memory-efficient inference and serving engine designed specifically for large language models. Attendees will gain insights into the architecture and integration of these powerful tools, driving innovation and efficiency in the deployment of AI solutions.</article>","contentLength":1345,"flags":null,"enclosureUrl":"https://www.youtube.com/v/K3NW-gV1OtA?version=3","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.14 Adds Support For The Microsoft Copilot Key Found On New Laptops","url":"https://linux.slashdot.org/story/25/01/23/2333254/linux-614-adds-support-for-the-microsoft-copilot-key-found-on-new-laptops?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1737678000,"author":"BeauHD","guid":253,"unread":true,"content":"The Linux 6.14 kernel now maps out support for Microsoft's \"Copilot\" key \"so that user-space software can determine the behavior for handling that key's action on the Linux desktop,\" writes Phoronix's Michael Larabel. From the report: A change made to the atkbd keyboard driver on Linux now maps the F23 key to support the default copilot shortcut action. The patch authored by Lenovo engineer Mark Pearson explains [...]. Now it's up to the Linux desktop environments for determining what to do if the new Copilot key is pressed. The patch was part of the input updates now merged for the Linux 6.14 kernel.","contentLength":608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating the Future: Exploring the Latest in Kubernetes Dashboard Dev... M. Maciaszczyk, S. Florek","url":"https://www.youtube.com/watch?v=7BV1QAgTCxI","date":1737677765,"author":"CNCF [Cloud Native Computing Foundation]","guid":363,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNavigating the Future: Exploring the Latest in Kubernetes Dashboard Development - Marcin Maciaszczyk &amp; Sebastian Florek, Plural\n\nJoin us for an insightful presentation on the latest updates from the Kubernetes SIG-UI, focusing on the evolution of the Kubernetes Dashboard project. Dive into a comprehensive overview of key changes, enhancements, and advancements, including a detailed exploration of the project's new architecture. Gain valuable insights into how these developments shape the future of Kubernetes management and user experience. Whether you're a seasoned Kubernetes user or new to the ecosystem, this presentation promises to provide valuable perspectives on the cutting-edge developments in Kubernetes Dashboard.</article>","contentLength":1049,"flags":null,"enclosureUrl":"https://www.youtube.com/v/7BV1QAgTCxI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Navigating Cross SIG Collaborations with SIG Docs - Panel","url":"https://www.youtube.com/watch?v=wrbWvv6Un2U","date":1737677670,"author":"CNCF [Cloud Native Computing Foundation]","guid":362,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNavigating Cross SIG Collaborations with SIG Docs - Rey Lejano &amp; Savitha Raghunathan, Red Hat; Divya Mohan, SUSE; Xander Grzywinski, Defense Unicorns\n\nAs one of the largest open source projects, Kubernetes is divided into twenty-four Special Interest Groups (SIGs). All SIGs share a common goal of advancing the project, and collaboration across SIGs is required to do so. In this session, learn how SIG Docs collaborates with other SIGs on Kubernetes releases, improving Kubernetes security and driving Kubernetes adoption with documentation.</article>","contentLength":862,"flags":null,"enclosureUrl":"https://www.youtube.com/v/wrbWvv6Un2U?version=3","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes SIG Storage: Intro & Deep Dive - Michelle Au, Xing Yang & Hemant Kumar","url":"https://www.youtube.com/watch?v=DkpQSCX6KqQ","date":1737677622,"author":"CNCF [Cloud Native Computing Foundation]","guid":361,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKubernetes SIG Storage: Intro &amp; Deep Dive - Michelle Au, Google; Xing Yang, VMware by Broadcom; Hemant Kumar, Red Hat\n\nKubernetes SIG Storage is responsible for ensuring that different types of file and block storage are available wherever a container is scheduled, storage capacity management (container ephemeral storage usage, volume resizing, etc.), influencing scheduling of containers based on storage (data gravity, availability, etc.), and generic operations on storage (snapshotting, etc.). SIG Storage also has a project that provides APIs for object storage support in Kubernetes. In this session, we will deep dive into some projects that SIG Storage is currently working on, provide an update on the current status, and discuss what might be coming in the future.</article>","contentLength":1095,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DkpQSCX6KqQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"SIG-Node: Intro and Deep Dive - Sergey Kanzhelev & Dawn Chen, Google; Mrunal Patel, Red Hat","url":"https://www.youtube.com/watch?v=bb0Op1G6XjQ","date":1737677538,"author":"CNCF [Cloud Native Computing Foundation]","guid":360,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSIG-Node: Intro and Deep Dive - Sergey Kanzhelev &amp; Dawn Chen, Google; Mrunal Patel, Red Hat\n\nKubernetes SIG Node maintainers track session will cover the latest updates in the Kubernetes Node subsystem. The emergence of Generative AI has introduced new challenges and workload behaviors. And SIG Node is up for the challenge. SIG Node owns components and interactions between pods and host resources, including the Kubelet, Container Runtime Interface, and Node API. SIG Node is responsible for the Pod’s lifecycle from allocation to teardown, to liveness checks and shared resource management. We work with various container runtimes, kernels, networking, storage, and more; anything a pod touches is SIG Node’s responsibility! The session will be led by Kubernetes SIG Node leads and will be interesting for seasoned contributors as well as people seeking to get involved in the project. Attendees will leave the session with a better understanding of the latest developments in the Kubernetes Node subsystem. The session is open to all Kubernetes users, regardless of experience level.</article>","contentLength":1411,"flags":null,"enclosureUrl":"https://www.youtube.com/v/bb0Op1G6XjQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"KubeVirt: Enhancements and the Road Ahead - Vladik Romanovsky & David Vossel, Red Hat","url":"https://www.youtube.com/watch?v=LfLahyTIIc8","date":1737677348,"author":"CNCF [Cloud Native Computing Foundation]","guid":359,"unread":true,"content":"<article>KubeVirt: Enhancements and the Road Ahead - Vladik Romanovsky &amp; David Vossel, Red Hat\n\nIt's been a big year for KubeVirt. Join us for a detailed update on major advancements introduced over the past year and our plans, including CNCF Graduation. We'll cover some of our recent features: \"VM rollout strategy,\" which changes the update management for running virtual machines; \"VM Volume migration,\" which provides a declarative API to move data between volumes; and we introduce the \"Application Aware Quota\" operator, a solution that addresses the limitations of Kubernetes' native resource quota system and provides an alternative implementation of resource counting. Looking forward, we will also discuss our desire to improve the control over migration convergence, support for (DRA) Dynamic Resource Allocation to optimize resources handling and allocation, and introduce SWAP support for virtual machines, enabling performance improvements and flexibility. This session is designed to provide valuable insights for current users and those who are new to KubeVirt.</article>","contentLength":1069,"flags":null,"enclosureUrl":"https://www.youtube.com/v/LfLahyTIIc8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cloud Native Storage: The CNCF Storage TAG Projects, Technology...- A. Chircop, R. Spazzoli, X Yang","url":"https://www.youtube.com/watch?v=DjePDGyEAvI","date":1737677263,"author":"CNCF [Cloud Native Computing Foundation]","guid":358,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nCloud Native Storage: The CNCF Storage TAG Projects, Technology &amp; Landscape - Alex Chircop, Akamai &amp; Raffaele Spazzoli, Red Hat; Xing Yang, VMWare by Broadcom\n\nThis talk will introduce the CNCF Storage TAG and discuss how the TAG operates, how we work with CNCF Storage projects, and the work we have done to build guidance and write whitepapers for the ecosystem. During this session we will cover an overview of storage projects in the CNCF, including the broader ecosystem, as well as projects that are currently being reviewed. We will also share updates of our latest work including the CNCF Storage Whitepaper, Performance and Benchmarking whitepaper, Cloud Native Disaster Recovery whitepaper, and the Data on Kubernetes whitepapers on database patterns and AI/ML workloads. Join us to find out how to contribute and participate in the CNCF storage community and discover practical guidance on how to use cloud native storage in your environments.</article>","contentLength":1273,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DjePDGyEAvI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Linkerd Update: Egress, Rate Limiting, Federated Services, and more William Morgan, Linkerd","url":"https://www.youtube.com/watch?v=aOt62I2bkxk","date":1737677124,"author":"CNCF [Cloud Native Computing Foundation]","guid":357,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLinkerd Update: Egress, Rate Limiting, Federated Services, and more  \nWilliam Morgan, Director, Linkerd\n\nThe pace of feature delivery in Linkerd has never been higher. In this whirlwind project update by Linkerd maintainers and directors, you'll learn about the latest developments and upcoming features. We'll discuss new support for egress traffic control and visibility, ingress traffic handling, UX improvements to multicluster, new support for IPv6, and more. Come prepared to learn about the world's fastest, lightest service mesh!</article>","contentLength":856,"flags":null,"enclosureUrl":"https://www.youtube.com/v/aOt62I2bkxk?version=3","enclosureMime":"","commentsUrl":null},{"title":"Divide and Conquer: Master GPU Partitioning and Visualize Savings with OpenCost - Kayse Yu","url":"https://www.youtube.com/watch?v=sH699gmFC0o","date":1737677069,"author":"CNCF [Cloud Native Computing Foundation]","guid":356,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nDivide and Conquer: Master GPU Partitioning and Visualize Savings with OpenCost - Kayse Yu \n\nKubernetes is the ideal platform for running AI and ML workloads, such as LLMs. GPU nodes are often used for their parallel processing capabilities and higher performance benefits; however, they are known to be costly. Many factors impact the cost of running AI/ML workloads such as GPU utilization, GPU VM size, idle time, etc. These costs are often ignored and considered inherent in running GPU workloads. But if running workloads at scale and left unoptimized, costs will quickly spin out of control. In this talk, we leverage NVIDIA DCGM exporter with Prometheus for GPU metrics monitoring alongside OpenCost to measure the Kubernetes spend of our GPU workloads. We will provide an overview of OpenCost, highlighting its role in bridging the gap between the developer and platform teams through visibility and accountability of spend. We will demonstrate how to use the NVIDIA GPU Operator and how techniques such as partitioning can lead to significant cost savings.</article>","contentLength":1384,"flags":null,"enclosureUrl":"https://www.youtube.com/v/sH699gmFC0o?version=3","enclosureMime":"","commentsUrl":null},{"title":"Managing and Distributing AI Models Using OCI Standards and Harbor - Steven Zou","url":"https://www.youtube.com/watch?v=0eiXSogHxmQ","date":1737676922,"author":"CNCF [Cloud Native Computing Foundation]","guid":355,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nManaging and Distributing AI Models Using OCI Standards and Harbor - Steven Zou VMware by Broadcom\n\nJust as container images are vital to cloud-native technology, AI models are crucial to AI technology. Effectively, conveniently, and safely managing, maintaining, and distributing AI models is critical for supporting workflows like AI model training, inference, and application deployment. This presentation explores AI model management based on OCI standards and the Harbor project. Standardizing AI model structures and characteristics using OCI specifications and extension mechanisms like OCI Reference to link datasets and dependencies. When large models require efficient loading or privacy considerations, model replication or proxy with upstream repositories like Hugging Face becomes essential. Enhancing model distribution security through signing, vulnerability scanning, and policy-based governance is often necessary. Additionally, introducing acceleration mechanisms such as P2P can significantly improve the efficiency of large model loading.</article>","contentLength":1377,"flags":null,"enclosureUrl":"https://www.youtube.com/v/0eiXSogHxmQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"Unlocking the Future of GPU Scheduling in Kubernetes with Reinforcement Learning- N. Goyal, A. Gupta","url":"https://www.youtube.com/watch?v=fzT6Ot_PTQ0","date":1737676857,"author":"CNCF [Cloud Native Computing Foundation]","guid":354,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nUnlocking the Future of GPU Scheduling in Kubernetes with Reinforcement Learning - Nikunj Goyal, Adobe Systems &amp; Aditi Gupta, Disney Plus Hotstar\n\nScaling up Multi GPU setup using Kubernetes for large scale ML projects has been a hot topic equally stressed upon among both the AI and cloud community. While Kubernetes is able to providing computing power by scheduling GPU nodes, certain issues like resource fragmentation and low utilization plague the performance and results in cost issues. \n\nWhy Reinforcement Learning (RL) in particular one would ask. Unlike the other algorithms, RL shines in its unique ability to continuously adapt to changing environments and efficiently handle Complex and Multi-dimensional Objectives making it particularly suitable for the dynamic and heterogeneous nature of Kubernetes clusters. In this talk, we shall explore the current landscape of GPU scheduling and some state of the art RL algorithms proposed for scheduling. Their current impact on Kubernetes and the possible use of RLHF shall be dived deep into. We hope that audience gain more insights into these new ways of scheduling GPUs on Kubernetes.</article>","contentLength":1465,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fzT6Ot_PTQ0?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Stargate situation is crazy... Elon vs Altman beef intensifies","url":"https://www.youtube.com/watch?v=YrHsw4Oja7w","date":1737658527,"author":"Fireship","guid":375,"unread":true,"content":"<article>What is the new Startgate project just announced by OpenAI and why is Elon Musk calling it fake? Learn how the new Stargate datacenters could affect the future of artificial intelligence development. \n\n#tech #ai #thecodereport \n\n💬 Chat with Me on Discord\n\nhttps://discord.gg/fireship\n\n🔗 Resources\n\nElon vs Sam Altman https://youtu.be/Sf4WqHBCYSY\nDeepSeek R1 first look https://youtu.be/-2k1rcRzsLA\n\n📚 Chapters\n\n🔥 Get More Content - Upgrade to PRO\n\nUpgrade at https://fireship.io/pro\nUse code YT25 for 25% off PRO access \n\n🎨 My Editor Settings\n\n- Atom One Dark \n- vscode-icons\n- Fira Code Font\n\n🔖 Topics Covered\n\n- What is Project Stargate for AI infrastructure?\n- Elon Musk vs Sam Altman controversy\n- Grok 3 vs OpenAI o1</article>","contentLength":739,"flags":null,"enclosureUrl":"https://www.youtube.com/v/YrHsw4Oja7w?version=3","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Open-source AI video editor","url":"https://github.com/fal-ai-community/video-starter-kit","date":1737657278,"author":"drochetti","guid":183,"unread":true,"content":"<p>Hey HN community! I'm one of the lead devs of this project at fal.ai and we created an open source lightweight video editor powered by the latest media AI models. The main goal was to tackle some challenges when dealing with complex media handling and encoding on the browser.</p><p>It all started as an internal experiment but as we tackled some of the issues it was clear there could be some value sharing it with the open source community.</p><p>Some of the key points and tech stack details:</p><p>- It uses IndexedDb, so all data is local (i.e. no auth, no cloud db)</p><p>- Multiple AI models for video, image, music and voice-over. APIs are provided by fal.ai</p><p>- Built with the typical React+Next.js, Shadcn front-end</p><p>- Used remotion.dev for the realtime video preview (this is such a great project, without it the codebase would be twice as large)</p><p>- File uploads so you can bring your own media by uploadthing.com</p><p>- ffmpeg for encoding the final video and also some ui tricks, like the audio waveform</p><p>We deployed a version of it and for now it's free to use. We do plan to add some rate limiting and a bring your own API Key next, but it's open source and I'm curious about what the community will build on top of it, or derive from it. Customize your own video app and if you do, please share.</p><p>If you have any questions, hit me up!</p>","contentLength":1305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42806616"},{"title":"Thank HN: My bootstrapped startup got acquired today","url":"https://news.ycombinator.com/item?id=42806247","date":1737655085,"author":"paraschopra","guid":231,"unread":true,"content":"Hello HN,<p>Today, I sold the company to a private equity firm for $200mn.</p><p>I was a 22 year old fresh graduate when I launched VWO on HN and got initial users. Feedback from people like @patio11 helped me get to PMF. And now, 15 years later, \"site:ycombinator.com\" is what I appended when I wanted to search for advice on what to keep in mind while selling my company.</p><p>Thank you HN for sharing inspiration and wisdom all along. I honestly don't think I would have been an entrepreneur had it not been for hacker news.</p><p>Every single day, HN is the first website I open! I'm feeling very grateful towards the community. Thanks @dang, and thank you Paul Graham for your essays and for creating this beautiful corner of the internet!</p>","contentLength":721,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42806247"},{"title":"Playing with the classification report","url":"https://www.youtube.com/watch?v=765qaIk30Rs","date":1737623293,"author":"probabl","guid":381,"unread":true,"content":"<article>In this video we will play around with a confusion matrix widget that will help us understand how the numbers in the classification report in scikit-learn are created. The classification report is a great utility, but it can help to remind oneself of what the numbers really mean. \n\nScikit-learn documentation:\nhttps://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html\n\nAppendix with notebooks:\nhttps://github.com/probabl-ai/youtube-appendix/tree/main/16-metrics\n\nWebsite: https://probabl.ai/\nLinkedIn: https://www.linkedin.com/company/probabl\nTwitter: https://x.com/probabl_ai\nBluesky: https://bsky.app/profile/probabl.bsky.social\nDiscord: https://discord.probabl.ai\n\nWe also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:\nhttps://rss.com/podcasts/sample-space/\n\n#probabl</article>","contentLength":879,"flags":null,"enclosureUrl":"https://www.youtube.com/v/765qaIk30Rs?version=3","enclosureMime":"","commentsUrl":null}]}