{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Upgrading cluster in-place coz I am too lazy to do blue-green","url":"https://www.reddit.com/r/kubernetes/comments/1mxuf5v/upgrading_cluster_inplace_coz_i_am_too_lazy_to_do/","date":1755931848,"author":"/u/suman087","guid":572,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"K3S with iSCSI storage (Compellent/Starwind VSAN)","url":"https://www.reddit.com/r/kubernetes/comments/1mxr3z0/k3s_with_iscsi_storage_compellentstarwind_vsan/","date":1755920539,"author":"/u/Norava","guid":567,"unread":true,"content":"<p>Hey all! I have a 3 master 4 node K3S cluster installed on top of my Hyper-V S2D cluster in my lab and currently I'm just using Longhorn + each node having a 500gb vhd attached to serve as storage but as I'm using this to learn kube I wanted to try to work on building more scalable storage. </p><p>To that end I'm trying to figure out how to get any form of basic networked storage for my K3S cluster. In doing research I'm finding NFS is much to slow to use in prod so I'm trying to see if there's a way to set up ISCSI LUNs attached to the cluster / workers but I'm not seeing a clear path to even get started</p><p>I initially pulled out an old Dell SAN (A Compellent Scv2020) that I'm trying to get running but that right now is out of band due to it missing it's SCOS but I do know if the person who I found has an iso for SCOS I could get this running as ISCSI storage so I took 2 R610s I had laying around and made a basic Starwind vSAN but I cannot for the life of me figure out HOW to expose ANY LUNs to the k3s cluster. </p><p>My end goal is to have something to host storage that's both more scalable than longhorn and vhds that also can be backed up by Veeam Kasten ideally as I'm in big part also trying to get dr testing with Kasten done as part of this config as I determine how to properly handle backups for some on prem kube clusters I'm responsible for in my new roles that we by compliance couldn't use cloud storage for</p><p>I see democratic-csi mentioned a lot but that appears to be orchestration of LUNs or something through your vendors interface that I cannot find on Starwind and that I don't SEE an EOL SAN like the scv2020 having in any of my searches. I see I see CEPH mentioned but that looks like it's going to similarly operate with local storage like longhorn or requires 3 nodes to get started and the hosts I have to even perform that drastically lack the bay space a full SAN does (Let alone electrical issues I'm starting to run into with my lab but thats beyong this LOL) Likewise I see democratic could work with TrueNAS scale but that also requires 3 nodes and again will have less overall storage. I was debating spinning a Garage node for this and running s3 locally but I'm reading if I want to do ANYTHING with database or heavy write operations is doomed with this method and nfs storage similarly have such issues (Supposedly) Finally I've been through a LITANY of various csi github pages but nearly all of them seem either dead or lacking documentation on how they work</p><p>My ideal would just be connecting a LUN into the cluster in a way I can provision to it directly so I can use the SAN but my understanding is I can't exactly like, create a shared VHDX in Hyper-v and add that to local storage or longhorn or something without basically making the whole cluster either extremely manual or extremely unstable correct?</p>","contentLength":2840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.34 is coming with some interesting security changes ‚Äî what do you think will have the biggest impact?","url":"https://www.armosec.io/blog/kubernetes-1-34-security-enhancements/","date":1755890863,"author":"/u/Swimming_Version_605","guid":571,"unread":true,"content":"<p>Kubernetes v1.34 is coming soon, and it brings a rich batch of security upgrades ‚Äì from alpha features that hint at the future of zero-trust Kubernetes, to mature enhancements making their way into stable releases. Whether you‚Äôre managing a production cluster or exploring new security patterns, this release has something worth your attention.</p><div><div><h2>Kubernetes Security ‚Äì The Ultimate Guide</h2><div><p>Dive deep into the ever evolving landscape of Kubernetes security, explore best practices, and discover potential pitfalls.</p></div><a href=\"https://landing.armosec.io/kubernetes_security_best_practices\">Learn More</a></div></div><h2>üîê What‚Äôs New in Kubernetes 1.34 Security</h2><h3>Built‚Äëin Mutual TLS for Pods (Alpha)&nbsp;</h3><p>Pods can now request short-lived X.509 certificates from the Kubernetes API server and use them to authenticate via mutual TLS. This enables a clean and native approach to in-cluster workload identity without relying on external tools or sidecars.</p><h3>Fine‚ÄëGrained Anonymous API Endpoint Control (Stable)&nbsp;</h3><p>Rather than disabling anonymous access cluster-wide, you can now configure it to apply only to specific safe paths (like /healthz, /livez, and /readyz). This prevents overly permissive anonymous access while preserving functionality for monitoring and load balancers.</p><h3><a href=\"https://www.armosec.io/blog/a-guide-for-using-kubernetes-rbac/\" target=\"_blank\" rel=\"noreferrer noopener\">RBAC</a> with Field &amp; Label Selectors for List/Delete (Stable)&nbsp;</h3><p>You can now restrict access to resources based on selectors in list, watch, and deleteCollection operations. For example, limit a kubelet to view only the pods on its node using spec.nodeName=$NODE.</p><h3>External JWT Signing via KMS or HSM (Beta)&nbsp;</h3><p>ServiceAccount tokens can now be signed using an external KMS or HSM via a new gRPC interface. This improves key security by enabling rotation, offloading signing from the API server, and aligning with compliance needs.</p><h3>Short-Lived Pod-Scoped Tokens for ImagePull (Beta)&nbsp;</h3><p>No more long-lived imagePullSecrets. Kubernetes can now use short-lived, per-pod tokens automatically generated for accessing private registries. These tokens are OIDC-compliant and auto-rotated by the system.</p><h3>CEL-Based In-Process Mutating Admission Policies (Beta)&nbsp;</h3><p>Kubernetes now supports <a href=\"https://www.armosec.io/blog/kubernetes-admission-controller/\" target=\"_blank\" rel=\"noreferrer noopener\">mutating admission policies</a> written using CEL (Common Expression Language) directly in the API server‚Äîno external webhook required. This simplifies setup and improves performance while supporting re-evaluation logic.</p><p>ARMO‚Äôs Kubescape, the CNCF‚Äôs Incubating open-source Kubernetes security platform, will enhance its<a href=\"https://github.com/kubescape/cel-admission-library/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> CEL admission control library</a> in the upcoming release to support these new in-process mutating policies. This will allow users to define and enforce mutating admission policies directly within Kubescape, leveraging the same CEL framework as Kubernetes itself.</p><h3>OCI Artifact Volumes (Beta)&nbsp;</h3><p>You can now mount artifacts stored in OCI registries directly into pods as read-only volumes. This is useful for securely distributing config files, binaries, or ML models without baking them into container images.</p><h3>üß† Why These Changes Matter</h3><figure><table><thead><tr></tr></thead><tbody><tr><td>Enables pod-to-API secure identity</td><td>Test alpha feature in dev clusters</td></tr><tr><td>Prevents overexposed unauthenticated access</td></tr><tr><td>Enforces least privilege at node/pod granularity</td><td>Update roles with selectors</td></tr><tr><td>Eliminates local key exposure</td><td>Integrate with existing KMS</td></tr><tr><td>Prevents static secret leakage</td><td>Migrate from imagePullSecrets</td></tr><tr><td>Simplifies secure mutation logic</td><td>Define CEL-based policies</td></tr><tr><td>Secure delivery of external files</td><td>Replace sidecar/manual content injection</td></tr></tbody></table></figure><p>The Kubernetes 1.34 release reflects a growing focus on , , and <strong>native, reliable policy enforcement</strong>. From in-cluster identities to hardened token workflows and registry access, these updates make it easier for platform teams to deliver secure infrastructure ‚Äì without reinventing the wheel.</p><p>Stay secure, stay curious.</p><p>‚Äî <a href=\"https://www.armosec.io\" target=\"_blank\" rel=\"noreferrer noopener\"></a><a href=\"https://github.com/kubescape/kubescape\" target=\"_blank\" rel=\"noreferrer noopener\"></a><em>, the open-source Kubernetes security platform</em> and one of the leading <a href=\"https://www.armosec.io/platform/kubernetes-security-posture-management/\">KSPM</a> solutions. </p><div><div><h2>Quickly ensure your Kubernetes is secured.</h2><div><p>Follow this simple checklist and make sure your Kubernetes security is covered in just a few steps.</p></div><a href=\"https://landing.armosec.io/kubernetes_checklist\">Read Now</a></div></div>","contentLength":3881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxfxsq/kubernetes_v134_is_coming_with_some_interesting/"},{"title":"OpenBao installation on Kubernetes - with TLS and more!","url":"https://nanibot.net/posts/vault","date":1755885253,"author":"/u/-NaniBot-","guid":570,"unread":true,"content":"<div><p>OpenBao is an open-source fork of HashiCorp‚Äôs Vault, created to ensure the project remains community-driven and permissively licensed. It provides a robust, transparent, and accessible solution for secrets management and data protection, offering a viable alternative for users who relied on Vault‚Äôs original open-source model.</p><p>The default Helm installation of OpenBao is enough for a dev environment but it needs some modifications for a full-fledged production deployment. In this blog post we‚Äôll learn about how a typical production deployment for OpenBao would look like.</p><p> I‚Äôm  new to OpenBao myself. Apologies for any mistakes/inaccuracies in my blog post. Feel free to e-mail me if you find something wrong.</p><p><code>mail: nanibot@nanibot.net</code></p><p>Here‚Äôs all the things that we‚Äôre going to configure for our OpenBao cluster:</p><ol><li><p>End-to-end TLS encryption for network traffic. Includes the OpenBao UI (with proxy SSL support!)</p></li><li><p>High availability via OpenBao‚Äôs internal Raft implementation.</p></li><li><p>Auto-unseal without relying on a cloud KMS solution ( This might  be secure - depending on whether you feel comfortable storing the unseal key as a kubernetes secret or not)</p></li></ol><p> Currently, static unseal is only available in a nightly build (<code>openbao/openbao-nightly:2.4.0-nightly1752150785</code>) but is planned to be released as part of the 2.4.0 release</p><ul><li><p>I‚Äôll use  for creating the necessary certificates and  for exposing the UI</p></li><li><p>I‚Äôll assume the chart is going to be installed in the  namespace and the release is called </p></li></ul><ol><li>Certificate to be used for TLS. In this example, I‚Äôm using a wildcard certificate issued by my own CA. The certificate is stored in a kubernetes secret named <code>internal-wildcard-cert-secret</code> in the  namespace</li></ol><pre tabindex=\"0\"><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: internal-wildcard-cert\n  namespace: vault-system\nspec:\n  secretName: internal-wildcard-cert-secret\n  duration: 2160h\n  renewBefore: 720h\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n    rotationPolicy: Always\n  subject:\n    organizations:\n      - Umbrella\n    organizationalUnits:\n      - nanibot.net\n  dnsNames:\n    - \"vault-production-openbao-active\"\n    - \"*.vault-production-openbao-internal\"\n    - \"*.vault-production-openbao-internal.vault-system\"\n    - \"*.vault-production-openbao-internal.vault-system.svc\"\n    - \"*.vault-production-openbao-internal.vault-system.svc.cluster.local\"\n  ipAddresses:\n    - \"127.0.0.1\"\n  issuerRef:\n    name: pki-production-selfsigned-issuer\n    kind: ClusterIssuer\n</code></pre><p> The dnsName entry <code>vault-production-openbao-active</code> refers to the Kubernetes service that‚Äôs created by the Helm chart. This will also be our API Address - the hostname that the Vault API will be exposed at.</p><ol start=\"2\"><li>Unseal key for static auto-unseal</li></ol><p>We need to create a kubernetes secret containing the unseal key for static auto-unseal to work. We can do this by running the following commands:</p><pre tabindex=\"0\"><code>openssl rand -out unseal-umbrella-1.key 32\nkubectl create secret generic unseal-key --from-file=unseal-umbrella-1.key=./unseal-umbrella-1.key\n</code></pre><pre tabindex=\"0\"><code>global:\n  tlsDisable: false\nserver:\n  image:\n    repository: \"openbao/openbao-nightly\"\n    tag: \"2.4.0-nightly1752150785\"\n  extraEnvironmentVars:\n    BAO_CACERT: \"/certs/ca.crt\"\n  ha:\n    enabled: true\n    apiAddr: \"https://vault-production-openbao-active:8200\"\n    raft:\n      enabled: true\n      config: |\n        ui = true\n\n        listener \"tcp\" {\n          address = \"[::]:8200\"\n          cluster_address = \"[::]:8201\"\n          tls_cert_file = \"/certs/tls.crt\"\n          tls_key_file = \"/certs/tls.key\"\n        }\n\n        storage \"raft\" {\n          path = \"/openbao/data\"\n        }\n\n        seal \"static\" {\n          current_key_id = \"umbrella-1\"\n          current_key = \"file:///keys/unseal-umbrella-1.key\"\n        }\n\n        service_registration \"kubernetes\" {}\n  auditStorage:\n    enabled: true\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      nginx.ingress.kubernetes.io/proxy-ssl-verify: \"on\"\n      nginx.ingress.kubernetes.io/proxy-ssl-name: \"vault-production-openbao-active\"\n      nginx.ingress.kubernetes.io/proxy-ssl-secret: \"vault-system/internal-wildcard-cert-secret\"\n    ingressClassName: \"nginx\"\n    hosts:\n      - host: vault.nanibot.net\n    tls:\n      - secretName: public-wildcard-cert-secret\n        hosts:\n          - vault.nanibot.net\n  volumes:\n    - name: unseal-key\n      secret:\n        secretName: unseal-key\n    - name: certs\n      secret:\n        secretName: internal-wildcard-cert-secret\n  volumeMounts:\n    - mountPath: /keys\n      name: unseal-key\n      readOnly: true\n    - mountPath: /certs\n      name: certs\n      readOnly: true\nui:\n  enabled: true\n</code></pre><ol><li><p>We enable TLS by setting  to . This enables https endpoints for the relevant services.</p></li><li><p>We use the nightly build of OpenBao which has support for static auto-unseal (<code>openbao/openbao-nightly:2.4.0-nightly1752150785</code>).</p></li><li><p> is set to the path of our CA certificate so that OpenBao can verify the TLS certificate of other nodes in the cluster.</p></li><li><p>We enable HA and Raft storage.</p></li><li><p>We configure the Raft listener to use TLS and bind to all interfaces. We also provide the paths to our TLS certificate and key.</p></li><li><p>We configure static auto-unseal using a file-based unseal key.</p></li><li><p>apiAddr is set to the DNS name of the active OpenBao node (Kubernetes service created by the Helm chart). This is required for the UI to work properly with proxy SSL.</p></li><li><p>proxy-ssl-name is set to the DNS name of the active OpenBao node. This is required for the UI to work properly with proxy SSL.</p></li><li><p>Other  parameters are set to ensure that the ingress controller can verify the TLS certificate of the OpenBao server.</p></li><li><p>We enable the UI by setting  to .</p></li><li><p>Volumes and volume mounts are added for the unseal key and TLS certificates.</p></li></ol><ol><li><p>Install the helm chart using the above values.yaml file</p></li><li><p>Initialize the OpenBao cluster by running the following command (Assuming the pod name is <code>vault-production-openbao-0</code>):</p></li></ol><pre tabindex=\"0\"><code>kubectl exec -it vault-production-openbao-0 -- bao operator init\n</code></pre><ol start=\"3\"><li><p>Store the unseal key(s) and the root token somewhere safe</p></li><li><p>Join the other nodes to the cluster by running the following command on each of them:</p></li></ol><pre tabindex=\"0\"><code>kubectl exec -it vault-production-openbao-1 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200\nkubectl exec -it vault-production-openbao-2 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200\n</code></pre><p>That‚Äôs it! You should now have a fully functional OpenBao cluster running on Kubernetes with TLS, HA and auto-unseal support.</p><p>The Web UI should be accessible at <code>https://vault.nanibot.net</code> (or whatever host you configured in the ingress).</p></div>","contentLength":6709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxdgvd/openbao_installation_on_kubernetes_with_tls_and/"},{"title":"Quick background and Demo on kagent - Cloud Native Agentic AI - with Christian Posta and Mike Petersen","url":"https://youtube.com/live/KUOIRZsWv38","date":1755882405,"author":"/u/mpetersen_loft-sh","guid":569,"unread":true,"content":"<div><p>Christian Posta gives some background on kagent, what they looked into when building agents on Kubernetes. Then I install kagent in a vCluster - covering most of the quick start guide + adding in a self hosted LLM and ingress.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mpetersen_loft-sh\"> /u/mpetersen_loft-sh </a>","contentLength":266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxc7w5/quick_background_and_demo_on_kagent_cloud_native/"},{"title":"How to run database migrations in Kubernetes","url":"https://packagemain.tech/p/database-migrations-in-kubernetes","date":1755861685,"author":"/u/der_gopher","guid":568,"unread":true,"content":"<p>In the era of microservices and Kubernetes, managing database migrations has become more complex than ever. Traditional methods of running migrations during application startup are no longer sufficient. </p><p>This article explores various approaches to handling database migrations in a Kubernetes environment, with a focus on Golang-based solutions.</p><p>Kubernetes introduces new challenges for database migrations:</p><ul><li><p>Multiple replicas starting simultaneously.</p></li><li><p>Need for coordination to avoid concurrent migrations.</p></li><li><p>Separation of concerns between application and migration logic.</p></li></ul><p><a href=\"https://packagemain.tech/i/149097592/database-migrations\" rel=\"\">post</a></p><ul><li><p>Widely used and supports numerous databases.</p></li><li><p>Supports various migration sources (local files, S3, Google Storage).</p></li></ul><ul><li><p>Supports main SQL databases.</p></li><li><p>Allows migrations written in Go for complex scenarios.</p></li><li><p>Flexible versioning schemas.</p></li></ul><ul><li><p>Powerful database schema management tool</p></li><li><p>Supports declarative and versioned migrations.</p></li><li><p>Offers integrity checks and migration linting.</p></li><li><p>Provides GitHub Actions and Terraform provider.</p></li></ul><p>A naive implementation would be to run the code of the migration directly inside your main function before you start your server.</p><p><em><strong>Example using golang-migrate:</strong></em></p><pre><code>package main\n\nimport (\n    \"database/sql\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n\n    \"github.com/golang-migrate/migrate/v4\"\n    \"github.com/golang-migrate/migrate/v4/database/postgres\"\n    _ \"github.com/golang-migrate/migrate/v4/source/file\"\n    _ \"github.com/lib/pq\"\n)\n\nfunc main() {\n    // Database connection parameters\n    url := \"postgres://user:pass@localhost:5432/dbname\"\n\n    // Connect to the database\n    db, err := sql.Open(\"postgres\", url)\n    if err != nil {\n        log.Fatalf(\"could not connect to database: %v\", err)\n    }\n    defer db.Close()\n\n    // Run migrations\n    if err := runMigrations(db); err != nil {\n        log.Fatalf(\"could not run migrations: %v\", err)\n    }\n\n    // Run the application, for example start the server\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        log.Fatalf(\"server failed to start: %v\", err)\n    }\n}\n\nfunc runMigrations(db *sql.DB) error {\n    driver, err := postgres.WithInstance(db, &amp;postgres.Config{})\n    if err != nil {\n        return fmt.Errorf(\"could not create database driver: %w\", err)\n    }\n\n    m, err := migrate.NewWithDatabaseInstance(\n        \"file://migrations\", // Path to your migration files\n        \"postgres\",          // Database type\n        driver,\n    )\n    if err != nil {\n        return fmt.Errorf(\"could not create migrate instance: %w\", err)\n    }\n\n    if err := m.Up(); err != nil &amp;&amp; err != migrate.ErrNoChange {\n        return fmt.Errorf(\"could not run migrations: %w\", err)\n    }\n\n    log.Println(\"migrations completed successfully\")\n    return nil\n}</code></pre><p>However, these could cause different issues like your migrations being slow and Kubernetes considering the pod didn‚Äôt start successfully and therefore killing it. You could run those migrations in a Go routine, but how do you handle failures then? </p><p>In case when multiple pods are created at the same time, you would have a potential concurrency problem. </p><p>It also means your migrations need to be inside your Docker image.</p><p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\" rel=\"\">initContainers</a></p><p>If the initContainer fails, the blue/green deployment from Kubernetes won‚Äôt go further and your previous pods stays where they are. It prevents having a newer version of the code without the planned migration. </p><pre><code>initContainers:\n  - name: migrations\n    image: migrate/migrate:latest\n    command: ['/migrate']\n    args: ['-source', 'file:///migrations', '-database','postgres://user:pass@db:5432/dbname', 'up']</code></pre><p><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\" rel=\"\">Kubernetes Job </a></p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrate\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: your-migration-image:latest\n        command: ['/app/migrate']</code></pre><p>You can also combine it with initContainers making sure that the pod starts only when the job is successful.</p><pre><code>initContainers:\n  - name: migrations-wait\n    image: ghcr.io/groundnuty/k8s-wait-for:v2.0\n    args:\n      - \"job\"\n      - \"my-migration-job\"</code></pre><p><a href=\"https://helm.sh/docs/topics/charts_hooks/\" rel=\"\">hooks</a></p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-migrations\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: migrations\n          image: your-migrations-image:tag\n          command: [\"./run-migrations.sh\"]</code></pre><p>There are pre-install and post-install hooks. </p><ol><li><p>Decoupling Migrations from Application Code</p><ol><li><p>Create separate Docker image for migrations.</p></li><li><p>Use tools like Atlas to manage migrations independently.</p></li></ol></li><li><p>Version Control for Migrations</p><ol><li><p>Store migration files in your Git repository.</p></li><li><p>Use sequential or timestamp-based versioning.</p></li></ol></li><li><ol><li><p>Ensure migrations can be run multiple times without side effects.</p></li></ol></li><li><ol><li><p>Implement and test rollback procedures for each migration.</p></li></ol></li><li><ol><li><p>Use tools like Atlas Cloud for visibility into migration history.</p></li></ol></li></ol><p>Managing database migrations in a Kubernetes environment requires careful planning and execution. </p><p>By leveraging tools like golang-migrate, goose, or atlas, and following best practices, you can create robust, scalable, and maintainable migration strategies. </p><p>Remember to decouple migrations from application code, use version control, and implement proper monitoring to ensure smooth database evolution in your Kubernetes-based architecture.</p>","contentLength":5325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mx3rq2/how_to_run_database_migrations_in_kubernetes/"}],"tags":["dev","reddit","k8s"]}