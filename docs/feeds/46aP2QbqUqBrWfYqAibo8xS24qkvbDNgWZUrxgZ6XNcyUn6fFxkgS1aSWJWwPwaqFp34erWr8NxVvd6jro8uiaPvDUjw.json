{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Calico vs Cilium as CNI","url":"https://www.reddit.com/r/kubernetes/comments/1ib2dx9/calico_vs_cilium_as_cni/","date":1737962080,"author":"/u/Flimsy_Tomato4847","guid":298,"unread":true,"content":"<p>I am building an onprem Cluster with 2 HA Proxy Setup, 3 Master and 2 Worker Nodes. For Services I want to implement an nginx Ingress to route the traffic to the endpoints. </p><p>Planning to implement Harbor as Image Registry in Gitlab and then use Security Features for „hardening“ the Cluster network.</p><p>What do you think is for this use case the better CNI ? </p><p>Cilium is since the Cisco takeover in critics because we all know that in long term Cisco is mostly interested in money and not in developing products. I know that Cncf gratuated means that at least one project contributor is not from Cisco. </p><p>So i am a bit more interested in Calico and Security Features.</p>","contentLength":661,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event driven restart of Pods?","url":"https://www.reddit.com/r/kubernetes/comments/1iax3fa/event_driven_restart_of_pods/","date":1737943849,"author":"/u/ButterscotchWeak1192","guid":301,"unread":true,"content":"<p>Context: we have a particular Pod which likes to hang, for unknown to us reasons and conditions (it's external software, we can't modify, and logs don't show anything).</p><p>The most accurate way to tell when it's happening is by checking a liveness probe. We have monitoring set up for particular URL and we can check for non 2xx status.</p><p>This chart we talk about deploys  Pod as well as  Pods. Each is separate Deployment.</p><p>The issue: when  Pod fails it's liveness probe, it gets restarted by k8s. But we also need to restart  nodes, because for some reason it looks like they lose connection in such way that they don't pick up work, and only restart helps. And  in this case .  Pod first, then .</p><p>Restart in case of liveness probe restarts only affected Pod. Currently, to restart workers too, I installed <a href=\"https://keda.sh/\">KEDA</a> in cluster and created <a href=\"https://keda.sh/docs/2.14/concepts/scaling-jobs/\">ScaleJob</a> object to trigger deployment restart. As trigger we use <code>kube_pod_container_status_restarts_total</code> Prometheus query:</p><pre><code>apiVersion: keda.sh/v1alpha1 kind: ScaledJob metadata: name: n8n-restart-job-scaler namespace: company spec: jobTargetRef: kind: Job name: n8n-worker-restart-job spec: jobTargetRef: template: spec: containers: - name: kubectl image: bitnami/kubectl:latest # imagePullPolicy: Always command: [\"/bin/sh\", \"-c\"] args: [\"kubectl rollout restart deployment n8n-worker -n company\"] backoffLimit: 4 pollingInterval: 15 # Check every 15 seconds (default: 30) successfulJobsHistoryLimit: 1 # How many completed jobs should be kept. failedJobsHistoryLimit: 1 # How many failed jobs should be kept. triggers: - type: prometheus metadata: serverAddress: https://&lt;DOMAIN&gt;.com/select/0/prometheus metricName: pod_liveness_failure threshold: \"1\" # Triggers when any liveness failure alert is active query: increase(kube_pod_container_status_restarts_total{pod=~\"^n8n-[^worker].*$\"}[1m]) &gt; 0 </code></pre><p>This kind of works. I mean it succesfully triggers restarts. But: - in current setup it triggers multiple restarts when there was only single liveness probe failure. This extends downtime<p> - depending on different settings for check time, there might be a slight delay between time of event, and time of triggering</p></p><p>I've been thinking about more event-driven workflow. So that when event in cluster happens, I can perform matching action. but I don't know what options would be most suitable for this task.</p><p>What do you suggest here? Maybe you've had such problem? How would you deal with it?</p><p>if something is unclear or I didn't provide something, ask below and I'll provide more info.</p>","contentLength":2504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microk8s - User \"system:node:k8snode01\" cannot list resource \"pods\" in API group","url":"https://www.reddit.com/r/kubernetes/comments/1iat7jr/microk8s_user_systemnodek8snode01_cannot_list/","date":1737932605,"author":"/u/myridan86","guid":297,"unread":true,"content":"<p>For some reason, I started receiving this error on one of the nodes. Apparently everything is working, some pods were crashing, but I've already removed them and they started up normally...</p><p>I looked for the message below on the internet, but I didn't find much...</p><p><strong>Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68418]: Error from server (Forbidden): pods is forbidden: User \"system:node:k8snode01\" cannot list resource \"pods\" in API group \"\" at the cluster scope: can only list/watch pods with spec.nodeName field selector</strong></p><pre><code>Jan 26 19:27:13 k8snode01 sudo[68404]: root : PWD=/var/snap/microk8s/7589 ; USER=root ; ENV=PATH=/snap/microk8s/7589/usr/bin:/snap/microk8s/7589/bin:/snap/microk8s/7589/usr/sbin:/snap/microk8s/7589/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin LD_LIBRARY_PATH=/var/lib/snapd/lib/gl:/var/lib/snapd/lib/gl32:/var/lib/snapd/void:/snap/microk8s/7589/lib:/snap/microk8s/7589/usr/lib:/snap/microk8s/7589/lib/x86_64-linux-gnu:/snap/microk8s/7589/usr/lib/x86_64-linux-gnu:/snap/microk8s/7589/usr/lib/x86_64-linux-gnu/ceph: PYTHONPATH=/snap/microk8s/7589/usr/lib/python3.8:/snap/microk8s/7589/lib/python3.8/site-packages:/snap/microk8s/7589/usr/lib/python3/dist-packages ; COMMAND=/snap/microk8s/7589/bin/ctr --address=/var/snap/microk8s/common/run/containerd.sock --namespace k8s.io container ls -q Jan 26 19:27:13 k8snode01 sudo[68404]: pam_unix(sudo:session): session opened for user root(uid=0) by (uid=0) Jan 26 19:27:13 k8snode01 sudo[68404]: pam_unix(sudo:session): session closed for user root Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68418]: Error from server (Forbidden): pods is forbidden: User \"system:node:k8snode01\" cannot list resource \"pods\" in API group \"\" at the cluster scope: can only list/watch pods with spec.nodeName field selector Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: Traceback (most recent call last): Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/scripts/kill-host-pods.py\", line 104, in &lt;module&gt; Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: main() Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/usr/lib/python3/dist-packages/click/core.py\", line 764, in __call__ Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: return self.main(*args, **kwargs) Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/usr/lib/python3/dist-packages/click/core.py\", line 717, in main Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: rv = self.invoke(ctx) Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/usr/lib/python3/dist-packages/click/core.py\", line 956, in invoke Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: return ctx.invoke(self.callback, **ctx.params) Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/usr/lib/python3/dist-packages/click/core.py\", line 555, in invoke Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: return callback(*args, **kwargs) Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/scripts/kill-host-pods.py\", line 84, in main Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: out = subprocess.check_output([*KUBECTL, \"get\", \"pod\", \"-o\", \"json\", *selector]) Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/usr/lib/python3.8/subprocess.py\", line 415, in check_output Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: return run(*popenargs, stdout=PIPE, timeout=timeout, check=True, Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: File \"/snap/microk8s/7589/usr/lib/python3.8/subprocess.py\", line 516, in run Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: raise CalledProcessError(retcode, process.args, Jan 26 19:27:13 k8snode01 microk8s.daemon-apiserver-kicker[68393]: subprocess.CalledProcessError: Command '['/snap/microk8s/7589/kubectl', '--kubeconfig=/var/snap/microk8s/7589/credentials/kubelet.config', 'get', 'pod', '-o', 'json', '-A']' returned non-zero exit status 1. </code></pre><p>If anyone has any idea what it could be... because memory, disk, processing, network... I've already checked.</p>","contentLength":4426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"k3s pods networking","url":"https://www.reddit.com/r/kubernetes/comments/1iaszwq/k3s_pods_networking/","date":1737932042,"author":"/u/crewman4","guid":296,"unread":true,"content":"<p>im not used to \"onprem\" k8s and am testing setting up an k3s in my homelab and i cant get it to work. ive been testing this on debian server and whatever i do, fresh installs and such, i cant enter a pod and wget an external internet site. all sites point to some IP (213.163.146.142:443)</p><p>Non-authoritative answer:</p><p>i can resolve dns , but thats hosted internally. everything else works from debian server and no firewalls active. ive been chatGPTing for hours but im stuck. ive rolled new servers and tested everything :P</p>","contentLength":519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Way to Collect Traces for Tempo","url":"https://www.reddit.com/r/kubernetes/comments/1iaq420/best_way_to_collect_traces_for_tempo/","date":1737924977,"author":"/u/Sule2626","guid":300,"unread":true,"content":"<p>I'm currently using Prometheus, Grafana, and Loki in my stack, and I'm planning to integrate Tempo for distributed tracing. However, I'm still exploring the best way to collect traces efficiently.</p><p>I've looked into Jaeger and OpenTelemetry:</p><ul><li> seems to require a relatively large infrastructure, which feels like overkill for my use case.</li><li> looks promising, but it overlaps with some functionality I already have covered by Prometheus (metrics) and Loki (logs).</li></ul><p>Does anyone have recommendations or insights on the most efficient way to implement tracing with Tempo? I'm particularly interested in keeping the setup lightweight and complementary to my existing stack.</p>","contentLength":658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Has the behaviour of maxUnavailable and maxSurge for RollingUpdates changed since v1.21.9","url":"https://www.reddit.com/r/kubernetes/comments/1iam71l/has_the_behaviour_of_maxunavailable_and_maxsurge/","date":1737916265,"author":"/u/mrnadaara","guid":299,"unread":true,"content":"<p>We've deployed a new cluster with v1.30.7 and tried to deploy a deployment with a maxSurge of 1 and maxUnavailable with 0. The new pod remains stuck in Pending with the following reasons:</p><p>0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.</p><p>Changing maxUnavailable to 1 fixes it but I'm curious as to why it fails with the new version when it worked fine in the old version. It exceeds the replica count when doing a rolling update so it makes sense the pod wouldn't be scheduled until the old one is deleted, but since we've set the maxUnavailable to 0 the old pods are never deleted. This in theory shouldn't have worked in the old version as well. Am I misconstruing things here?</p>","contentLength":757,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit"]}