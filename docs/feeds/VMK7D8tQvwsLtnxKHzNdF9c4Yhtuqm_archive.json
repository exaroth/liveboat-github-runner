{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HN","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"We were wrong about GPUs","url":"https://fly.io/blog/wrong-about-gpu/","date":1739572591,"author":"mxstbr","guid":146,"unread":true,"content":"<div><p>We’re building a public cloud, on hardware we own. We raised money to do that, and to place some bets; one of them: GPU-enabling our customers. A progress report: GPUs aren’t going anywhere, but: GPUs aren’t going anywhere.</p></div><p>A Fly Machine is a <a href=\"https://fly.io/blog/docker-without-docker/\">Docker/OCI container</a> running inside a hardware-virtualized virtual machine somewhere on our global fleet of bare-metal worker servers. A GPU Machine is a Fly Machine with a hardware-mapped Nvidia GPU. It’s a Fly Machine that can do fast CUDA.</p><p>Like everybody else in our industry, we were right about the importance of AI/ML. If anything, we underestimated its importance. But the product we came up with probably doesn’t fit the moment. It’s a bet that doesn’t feel like it’s paying off.</p><p><strong>If you’re using Fly GPU Machines, don’t freak out; we’re not getting rid of them.</strong> But if you’re waiting for us to do something bigger with them, a v2 of the product, you’ll probably be waiting awhile.</p><p>GPU Machines were not a small project for us. Fly Machines run on an idiosyncratically small hypervisor (normally Firecracker, but for GPU Machines <a href=\"https://github.com/cloud-hypervisor/cloud-hypervisor\">Intel’s Cloud Hypervisor</a>, a very similar Rust codebase that supports PCI passthrough). The Nvidia ecosystem is not geared to supporting micro-VM hypervisors.</p><p>GPUs <a href=\"https://googleprojectzero.blogspot.com/2020/09/attacking-qualcomm-adreno-gpu.html\">terrified our security team</a>. A GPU is just about the worst case hardware peripheral: intense multi-directional direct memory transfers</p><div><p>(not even bidirectional: in common configurations, GPUs talk to each other)</p></div><p>with arbitrary, end-user controlled computation, all operating outside our normal security boundary.</p><p>We did a couple expensive things to mitigate the risk. We shipped GPUs on dedicated server hardware, so that GPU- and non-GPU workloads weren’t mixed. Because of that, the only reason for a Fly Machine to be scheduled on a GPU machine was that it needed a PCI BDF for an Nvidia GPU, and there’s a limited number of those available on any box. Those GPU servers were drastically less utilized and thus less cost-effective than our ordinary servers.</p><p>We funded two very large security assessments, from <a href=\"https://www.atredis.com/\">Atredis</a> and <a href=\"https://tetrelsec.com/\">Tetrel</a>, to evaluate our GPU deployment. Matt Braun is writing up those assessments now. They were not cheap, and they took time.</p><p>Security wasn’t directly the biggest cost we had to deal with, but it was an indirect cause for a subtle reason.</p><p>We could have shipped GPUs very quickly by doing what Nvidia recommended: standing up a standard K8s cluster to schedule GPU jobs on. Had we taken that path, and let our GPU users share a single Linux kernel, we’d have been on Nvidia’s driver happy-path.</p><p>Alternatively, we could have used a conventional hypervisor. Nvidia suggested VMware (heh). But they could have gotten things working had we used QEMU. We like QEMU fine, and could have talked ourselves into a security story for it, but the whole point of Fly Machines is that they take milliseconds to start. We could not have offered our desired Developer Experience on the Nvidia happy-path.</p><p>Instead, we burned months trying (and ultimately failing) to get Nvidia’s host drivers working to map <a href=\"https://www.nvidia.com/en-us/data-center/virtual-solutions/\">virtualized GPUs</a> into Intel Cloud Hypervisor. At one point, we hex-edited the closed-source drivers to trick them into thinking our hypervisor was QEMU.</p><p>I’m not sure any of this really mattered in the end. There’s a segment of the market we weren’t ever really able to explore because Nvidia’s driver support kept us from thin-slicing GPUs. We’d have been able to put together a really cheap offering for developers if we hadn’t run up against that, and developers love “cheap”, but I can’t prove that those customers are real.</p><p>On the other hand, we’re committed to delivering the Fly Machine DX for GPU workloads. Beyond the PCI/IOMMU drama, just getting an entire hardware GPU working in a Fly Machine was a lift. We needed Fly Machines that would come up with the right Nvidia drivers; our stack was built assuming that the customer’s OCI container almost entirely defined the root filesystem for a Machine. We had to engineer around that in our  orchestrator. And almost everything people want to do with GPUs involves efficiently grabbing huge files full of model weights. Also annoying!</p><p>And, of course, we bought GPUs. A lot of GPUs. Expensive GPUs.</p><p>The biggest problem: developers don’t want GPUs. They don’t even want AI/ML models. They want LLMs.  may have smart, fussy opinions on how to get their models loaded with CUDA, and what the best GPU is. But  don’t care about any of that. When a software developer shipping an app comes looking for a way for their app to deliver prompts to an LLM, you can’t just give them a GPU.</p><p>For those developers, who probably make up most of the market, it doesn’t seem plausible for an insurgent public cloud to compete with OpenAI and Anthropic. Their APIs are fast enough, and developers thinking about performance in terms of “tokens per second” aren’t counting milliseconds.</p><div><p>(you should all feel sympathy for us)</p></div><p>This makes us sad because we really like the point in the solution space we found. Developers shipping apps on Amazon will outsource to other public clouds to get cost-effective access to GPUs. But then they’ll faceplant trying to handle data and model weights, backhauling gigabytes (at significant expense) from S3. We have app servers, GPUs, and object storage all under the same top-of-rack switch. But inference latency just doesn’t seem to matter yet, so the market doesn’t care.</p><p>Past that, and just considering the system engineers who do care about GPUs rather than LLMs: the hardware product/market fit here is really rough.</p><p>People doing serious AI work want galactically huge amounts of GPU compute. A whole enterprise A100 is a compromise position for them; they want an SXM cluster of H100s.</p><div><p>Near as we can tell, MIG gives you a UUID to talk to the host driver, not a PCI device.</p></div><p>We think there’s probably a market for users doing lightweight ML work getting tiny GPUs. <a href=\"https://www.nvidia.com/en-us/technologies/multi-instance-gpu/\">This is what Nvidia MIG does</a>, slicing a big GPU into arbitrarily small virtual GPUs. But for fully-virtualized workloads, it’s not baked; we can’t use it. And I’m not sure how many of those customers there are, or whether we’d get the density of customers per server that we need.</p><p><a href=\"https://fly.io/blog/cutting-prices-for-l40s-gpus-in-half\">That leaves the L40S customers</a>. There are a bunch of these! We dropped L40S prices last year, not because we were sour on GPUs but because they’re the one part we have in our inventory people seem to get a lot of use out of. We’re happy with them. But they’re just another kind of compute that some apps need; they’re not a driver of our core business. They’re not the GPU bet paying off.</p><p>Really, all of this is just a long way of saying that for most software developers, “AI-enabling” their app is best done with API calls to things like Claude and GPT, Replicate and RunPod.</p><p>A very useful way to look at a startup is that it’s a race to learn stuff. So, what’s our report card?</p><p>First off, when we embarked down this path in 2022, we were (like many other companies) operating in a sort of phlogiston era of AI/ML. The industry attention to AI had not yet collapsed around a small number of foundational LLM models. We expected there to be a diversity of  models, the world <a href=\"https://github.com/elixir-nx/bumblebee\" title=\"\">Elixir Bumblebee</a> looks forward to, where people pull different AI workloads off the shelf the same way they do Ruby gems.</p><p>But <a href=\"https://www.cursor.com/\" title=\"\">Cursor happened</a>, and, as they say, how are you going to keep ‘em down on the farm once they’ve seen Karl Hungus? It seems much clearer where things are heading.</p><p>GPUs were a test of a Fly.io company credo: as we think about core features, we design for 10,000 developers, not for 5-6. It took a minute, but the credo wins here: GPU workloads for the 10,001st developer are a niche thing.</p><p>Another way to look at a startup is as a series of bets. We put a lot of chips down here. But the buy-in for this tournament gave us a lot of chips to play with. Never making a big bet of any sort isn’t a winning strategy. I’d rather we’d flopped the nut straight, but I think going in on this hand was the right call.</p><p>A really important thing to keep in mind here, and something I think a lot of startup thinkers sleep on, is the extent to which this bet involved acquiring assets. Obviously, some of our <a href=\"https://fly.io/blog/the-exit-interview-jp/\" title=\"\">costs here aren’t recoverable</a>. But the hardware parts that aren’t generating revenue will ultimately get liquidated; like with <a href=\"https://fly.io/blog/32-bit-real-estate/\" title=\"\">our portfolio of IPv4 addresses</a>, I’m even more comfortable making bets backed by tradable assets with durable value.</p><p>In the end, I don’t think GPU Fly Machines were going to be a hit for us no matter what we did. Because of that, one thing I’m very happy about is that we didn’t compromise the rest of the product for them. Security concerns slowed us down to where we probably learned what we needed to learn a couple months later than we could have otherwise, but we’re scaling back our GPU ambitions without having sacrificed <a href=\"https://fly.io/blog/sandboxing-and-workload-isolation/\" title=\"\">any of our isolation story</a>, and, ironically, GPUs  are making that story a lot more important. The same thing goes for our Fly Machine developer experience.</p><p>We started this company building a Javascript runtime for edge computing. We learned that our customers didn’t want a new Javascript runtime; they just wanted their native code to work. <a href=\"https://news.ycombinator.com/item?id=22616857\" title=\"\">We shipped containers</a>, and no convincing was needed. We were wrong about Javascript edge functions, and I think we were wrong about GPUs. That’s usually how we figure out the right answers:  by being wrong about a lot of stuff.</p>","contentLength":9514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43053844"},{"title":"The hardest working font in Manhattan","url":"https://aresluna.org/the-hardest-working-font-in-manhattan/","date":1739569549,"author":"robinhouston","guid":144,"unread":true,"content":"<p>\n\t\tIn 2007, on my first trip to New York City, I grabbed a brand-new DSLR camera and photographed all the fonts I was supposed to love. I admired American Typewriter in all of the I &lt;3 NYC logos, watched Akzidenz Grotesk and Helvetica fighting over the subway signs, and even caught an occasional appearance of the flawlessly-named Gotham, still a year before it skyrocketed in popularity via Barack Obama’s first campaign. \n\t</p><p>\n\t\tBut there was one font I didn’t even notice, even though it was everywhere around me.\n\t</p><p>\t\t\n\t\tLast year in New York, I walked over 100 miles and took thousands of photos of one and one font only.\n\t</p><p>\t\t\n\t\tThe font’s name is Gorton.\n\t</p><p>\t\t\n\t\tIt’s hard to believe today that there was a time before I knew of Gorton and all its quirks and mysteries. The first time I realized the font even existed was some time in 2017, when I was researching for <a target=\"_blank\" href=\"https://shifthappens.site\">my book about the history of typing</a>. \n\t</p><p>\t\t\n\t\tMany keyboards, especially older ones, sported a particular distinctive font on their keycaps. It was unusually square in proportions, and a weird mélange of “mechanical” and “childish.”\n\t</p><p>\t\t \n\t\tThe more I looked at it, the more I realized how bizarre and amateurish it was. The G always felt like it was about to roll away on its side. There was a goofy wavy hook sticking out of Q. P and R were often too wide. &amp; and @ symbols would be laughed away in a type crit, and the endings of C felt like grabbing something next to it – a beginning of a ligature that never came.\n\t</p><p>\t\t\n\t\tThe strangeness extended to the digits. There was a top-flatted 3 resembling a Cyrillic letter, 7 sloping down in a unique way, a very geometric 4, an unusual – perhaps even naïve – symmetry between 6 and 9, and a conflation of O with 0 that would be a fireable offense elsewhere.\n\t</p><p>\t\t\t\n\t\tLooking at just a few keyboards, it was also obvious that it wasn’t just one rigid font. There were always variations, sometimes even on one keyboard. 0 came square, dotted, or slashed. The usually very narrow letter I sometimes sported serifs. The R and the 6 moved their middles higher or lower. There also seemed to be a narrower version of the font, deployed when a keycap needed a word and not just a letter. (Lowercase letters existed too, but not very often.) \n\t</p><p>\t\t\n\t\tMy first thought was: What a mess. Is this how “<a target=\"_blank\" href=\"https://fonts.ilovetypography.com/category/grotesque\">grotesque</a>” fonts got their name?\n\t</p><p>\t\t\n\t\tThen, the second thought: I kind of like it.\n\t</p><figcaption>The most distinctive letterforms of Gorton</figcaption><p>\t\t\n\t\tBut what font was it? What The Font website posited TT Rounds, Identifont suggested it could be Divulge, my early guess was DIN Rounded or something related to road signage. Whatever it was, a flat R clearly separated it from Helvetica, and the shapes were not as round as even the un-rounded Gotham’s.\n\t</p><p>\t\t\n\t\tA few places for keyboard nerds referred to the font as “Gorton,” but that phrase yielded zero results anywhere I typically looked for fonts I could download and install.\n\t</p><p>\t\t\t\t\n\t\tI originally thought this had to do with how keys were made. Only in newer keyboards are the letters printed on top of the keys, or charred from their surface by a laser. In older ones – those from the early 1960s laboratory computers, or the 1980s microcomputers –&nbsp;the way every key was constructed was by first molding the letter from plastic of one color, and then grabbing a different plastic and molding the key around the letter. A Gorton letter was as physical as the key itself. It made the keys virtually indestructible – the legend could not wear off any more than its key – and I imagined required some specialized keyboard-making machinery that came with the “keyboard font” already there.\n\t</p><figcaption>\n\t\tAn example of a “double-shot” key from above and from below\n\t</figcaption><p>\t\t\t\n\t\tBut then, I started seeing Gorton in other places.\n\t</p><p>\t\t\t\t\t\n\t\tHours of looking at close-ups of keys made me sensitive to the peculiar shapes of some of its letters. No other font had a Q, a 9, or a C that looked like this.\n\t</p><p>\t\t\t\t\t\n\t\tOne day, I saw what felt like Gorton it on a ferry traversing the waters Bay Area. A few weeks later, I spotted it on a sign in a national park. Then on an intercom. On a street lighting access cover. In an elevator. At my dentist’s office. In an alley. \n\t</p><p>\t\t\t\t\t\t\n\t\tThese had one thing in common. All of the letters were carved into the respective base material – metal, plastic, wood. The removed shapes were often filled in with a different color, but sometimes left alone.\n\t</p><p>\t\t\t\t\t\n\t\tAt one point someone explained to me Gorton must have been a routing font, meant to be carved out by a milling machine rather than painted on top or impressed with an inked press.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSome searches quickly led me to George Gorton Machine Co., a Wisconsin-based company which produced various engraving machines. The original model 1 led to model 1A and then 3U and then, half a decade later, P1-2. They were all pantograph engravers: They allowed you to install one or more letter templates and then trace their shape by hand. A matching rotating cutter would mimic your movements, and the specially configured arms would enlarge or reduce the output to the size you wanted.\n\t\t</p><p>\t\t\t\t\t\t\t\n\t\tThis immediately explained both the metaphorical and literal rough edges of Gorton.\n\t</p><p>\t\t\t\t\t\n\t\tA lot of typography has roots in calligraphy – someone holding a brush in their hand and making natural but delicate movements that result in nuanced curves filled with thoughtful interchanges between thin and thick. Most of the fonts you ever saw follow those rules; even the most “mechanical” fonts have surprising humanistic touches if you inspect them close enough.\n\t</p><p>\t\t\t\n\t\tBut not Gorton. Every stroke of Gorton is exactly the same thickness (typographers would call such fonts “monoline”). Every one of its endings is exactly the same rounded point. The italic is merely an oblique, slanted without any extra consideration, and while the condensed version has some changes compared to the regular width, those changes feel almost perfunctory.\n\t</p><p>\t\n\t\tMonoline fonts are not respected highly, because every type designer will tell you: This is not how you design a font. \n\t</p><p>\t\t\n\t\tIt seemed at this point that perhaps P1-2 and its predecessors were a somewhat popular machining product during the 20th century’s middle decades. But casual research through materials preserved by some of George Gorton Machine Company’s fans – including <a target=\"_blank\" href=\"http://gorton-machine.org/\">the grandson of the founder</a> – revealed something even more interesting. Gorton the font was a lot older than I expected. \n\t</p><p>\t\t\n\t\tI found a 1935 catalog showing the very same font. Then one from 1925. And then, there was one all the way from 1902, showing the shapes I was starting to be mildly obsessed with.\n\t</p><p>\t\t\t\n\t\tTo put it in perspective: the font I first assumed was a peer to 1950s Helvetica was already of retirement age the day Helvetica was born. Gorton was older than Gill Sans, Futura, or Johnston’s London Underground font. It was contemporaneous to what today we recognize as the first modern sans serif font, Akzidenz-Grotesk, released but three years before the end of the century.\n\t</p><p>\n\t\tImagine how stripped down and exotic Gorton must have felt right next to George Gorton Machine’s then-current logo!\n\t</p><p>\t\t\t\t\t\n\t\tI started researching Gorton more. Unfortunately, as I already suspected, no one ever wrote “I used Gorton to typeset this,” because Gorton was a tenuous name at best. It was the first font, and perhaps originally the  font that came with the engraver, so it suffered a nameless fate, familiar later to many bespoke bitmap fonts adoring the screens of early computers.\n\t</p><p>\t\t\t\t\t\n\t\tThe difference from these fonts, however, was that Gorton was meant to travel. And so, since searching for it by name was impossible, for months and years I just kept looking around for the now-familiar shapes.\n\t</p><p>\n\t\tGorton wasn’t just on computer keyboards, intercom placards, and sidewalk messages visited by many shoes. Gorton was there on typewriter keyboards, too. And on office signs and airline name tags. On boats, desk placards, rulers, and various home devices from fridges to tape dispensers.\n\t</p><p>\t\t\t\t\t\n\t\tIt was also asked to help in situations other fonts rarely did. I spotted Gorton on overengineered buttons that were put to heavy industrial and military use. I saw it carved into surfaces of traffic control devices, elevators and escalators, locomotives and subway trains, submarines and jet fighters. Gorton made its way to peace- and wartime nuclear facilities, it was there on the elevator at the Kennedy Space Center with labels marked EARTH and SPACE… and it went  and then the Moon, as key legends on Apollo’s onboard computer.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut why? Why would anyone choose this kind of an ugly font where so many nicer fonts have already been around for ages?\n\t</p><p>\t\t\t\t\t\n\t\tSome of it might be the power of the default. Popular engraving equipment comes with a built-in font that’s so standard it reuses the router’s name? Of course you will see it, the same way you saw a lot of Arial in the 1990s, or Calibri today.\n\t</p><p>\n\t\tGorton was also convenient. If your previous engraving work required you do to the routing equivalent of handwriting or lettering – every letter done by hand – then a modern font you could simply  and one designed with “a minimum of sharp corners for rapid tracing with a smooth stroke,” must have felt like a breath of fresh air.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut why engraving to begin with? Because the affordable and casual printing options we enjoy today – the office laser printers and home inkjets, the FedEx Kinko’s, the various cheap labelers – weren’t there. Even things that today feel obsolete, like dot matrix printers, Letraset, and popular letter stencils, were yet to be invented. Often, your only realistic option was the complicated and time-consuming lettering by hand.\n\t</p><p>\t\t\t\n\t\tOn top of that, Gorton’s longevity must have felt attractive. Ink smudges. Paint fades away. Paper can catch fire (quickly) or germs (slowly). Carve something into plastic, on the other hand, and it can survive decades. Substitute plastic for metal, and you just turned decades into centuries. The text is not added atop heavy-duty material. The text  the material.\n\t</p><figcaption>Various items from the 20th century typeset in Gorton</figcaption><p>\t\t\t\t\t\n\t\tI felt good about all my findings: What a strange story of a strange routing font! \n\t</p><p>\n\t\tBut it turns out I was just getting started. Because soon, I noticed Gorton as ink on paper, and as paint on metal.\n\t</p><p>We’re used to the flexibility of fonts today. Fonts as bits inside a computer can become a website, paint on paper, CNC routing, a wall projection, and many other things. But those freedoms weren’t as easy back when fonts were made out of metal. Life’s not as much fun outside of the glamor of a TTF file, and a routing font couldn’t immediately become a regular font – so seeing Gorton being additive and not subtractive was an unexpected discovery.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tIt turns out that there developed a small cottage industry of things that extended Gorton past its engraving origins.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tA company called Keuffel &amp; Esser Co. grabbed Gorton’s machines, and used them to create lettering sets called Leroy. This was Gorton abstracted away – still a pantograph, but cheap, small, completely manual, and a vastly simplified one: no possibility to make things bigger and smaller, and no carving –&nbsp;instead, you’d mount a special pen and draw letters by tracing them.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tAnother company, Wood-Regan Instrument Co., made a similar set called (semi-eponymously) Wrico. But then, they simplified the process even more. Instead of a pantograph, they offered for sale a set of simple lettering guides used to guide your pen directly on paper.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSome of the traditional draftspeople pooh-poohed these inventions – one handbook wrote “[Those are] of value chiefly to those who are not skilled in lettering. A professional show-card writer could work better and faster without it. A Leroy or Wrico lettering set permits work that is neat, rapid, and nearly foolproof, if not inspired.”\n\t</p><p>\t\t\t\n\t\tBut the products ended up being popular and influential. Their output appeared in many technical documents, but spread even a bit further than that. Eventually, there were stencils made by Unitech, Lutz, Tacro, Teledyne Post, Tamaya, Tech Graphic, Ridgway’s, Faber Castell, Zephyr, Charvoz, Rotring, Pickett, and probably many more.\n\t</p><p>\t\t\t\n\t\tThen, both EC Comics and All-Star Comics <a target=\"_blank\" href=\"https://kleinletters.com/Blog/wizards-of-leroy-and-wrico-lettering/\">used Leroy in the 1940s and 1950s</a>, most notably in the first comic book that introduced Wonder Woman. This was Gorton spreading further than just technical documents, and inspiring more people.\n\t</p><p>\t\t\t\n\t\tElsewhere silkscreening – a pretty cool technique of applying paint on surfaces through a wet mesh of fabric – took Gorton and Leroy in a different direction, by allowing paint on metal.\n\t</p><p>\t\t\t\n\t\tThere was more. The popular plastic letters attached to felt boards, popularized by restaurants decades ago, and more recently revisited by Instagram mom influencers, also clearly derive from Gorton and Leroy.\n\t</p><p>\n\t\tI also counted at least three different systems of “Gorton movable type” – some where you could assemble physical letters, and some where you could impress them into soft materials using steel types – and I imagine there were probably more.\n\t</p><p>\t\t\t\n\t\tLetraset, a cheap technique of applying a font by rubbing a letter from a provided sheet onto paper, popular throughout the 1960s, introduced first- or second-hand Leroy too – and so did a few competitors.\n\t</p><p>\t\t\t\t\t\n\t\tIn the regulatory space, the U.S. military canonized Gorton in 1968 as a standard called MIL-SPEC-33558 for aircraft and other equipment dials, cancelled it in 1998… then brought it back again in 2007. NATO and STANAG followed. ANSI, American standardization body, made a more rounded Leroy an official font for technical lettering via <a target=\"_blank\" href=\"https://archive.org/details/ansi-y14.2m-1971-line-conventions-and-lettering\">ANSI Y14.2M</a>, and so did institutions like the US National Park Service.\n\t</p><p>\t\t\t\t\n\t\tGorton went on and on and on. The early <a target=\"_blank\" href=\"https://hackaday.com/2021/03/30/hershey-fonts-not-chocolate-the-origin-of-vector-lettering/\">Hershey vector fonts</a>, developed on very early computers and still popular in CAD applications today, were also derived from Gorton/Leroy shapes, simplified so that the already-simple curves weren’t even necessary –&nbsp;any letter could now be drawn by a series of straight lines.\n\t</p><p>\t\t\t\t\t\n\t\tAnd even in the first universe Gorton inhabited things weren’t standing still. \n\t</p><p>\n\t\tAs the engraving industry learned what’s popular and what is not, the offerings started getting more and more sophisticated. A promotional booklet called “The Whereabouts of 230 Engraving Machines” listed Gorton customers ranging from biscuit makers to fire engine constructors. <a target=\"_blank\" href=\"http://gorton-machine.org/forms/form_933/index.html\">Other</a><a target=\"_blank\" href=\"http://gorton-machine.org/forms/form_1370a/index.html\">catalogs</a><a target=\"_blank\" href=\"http://gorton-machine.org/forms/form_2070/index.html\">proudly listed</a> applications like book covers, billiard balls, organ keys, and toothbrushes, as well as “tools making more tools” – using Gorton engravers to create legends for other machines.\n\t</p><p>\t\t\n\t\tAfter you bought your pantograph engraver, you could buy attachments for sometimes surprising use cases:\n\t</p><p>\n\t\tThe original machine-shop pantographs were supplanted by smaller portable units (called Pantoetchers) on one side, and by increasingly complex  devices on the other. First generation of those were still huge room-size endeavors with Nixie tubes and complex interfaces labeled… in Gorton itself. \n\t</p><p>\n\t\tBut the technology matured quickly and soon more and more early manual “tracer-guided” pantographs that forced the operator to put letters side by side and then trace them by hand, were superseded by <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Numerical_control\">computerized ones</a>, with both the composition and the routing completely automated. They came from George Gorton Machine Co., and from competitors like New Hermes or H.P. Preis.\n\t</p><p>\t\t\t\t\n\t\tYou no longer had to buy the chromium-plated brass alphabets weighing up to 13 pounds, choosing the right size from 3/8´´ to 3´´ ahead of time (pantographs allowed for reductions and enlargements, but only gave you a few steps within a specific range.) \n\t</p><p>\n\t\tNow, fonts came as digits or formulas built into computer memory, or – for a moment in time –&nbsp;as separate cartridges you’d insert in eager slots. (And yes, before you ask:&nbsp;there were <a target=\"_blank\" href=\"https://archive.org/details/gorton-master-copy-type-for-all-pantograph-machines/page/n5/mode/2up\">other routing monoline fonts</a>, too. But I really don’t care about any of them.)\n\t</p><p>\t\t\t\t\t\n\t\tIt was the same story as in word processing right next door, where old-fashioned Gutenberg-era typesetting was being replaced by increasingly smaller and cheaper computers equipped with first-laughable-then-capable software.\n\t</p><p>\t\t\t\t\t\n\t\tAnd automation came for the Leroy branch of the tree as well. A few companies grabbed Leroy lettering templates and abstracted them away once more. They created curious small typewriter/plotter hybrids where typing letters on a keyboard would make the pen draw them on paper for you. (I own one of them, a Max Cadliner. It might be one of the strangest typewriters I’ve seen – a weird combination of a machine pretending to be another machine pretending to be a human hand.)\n\t</p><p>\t\t\t\t\n\t\tIf this was a Gorton typewriter, there were also Gorton , even more sophisticated 1980s machines whose text could be programmed in advance rather than typed one line at a time, and mixed with graphics.\n\t</p><p>\t\t\t\n\t\tI don’t think the – by now 80 years and counting – fractal explosion of Gorton made its original creators rich.\n\t</p><p>\t\t\t\n\t\tCopy protection in the world of typography is complicated. The font’s name can be trademarked and other companies legally prevented from using it, and you can’t just grab matrices or font files and copy them without appropriate licenses. But take any text output using a font and then redraw it – and you are within your right to do so, and even to sell the final result. At least in America, or in some other countries until somewhat recently, the shapes of the letters themselves are not legally protected.\n\t</p><p>\t\t\t\n\t\tThis is why Keuffel &amp; Esser, Wood-Regan Instrument, and Letraset could potentially grab Gorton and claim it their own, as long as they didn’t name it Gorton. \n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut of course, Gorton was barely named “Gorton” to begin with. In the early days of George Gorton pantographs, as the default pantograph font, it came without a name. (The font sets for purchase were called “standard copies.”) Then, as other fonts were added, it was retroactively named Gorton Normal – the name of the company and the most generic word possible.\n\t</p><p>\t\t\t\t\t\n\t\tLeroy lettering sets started with one font, so similarly to Gorton the font started to be known as “Leroy,” then “Series C,” then “Gothic.” New Hermes called it simply “Block,” Letraset went with “Engineering Standard,” and Rotring – another producer of little computerized plotters&nbsp;– with “Universal.” I’ve also seen “A style,” “Plain Gothic,” and, mysteriously, “Standpoint.” \n\t</p><p>\t\t\t\t\t\t\t\n\t\tI don’t think this was meant to be disrespectful. “Standard,“ “Universal,” “A style” might not have had the connotations of “generic” we associate with them today, but rather meaning “the only one you need,” “approved of by millions,” or “the ultimate.”\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut there  one name that felt somewhat inconsiderate. It appeared in one product in the 1980s, a few decades after the birth of another font whose name became recognizable and distinguished. In that product, Gorton was referred to as “Linetica.”\n\t</p><figcaption>A few rare examples of Gorton Extended in use</figcaption><p>\t\t\t\t\t\t\n\t\tEach of these reappearances made small changes to the shapes of some letters. Leroy’s ampersand was a departure from Gorton’s. Others softened the middle of the digit 3, and Wrico got rid of its distinctive shape altogether. Sometimes the tail of the Q got straightened, the other times K cleaned up. Punctuation –&nbsp;commas, quotes, question marks –&nbsp;was almost always redone. But even without hunting down the proof confirming the purchase of a Gorton’s pantograph or a Leroy template set as a starting point, the lineage of its lines was obvious. (The remixes riffed off of Gorton Condensed or the normal, squareish edition… and at times both. The extended version – not that popular to begin with – was often skipped.)\n\t</p><figcaption>Classic Gorton vs. Gorton Modified</figcaption><p>\t\t\t\t\t\t\n\t\tThe only “official” update to Gorton I know of, and one actually graced with a name, was Gorton Modified. It was made some time in the 1970s by one of the main keyboard keycap manufacturers, Comptec (later Signature Plastics). It was almost a fusion of Gorton and Futura, with more rounded letterforms. Gone was the quirkiness of 3, 7, Q, C, and the strange, tired ampersand. This is the version people might recognize from some of the 1980s computers, or mechanical keyboards today. \n\t</p><p>\n\t\tIt is also that last Gorton that mattered.\n\t</p><figcaption>A collection of movies and TV shows featuring Gorton</figcaption><p>\t\t\t\t\t\t\t\n\t\tMy every walk in Chicago or San Francisco was counting down “time to Gorton” – sometimes mere minutes before I saw a placard or an intercom with the familiar font.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tThis might be embarrassing to admit, but I have never been so happy seeing a font in the wild, particularly as there was almost always some new surprise – a numero, a line going through the Z, a new use, or a new imperfection. And, for a font that didn’t exist, I saw it surprisingly often.\n\t</p><p>\t\t\t\t\t\t\t\t\t\n\t\tI even spotted Gorton a few times in Spain, or the U.K., and didn’t make too much of it, not thinking about the likelihood of machines from George Gorton’s company in a small town of Racine, Wisconsin making it all the way to different continents. In hindsight I should have.\n\t</p><figcaption>Gorton on old British cars, with a particularly delightful Rolls Royce logo made by a simple duplication of the classic Gorton letter R</figcaption><p>\t\t\t\t\t\t\n\t\tIt was only on a trip to Australia where something started connecting. Here, once more, I saw Gorton on the streets, put to work in all sorts of unglamorous situations:\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSome letterforms in the above photos felt slightly odd, and so did Gorton on the heavy machinery in an abandoned shipyard on an island near Sydney:\n\t</p><p>\t\t\t\t\t\t\t\n\t\tAnd a visit to a naval museum cemented it all:\n\t</p><p>\n\t\tIt was Gorton, although with some consistent quirks: 2, 5, 6, and 9 were shorter, the centers of M and W didn’t stretch all the way across, and the distinctive shape of S was slightly different here.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tFortunately, this time around, a type designer familiar with my now-public obsession with Gorton clued me in. Gorton didn’t actually originate from Racine, Wisconsin in the late 19th century. It started a bit earlier, and quite a bit further away, at a photographic lens maker in the U.K. called Taylor, Taylor &amp; Hobson. \n\t</p><p>\t\t\t\t\t\t\t\n\t\tIn 1894, TT&amp;H needed some way to put markings on their lenses. This being late 19th century, their options were limited to manual engraving, which must have felt tricky given the small font sizes necessary. So the company did what makers sometimes do – instead of searching for a solution that might not have even existed, they made new types of machines to carve out letters, and then designed a font to be used with them.\n\t</p><p>\n\t\tI don’t know how this first proto-Gorton was designed – unfortunately, Taylor, Taylor &amp; Hobson’s history seems sparse and despite personal travels to U.K. archives, I haven’t found anything interesting – but I know simple technical writing standards existed already, and likely influenced the appearance of the newfangled routing font.\n\t</p><figcaption>From a 1895 “Free-hand lettering” book by Frank T. Daniels</figcaption><p>\t\t\t\t\t\n\t\tThis was perhaps the first modern pantograph engraver, and perhaps even the arrival of a concept of an engraving font – the first time technical writing was able to be replicated consistently via the aid of the machine.\n\t</p><p>\t\t\t\n\t\tNo wonder that other companies came knocking. Only a few years later, still deep within the 19th century, Taylor, Taylor &amp; Hobson <a target=\"_blank\" href=\"http://gorton-machine.org/archives/TTH_license_1898/index.html\">licensed their stuff to a fledgling American company</a> named after its founder. Gorton Model 1 was the first U.S. version of the engraver, and the TT&amp;H font must have been slightly adjusted on arrival. \n\t</p><figcaption>A Taylor-Hobson pantograph in use in 1942</figcaption><p>\t\t\t\n\t\tThis adds to the accomplishments of Gorton – the font was actually  than even Akzidenz-Grotesk, and has been used on World War II equipment and later on on British rifles and motorcycles (and 3,775 finger posts in <a target=\"_blank\" href=\"https://www.yorkshiredales.org.uk/behind-the-signs-the-man-and-the-machine/\">one of the UK’s national parks</a>), but it complicates the story of the name even more. Turns out, the font without a name has even less of a name than I suspected.\n\t</p><p>\t\t\t\n\t\tIf the Taylor, Taylor &amp; Hobson (or, Taylor-Hobson, as their engravers were known) “branch” of Gorton were more used, should it usurp the at least somewhat popular Gorton name? Or should it just because it was first and the letterform changes were small? Does it matter? Where does one font end and another begin? (Unsurprisingly, TT&amp;H didn’t properly name the font either, eventually calling it “A style” for regular and “C style” for condensed variants. Google searches for “taylor hobson font” are a lot more sparse than those for Gorton.)  \n\t</p><div><div><div>GortonGorton Condensed</div></div></div><figcaption>The Gorton quasisuperfamily</figcaption><p>\n\t\tIn the end, I’m sticking with Gorton for the whole branch since that feels the most well-known name, but I feel ill-equipped to make that call for everyone. You might choose to call it Gorton, Leroy, TT&amp;H, Taylor-Hobson, or one of the many other names. (Just, ideally, not Linetica.)\n\t</p><figcaption>A comparison of all major editions of Gorton</figcaption><p>\t\t\t\t\t\n\t\tAnd so, throughout the 20th century, Gorton has lived two parallel lives –&nbsp;one originating in the U.K. and later expanding to its colonies and the rest of Europe, and another one in America. \n\t</p><p>\t\t\t\t\t\t\t\n\t\tI am still tracing various appearences of Gorton and perhaps you, dear reader, will help me with that. (<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Frequency_illusion\">Chances are</a>, you will see Gorton later today!) I’m curious about whether Gorton made it to Eastern Europe, Africa, or Asia. I’m interested in seeing if it appeared in Germany where the objectively better-designed DIN fonts became much more popular in Gorton’s niche.\n\t</p><p>\n\t\tThe history of this strange font spans over a century and I’ve seen it in so many countries by now, used in so many situations. But it’s impossible for me to say Gorton is the most hard-working font in the world.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tTo this title, there are many contenders. Garamond has a head start of 300+ years and has been released in more versions than letters in any alphabet. Helvetica is so famous and used so much that even its ugly copy, Arial, became a household name. Whatever font MS Office or a popular operating system appoint to be “the default” – from Times New Roman through Calibri to Roboto – immediately enjoys the world premiere that any Hollywood movie would be envious of. There is even a 5×7 pixel font originally started by Hitachi that you can see everywhere on cheap electronic displays in cash registers and intercoms.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tBut there is one place in the world where Gorton pulls triple duty, and I feel confident in saying at least this: Gorton is the hardest working font in Manhattan.\n\t</p><p>\t\t\t\t\t\t\n\t\tIn 2007, on my first trip to New York City, I grabbed my brand-new DSLR camera and photographed all the fonts I was supposed to love: American Typewriter, Helvetica, Gotham. But, in hindsight, I missed the most obvious one.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tGorton is everywhere in Manhattan. It’s there in the elevators, in the subway, on ambulances, in various plaques outside and inside buildings. And god knows it’s there on so, so many intercoms.\n\t</p><p>\t\t\t\t\t\n\t\tI wouldn’t be surprised if there weren’t a single block without any Gorton in a whole of Manhattan.\n\t</p><figcaption>A complete inventory of Gorton outside, near my hotel, between 5th and 7th avenues and 25th and 35th streets. I didn’t have access to the interiors of most buildings.</figcaption><p>\t\n\t\tThe omnipresence of Gorton makes it easy to collect all the type crimes layered on top of the font’s already dubious typographical origins. Walking through Manhattan, you can spot the abominable lowercase that should better be forgotten:\n\t</p><p>\t\t\t\t\t\t\t\n\t\tYou can see all sorts of kerning mistakes:\n\t</p><p>\t\t\t\t\t\t\t\t\t\n\t\tYou will notice the many, many routing imperfections – an unfinished stroke, a shaky hand, or services of a pantograph that never felt the loving touch of regular maintenance:\n\t</p><p>\n\t\tThere are all the strange decisions to haphazardly mix various styles of Gorton, or even to mix Gorton with other fonts:\n\t</p><p>\t\t\t\t\t\t\t\t\t\t\t\n\t\tYou can even spot reappearing strange characters like a weirdly deep 3, or a flattened 4:\n\t</p><p>\n    I wish I understood how they came to be, but I have a hunch. The nature of pantographic reproduction is that Gorton carved into metal is not that far away from the original Gorton font template you started with! So in addition to the George Gorton and Taylor Hobson originals, and the other named and above-the-table copies, they might have been bigger or smaller Gorton . I have one myself, carved into acrylic, of unknown provenance and even more nameless than I thought possible for an already name-free font.\n  </p><p>\n\t\tBut New York Gorton holds pleasant surprises, too. Despite the simplicity of Gorton itself, the combinations of font sizes, cutter sizes, materials, reproductions, and applications can still yield some striking effects:\n\t\n\t</p><figcaption>\n\t\t\tAll my Gorton walks in Manhattan in 2024\n\t\t</figcaption>\n\n\t\tThis was what made me walk 100 miles. Over and over again, Gorton found ways to make itself interesting. Without hyperbole, I consider the above photos simply beautiful.\n\t<p>\n\t\tIn a city that never sleeps, Gorton wasn’t allowed to sleep, either. Even in the richest and most glamorous neighborhoods of Manhattan, the font would be there, doing the devil’s work without complaining. Gorton made Gotham feel bougie; American Typewriter touristy.\n\t</p><p>\n\t\tAnd once in a while, I’d find Gorton that would wink at me with a story –&nbsp;followed by that aching in the heart as I realized I’d never know what the story was.\n\t</p><p>\t\t\t\n\t\tYou’re not supposed to fall in love with an ugly font. No one collects specimens of Arial. No one gets into eBay fights for artifacts set in Papyrus. No one walks a hundred miles in a hot New York summer, sweating beyond imagination, getting shouted at by security guys, to capture photos of Comic Sans.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tSo why do I love Gorton so much? \n\t</p><p>\t\t\t\t\t\t\t\n\t\tThe Occam’s Razor seems sharp on this one. Perhaps I like it because I’m a boy and Gorton is often attached to heavy machinery. \n\t</p><p>\t\t\t\t\n\t\tBut there must be more to it. Perhaps it’s all about the strange contrasts Gorton represents. The font is so ubiquitous, but also profoundly unrecognizable, sporting no designer and no name. Gorton is a decidedly toy-like, amateurish font deployed to for some of the most challenging type jobs: nuclear reactors, power plants, spacecraft. More than most other fonts, Gorton feels it’s been made by machines for machines –&nbsp;but in its use, it’s also the font that allows you to see so many human mistakes and imperfections.\n\t</p><p>\t\t\t\t\n\t\tGorton also feels mistake-friendly. The strange limitations of Gorton mean that some of the transgressions of other fonts don’t apply here. The monoline nature of the font means that messing with the size of Gorton is okay: Shrinking the font for small caps or superscript, for example, gives you still-valid letterforms, almost by accident. \n\t</p><p>\n\t\tStretching or slanting Gorton is not as much a typographical crime as it would be with other fonts because you don’t stretch the tip of the router itself.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tThere are genuinely moments where I felt Gorton gave people freedoms to maul it decades before variable fonts allowed us similar flexibiity.\n\t\tAnd on top of that, the simplicity of the letterforms themselves feels compatible with the typical naïveté of Gorton’s typesetting. \n\t</p><figcaption>Various accessories and attachments allowing you to shift Gorton around in a way other fonts would not allow</figcaption><p>\n    Sure, there are really bad renditions that are inexcusable. \n\n\t\tBut most of the time, the imperfections and bad decisions are what makes Gorton come alive. They don’t feel like a profound misunderstandings of typography, typesetting, or Gorton itself. They don’t feel like abuses or aberrations. No, they feel exactly how Gorton was supposed to be used – haphazardly, without much care, to solve a problem and walk away. (Later routing fonts copied Helvetica, but seeing Helvetica in this context with all the same mistakes grates so much more.)\n\t</p><p>\t\t\t\n\t\tThe transgressions are not really transgressions. They all feel honest. The font and its siblings just show up to work without pretense, without ego, without even sporting a nametag. Gorton isn’t meant to be admired, celebrated, treasured. It’s meant to do some hard work and never take credit for it. Gorton feels like it was always a font, and never a typeface. (Depending on how rigid you are with your definitions, some versions of Gorton – especially those without instructions on how letters are positioned against each other – might not even <a target=\"_blank\" href=\"https://mastodon.design/@fhardwig@mastodon.social/113515144112560218\">classify as a font</a>!)\n\t</p><p>\t\t\t\t\t\n\t\tAnd I think I love Gorton because over the years I grew a little tired of the ultra flat displays rendering miniature pixels with immaculate precision. \t\n\t\tWith Gorton, carving into metal or plastic means good-looking fixes are impossible:\n\t</p><p>\n\t\tAnd unsurprising given its roots, Gorton has dimensionality that most fonts cannot ever enjoy: A routing tip picked in the 1980s and a sun coming in from just the right angle forty years later can create a moment that thousands of letterpress cards can only dream of.\n\t</p><p>\n\t\tPerhaps above everything else, Gorton is all about . \n  </p><p>\n    Every kind of engraving has it, of course. But these are not precise submillimeter letters at the bottom of your MacBook Pro or Apple Watch. This is the utilitarian, often harried, sometimes downright  Gorton, carved into steel of a  \n\t\tmid-century intercom and filled in with <a target=\"_blank\" href=\"https://youtu.be/llzdLgMurvw?si=8S7px9gg8iH4iav2&amp;t=101\">special paste or wax</a>, or put on an office placard made out of a special two-layer material made especially so engraving it reveals the second color underneath, without the need for infill. \n\t</p><p>\t\t\t\n\t\t(This is also true when it comes to the original reason I learned of Gorton. Letters on keycaps show the same artifacts – you just have to look very, very closely.)\n\t</p><p>\n\t\tThat’s the last, and perhaps the best thing to fall in love with. \n\t</p><p>\n\t\tYou won’t be able to fully appreciate it here, of course, but maybe this will give an approximation of how beautiful Gorton’s non-beauty can be:\n\t</p><p>\t\t\t\t\t\n\t\tThis has been a strange thing to write. Gorton has been around for over 135 years and used in so many countries for so many reasons, and yet I found no single article about it. \n\t</p><p>\t\t\t\t\t\n\t\tI feel the burden of being an amateur historian, wanting to know and share so much more, but only being able to provide little. I don’t know the full extent of Gorton’s use. I don’t know who designed it. My chronology is rickety and pieced together from a few breadcrumbs. I dream of seeing the original drawings or drafts once laid on the tables of Taylor, Taylor &amp; Hobson offices, or some notes, or some correspondence. I fear they might no longer exist.\n\t</p><p>\t\t\t\t\t\n\t\tAlso, if part of the allure of Gorton is shying away from the limelight and not being admired, am I doing it a disservice by writing about it?\n\t</p><p>\t\t\t\t\t\n\t\tBut mostly, I can’t shake the feeling that we all missed a window. That this essay can’t be just a celebration, but also needs to be the beginnings of a eulogy.\n\t</p><p>\t\t\t\t\t\n\t\tWalking around New York, you get a sense that even Gorton carved into metal can disappear. Some of the signs are rusted or destroyed beyond repair. Others get replaced by more modern, charmless equivalents.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tGorton itself is obsolete. All <a target=\"_blank\" href=\"https://spkeyboards.com/\">the keyboards that use Gorton Modified</a> you can still buy new today are tipping a hat to nostalgia. The omnipresence of Gorton in New York City is already time shifted from its decades of glory, a simple confirmation of what Robert Moses knew so well: that once built, cities don’t change all that much. But few of the new placards use Gorton, and none of the new intercoms do. \n\t</p><p>\n\t\tTaylor, Taylor &amp; Hobson went through multiple splits and mergers and survives as a subsidiary of Ametek, chiefly working on measuring devices. George Gorton Machine Co. from Racine has been bought by Kearny &amp; Trecker, which became Cross &amp; Trecker, was acquired by Giddings &amp; Lewis, and then acquired  by ThyssenKrupp, but not before the Gorton branch was spun off as Lars, and in a sequence of events now resembling a telenovella, eventually bought by Famco in 1987. I do not believe any corporate grandchildren of TT&amp;H and George Gorton’s company are today selling Gorton in any capacity.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tIt will take decades, perhaps even centuries, but one day the last of this font will be gone. The modern recreations (<a target=\"_blank\" href=\"https://aresluna.org/the-hardest-working-font-in-manhattan/the-hardest-working-font-in-manhattan/recreations\">I eventually found quite a few</a>) won’t help. They are perhaps all missing a point, anyway.\n\t</p><p>\n\t\tBut there’s a somewhat silver lining. Yes, when Gorton is carved into fresh metal, there might be nothing more pretty than seeing its depths glistening in the sun.\n\t</p><p>\n\t\tBut fresh, shining metal is at this point rare. Fortunately, the Gorton I love most is the weathered Gorton.\n\t</p><p>\t\t\t\t\t\t\t\n\t\tManhattan’s tired Gorton is the best variant of Gorton: infill cracked by hot summers followed by frigid winters, the surface scratched by keys or worn out by many finger presses, the routing snafus meeting decades of wear and tear. Gorton’s no stranger to water, snow, rust, or dirt.\n\t</p><p>\t\t\n\t\tThis is, perhaps, how you become gortonpilled. You learn to recognize the 7 with a crooked hook, the Q with a swung dash, the strange top-heavy 3, the simple R. You start noticing the endings of each character being consistently circular, rather than occasionally flat. A routing mistake, suspicious kerning, or the absence of lowercase are not a wrongdoing – they’re a .\n\t</p><p>\t\t\t\t\t\t\t\n\t\tYou find yourself enchanted with how this simple font went so very far. And then you touch the letters, just to be sure. If you can  them, chances are this is Gorton.\t\t\n\t</p>","contentLength":38055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43053419"},{"title":"AI is stifling new tech adoption?","url":"https://vale.rocks/posts/ai-is-stifling-tech-adoption","date":1739537105,"author":"kiyanwang","guid":182,"unread":true,"content":"<p>I propose that the advent and integration of AI models into the workflows of developers has stifled the adoption of new and potentially superior technologies due to training data cutoffs and system prompt influence.</p><p>I have noticed a bias towards specific technologies in multiple popular models and have noted anecdotally in conversation and online discussion people choosing technology based on how well AI tooling can assist with its usage or implementation.</p><p>While it has long been the case that developers have considered documentation and support availability when choosing software, AI’s influence dramatically amplifies this factor in decision-making, often in ways that aren’t immediately apparent and with undisclosed influence.</p><p>Large language models, especially those on the scale of many of the most accessible, popular hosted options, take humongous datasets and long periods to train. By the time everything has been scraped and a dataset has been built, the set is on some level already obsolete. Then, before a model can reach the hands of consumers, time must be taken to train and evaluate it, and then even more to finally deploy it.</p><p>Once it has finally released, it usually remains stagnant in terms of having its knowledge updated. This creates an AI knowledge gap. A period between the present and AI’s training cutoff. This gap creates a time between when a new technology emerges and when AI systems can effectively support user needs regarding its adoption, meaning that models will not be able to service users requesting assistance with new technologies, thus disincentivising their use.</p><p>The cutoff means that models are strictly limited in knowledge up to a certain point. For instance, Anthropic’s latest models have a cutoff of <a href=\"https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data\">April 2024</a>, and OpenAI’s latest models have cutoffs of <a href=\"https://platform.openai.com/docs/models\">late 2023</a>.</p><p>The influence of the popularisation of AI models has also led to the proliferation of AI slop content, which AI companies are likely trying to avoid introducing in their scraped data sets – potentially increasing this knowledge gap.</p><p>This knowledge gap doesn’t just stifle the adoption of new frameworks and tooling but also the adoption of new and potentially superior features introduced in updates to already in-use frameworks and tooling as a result of the models not yet including information about them in their training set. This is a lesser issue though, because a technology with existing market adoption is bound to have fanatics who will use it, create examples, and produce blog posts, documentation, and other media about it that will eventually end up in training data.</p><p>While some models are used in contexts where they are equipped with tooling, such as ChatGPT with its internet search capabilities, their prompts often only instruct them to use this functionality when explicitly requested by a user. Of course, plenty of implementations don’t have any access to any internet query functionality at all, such as Claude via its web interface, which will inform you it will only discuss things included in its training data.</p><p>Of course, if people are reluctant to adopt a new technology because of a lack of AI support, there will be fewer people likely to produce material regarding said technology, which leads to an overall inverse feedback effect. Lack of AI support prevents a technology from gaining the required critical adoption mass, which in turn prevents a technology from entering use and having material made for it, which in turn starves the model of training data, which in turn disincentivises selecting that technology, and so on and so forth.</p><p>Consider a developer working with a cutting-edge JavaScript framework released just months ago. When they turn to AI coding assistants for help, they find these tools unable to provide meaningful guidance because their training data predates the framework’s release. This forces developers to rely solely on potentially limited official documentation and early adopter experiences, which, for better or worse, tends to be an ‘old’ way of doing things and incentivises them to use something else.</p><p>I’ve anecdotally noticed that many AI tools have a ‘preference’ for React and Tailwind when asked to tackle a web-based task, or even to create any app involving an interface at all.</p><p>With Claude 3.5 Sonnet, which is generally my AI offering of choice given its superior coding ability, my “What personal preferences should Claude consider in responses?” profile setting includes the line “When writing code, use vanilla HTML/CSS/JS unless otherwise noted by me”. Despite this, Claude will  opt to generate new code with React, and in some occurrences even rewrite my existing code into React against my intent and without my consultation.</p><p>This can be evidenced when asking Claude to use a certain technology, such as Svelte Runes, where it will instead direct the user towards its preferred technology. For example:</p><blockquote><p> Build me something with Svelte Runes.</p></blockquote><blockquote><p> I apologize, but I am not able to create Svelte Runes components directly, as the artifact environment only supports certain technologies like React, HTML, and SVG. I can help you build something similar using React, or I can provide you with Svelte Runes code that you can use in your own Svelte project. Which would you prefer?</p><p>If you’d like to see a React version, I can create something with similar reactive functionality using React’s hooks. Or if you want Svelte Runes code specifically, I can write that for you to use in your own environment.</p></blockquote><p>To test if popular models do have built-in bias for React, I logged into the web interfaces of four of the current most popular AI chat platforms via my free account and asked the models to “Create me a web app. It can do anything.”. I repeated this with each of the four models three times in fresh chats with previous chats deleted and any shared context (such as ChatGPT’s memory feature) disabled or cleared between each test.</p><p>I asked Anthropic’s Claude 3.5 Sonnet with blank personal preferences and artifacts enabled. The first time it opted to create a mood tracking app, the second a reading list manager, and the third time a to-do list. Each time it chose to generate it using React with Tailwind CSS for styling. It generated each project in an artifact which offered an interactive preview directly in the interface.</p><p>I asked OpenAI’s ChatGPT 4o with both custom instructions and memory disabled. It produced a note-taking app the first time, a random quote generator the second time, and a to-do list the third time. All using React with Tailwind for styling and presented in <a href=\"https://openai.com/index/introducing-canvas\">canvases</a>. These canvases came with a preview button, which, much like with Claude, offered an interactive preview of the React project it built directly in the interface.</p><p>I asked Google’s Gemini 2.0 Flash with app activity disabled. All three times it built to-do list apps using vanilla HTML, CSS, and JavaScript. Each time, it also noted that I should consider using JavaScript frameworks on the frontend, namely React, Angular, and Vue - in that order.</p><p>I asked DeepSeek’s DeepSeek-V3, and it returned a to-do list app using vanilla HTML, CSS, and JavaScript. On the second request, it created a project outline for a to-do list app, but this time with Bootstrap, Node.js, Express.js, and SQLite. The third time, it created another outline for a note-taking app but with Node.js, Express.js, and MongoDB.</p><p>DeepSeek had the greatest variation in technology used but is the least accessible model and didn’t actually ‘create’ the app as requested – merely a general outline for it.</p><p>These tests show both Claude and ChatGPT have a strong preference for React and Tailwind,  that Gemini has a preference for HTML/CSS/JS but will recommend React and that DeepSeek is much more flexible and varied with its technological choices, though requires more prompting to produce an actual output.</p><p>I’d suggest that a beginner developer, or someone creating an app exclusively via prompting, is likely to use ChatGPT due to its position in the zeitgeist and use whatever output the model first produces without much second thought – thus influencing their tech selection without their realising.</p><p>Even if a developer does opt to use another framework or toolset, there is a chance that the model will prod them towards a selection more aligned with its system prompt, even going as far as rewriting their code to its ‘preference’ against user request, as evidenced by Claude ignoring my request for vanilla web technologies.</p><p>That is assuming that a specific framework or toolset hasn’t already been chosen by the user specifically because they have heard or experienced it being best handled by AI models. Also, while this very much applies to larger technical choices, such as what framework or general tooling a project may choose, it also trickles down into smaller decisions, such as what libraries or modules they may import.</p><p>I think it is evident that AI models are influencing technology, and that the technologies currently in use – especially those that reached popularity before November 2022, when ChatGPT was released, or that are otherwise in current data sets – will be around for a long time to come, and that AI models’ preferential treatment of them will expand their adoption and lifespan.</p><p>I think it would be prudent for AI companies to provide more transparent documentation of technology biases in their models, like they disclose that their models can make mistakes. These models are becoming a common part of developer knowledge and decision-making, and we’re letting the training and prompting decisions of OpenAI, Anthropic, etc shape the entire direction of software development.</p><p>As for further research on this topic, if one had a collection of system prompts over time, it may be possible to compare them to download trends of specified packages and identify correlations. Of course, there are a lot of other influences on the download and adoption of packages, so this may prove difficult and provide data too noisy and influenced by outside variables to be unusable.</p>","contentLength":10120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047792"},{"title":"Anyone can push updates to the doge.gov website","url":"https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/","date":1739518306,"author":"mahkeiro","guid":181,"unread":true,"content":"<div>\"THESE 'EXPERTS' LEFT THEIR DATABASE OPEN.\"</div>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045835"},{"title":"Zed now predicts your next edit with Zeta, our new open model","url":"https://zed.dev/blog/edit-prediction","date":1739515847,"author":"ahamez","guid":180,"unread":true,"content":"<p>Zed is built for speed. We've always strived for an editing experience that feels . But what's faster than instant?  A tool that anticipates your next move. That's why we're introducing <a href=\"https://zed.dev/edit-prediction\">edit prediction</a> in Zed, powered by <a href=\"https://huggingface.co/zed-industries/zeta\">Zeta</a>, our new open source model.</p><p>Here's a quick walkthrough:</p><div><figure><figcaption>Edit Prediction in action.</figcaption></figure></div><p>As you work, Zed now predicts your next edit, so you can apply it just by hitting . Once you accept a prediction, you can perform multiple follow-up edits by pressing  repeatedly, saving you time and keystrokes. We've received  of requests for this functionality, and we've poured our hearts into making it feel like a natural extension of the Zed experience.</p><p>You can use Zeta for free during this public beta by <a href=\"https://zed.dev/download\">downloading Zed</a> and signing in with your GitHub account. Edit prediction won't be free forever, but right now we're just excited to share and learn.</p><p>Edit prediction transforms  into a magical, universal key. But what about the existing uses of , such as indenting lines? And what happens when there's both an edit prediction  suggestions from your language server? We didn't want a powerful new feature to come at the expense of the existing editing experience in Zed.</p><p>When language server completions are visible, Zed won't preview the predicted edit until you press  or . As soon as you press the modifier, Zed previews the edit and hides the menu to enable an unobstructed review. On macOS, you can just hit  to confirm, or back out by releasing  to restore the language server completions menu.</p><p>On Linux,  is often reserved by the window manager, so we offer  as an alternative default. We chose  because it's on the QWERTY home row and represents rightward movement in Vim. If your Linux window manager doesn't claim , you're free to use that binding as well.</p><p><a href=\"https://huggingface.co/zed-industries/zeta\">Zeta</a> is derived from Qwen2.5-Coder-7B, and is fully open source, including <a href=\"https://huggingface.co/datasets/zed-industries/zeta\">an open dataset</a>. If you're working in an open source repository, we'd love your help improving Zeta by contributing to its dataset. Please bear with us initially, as we will be reviewing the submitted data before publishing to ensure everyone's safety and privacy. We're excited to figure this out and see a community effort form to make edit prediction better everywhere, most especially in Zed!</p><div><div><div><p>How Zed's Open-Source Edit Predictions Work</p></div><p>Richard Feldman and Antonio Scandurra talk about how Zed's new Edit Prediction feature works under the hood. This includes how the Zed team developed and open-sourced both the code and the dataset behind the fine-tuned Zeta language model that powers it!</p></div></div><p>Most coding models are trained on a \"fill in the middle\" task. You give them a prefix and a suffix, and they generate what goes in between.</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rs\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>This works for completing text at the cursor, but we wanted Zeta to predict edits at arbitrary locations, which doesn't fit into this structure.</p><p>In our experience, models aren't very good at producing granular edits, but they do excel at rewriting larger chunks of code. So that's where we started: given a list of recent edits and the cursor position, we asked the model to rewrite a snippet of text around the cursor, incorporating one or more edit predictions in the rewritten text.</p><p>Before writing a single line of code, we created a set of tests to check if our idea worked. Testing the output of a large language model is tricky because, on every run, you can get slightly different results even when feeding it the exact same input. This can be mitigated by using a temperature of  and, for providers that support it, providing a seed for the RNG.</p><p>That said, code can often be written in many different but equally valid ways. So even when Zeta's output differs from our expected answer, it might still be doing exactly what we want—just taking a different path to get there. This makes traditional unit testing approaches particularly challenging when working with LLMs.</p><p>This led us to take a different approach—instead of strict assertions, we used a larger LLM to evaluate Zeta's edits. By writing our test assertions in plain English and having Claude check if the results matched our intent, we could validate that Zeta was making sensible edits, even when its exact output differed between runs. This ended up being much more practical than trying to make brittle assertions about specific tokens.</p><p>Here's an example taken from our eval suite:</p><figure data-rehype-pretty-code-figure=\"\"><div><pre><code data-language=\"rs\" data-theme=\"dark-plus light-plus\"></code></pre></div></figure><p>We took our first stab at making those tests pass by using Qwen2.5-Coder-32B and giving it clear instructions for which types of edits we wanted it to predict. <a href=\"https://github.com/zed-industries/zed/blob/79a70b72b3968d102c6171f8bd2738ec7be8e94f/crates/zeta/src/complete_prompt.md\">Here's the initial system prompt</a> we used and you can look through the history to see how we kept changing it to pass the eval suite.</p><p>This worked out surprisingly well for the first 4-5 evals. However, as soon as we introduced more, we started noticing that it got harder and harder to pass them all consistently. Changing the prompt caused the new evals to pass, but made the old ones fail. Overall, it felt like a flaky process and we didn't feel confident this would lead to the system being robust enough to be used in production.</p><p>Moreover, using a 32b model wasn't really compatible with our strict latency requirements (more on that later).</p><p>After playing around with different approaches, we decided to go with supervised fine-tuning using <a href=\"https://unsloth.ai\">Unsloth</a> and LoRA. The idea was to teach Zeta two key things: figuring out what changes a developer might want next based on their recent edits, and then actually applying those changes cleanly to the code without introducing weird side effects.</p><p>But we had a classic chicken-and-egg problem—we needed data to train the model, but we didn't have any real examples yet. So we started by having Claude generate about 50 synthetic examples that we added to <a href=\"https://huggingface.co/datasets/zed-industries/zeta-dataset\">our dataset</a>. We then used that initial fine-tune to ship an early version of Zeta behind a feature flag and started collecting examples from our own team's usage.</p><p>This approach let us quickly build up a solid dataset of around 400 high-quality examples, which improved the model a lot! However, we kept running into edge cases that would trip the model up. The most annoying ones were when Zeta was working with a small piece of code in a larger file—it would sometimes get confused and make random deletions or insertions that had nothing to do with what the user was trying to do, and it didn't seem like adding more examples steered the model away from those mistakes.</p><p>To handle these edge cases, we conducted another pass using direct preference optimization (DPO). This technique let us go beyond simply showing the model what good edits look like—we could also teach it what edits . With DPO, we could fine-tune Zeta by providing both positive and negative examples, helping it learn the subtle differences between helpful and problematic edits.</p><p>We found that just ~150 carefully selected examples were enough to significantly improve Zeta's behavior on tricky cases. Of course, we think we can make it even better by expanding our training data with more diverse examples, and we're excited to keep pushing the boundaries here.</p><p>Like every feature in Zed, latency was a critical factor for edit prediction. When we started, we set aggressive performance targets: predictions should be delivered in under 200ms for the median case (p50) and under 500ms for the 90th percentile (p90). The challenge was that rewriting complete excerpts, while enabling multi-location edits, requires generating significantly more tokens than simple fill-in-middle approaches. Initially, this put us way over our latency budget.</p><p>However, there's a fascinating insight about how edit predictions work. When we rewrite a text snippet, the output often mirrors the input closely, with changes concentrated in specific spots. This pattern lets us parallelize token generation by using the input as a reference—a technique known as speculative decoding. We use n-gram search to identify promising jumping-off points in the input where we can start parallel token generation, giving us a significant speedup without sacrificing quality.</p><p>For edit predictions to feel responsive, we needed to solve multiple latency challenges in parallel. As discussed above, we tackled the model execution time through speculative decoding, but serving the model at scale presented its own set of hurdles. This was by far the most compute-intensive problem our team has ever tackled.</p><p>A few weeks out from launch, we ran a brief competitive process, and we ended up being really impressed with <a href=\"https://www.baseten.co\">Baseten</a>. Their performance engineers quickly optimized our open source model to run on their flexible infrastructure, achieving our target latencies while letting us retain full visibility into the details of the deployment, both for the Zed team and the entire Zed community. We plan to follow up with a guest post about what they learned optimizing <a href=\"https://huggingface.co/zed-industries/zeta\">our model</a>.</p><p>Latency is not just a function of compute; network transit time is a key driver of perceived speed. To cooperate with the laws of physics, we're launching with GPUs in both North America and Europe, and we hope to add more regions soon. We're also using <a href=\"https://workers.cloudflare.com/\">Cloudflare Workers</a> to handle your requests in a data center located as close to you as possible.</p><p>There's plenty more to explore to make edit predictions more powerful. We'll be fast-following with more experiments. We plan on sending more kinds of context to the model and continuing our experiments with fine-tuning, and we'll share updates as we grow and evolve the Zeta dataset.</p><p>We've learned a lot since we launched Zed AI last fall. The world is changing fast, and we're having a blast exploring and learning to build features that developers love. We're also excited to build with AI the Zed way. From our early days, we've been proponents of an open approach to building software, even when hard, and we see no reason to change that approach when it comes to working with AI. We hope you'll join us as a user, a contributor, or an employee, as we hustle to ship a golden future.</p>","contentLength":9947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43045606"},{"title":"Privacy Pass Authentication for Kagi Search","url":"https://blog.kagi.com/kagi-privacy-pass","date":1739476649,"author":"b3n","guid":179,"unread":true,"content":"<p>Today we are announcing a new privacy feature coming to Kagi Search. Privacy Pass is an authentication protocol first introduced by Davidson  in [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">1</a>], and recently standardized by the IETF as RFCs [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2—4</a>].  Our starting point was the excellent Rust implementation of the Privacy Pass protocols by <a href=\"https://github.com/raphaelrobert/privacypass\">Raphael Roberts</a>. At the same time, we are announcing the immediate availability of Kagi’s <a href=\"https://blog.kagi.com/kagi-privacy-pass#tor\">Tor onion service</a>.</p><p>In general terms, Privacy Pass allows “Clients” (generally users) to authenticate to “Servers” (like Kagi) in such a way that while the Server can verify that the connecting Client has the right to access its services, it cannot determine which of its rightful Clients is actually connecting. This is particularly useful in the context of a privacy-respecting paid search engine, where the Server wants to ensure that the Client can access the services, and the Client seeks strong guarantees that, for example, the searches are not associated with them.</p><p>As a privacy-respecting search engine, Kagi’s business model is such that we have no incentive to track what an individual user is searching for. We are in the business of selling a search product, not selling user data or attention.</p><p>Now, Privacy Pass adds another layer of trust: we can verify that you have the right to search without knowing who you are or what you’re searching for. It’s one thing to promise we won’t track you; it’s another to make it technically impossible. We jumped on the opportunity to implement Privacy Pass as soon as the IETF made it an official standard.</p><p>This matters, because for many users, privacy isn’t just about incentives and privacy policies; it’s about proof. When we cannot track you even if we wanted to, that’s genuine privacy.</p><p>Initially, we will be offering Privacy Pass to all our plans with unlimited searches: Professional, Ultimate, Family, and Team plans. Privacy Pass will not be available to Trial and Starter plans due to technical limitations at this moment (see below for more info).</p><p>\nTo get started with Kagi Privacy Pass right away:</p><ul><li>Download the newest version of Kagi’s <a href=\"https://kagi.com/orion\">Orion Browser</a> (for macOS/iOS/iPadOS) with Kagi Privacy Pass natively integrated. You will need at least 0.99.131 for macOS and 1.3.17 for iOS/iPadOS (they are expected to be rolling out globally today).</li></ul><ul><li>Download the newest version of <a href=\"https://play.google.com/store/apps/details?id=com.kagi.search&amp;hl=en_US\">Kagi for Android</a> app with Kagi Privacy Pass natively integrated. You will need to use at least version 0.29 (this is expected to roll out globally today).</li></ul><ul><li>If you are already using the Kagi Search extension, you will want to update it to the latest version (0.7.6 on <a href=\"https://addons.mozilla.org/firefox/addon/kagi-search-for-firefox\">Firefox</a>, 1.2.2.5 on <a href=\"https://chromewebstore.google.com/detail/kagi-search/cdglnehniifkbagbbombnjghhcihifij\">Chrome</a>) to avoid compatibility issues, or simply disable it.</li><li>Safari is not yet supported due to technical limitations, see the F.A.Q. below.</li></ul><p>In addition our implementation of Privacy Pass is open sourced and you can find it <a href=\"https://github.com/kagisearch/privacypass-extension\">here</a>.</p><p>When using Kagi Privacy Pass mode, you’ll be truly anonymous - which means your account settings won’t be available since we can’t identify which user you are.</p><p>But don’t worry - we’ve made it flexible. You can easily toggle Privacy Pass on or off based on your needs. Think of it as two modes: full features with normal privacy, or maximum privacy with core features. You choose what makes sense for you based on your context and needs.</p><p>Privacy Pass uses cryptography to allow a client to authenticate to a server by performing a protocol with two phases: token generation and token redemption.</p><p>In the initial “token generation” phase, the client interacts with the server to generate some authentication “tokens.”</p><p>For the server to willingly participate in this protocol, the client must prove their “right” to generate tokens.</p><p>In the case of Kagi’s users, this can be done by presenting their Kagi session cookie to the server.</p><p>The tokens eventually generated by the client at the end of this phase are indistinguishable from a randomly generated token from the server’s point of view. They cannot be traced back to the user who generated them, or to other tokens generated by the same user at the same or a different time.</p><p>After token generation is performed, a client can initiate a “token redemption” phase.</p><p>During this phase, the client actually accesses the services provided by the server, proving the client’s right to access the services by presenting one of the previously generated tokens.</p><p>Since the previously generated tokens are unknown and unpredictable to the server, the latter can only tell that the client has successfully completed token generation at some point.</p><p>Technically, we say that the techniques used by Privacy Pass result in the two phases being “unlinkable”. While the server is able to tell whether a token presented for redemption was previously generated by interacting with a rightful client, it cannot link the token to a specific token generation phase.</p><p>Crucially, tokens are single-use: servers keep track of which tokens have already been redeemed to avoid multiple redemptions. Furthermore, clients should not present the same token twice to prevent different redemption phases from being linked.</p><p>Tokens have a fixed life span. If they are too old, they will stop being redeemable. In that case, a new token generation phase must be initiated by the client to obtain new tokens.</p><p>As standardized in [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2 - 4</a>], the Privacy Pass protocol is able to accommodate many “architectures.” Our deployment model follows the original architecture presented by Davidson  [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">1</a>], called “Shared Origin, Attester, Issuer” in § 4 of [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2</a>].</p><p>Here, Kagi plays all the “Server roles” (Attester, Issuer, Origin), and Kagi users play the Client role via the new Kagi browser extensions for Privacy Pass, or via native support in Orion. This is what it looks like in practice:</p><ol><li>Once installed, and periodically, the browser extension will generate and store a large number of tokens.</li><li>The user can mark in the extension whether searches should be performed by authenticating classically via a session cookie, or by using Privacy Pass.</li><li>If the user chooses the second option, they will authenticate to Kagi during the search by redeeming one of the tokens it previously generated.</li></ol><p>Using Privacy Pass is as easy as clicking a toggle.</p><p>If you are using the latest version of <a href=\"https://kagi.com/orion\">Orion</a> for macOS,  select Kagi as your search engine in  and then enable the checkbox for showing Privacy Pass options on your toolbar.</p><p>From there you can easily toggle when you want to use Privacy Pass or standard authentication.</p><p>On iOS and iPadOS, Kagi Privacy Pass is natively supported in the latest version of the <a href=\"https://apps.apple.com/us/app/orion-browser-by-kagi/id1484498200\">Orion Browser</a> for iOS and iPadOS and takes just a few clicks to enable.</p><p>Our <a href=\"https://play.google.com/store/apps/details?id=com.kagi.search\">Android app</a> now supports Privacy Pass mode via an app shortcut. Launching the shortcut allows you to browse Kagi seamlessly in Privacy Pass mode. You can also add the shortcut to your home screen for quick access.</p><p>This feature lets you either use Kagi exclusively in Privacy Pass mode or switch effortlessly between modes.</p><h3>Chrome and Firefox browser extensions</h3><p>If you are using the Kagi Privacy Pass extension for <a href=\"https://chromewebstore.google.com/detail/kagi-privacy-pass/mendokngpagmkejfpmeellpppjgbpdaj\">Chrome</a> or <a href=\"https://addons.mozilla.org/firefox/addon/kagi-privacy-pass/\">Firefox</a>, once installed you should see the Kagi Privacy Pass icon on your toolbar.</p><p>Once installed, the extension automatically generates tokens. To use them, click the extension icon, and make sure the toggle is on.</p><p>Note that Safari is not supported at this moment; see the F.A.Q. below for more information.</p><p>As used by Kagi, Privacy Pass tokens offer various security properties (§ 3.3,  of [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">2</a>]).</p><p>These can be a little technical to capture. In a few words, they guarantee that users can trust that their searches authenticated via Privacy Pass cannot be linked to their accounts, and Kagi can rest assured that only legitimate users can correctly authenticate using Privacy Pass. Crucially, the guarantee for users is even against malicious servers that attempt to incorrectly implement the server-side computation, as long as the client-side implementation is correct.</p><p>Three of these security properties serve to protect our users:</p><ol><li><p>Generation-redemption unlinkability: Kagi cannot link the tokens presented during token redemption ( during search) with any specific token generation phase. This means that Kagi will not be able to tell who it is serving search results to, only that it is someone who presented a valid Privacy Pass token.</p></li><li><p>Redemption-redemption unlinkability: Kagi cannot link the tokens presented during two different token redemptions. This means that Kagi will not be able to tell from tokens alone whether two searches are being performed by the same user.</p></li><li><p>No redemption hijacking: an eavesdropper that observes any token generation phase, cannot use the observed information alone to “steal” the tokens from the intended user and redeem them themselves. This means that third parties snooping on a user’s token generation interaction will not be able to steal the tokens. This adds a layer of security on top of the confidentiality attained during token generation by using a TLS-protected connection.</p></li></ol><p>Two of these security properties serve to protect Kagi.</p><ol><li><p>Correctness: honestly generated tokens will pass Kagi’s validation.</p></li><li><p>One-more-forgery security: a malicious client cannot use knowledge of a correctly generated token to forge a new one. This means that valid tokens cannot be generated without correctly interacting with Kagi, and therefore valid tokens are evidence that the user owned a valid session cookie for a supported Kagi plan at the moment of generating the token.</p></li></ol><p>Naturally, online interactions are never fully described by a mathematical model.</p><p>While the Privacy Pass protocol  indeed guarantee that the server will not be able to link token generation and token redemption phases , in principle, a malicious server could still attempt to track clients via <a href=\"https://en.wikipedia.org/wiki/Side-channel_attack\">side-channel</a> information.</p><p>For example, if someone were to make the same specific request to a server at the same time every day (say, searching “lunch places near 123 Mulholland Drive, LA” at 11:58 AM), a server that records all searches being made could, in principle, guess that these searches are all made by the same person.</p><p>In this case, Privacy Pass would make it harder for the server to determine who this specific person is, but the server could nonetheless link searches to one another.</p><p>On a level beyond, it is well known that browsers can often have a unique “fingerprint” [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">5-7</a>]. Fingerprinting attacks heavily rely on side-channel signals that evade the Privacy Pass protocol, such as user-agent strings or IP addresses. For example, if a server receives a token generation request from a given IP address, and immediately after a token redemption request from the same address, it can likely conclude that the same individual is behind the request. For this reason, it is <a href=\"https://www.rfc-editor.org/rfc/rfc9576.html#section-4.1\">highly recommended</a> to separate token generation and redemption in time, or “in space” (by using an anonymizing service such as <a href=\"https://www.torproject.org/\">Tor</a> when redeeming tokens, see below).</p><p>Kagi’s Privacy Pass extension and native implementation in Orion take care, as much as we can, to uniform your browser fingerprint, by removing deanonymizing HTTP headers and cookies.</p><p>We see Privacy Pass as an important tool for increasing the anonymity guarantees we can offer to Kagi users.</p><p>Adopting state-of-the-art standards for new privacy enhancing technologies also signals to researchers and standardization bodies that there is a public demand for more privacy and anonymity tools in today’s digital world and incentivizes further scrutiny and development of privacy-enhancing technologies.</p><p>Together with launching Privacy Pass, we are also announcing that we now have a Tor onion service available, which allows access to Kagi directly from the Tor network. Kagi’s onion address is:</p><p>On its own, Tor will obscure your location by hiding your IP address. However, without Privacy Pass, you still need to be logged into your Kagi account to perform searches, making them all theoretically linkable back to a single account. As always, Kagi does not link searches to accounts or permanently record them; see <a href=\"https://kagi.com/privacy\">our Privacy Policy</a> for more info.</p><p>With Tor and Privacy Pass together, Kagi only knows that the search is being issued by a user who previously verified that they have an account authorized to receive tokens, but nothing about the user’s account, or where they’re located.</p><p>Privacy Pass support is provided:</p><p>This should accommodate users who want to install and use the extension across multiple browsers or computers. Please refer to our documentation for <a href=\"https://help.kagi.com/kagi/privacy/privacy-pass.html\">usage instructions</a>.</p><p>At first, Privacy Pass authentication will be available to users on any Kagi plan with unlimited searches. These plans will have a generous allocation of tokens (2000 to begin with) that they can generate monthly.</p><p>We are working on enabling this feature for Trial and Starter plans, which have access to a limited number of monthly searches. Therefore, they risk a worse user experience if their generated tokens are lost (for example, due to uninstalling the extension) and theoretically, users on this plan could redeem more tokens than the limit of searches allowed on their plan (again, we do not know who the user redeeming the tokens is, or what plan they are on). This makes it more technically challenging to support these plans with Privacy Pass, and we have left that for later.</p><p><strong>You mention “tokens.” Are blockchains involved in this protocol?</strong></p><p>Privacy Pass does not rely on any blockchain technology.</p><p>While the protocol makes use of various cryptographic primitives (specifically, elliptic curves and hash functions, as part of a “verifiable oblivious pseudorandom function” construction, [<a href=\"https://blog.kagi.com/kagi-privacy-pass#references\">8</a>]) and generates “tokens,” these are not generated, stored, or traded on a blockchain.</p><p><strong>You mention the client generating tokens. Is this process energy-intensive or storage-demanding?</strong></p><p>No. The generation of 500 search tokens requires approximately 1 second of computation on a consumer laptop, and is performed in the background when installing the extension. A few extra seconds may be required due to the time required to contact the server and get a response. Each token consists of 216 bytes, for a total of approximately 100 KiB of storage per token generation request.</p><p><strong>Is there a potential impact on the speed of search when using Privacy Pass?</strong></p><p>The initial generation of tokens takes about ~1 second for 500 tokens, plus the time required for contacting the server. This occurs infrequently and is done in the background when possible.</p><p>Currently, the token validation servers are only deployed in our us-central1 region, we plan to expand this shortly after launch.</p><p><strong>How many tokens am I able to generate?</strong></p><p>You can generate 2,000 tokens in one “epoch” (= one month). This should be enough for most users. If you need more than this, you can request additional tokens by contacting support@kagi.com.</p><p><strong>Do you plan to allow purchasing privacy pass tokens without having an account?</strong></p><p>Yes, this makes sense. This is possible because technically the extension does not care if you have an account or not. It just needs to be ‘loaded’ with valid tokens. And you can imagine a mechanism where you could also anonymously purchase them, eg. with monero, without ever creating an account at Kagi.  Let us know <a href=\"https://kagifeedback.org/d/6163-kagi-privacy-pass\">here</a> if you are excited about this, as it will help prioritize it.</p><p><strong>How can I submit feedback for Kagi Privacy Pass?</strong></p><p>We have a feedback thread open <a href=\"https://kagifeedback.org/d/6163-kagi-privacy-pass\">here</a>.</p><p><strong>Even if the extension implements anti-fingerprinting measures, Kagi will still be able to see my IP address, correct?</strong></p><p>Even with Privacy Pass authentication enabled, due to the way the TCP/IP stack works, we will be able to see your search request come from an IP address. As outlined in our <a href=\"https://kagi.com/privacy\">Privacy Policy</a>, your privacy is our priority, whether you are using Privacy Pass to authenticate or otherwise. If you are worried about us seeing your IP address, our suggestion is to connect to Kagi via Tor or through a VPN service you trust.</p><p><strong>How can Privacy Pass increase my privacy, if I have to send a session cookie to authenticate during token generation?</strong></p><p>While token generation is indeed not anonymous, Privacy Pass provides you with anonymity .</p><p>By providing the server with a Privacy Pass token instead of a session cookie when searching, you will guarantee that your searches cannot be  to any specific user account that generated Privacy Pass tokens, or to each other.</p><p>From the point of view of the server, your search query could have come from any of the users who previously generated Privacy Pass tokens.</p><p>The more users do so, the lower the probability that the server can guess it was you specifically who made a given search query.</p><p><strong>Token generation does not work in my Chrome/Firefox private window</strong></p><p>Correct, we need to authenticate you to create tokens (see above), and in the private window, the extension does not have access to your session cookie. Please use a normal browsing window while logged in to Kagi to generate tokens.</p><p>Note, generating tokens while in a private window will work in the <a href=\"https://kagi.com/orion\">Orion browser</a>.</p><p><strong>Can I use Kagi Assistant while using Privacy Pass?</strong></p><p>Not at this time, since Kagi Assistant is only available to Ultimate members. In Privacy Pass, we don’t have any account information, so we can’t validate what plan you’re on. We could issue tokens attached to different keys for different plans, but that also has privacy implications, see the discussion of personalization below.</p><p><strong>What Kagi services will be compatible with Privacy Pass at launch?</strong></p><p>At launch, Privacy Pass will only be used to authenticate Kagi Search. Soon to follow (in the next few weeks), we plan to expand support for Kagi Privacy Pass to:</p><ul><li>Kagi Translate and Kagi Maps</li><li>Kagi universal Summarizer and Ask questions about page</li></ul><p>Please disable Privacy pass to access these services for now.</p><p><strong>Since initial token generation happens in batches and the tokens expire, could tokens with similar expiration dates potentially be used to identify multiple searches from the same user?</strong></p><p>All tokens generated during month X expire at midnight of the first day of month X+2, to avoid this exact issue. Meaning a freshly generated token lasts until the end of the month following its generation (generate today, use all of Feb and March).</p><p><strong>If Kagi cannot track who exactly is performing search queries, will I have access to my account settings including customization and personalization?</strong></p><p>Since Kagi will not know who you are, we will not be able to serve you content tailored to your custom settings via Privacy Pass-protected search.</p><p>We have considered allowing users to send a small configuration with every request <code>(language, region, safe-search)</code> to automatically customize your search experience to some extent. However, we currently believe this would quickly result in a significant loss of anonymity for you and for other users. To illustrate this, we have examined the most common configurations of <code>(language, region, search-safe)</code> used on Kagi.com, and extrapolated how many Privacy Pass users would share them. Looking at the top 35 configurations, we see the following approximate numbers.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>Limiting the analysis to only the ten most common language settings, the effect is similar:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>This would mean that someone sending us  as their language configuration would automatically lose redemption-redemption unlinkability guarantees if approximately 1000 Kagi users used Privacy Pass.</p><p>While our extrapolation may be overly conservative, we won’t be enabling this level of “default” customization for users authenticating via Privacy Pass for the time being. We could reconsider if we find a better solution.</p><p>For manual search settings customization, you can always use bangs in your search query to enable basic settings for a specific query. For example, <a href=\"https://help.kagi.com/kagi/features/bangs.html#regional-bangs\">regional bangs</a> will let you focus your query on one region. For example prefixing your search with  will automatically search in the German region.</p><p>To access a fully customized search experience, you can always use the traditional login method and disable the use of Privacy Pass.</p><p><strong>Will Safari be supported?</strong></p><p>The Safari extensions API doesn’t support (as far as we know) removing cookies from requests, which means it will always authenticate with your logged-in account. We’re not aware of a way to change this. The alternative if you want a similar, native, WebKit-based browsing experience, is to use the <a href=\"https://kagi.com/orion\">Orion Browser</a> which has Kagi Privacy Pass natively integrated.</p><ol><li>Davidson, A., Goldberg, I., Sullivan, N., Tankersley, G., &amp; Valsorda, F. (2018). Privacy pass: Bypassing internet challenges anonymously. Proceedings on Privacy Enhancing Technologies. <a href=\"https://doi.org/10.1515/popets-2018-0026\">Paper</a>.</li><li>Davidson, A., Iyengar, J., &amp; A. Wood, C. (2024). The Privacy Pass Architecture. <a href=\"https://www.rfc-editor.org/rfc/rfc9576.html\">RFC 9576</a>.</li><li>Pauly, T., Valdez, S., &amp; A. Wood, C. (2024). The Privacy Pass HTTP Authentication Scheme. <a href=\"https://www.rfc-editor.org/rfc/rfc9577.html\">RFC 9577</a>.</li><li>Celi, S., Davidson, A., Valdez, S., &amp; Wood, C. A. (2024). Privacy Pass Issuance Protocols. <a href=\"https://www.rfc-editor.org/rfc/rfc9578.html\">RFC 9578</a>.</li><li>Eckersley, P. (2010). How unique is your web browser? Proceedings on Privacy Enhancing Technologies. <a href=\"https://dl.acm.org/doi/10.5555/1881151.1881152\">Paper</a>.</li><li>Davidson, A., Faz-Hernandez, A., Sullivan, N., &amp; A. Wood, C. (2023). Oblivious Pseudorandom Functions (OPRFs) Using Prime-Order Groups. <a href=\"https://www.rfc-editor.org/rfc/rfc9497.html\">RFC 9497</a>.</li></ol><blockquote><p>Do you like how this post reads? It was proofread with <a href=\"https://translate.kagi.com\">Kagi Translate’s</a> proofreading option. To proofread any web page, just use <code>https://translate.kagi.com/proofread/[URL]</code>.</p></blockquote>","contentLength":21275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43040521"},{"title":"Phind 2: AI search with visual answers and multi-step reasoning","url":"https://www.phind.com/blog/phind-2","date":1739470829,"author":"rushingcreek","guid":178,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43039308"},{"title":"LibreOffice still kicking at 40, now with browser tricks and real-time collab","url":"https://www.theregister.com/2025/02/13/libreoffice_wasm_zetaoffice/","date":1739465983,"author":"LinuxBender","guid":177,"unread":true,"content":"<p> LibreOffice is a big, mature chunk of code now, but that doesn't make it impossible to teach it impressive new tricks. Some of them could make it more important than ever.</p><p>The open-source office suite had its own <a target=\"_blank\" href=\"https://fosdem.org/2025/schedule/track/libreoffice/\" rel=\"nofollow\">program stream</a> at FOSDEM, including the <a target=\"_blank\" href=\"https://fosdempwa.com/event/5735?year=2025&amp;day=null&amp;test=false\" rel=\"nofollow\">pre-announcement</a> of the new <a target=\"_blank\" href=\"https://wiki.documentfoundation.org/ReleaseNotes/25.2\" rel=\"nofollow\">LibreOffice release 25.2</a>. It has been around in some form since 1985, so this version marks its 40th year. It's middle-aged and, almost inevitably, that means it's big, a bit saggy in places, it definitely has some issues, and it doesn't look as good as it did a couple of decades ago.</p><p>But that doesn't mean it can't be taught interesting things. Even a 40-year-old can acquire new skills and take on entirely new roles.</p><p>It's the same sort of functionality that you get from Google Docs, and indeed this is already possible using the <a target=\"_blank\" href=\"https://www.theregister.com/2022/07/01/collabora_releases_code_2205_webbased/\">Collabora Online web-based version of LibreOffice</a>. The big difference is that such tools run in a browser, so you need to be online. What makes the CRDT implementation different is that this is a local app, working on a local file, but using a network copy to keep changes in sync. The idea is to free you from keeping your apps and data on someone else's computer, without losing the handy collaborative features that web apps bring.</p><p>We also met up with Thorsten for a demo of one of his other babies, <a target=\"_blank\" href=\"https://zetaoffice.net/\" rel=\"nofollow\">ZetaOffice</a>. This is a version of LibreOffice built for the Wasm runtime, which means it can run inside a browser. He demonstrated it to us on an AMD Ryzen-powered developer-spec ThinkPad, but the same binaries could run on any OS and on any CPU, such as ARM hardware. ZetaOffice went into public beta <a target=\"_blank\" href=\"https://blog.allotropia.de/2024/11/08/launching-zetaoffice/\" rel=\"nofollow\">last November</a>, and a first release is looming very soon.</p><p>He showed us it running in multiple modes. You can embed any of the LibreOffice apps into a web page, complete with their normal user interface and so on, much like Google Apps. This enables a website to offer a very rich editing experience of text, spreadsheets, presentations, or anything else. ZetaOffice can also be embedded, scripted, and controlled using the <a target=\"_blank\" href=\"https://github.com/allotropia/zetajs\" rel=\"nofollow\">zetajs wrapper</a>, so you can call it to display embedded rich objects in web pages. We were rather taken by a demo that displayed a moving line chart of ping times to the  website. A script was extracting the times from the ping command, adding lines to a LibreOffice Calc spreadsheet, graphing the last dozen or so lines and embedding the graph in a web page.</p><p>He told us that the Allotropia development team sees ZetaOffice as orthogonal to Collabora Office (or COOL for short). COOL runs in the browser and has simultaneous multiuser editing. ZetaOffice runs in the browser too, but it's perfectly able to run locally and doesn't need an uplink. For now, though, it's single user. The company will make a cross-platform version that is also available for local installation.</p><p>The real power comes from the scriptability and integration with JavaScript, though. We are sure lots of people will find uses for the ability to embed any document LibreOffice can open into any web page, control and automate it from JavaScript, with the choice of a full local user interface – or none, just the content, controlled with a JavaScript UI instead. The code is all under the MIT licence, so it's all-FOSS, and all the work is being upstreamed to LibreOffice itself.</p><p>This is big stuff in more than one way. LibreOffice isn't lightweight anyway. Opening a document in a web page can pull in a gigabyte or so of code, and the memory footprint is a bit more than that. It is still in beta, though, and the team is working hard on modularizing the code into multiple smaller chunks so that it will load much faster and take less memory.</p><p>The potential flexibility here&nbsp;is impressive, though. You could have a portable app on a USB key that was usable on a PC or Mac or Chromebook, regardless of the OS, without installation. Web apps suddenly acquire full rich cross-platform document handling and editing, including, of course, all the main Microsoft files and formats.</p>","contentLength":3989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43038200"},{"title":"Resigning as Asahi Linux project lead","url":"https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/","date":1739460897,"author":"Shank","guid":176,"unread":true,"content":"<p>Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them “jailbreaks” these days) to allow users to run their own unofficial apps on the Nintendo Wii.</p><p>I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people’s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn’t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.</p><p>When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.</p><p>I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.</p><p>Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I’ve already spoken at length about and I won’t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience.</p><p>But then also came the entitled users. This time, it wasn’t about stealing games, it was about features. “When is Thunderbolt coming?” “Asahi is useless to me until I can use monitors over USB-C” “The battery life sucks compared to macOS” (nobody ever complained when compared to x86 laptops…) “I can’t even check my CPU temperature” (yes, I seriously got that one). (<em>Edit: This wasn’t just a few instances; I’ve seen variations on the first three posted hundreds of times by now, including takes like “Thunderbolt/DP Alt are never going to happen”. A few times is fine, but the same thing repeated over and over again every day while we’re trying to make these things happen will get to anyone.</em>)</p><p>And, of course, “When is M3/M4 support coming?”</p><p>For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were “alpha” and “unstable” and “not suitable for a daily driver” (despite <a href=\"https://stats.asahilinux.org\">thousands of users</a>, myself included, daily driving it and even using it for servers).</p><p>No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.</p><p>I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.</p><p>Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won’t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).</p><p>I did make some progress in 2024, but this left me in a very vulnerable position. I hadn’t gotten nearly as much Asahi work done as I’d liked, and the users weren’t getting any quieter about demanding more features and machine support.</p><p>We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).</p><p>I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. “Full” DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn’t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did <a href=\"https://lore.kernel.org/lkml/fda8b831-1ffc-4087-8e7b-d97779b3ecc5@marcan.st/T/#u\">try</a> to bring the subject up on the mailing lists, but it didn’t get much response).</p><p>The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won’t repeat them in detail here. Suffice it to say, I consider Linus’ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.</p><p>As you know, this is deeply personal to me, as we’ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.</p><p>Some might be wondering why we can’t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there’s more.</p><p>In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that’s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a <a href=\"https://pagure.io/fedora-asahi/mesa-asahi-flatpak\">partial workaround</a> for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.</p><p>I’m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated <a href=\"https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/\">“We\nare the ‘thin blue line’”</a>, and nobody cared, which just further confirmed to me that I don’t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to <a href=\"https://lore.kernel.org/lkml/20240828211117.9422-1-wedsonaf@gmail.com/\">quit</a>.</p><p>But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than <a href=\"https://bunny.net/\">bunny.net</a> giving us free CDN credits for the hosting).</p><p>I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I’m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won’t touch the Linux kernel with a 10-foot pole.</p><p>When Apple released the M1, Linus Torvalds <a href=\"https://thenextweb.com/news/linus-torvalds-wants-apples-new-m1-powered-macs-to-run-linux\">wished it could run Linux</a>, but didn’t have much hope it would ever happen. We made it happen, and Linux 5.19 was <a href=\"https://lore.kernel.org/lkml/CAHk-=wgrz5BBk=rCz7W28Fj_o02s0Xi0OEQ3H1uQgOdFvHgx0w@mail.gmail.com/T/#u\">released from an M2 MacBook Air running Asahi Linux</a>. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him <a href=\"https://gist.github.com/marcan/fe70ee6648f3d5ae94eb8332265b8d95\">an invitation</a> to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.</p><p>Back in 2011, Con Kolivas <a href=\"https://web.archive.org/web/20110707151924/http://apcmag.com/why_i_quit_kernel_developer_con_kolivas.htm\">left the Linux kernel community</a>. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you’re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.</p><p>When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it’s not fun any more. I have an M3 Pro in a box and I haven’t even turned it on yet. I dread doing the bring-up work. It doesn’t feel worth the trouble.</p><p>I miss having free time where I can relax and not worry about the features we haven’t shipped yet. I miss <a href=\"https://youtube.com/@TsuiokuCircuit\">making music</a>. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven’t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.</p><p>I’m resigning as lead of the Asahi Linux project, effective immediately. The project will <a href=\"https://asahilinux.org/2025/02/passing-the-torch/\">continue on without me</a>, and I’m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the <a href=\"https://opencollective.com/asahilinux\">Asahi Linux OpenCollective</a> (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).</p><p>I want to thank the entire Asahi Linux team, without whom I would’ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.</p><p>If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: <a href=\"mailto:marcan@marcan.st\">marcan@marcan.st</a>.</p><p>: A lot of the discussion around this post and the interactions that led to it brings up the term “brigading”. Please read <a href=\"https://hachyderm.io/@chandlerc/114001000657957325\">this excellent Fedi post</a> for a discussion of what is and isn’t brigading.</p>","contentLength":15204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43036904"},{"title":"Why young parents should focus on building trust with their kids","url":"https://desunit.com/blog/marshmallow-test-and-parenting/","date":1739430767,"author":"desunit","guid":175,"unread":true,"content":"<p>Today, when my 1.9-year-old daughter tried to grab and eat an unwashed orange, I asked her to give it to me so I could wash and peel it if she could wait for a moment. She did. It got me thinking about the marshmallow experiment, where the idea was that patience equals success. But … it wasn’t the whole story.</p><p>The marshmallow experiment is famous: a little kid in a room staring at a marshmallow. If they wait 15 minutes, they get two marshmallows instead of just one. Some kids would poke the marshmallow, lick it, or just gobble it up. Others found clever ways to distract themselves – singing, closing their eyes, even falling asleep. The results – children who waited supposedly went on to achieve higher scores in school and better life outcomes. The message was clear: if you can delay gratification, you’re set for life.&nbsp;But later studies revealed some serious holes in that conclusion.</p><p>First of all, it’s not just about willpower. A follow-up study showed that kids from stable, reliable homes were much more likely to wait than kids from unpredictable ones. If you’re a kid and the adults in your life constantly break promises, why would you trust them this time? Why wait for the second marshmallow if history tells you it might not show up? Waiting isn’t a character trait; it’s a strategy. And strategies are shaped by experience.</p><p>Kids are like tiny detectives, watching and learning from every single thing we do. My toddler loves routines – the same bedtime story, the same morning/evening ritual. He thrives on knowing what’s next. So when I tell her to wait and then actually deliver, I’m teaching her something important: trust. But if I say “five more minutes” and don’t follow through, I’m sending a different message – that waiting might not be worth it.</p><p>Back to the marshmallow test. Another big revelation came when researchers looked at economic backgrounds. Kids from wealthier families waited longer than kids from low-income families. Not because they had more self-control, but because their environment made waiting feel safer. If you grow up knowing there will always be food on the table, waiting for an extra marshmallow isn’t a big deal. But if your life is more uncertain, grabbing what you can when you can make total sense.&nbsp;It’s a survival skill, not a lack of discipline.</p><p>This reminds me of a moment last week. My daughter asked me to swing her in my arms, but I was busy and promised her we’d do it once I finished my work. She switched to another task and waited. When I finally asked, “Would you like to have fun now?” she cheerfully screamed, “Yes!” Would she have trusted me if I forgot about it? Probably not.</p><p>Kids crave predictability. That’s why they ask for the same bedtime story 300 nights in a row. It’s why they get upset if the routine changes without warning. Predictability equals safety. And safety builds trust. When they trust us, they’re willing to wait, to try, to take risks.</p><p>So, here’s where the marshmallow test gets it wrong: it’s not just about the kid’s ability to wait; it’s about the adult’s ability to create an environment worth waiting in. If we, as parents, and teachers, aren’t consistent, why would a kid believe us when we say, “Just hold on, something better is coming”?</p><p>Another thing I’ve noticed is how much modeling matters. My daughter watches everything I do. If I tell her to wait and then lose my patience two seconds later because the internet is slow, what’s the lesson there? Waiting is for other people? The best way to teach patience is to live it – easier said than done …</p><p>The marshmallow test also doesn’t account for cultural differences. In some cultures, waiting is baked into daily life. Think about Japan, where kids are often taught to wait quietly for meals or gifts. Compare that to the US, where instant gratification is practically a way of life. These cultural norms shape how kids approach situations like the marshmallow test. It’s not just about personality; it’s about the world they live in.</p><p>In 2012, researchers added a modification to the marshmallow experiment. Before the test, kids were split into two groups. One group experienced a broken promise (they were told they’d get crayons to draw with, but never did). The other group experienced a kept promise (they actually got the crayons). Then came the marshmallow test. Which group waited longer? The kids who saw promises kept. The trust made all the difference.</p><p>So what’s the takeaway here? It’s simple, really: as parents, we set the tone. Our actions, promises, and reliability shape how our kids see the world. Are we building an environment where they feel safe enough to wait? Or are we teaching them that they need to grab what they can, when they can?</p><p>The marshmallow test highlights how our actions shape our kids’ lives. Small gestures – like keeping promises, sticking to “just five more minutes,” or reading bedtime stories – build trust and security. These moments might even help them wait for their own second marshmallow – or decide they don’t need to. Ultimately, it’s about the environment we are creating for them. And who knows, maybe one day they will pass their own version of the marshmallow test – not because they have to, but because they know they can.</p>","contentLength":5341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43033463"},{"title":"Leaking the email of any YouTube user for $10k","url":"https://brutecat.com/articles/leaking-youtube-emails","date":1739359182,"author":"brutecat","guid":174,"unread":true,"content":"<a href=\"https://brutecat.com/\" data-svelte-h=\"svelte-5sfkfb\">&lt; Back</a><h2>Leaking the email of any YouTube user for $10,000</h2><img src=\"https://brutecat.com/assets/youtube-email-disclosure.png\" alt=\"\"><p>Some time ago, I was looking for a research target in Google and was digging through the <a href=\"https://staging-people-pa.sandbox.googleapis.com/$discovery/rest?key=AIzaSyBOh-LSTdP2ddSgqPk6ceLEKTb8viTIvdw\">Internal People API (Staging)</a> discovery document until I noticed something interesting:</p><pre><code></code></pre><p>It seemed the Google-wide block user functionality was based on an obfuscated Gaia ID as well as a display name for that blocked user. The obfuscated Gaia ID is just a Google account identifier.‎</p><p>So, if you block someone on YouTube, you can leak their Google account identifier? I tested it out. I went to a random livestream, blocked a user and sure enough, it showed up in <a href=\"https://myaccount.google.com/blocklist\">https://myaccount.google.com/blocklist</a>‎</p><p>The fallback name was set as their channel name  and the profile ID was their obfuscated Gaia ID ‎</p><p>This was super strange to me because YouTube should never leak the underlying Google account of a YouTube channel. In the past, there's been several bugs to <a href=\"https://sector035.nl/articles/2022-35\">resolve these to an email address</a>, so I was confident there was still a Gaia ID to Email in some old obscure Google product.</p><h3>\n              Escalating this to 4 billion YouTube channels\n            </h3><p>So, we can leak the Gaia ID of any live chat user, but can we escalate this to all channels on YouTube? As it turns out, when you click the 3 dots just to open the context menu, a request is fired:‎</p><pre><code>www.youtube.com\n&lt;redacted&gt;</code></pre><pre><code> OK\napplication/json; charset=UTF-8\nscaffolding on HTTPServer2\n\n</code></pre><p>That  is nothing more than just base64 encoded protobuf, which is a common encoding format used throughout Google.‎</p><p>If we try decoding that  params:</p><pre><code>$ echo -n \"Q2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZMUFBV0FGaUx3b1ZNVEV6T1RBM05EWTJOVE0zTmpjd016Y3dOVGt3RWhaVFJTMWhXVTlpTFhWRFp6QTFPWEZJVW1selgyOTNjQUElM0Q=\" | base64\n -d | sed 's/%3D/=/g' | base64 -d | protoc --decode_raw\n1 {\n  5 {\n    1: \"UChs0pSaEoNLV4mevBFGaoKA\"\n    2: \"36YnV9STBqc\"\n  }\n}\n10: 0\n11: 1\n12 {\n  1: \"113907466537670370590\"\n  2: \"SE-aYOb-uCg059qHRis_ow\"\n}\n14: 0</code></pre><p>It actually just contains the Gaia ID of the user we want to block, we don't even need to block them!</p><p>Let's check out the  requests params too:</p><pre><code>$ echo -n \"R2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZeklhQ2hoVlExTkZMV0ZaVDJJdGRVTm5NRFU1Y1VoU2FYTmZiM2M9\" | base64 -d | sed 's/%3D/=/g' | base64 -d | protoc --decode_raw\n3 {\n  5 {\n    1: \"UChs0pSaEoNLV4mevBFGaoKA\"\n    2: \"36YnV9STBqc\"\n  }\n}\n6 {\n  1: \"UCSE-aYOb-uCg059qHRis_ow\"\n}</code></pre><p>Seems to just contain the channel ID of the channel we're blocking, the livestream video ID and livestream author ID. Let's try to fake the request params with our own target's channel ID.‎</p><pre><code>$  -n  |  -d | sed  |  -d | sed  |  | \nR2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZeklhQ2hoVlEwUXlURnBCVkRGcQpNVVI1VmxoeE1sSXlRbVIxYzFFPQo=</code></pre><p>Testing this on <code>/youtubei/v1/live_chat/get_item_context_menu</code>:</p><pre><code>...\n\"moderateLiveChatEndpoint\":{\"params\":\"Q2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZMUFBV0FGaUx3b1ZNVEF6TWpZeE9UYzBNakl4T0RJNU9Ea3lNVFkzRWhaRU1reGFRVlF4YWpGRWVWWlljVEpTTWtKa2RYTlJjQUElM0Q=\"}\n...</code></pre><pre><code>echo -n \"Q2lrcUp3b1lWVU5vY3pCd1UyRkZiMDVNVmpSdFpYWkNSa2RoYjB0QkVnc3pObGx1VmpsVFZFSnhZMUFBV0FGaUx3b1ZNVEF6TWpZeE9UYzBNakl4T0RJNU9Ea3lNVFkzRWhaRU1reGFRVlF4YWpGRWVWWlljVEpTTWtKa2RYTlJjQUElM0Q=\" | base64 -d | sed 's/%3D/=/g' | base64 -d | protoc --decode_raw\n1 {\n  5 {\n    1: \"UChs0pSaEoNLV4mevBFGaoKA\"\n    2: \"36YnV9STBqc\"\n  }\n}\n10: 0\n11: 1\n12 {\n  1: \"103261974221829892167\"\n  2: \"D2LZAT1j1DyVXq2R2BdusQ\"\n}\n14: 0</code></pre><p>We can leak the Gaia ID of the channel - </p><h3>\n              The missing puzzle piece: Pixel Recorder\n            </h3><p>I told my friend <a href=\"https://schizo.org\">nathan</a> about the YouTube Gaia ID leak and we started looking into old forgotten Google products since they probably contained some bug or logic flaw to resolve a Gaia ID to an email. <a href=\"https://recorder.google.com\">Pixel Recorder</a> was one of them. Nathan made a test recording on his Pixel phone and synced it to his Google account so we could access the endpoints on the web at <a href=\"https://recorder.google.com\">https://recorder.google.com</a>:‎</p><p>When we tried sharing the recording to a test email, that's when it hit us:</p><pre><code>pixelrecorder-pa.clients6.google.com\n&lt;redacted&gt;\n80\n&lt;redacted&gt;\nAIzaSyCqafaaFzCP07GzWUSRw0oXErxSlrEX2Ro\napplication/json+protobuf\nhttps://recorder.google.com/\n\n</code></pre><pre><code> OK\napplication/json+protobuf; charset=UTF-8\nESF\n138\n\n</code></pre><p>This endpoint was taking in the obfuscated Gaia ID and... ‎</p><p>We tested this with the obfuscated Gaia ID  we got from blocking that user on YouTube a while back and :</p><pre><code> OK\napplication/json+protobuf; charset=UTF-8\nESF\n138\n\n</code></pre><h3>\n              A small problem: preventing notification to the target\n            </h3><p>It seems that whenever we share a recording with a victim, they receive an email that looks like this:‎</p><p>This is , and it would lower the impact of the bug quite a lot. On the share pop-up, there didn't seem to be any option to disable notifications.</p><p>I tried leaking the full request proto via my tool <a href=\"https://github.com/ddd/googleapi_tools/tree/main/req2proto\">req2proto</a>, but there was nothing about disabling the email notification:</p><pre><code>syntax = ;\n\n java.com.google.wireless.android.pixel.recorder.protos;\n\n;\n\n {\n   recording_id = ;\n   delete_obfuscated_gaia_ids = ;\n  ShareUser update_shared_users = ;\n   sharing_message = ;\n}\n\n {\n   obfuscated_gaia_id = ;\n  java.com.google.wireless.android.pixel.recorder.sharedclient.acl.protos.ResourceAccessRole role = ;\n   email = ;\n}</code></pre><p>Even trying to add and remove the user at the same time didn't work, the email was still sent. But that's when we realized - if it's including our recording title in the email subject, perhaps it wouldn't be able to send an email if our recording title was too long.‎</p><p>We hacked together a quick python script to test this out:</p><pre><code> requests\n\nBASE_URL = \n\nheaders = {\n    : ,\n    : ,\n    : ,\n    : \n}\n\n():\n    payload = \n    response = requests.post(BASE_URL +  + , headers=headers, data=payload)\n     response.status_code != :\n        (, response.json())\n        exit()\n    :\n        response = response.json()\n    :\n        (, response.text)\n        exit()\n\n     response[][]\n\n():\n    x = *\n    payload = \n    response = requests.post(BASE_URL +  + , headers=headers, data=payload)\n     response.status_code != :\n        (, response.json())\n        exit()\n\n():\n    share_id = ()\n    headers[] = ( )\n    headers[] = ()\n    uuid = get_recording_uuid(share_id)\n    (, uuid)\n    update_recording_title(uuid)\n    ()\n\n __name__ == :\n    main()</code></pre><p>... and the recording title was now <strong>2.5 million letters long!</strong> There wasn't any server-side limit to the length of a recording name.‎</p><p>Trying to share the recording with a different test user...  No notification email.‎</p><p>We basically have the full attack chain, we just have to put it together.‎</p><ol><li>Leak the obfuscated Gaia ID of the YouTube channel from the Innertube endpoint </li><li>Share the Pixel recording with an extremely long name with the target to convert the Gaia ID to an email</li><li>Remove the target from the Pixel recording (cleanup)‎</li></ol><p>Here's a POC of the exploit in action:</p><ul><li>15/09/24 - Report sent to vendor</li><li>16/09/24 - Vendor triaged report</li><li>03/10/24 - Panel marks it as duplicate of existing-tracked bug, does botched patch of initial YouTube obfuscated Gaia ID disclosure</li><li>03/10/24 - Clarified to vendor that they haven't recognized Pixel recorder as vulnerability itself (since obfuscated Gaia IDs are leaked for Google Maps/Play reviewers) and provided vendor a work-around method to once again leak YouTube channel obfuscated Gaia IDs</li><li>05/11/24 -  Rationale: Exploitation likelihood is medium. Issue qualified as an abuse-related methodology with high impact.</li><li>03/12/24 - Product team sent report back to panel for additional reward consideration, coordinates disclosure for 03/02/2025</li><li>12/12/24 - <strong>Panel awards an additional $7,500.</strong> Rationale: Exploitation likelihood is high. Issue qualified as an abuse-related methodology with high impact. Applied 1 downgrade from the base amount due to complexity of attack chain required.</li><li>29/01/25 - Vendor requests extension for disclosure to 12/02/2025</li><li>09/02/25 - Confirm to vendor that both parts of the exploit have been fixed (T+147 days since disclosure)</li><li>12/02/25 - Report disclosed</li></ul><p data-svelte-h=\"svelte-uqqsu8\">You can contact me via\n\t\t\t\t<a href=\"https://signal.me/#eu/oT3XakzbgN6x5fLe37yBwfssy4KVje3aqZyCVArWxofNQ7FbnOdHfpfs3sn6Lfn6\"><img src=\"data:image/svg+xml,%3c!DOCTYPE%20svg%20PUBLIC%20'-//W3C//DTD%20SVG%201.1//EN'%20'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'%3e%3c!--%20Uploaded%20to:%20SVG%20Repo,%20www.svgrepo.com,%20Transformed%20by:%20SVG%20Repo%20Mixer%20Tools%20--%3e%3csvg%20width='800px'%20height='800px'%20viewBox='0%200%2048%2048'%20id='Layer_2'%20data-name='Layer%202'%20xmlns='http://www.w3.org/2000/svg'%20fill='%23ffffff'%20stroke='%23ffffff'%3e%3cg%20id='SVGRepo_bgCarrier'%20stroke-width='0'/%3e%3cg%20id='SVGRepo_tracerCarrier'%20stroke-linecap='round'%20stroke-linejoin='round'/%3e%3cg%20id='SVGRepo_iconCarrier'%3e%3cdefs%3e%3cstyle%3e.cls-1{fill:none;stroke:%23ffffff;stroke-linejoin:round;}%3c/style%3e%3c/defs%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M27.32,45.25A23.08,23.08,0,0,1,24,45.5a22.26,22.26,0,0,1-3.26-.25m14.44-2.88a21,21,0,0,1-6.08,2.51M41.36,36.7a21.63,21.63,0,0,1-4.66,4.65m5.65-6.16,2.54-6.08a21.25,21.25,0,0,1-2.52,6.07m2.88-14.42A23.33,23.33,0,0,1,45.5,24a22.43,22.43,0,0,1-.25,3.28m-42.46,0A22.46,22.46,0,0,1,2.5,24a22.43,22.43,0,0,1,.25-3.28m39.63-7.89a21.7,21.7,0,0,1,2.51,6.08m-41.69,0a21.19,21.19,0,0,1,2.52-6.06h0m31-6.2a21.19,21.19,0,0,1,4.66,4.65m-34.71,0A21.63,21.63,0,0,1,11.3,6.64M29.09,3.1a21.57,21.57,0,0,1,6.07,2.53h0m-22.33,0a21.07,21.07,0,0,1,6.09-2.51m1.84-.37A21.88,21.88,0,0,1,24,2.5a22.29,22.29,0,0,1,3.27.25'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M18.87,44.87a20.92,20.92,0,0,1-5-1.95l-2.24.51'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M4.61,36.38l.51-2.2a21.7,21.7,0,0,1-2-5'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M10,43.85l-4.08,1a2.19,2.19,0,0,1-2.66-1.56,2.27,2.27,0,0,1,0-1.1l1-4.08'/%3e%3cpath%20stroke-width='2px'%20class='cls-1'%20d='M24,6.41a17.59,17.59,0,0,0-14.83,27l-1.65,7.1,7.16-1.64A17.59,17.59,0,1,0,24,6.41Z'/%3e%3c/g%3e%3c/svg%3e\" alt=\"signal icon\"></a>\n\t\t\t\tor\n\t\t\t\t<a href=\"https://brutecat.com/cdn-cgi/l/email-protection#7c0e1313083c1e0e0908191f1d08521f1311\"><img src=\"data:image/svg+xml,%3c!DOCTYPE%20svg%20PUBLIC%20'-//W3C//DTD%20SVG%201.1//EN'%20'http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd'%3e%3c!--%20Uploaded%20to:%20SVG%20Repo,%20www.svgrepo.com,%20Transformed%20by:%20SVG%20Repo%20Mixer%20Tools%20--%3e%3csvg%20width='800px'%20height='800px'%20viewBox='0%200%2024%2024'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20id='SVGRepo_bgCarrier'%20stroke-width='0'/%3e%3cg%20id='SVGRepo_tracerCarrier'%20stroke-linecap='round'%20stroke-linejoin='round'/%3e%3cg%20id='SVGRepo_iconCarrier'%3e%3cpath%20d='M4%207.00005L10.2%2011.65C11.2667%2012.45%2012.7333%2012.45%2013.8%2011.65L20%207'%20stroke='%23ffffff'%20stroke-width='1'%20stroke-linecap='round'%20stroke-linejoin='round'/%3e%3crect%20x='3'%20y='5'%20width='18'%20height='14'%20rx='2'%20stroke='%23ffffff'%20stroke-width='1'%20stroke-linecap='round'/%3e%3c/g%3e%3c/svg%3e\" alt=\"email icon\"></a></p>","contentLength":8057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43024221"},{"title":"Smuggling arbitrary data through an emoji","url":"https://paulbutler.org/2025/smuggling-arbitrary-data-through-an-emoji/","date":1739352248,"author":"paulgb","guid":173,"unread":true,"content":"<blockquote><p>With ZWJ (Zero Width Joiner) sequences you could in theory encode an unlimited amount of data in a single emoji.</p></blockquote><p>Is it really possible to encode arbitrary data in a single emoji?</p><p> yes, although I found an approach without ZWJ. In fact, you can encode data in  unicode character. This sentence has a hidden message󠅟󠅘󠄐󠅝󠅩󠄜󠄐󠅩󠅟󠅥󠄐󠅖󠅟󠅥󠅞󠅔󠄐󠅤󠅘󠅕󠄐󠅘󠅙󠅔󠅔󠅕󠅞󠄐󠅝󠅕󠅣󠅣󠅑󠅗󠅕󠄐󠅙󠅞󠄐󠅤󠅘󠅕󠄐󠅤󠅕󠅨󠅤󠄑. (Try pasting it into <a href=\"https://emoji.paulbutler.org/?mode=decode\">this decoder</a>)</p><p>Unicode represents text as a sequence of , each of which is basically just a number that the Unicode Consortium has assigned meaning to.\nUsually, a specific codepoint is written as , where  is a number represented as uppercase hexadecimal.</p><p>For simple latin-alphabet text, there is a one-to-one mapping between Unicode codepoints and characters that appear on-screen. For example,\n represents the character .</p><p>For other writing systems, some on-screen characters may be represented by multiple codepoints. The character की\n(in <a href=\"https://en.wikipedia.org/wiki/Devanagari\">Devanagari script</a>) is represented by a consecutive pairing of the codepoints  and .</p><p>Unicode designates 256 codepoints as “variation selectors”, named VS-1 to VS-256. These have no on-screen representation of their own, but are used to modify\nthe presentation of the preceeding character.</p><p>Most unicode characters do not have variations associated with them. Since unicode is an evolving standard and aims to be future-compatible,\nvariation selectors are supposed to be preserved during transformations, even if their meaning is not known by the code handling them.\nSo the codepoint  (“g”) followed by  (VS-2) renders as a lowercase “g”, exactly the same as  alone. But if you copy and paste it, the\nvariation selector will tag along with it.</p><p>Since 256 is exactly enough variations to represent a single byte, this gives us a way to “hide” one byte of data in any other unicode codepoint.</p><p>As it turns out, the <a href=\"https://www.unicode.org/versions/Unicode16.0.0/\">Unicode spec</a> does not specifically say anything about sequences\nof multiple variation selectors, except to imply that they should be ignored during rendering.</p><p><em>See where I’m going with this?</em></p><p>We can concatenate a sequence of variation selectors together to represent any arbitrary byte string.</p><p>For example, let’s say we want to encode the data <code>[0x68, 0x65, 0x6c, 0x6c, 0x6f]</code>, which represents the text “hello”. We can do this by\nconverting each byte into a corresponding variation selector, and then concatenating them together.</p><p>To convert from a byte to a variation selector, we can do something like this Rust code:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">(byte: ) -&gt;  {\n     byte  {\n        char::from_u32( byte ).unwrap()\n    }  {\n        char::from_u32( (byte ) ).unwrap()\n    }\n}\n</code></pre></div><p>To encode a series of bytes, we can concatenate a number of these variation selectors after a base character.</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">(base: , bytes: []) -&gt; String {\n     result  String::new();\n    result.push(base);\n     byte  bytes {\n        result.push(byte_to_variation_selector(byte));\n    }\n    result\n}\n</code></pre></div><p>Then to encode the bytes <code>[0x68, 0x65, 0x6c, 0x6c, 0x6f]</code>, we can run:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">() {\n    println(, encode(, [, , , , ]));\n}\n</code></pre></div><p>It just looks like a regular emoji, but try pasting it into <a href=\"https://emoji.paulbutler.org/?mode=decode\">the decoder</a>.</p><p>If we instead use the debug formatter, we see what’s going on:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">() {\n    println(, encode(, [, , , , ]));\n}\n</code></pre></div><pre><code>\"😊\\u{e0158}\\u{e0155}\\u{e015c}\\u{e015c}\\u{e015f}\"\n</code></pre><p>This reveals the characters that were “hidden”&nbsp;in the original output.</p><p>Decoding is similarly straightforward.</p><div><pre tabindex=\"0\"><code data-lang=\"rust\">(variation_selector: ) -&gt; Option {\n     variation_selector  variation_selector ;\n     (..).contains(variation_selector) {\n        Some((variation_selector ) )\n    }  (..).contains(variation_selector) {\n        Some((variation_selector ) )\n    }  {\n        None\n    }\n}\n\n(variation_selectors: ) -&gt; Vec {\n     result  Vec::new();\n    \n     variation_selector  variation_selectors.chars() {\n         Some(byte)  variation_selector_to_byte(variation_selector) {\n            result.push(byte);\n        } result.is_empty() {\n             result;\n        }\n            }\n\n    result\n}\n</code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"rust\"> std::::from_utf8;\n\n() {\n     result  encode(, [, , , , ]);\n    println(, from_utf8(decode(result)).unwrap()); }\n</code></pre></div><p>Note that the base character does not need to be an emoji – the treatment of variation selectors is the same\nwith regular characters. It’s just more fun with emoji.</p><p>To be clear, this is an abuse of unicode and you shouldn’t do it. If your mind is wandering to practical use cases for this, shut it down.</p><p>That said, I can think of a couple of nefarious ways this could be (ab)used:</p><h3>1. Sneaking data past human content filters</h3><p>Since data encoded this way are invisible once rendered, a human moderator or reviewer will not know they are there.</p><p>There are techniques for using subtle variations in text to “watermark” a message, so that if it is sent to a number of people and then\nleaked, it’s possible to trace it to the original recipient. Variation selector sequences are a way to do this that survives most copy/pastes\nand allows arbitrary data density. You could go so far as to watermark  if you wanted to.</p><h2>Addendum: can an LLM decode it?</h2><p>Since this made it on <a href=\"https://news.ycombinator.com/item?id=43023508\">Hacker News</a>, some people have asked about how LLMs\ndeal with this hidden data.</p><p>Generally, tokenizers  seem to preserve the variation selectors as tokens, so in theory the model has access to them.\n<a href=\"https://platform.openai.com/tokenizer\">OpenAI’s tokenizer</a> is a good sanity check of this:</p><p>Overall though, models don’t even seem willing to try decoding them internally. However, when paired with a code interpreter, some models are\nactually able to solve them!</p><p>Here’s an example of Gemini 2 Flash solving one in a remarkable seven seconds, using <a href=\"https://block.github.io/goose/\">Codename Goose</a> and\n<a href=\"https://forevervm.com/\">foreverVM</a> (disclaimer: I work on foreverVM).</p>","contentLength":5705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43023508"},{"title":"The year I didn't survive","url":"https://bessstillman.substack.com/p/the-year-i-didnt-survive","date":1739326072,"author":"LaurenSerino","guid":172,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43020983"},{"title":"I tasted Honda’s spicy rodent-repelling tape and I will do it again (2021)","url":"https://haterade.substack.com/p/i-tasted-hondas-spicy-rodent-repelling","date":1739286488,"author":"voxadam","guid":171,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43013615"},{"title":"Backblaze Drive Stats for 2024","url":"https://www.backblaze.com/blog/backblaze-drive-stats-for-2024/","date":1739285745,"author":"TangerineDream","guid":170,"unread":true,"content":"<p>As of December 31, 2024, we had 305,180 drives under management. Of that number, there were 4,060 boot drives and 301,120 data drives. This report will focus on those data drives as we review the Q4 2024 annualized failure rates (AFR), the 2024 failure rates, and the lifetime failure rates for the drive models in service as of the end of 2024. Along the way, we’ll share our observations and insights on the data presented, and, as always, we look forward to you doing the same in the comments section at the end of the post.</p><h2>Q4 2024 hard drive failure rates</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. For our evaluation, we removed from consideration 487 drives, as they did not meet the criteria to be included. We’ll discuss the criteria we used in the next section of this report. Removing these drives leaves us with 300,633 hard drives to analyze. The table below shows the annualized failure rates for Q4 2024 for this collection of drives.</p><ul><li>. Seagate 24TB drives (model: ST24000NM002H) arrived in early December. The 1,200 drives filled one Backblaze Vault with no failed drives through the end of Q4. The 24TB Seagate drives join the 20TB Toshiba and 22TB WDC drive models in the 20-plus capacity club as we continue to dramatically increase storage capacity while optimizing existing storage server space.</li><li><strong>Zero failures for the quarter</strong>. Five drive models had zero failures for the quarter starting with the 24TB Seagate drive model noted above. The others are the 4TB HGST (model: HMS5C4040ALE640), the 8TB Seagate (model: ST8000NM000A), the 14TB Seagate (model: ST14000NM000J), and the 16TB Seagate (model: ST16000NM002J). All of the zeroes come with the caveat of having a relatively small number of drives and drive days, but zero failures in a quarter is always a good thing.</li><li><strong>The 4TB drives are nearly extinct</strong>. The 4TB drive count decreased by another 1,774 drives in Q4. (I discussed exactly <a href=\"https://www.backblaze.com/blog/how-backblaze-scales-our-storage-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\">how we migrate them</a> in more detail if you want to dig in.) The remaining ~4,000 drives should be gone by the end of Q1 2025. They will be replaced by the incoming 20TB, 22TB, and 24TB drives. It should be noted that out of the 4TB drives in operation in Q4, only one failed, so those 20-plus TB drives have a lot to live up to from a failure perspective.</li><li><strong>The quarterly failure rate is down.</strong> The AFR for Q4 dropped from 1.89% in Q3 to 1.35% in Q4. While all drive sizes delivered some improvement from Q3 to Q4, one of the primary drivers is the addition of over 14,000 new 20-plus TB drives. As a group, these drives delivered an AFR of 0.77% for the quarter.</li></ul><p>We noted earlier we removed 487 drives from consideration when we produced the table above covering Q4 2024. There are two primary reasons we did not consider these drive models.</p><ul><li>. These are drives of a given model that we monitor and collect Drive Stats data on, but are not considered production drives at this time. For example, drives undergoing certification testing to determine if they are performant enough for our environment are not included in our Drive Stats calculations.</li><li><strong>Insufficient data points. </strong>When we calculate the annualized failure rate for a drive model for a given period of time (quarterly, annual, or lifetime), we want to ensure we have enough data to reliably do so. Therefore we have defined criteria for a drive model to be included in the tables and charts for the specified period of time. Models that do not meet these criteria are not included in the tables and charts for the period in question.</li></ul><table><thead><tr></tr></thead><tbody></tbody></table><p>Regardless of whether or not a given drive model is included in the charts and tables, all of the data for all of the drives we use is included in our Drive Stats dataset which you can download by visiting our <a href=\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data\" target=\"_blank\" rel=\"noreferrer noopener\">Drive Stats page</a>.</p><p>As with the Q4 quarterly results, we will apply these criteria to the annual and lifetime charts that follow in this report.</p><h2>2024 annual hard drive failure rates</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. We removed nine drive models consisting of 2,012 drives from consideration as they did not meet the annual criteria we have defined. This leaves us with 298,954 drives divided across 27 different drive models. The table below shows the AFRs for 2024 for this collection of drives.</p><ul><li>. There were no qualifying drive models with zero failures in 2024. That said, the 16TB Seagate (model: ST16000NM002J) got close by recording just one drive failure back in Q3, giving the drive an AFR of 0.22% for 2024.&nbsp;</li><li>. During 2024, our data center techs installed 53,337 drives. If we assume there are 2,080 work hours a year (52 weeks times 40 hours), that math is , and that means our intrepid DC techs installed 26 drives per hour. Busy, busy, busy!&nbsp;</li><li>While there were 1,200 new 24TB Seagate drives added in 2024, they were installed in early December and did not accumulate enough drive days to make the cut for the annual, or lifetime, tables. Including the 24TB Seagate drive, there were three models that missed out on being included in the 2024 annual tables, these drive models are listed below.</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>As a reminder, a drive model needs to have over 250 drives by the end of Q4 and accumulate at least 50,000 drive days during 2024 to be included in the annual tables.</p><h2>Comparing Drive Stats for 2022, 2023, and 2024</h2><p>The table below compares the annual failure rates by drive model for each of the last three years. The table includes just those drive models which met the annual criteria as of the end of 2024. The data for each year is inclusive of that year only for the operational drive models present at the end of each year. The table is sorted by drive size and then AFR.</p><ul><li> The 2024 AFR for all drives listed was 1.57%, this is down from 1.70% in 2023.&nbsp; We expect the overall failure rates to continue to fall in 2025, but we will be watching the following for indicators.\n<ul><li><strong>The failure rates of the 8TB and 12TB drive models. </strong>All of the models will exceed their five years of service. In general, the failure rate will noticeably increase as the drives exceed five years of service. And, while there are outliers like the current HGST 4TB drives, you can’t assume that will happen.</li><li><strong>The failure rates of the 14TB and 16TB drive models.</strong> These models are approaching middle age—three to five years in operation. This is where, according to <a href=\"https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/\" target=\"_blank\" rel=\"noreferrer noopener\">the bathtub curve</a>, their failure rates could gradually increase—but not as severely as when they exceed five years.&nbsp;</li><li><strong>The failure rates for the 20TB, 22TB, and 24TB drives models.</strong> These drives will enter the flat portion of the bathtub curve, that is where their failure rate should be the lowest.</li></ul></li></ul><h2>Annualized failure rates vs. drive size</h2><p>Now, we can dig into the numbers to see what else we can learn. We’ll start by looking at the quarterly annualized failure rate by drive size over the last three years.</p><p>Let’s take a look at the different drive sizes and how they affect the overall annualized failure rate over time.</p><p>. The 4TB (blue line) drives and 10TB (gold line) drives have had little impact over the last year on the overall failure rate as each finished the year with a relatively small number of drives. Still, the wild ride delivered by the 10TB drives keeps our DC techs on their toes.&nbsp;</p><p>. The 8TB (gray line) drives and 12TB (purple line) drives range in age from five to eight years and as such their overall failure rates should be increasing over time. The 12TB drives are following that pattern moving up from about 1% AFR back in 2021 to just about 3% in 2024. The failure rates of the 8TB drives, while erratic from quarter-to-quarter, have a nearly flat trendline over the same period.</p><p>. The 14TB (green line) and 16TB (azure* line) drives comprise 57% of the drives in service and on average they range in age from two to four years. They are in the prime of their working lives. As such, they should have low and stable failure rates, and as you can see, they do.</p><p>*&nbsp; Maybe azure isn’t quite right, but robin’s egg blue seemed a bit pretentious.</p><p>. The 22TB (orange line) drives are in their early days as we continue to add more drives on a regular basis. Once the drive population settles down, we’ll have a better sense of the AFR direction. Still, the early results are solid with a lifetime AFR of 1.06%.</p><h2>Annualized failure rates vs. manufacturer</h2><p>One of the more popular ways we can look at this data is by the drive manufacturer as we’ve done below.</p><p>To complete the picture, the chart below uses the same data, but displays just the linear trendlines for each of the manufacturers over the same three-year period.</p><p>. While the HGST trendline is not pretty, it doesn’t tell the entire story. Looking at the first chart, until Q4 2023, the HGST drives were at or below the average for all of the drives, that is all manufacturers. At that point, HGST has exceeded the average, and then some. The table below contains results for just the HGST drives for 2024. We’ve sorted them, high to low, by the 2024 AFR.</p><p>As you can see, there are two 12TB drive models driving the high AFR for the HGST drives. The HUH721212ALN604 model began showing signs of an increased quarterly AFR in Q1 2023 and the HUH721212ALE604 model followed suit in Q3 2024. Without these drive models, the 2024 AFR for HGST drive would be 0.55%.</p><p>. The quarterly AFR trendline decreased for the Seagate drives from 2022 through 2024. While the decrease was slight, from 2.25% to 2.0%, Seagate was the only manufacturer to do so. The decrease appears, at least in part, to be due to the removal of the Seagate 4TB drives during that period.&nbsp;</p><p>. Over the 2022 to 2024 period, the quarterly AFR for the Toshiba drive models varied within a fairly narrow range between 0.80% and 1.52%, with most quarters hovering slightly around 1.2%. Most importantly, none of the individual drive models were outliers, as the highest quarterly AFR for any Toshiba drive model was 1.58%. We like consistency.&nbsp;</p><p>. While WDC drive models delivered a similar level of consistency as the Toshiba models, they did so with a lower AFR each quarter. From 2022 through 2024, the range of quarterly AFR values for the WDC models was 0.0% to 0.85%. The 0.0% AFR was in Q1 2022 when none of the 12,207 WDC drives in operation failed during that quarter.</p><h2>Lifetime hard drive stats</h2><p>As of the end of 2024, Backblaze was monitoring 301,120 hard drives used to store data. Applying our drive criteria noted above for the lifetime period, we removed 11 drive models consisting of 2,736 drives from consideration as they did not meet the lifetime criteria we defined. This leaves us with 298,230 drives divided across 25 different drive models. The table below shows the lifetime AFRs for this collection of drives.</p><p>The current lifetime AFR for all of the drives is 1.31%. This is down from 1.46% in 2023. The drop is primarily due to the completion of the migration of the 4TB Seagate drives in 2024, which left us with only two of these drives still in operation as of the end of 2024. As a consequence, the 79 million drive days and over 5,600 drive failures racked up by the 4TB Seagate drives by the end of 2023 are not included in the data presented in the 2024 lifetime table above.&nbsp;&nbsp;</p><p>In the final table below, we’ve taken the lifetime table and sorted out the drive models that have a lifetime AFR of 1.50% or less by drive size.</p><p>A couple of caveats as you review the table.</p><ul><li>There is enough data for each model to say the AFR values are solid. That said, everything could change tomorrow. In general, the hard drive failure rate follows the bathtub curve as the drives age—unless it doesn’t. Some drives refuse to fail as they age, like the 4TB HGST drives. Other drives are great, and then “hit the wall” and bend the failure curve upward, fast.</li><li>A drive model with a 1% annualized failure rate means that you can expect one drive out of 100 to fail in a year. If you’re a personal drive user, that one drive could be yours. If you have exactly one drive, your personal annualized failure rate is 100%. In other words, always have a backup, and don’t forget to test it.</li></ul><p>I have been authoring the various Drive Stats reports for the past ten years and this will be my last one. I am retiring, or perhaps in Drive Stats vernacular, it would be “migrating.” Either way, after 10 years in the U.S. Air Force and 30+ years in Silicon Valley Tech, it is time. Drive Stats will continue with <a href=\"https://www.backblaze.com/blog/author/stephanie/\" target=\"_blank\" rel=\"noreferrer noopener\">Stephanie Doyle</a> and <a href=\"https://www.backblaze.com/blog/author/davidjohnson/\" target=\"_blank\" rel=\"noreferrer noopener\">David Johnson</a> as the replacement drive models beginning with the Q1 2025 report. I wish them well.</p><p>I want to say thank you to each of you who have taken your time to peruse and engage with the Drive Stats reports and data over the last 10 years. And, thank you as well for the comments, questions, and discussions that raced and raged across the various communities that care about something as mundane and awesome as a hard drive. It has been quite the ride—thanks again.</p><h2>The Hard Drive Stats data</h2><p>The complete data set used to create the tables and charts in this report is available on our&nbsp;<a href=\"https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/\" target=\"_blank\" rel=\"noreferrer noopener\">Hard Drive Test Data</a>&nbsp;page. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data itself to anyone; it is free.</p><p>Good luck, and let us know if you find anything interesting.</p>","contentLength":13388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43013431"},{"title":"Boring tech is mature, not old","url":"https://rubenerd.com/boring-tech-is-mature-not-old/","date":1739282902,"author":"mikece","guid":169,"unread":true,"content":"<p>I’ve talked before about how I think NetBSD is “boring”, and that it’s among the highest forms of praise I can give tech as a sysadmin and architect. But I’ve never elaborated why that is.</p><p>The opposite of being bored is to be surprised, and that’s not something a sysadmin desires when building, maintaining, scaling, troubleshooting, upgrading, or even replacing a complex system… especially when you’re woken up at 03:30 by a monitoring server. That’s also why the phrase <em>work doing what you love and you never work a day again in your life</em> is an empty platitude of nonsense. But I digress.</p><p>Boring tech behaves in predictable ways. It’s a well trodden path others have evaluated, optimised, troubleshooted, and understood. Using tech that has been subjected to all those people hours of use means you’re less likely to run into edge cases, unexpected behaviour, or attributes and features that lack documentation or community knowledge. In other words, when something goes wrong, can you turn to someone or something?</p><p>Likewise, tech (generally) doesn’t exist in a vacuum. It interacts with other components and systems, some of which are even conveniently under our control, sometimes. Multiply out the potential for surprises by the number of components and their relative maturity, and your head can start to spin.</p><p>This isn’t to say there isn’t room for innovation, or that staying put is a guaranteed recipe for success. What it does teach is that it pays to make informed decisions, and that often times the understood, reliable, boring tech will get you there over something new, shiny or propped up with marketing spin. The number of people I’ve talked with who’ve replaced complicated K8s clusters with a few VMs and seen massive improvements in reliability, cost, and uptime would make some people at the Orange Peanut Gallery more than a little perturbed, for example.</p><blockquote><p>I used to agree with this. Now I don’t think so any more. You should prefer “boring” tech, and boring should be read as has been around for a while and therefore is well understood.</p><p>Ubiquity is a bad test for well-understood technology. Age as a test for ubiquity is also bad.</p></blockquote><p>There are a few assumptions here:</p><ul><li><p>Boring should be read as something being around for a while. I wouldn’t necessarily agree with that. I’ve been a DBA, and I never in a million years would call Oracle “boring”. It’s fiendishly complicated and difficult to maintain, and commands above-average salaries in part for that reason. A sign saying “there be Ellison dragons” isn’t boring, it’s frankly terrifying.</p></li><li><p>Ubiquity is a test for “boringness”. He’s right here; while age gives something an opportunity to become ubiquitous, it’s not a guarantee. I’d look to the BSDs here; I’d consider them boring, but they’re not exactly widely deployed compared to Penguins.</p></li><li><p>Boring should be read as well-understood. That’s true. Time gives something more of a chance of being well-understood, until suddenly it doesn’t and nobody is around with sufficient knowledge and inclination to maintain your COBOL stack. At that stage, I’d say you have a decidedly  issue on your hands.</p></li></ul><p>I’d conclude by suggesting boring tech isn’t old, but . Maturity not just in the software, but its documentation, community, and track record.  is often used as an analogue for maturity, but it’s not the same thing. Otherwise I’d be more mature than my Zoomer friends, and I very much doubt that to be the case. .</p><p>This is why I don’t hesitate to call NetBSD boring, and why I say that’s a compliment.</p>","contentLength":3598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43012862"},{"title":"Firing programmers for AI is a mistake","url":"https://defragzone.substack.com/p/techs-dumbest-mistake-why-firing","date":1739266962,"author":"frag","guid":168,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43010814"},{"title":"Musk-led group makes $97B bid for control of OpenAI","url":"https://www.reuters.com/markets/deals/elon-musk-led-group-makes-974-billion-bid-control-openai-wsj-reports-2025-02-10/","date":1739220138,"author":"jdoliner","guid":167,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43004889"},{"title":"Undergraduate shows that searches within hash tables can be much faster","url":"https://www.quantamagazine.org/undergraduate-upends-a-40-year-old-data-science-conjecture-20250210/","date":1739207109,"author":"Jhsto","guid":166,"unread":true,"content":"<p>Together, Krapivin (now a graduate student at the University of Cambridge), Farach-Colton (now at New York University) and Kuszmaul demonstrated in a <a href=\"https://arxiv.org/abs/2501.02305\">January 2025 paper</a> that this new hash table can indeed find elements faster than was considered possible. ln so doing, they had disproved a conjecture long held to be true.</p><p>“It’s an important paper,” said <a href=\"https://ajhconway.com/\">Alex Conway</a> of Cornell Tech in New York City. “Hash tables are among the oldest data structures we have. And they’re still one of the most efficient ways to store data.” Yet open questions remain about how they work, he said. “This paper answers a couple of them in surprising ways.”</p><p>Hash tables have become ubiquitous in computing, partly because of their simplicity and ease of use. They’re designed to allow users to do exactly three things: “query” (search for) an element, delete an element, or insert one into an empty slot. The first hash tables date back to the early 1950s, and computer scientists have studied and used them ever since. Among other things, researchers wanted to figure out the speed limits for some of these operations. How fast, for example, could a new search or insertion possibly be?</p><p>The answer generally depends on the amount of time it takes to find an empty spot in a hash table. This, in turn, typically depends on how full the hash table is. Fullness can be described in terms of an overall percentage — this table is 50% full, that one’s 90% — but researchers often deal with much fuller tables. So instead, they may use a whole number, denoted by , to specify how close the hash table is to 100% full. If  is 100, then the table is 99% full. If  is 1,000, the table is 99.9% full. This measure of fullness offers a convenient way to evaluate how long it should take to perform actions like queries or insertions.</p><p>Researchers have long known that for certain common hash tables, the expected time required to make the worst possible insertion — putting an item into, say, the last remaining open spot — is proportional to . “If your hash table is 99% full,” Kuszmaul said, “it makes sense that you would have to look at around 100 different positions to find a free slot.”</p><p>In a <a href=\"https://dl.acm.org/doi/10.1145/3828.3836\">1985 paper</a>, the computer scientist <a href=\"https://amturing.acm.org/award_winners/yao_1611524.cfm\">Andrew Yao</a>, who would go on to win the A.M. Turing Award, asserted that among hash tables with a specific set of properties, the best way to find an individual element or an empty spot is to just go through potential spots randomly — an approach known as uniform probing. He also stated that, in the worst-case scenario, where you’re searching for the last remaining open spot, you can never do better than . for 40 years, most computer scientists assumed that Yao’s conjecture was true.</p><p>Krapivin was not held back by the conventional wisdom for the simple reason that he was unaware of it. “I did this without knowing about Yao’s conjecture,” &nbsp;he said. His explorations with tiny pointers led to a new kind of hash table — one that did not rely on uniform probing. And for this new hash table, the time required for worst-case queries and insertions is proportional to (log ) — far faster than . This result directly contradicted Yao’s conjecture. Farach-Colton and Kuszmaul helped Krapivin show that (log ) is the optimal, unbeatable bound for the popular class of hash tables Yao had written about.</p><p>“This result is beautiful in that it addresses and solves such a classic problem,” said <a href=\"http://www.cs.cmu.edu/~guyb/\">Guy Blelloch</a> of Carnegie Mellon.</p><p>“It’s not just that they disproved [Yao’s conjecture], they also found the best possible answer to his question,” said <a href=\"https://cs.uwaterloo.ca/about/people/sassadi\">Sepehr Assadi</a> of the University of Waterloo. &nbsp;“We could have gone another 40 years before we knew the right answer.”</p>","contentLength":3730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43002511"},{"title":"The Anthropic Economic Index","url":"https://www.anthropic.com/news/the-anthropic-economic-index","date":1739196862,"author":"meetpateltech","guid":165,"unread":true,"content":"<p>In the coming years, AI systems will have a major impact on the ways people work. For that reason, we're launching the <a href=\"https://www.anthropic.com/economic-index\">Anthropic Economic Index</a>, an initiative aimed at understanding AI's effects on labor markets and the economy over time.</p><p>The Index’s <a href=\"https://assets.anthropic.com/m/2e23255f1e84ca97/original/Economic_Tasks_AI_Paper.pdf\">initial report</a> provides first-of-its-kind data and analysis based on millions of anonymized conversations on <a href=\"http://claude.ai\">Claude.ai</a>, revealing the clearest picture yet of how AI is being incorporated into real-world tasks across the modern economy.</p><p>We're also <a href=\"https://huggingface.co/datasets/Anthropic/EconomicIndex/\">open sourcing the dataset</a>used for this analysis, so researchers can build on and extend our findings. Developing policy responses to address the coming transformation in the labor market and its effects on employment and productivity will take a range of perspectives. To that end, we are also inviting economists, policy experts, and other researchers to <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfDEdY-mT5lcXPaDSv-0Ci1rSXGlbIJierxkUbNB7_07-kddw/viewform?usp=dialog\">provide input</a> on the Index.</p><p>The main findings from the Economic Index’s first paper are:</p><ul><li>Today, usage is concentrated in software development and technical writing tasks. Over one-third of occupations (roughly 36%) see AI use in at least a quarter of their associated tasks, while approximately 4% of occupations use it across three-quarters of their associated tasks.</li><li>AI use leans more toward augmentation (57%), where AI collaborates with and enhances human capabilities, compared to automation (43%), where AI directly performs tasks.</li><li>AI use is more prevalent for tasks associated with mid-to-high wage occupations like computer programmers and data scientists, but is lower for both the lowest- and highest-paid roles. This likely reflects both the limits of current AI capabilities, as well as practical barriers to using the technology.</li></ul><p>See below for further details on our initial findings.</p><h2>Mapping AI usage across the labor market</h2><p>Our <a href=\"https://assets.anthropic.com/m/2e23255f1e84ca97/original/Economic_Tasks_AI_Paper.pdf\">new paper</a> builds on a long line of research on the labor market impact of technologies, from the Spinning Jenny of the Industrial Revolution to the car-manufacturing robots of the present day. We focus on the ongoing impact of AI. We don’t survey people on their AI use, or attempt to forecast the future; instead, we have direct data on how AI is actually being used.</p><h3><strong>Analyzing occupational tasks</strong></h3><p>Our research began with an important <a href=\"https://academic.oup.com/qje/article-abstract/118/4/1279/1925105\">insight from the economics literature</a>: sometimes it makes sense to focus onrather than. Jobs often share certain tasks and skills in common: for example, visual pattern recognition is a task performed by designers, photographers, security screeners, and radiologists.</p><p>Certain tasks lend themselves better to being automated or augmented by a new technology than others. We’d therefore expect AI to be adopted selectively for different tasks across different occupations, and that analyzing tasks—in addition to jobs as a whole—would give us a fuller picture of how AI is being integrated into the economy.</p><h3><strong>Using Clio to match AI use to tasks</strong></h3><p>This research was made possible by Claude insights and observations, or\"<a href=\"https://www.anthropic.com/research/clio\">Clio</a>\", an automated analysis tool that allows us to analyze conversations with Claude while preserving user privacy. We used Clio on a dataset of approximately one million conversations with Claude (specifically, Free and Pro conversations on <a href=\"http://claude.ai\">Claude.ai</a>), and used it to organize the conversations by occupational task.</p><p>We chose tasks according to the classification made by the U.S. Department of Labor, which maintains a database of around 20,000 specific work-related tasks called the Occupational Information Network, or <a href=\"https://www.onetonline.org/\">O*NET</a>. Clio matched each conversation with the O*NET task that best represented the role of the AI in the conversation (the process is summarized in the figure below). We then followed the O*NET scheme for grouping the tasks into the occupations they best represented, and the occupations into a small set of overall categories:  and so on.</p><p> The tasks and occupations with by far the largest adoption of AI in our dataset were those in the “computer and mathematical” category, which in large part covers software engineering roles. 37.2% of queries sent to Claude were in this category, covering tasks like software modification, code debugging, and network troubleshooting.</p><p>The second largest category was “arts, design, sports, entertainment, and media” (10.3% of queries), which mainly reflected people using Claude for various kinds of writing and editing. Unsurprisingly, occupations involving a high degree of physical labor, such as those in the “farming, fishing, and forestry” category (0.1% of queries), were least represented.</p><p>We also compared the rates in our data to the rates at which each occupation appeared in the labor market in general. The comparisons are shown in the figure below.</p><p><strong>Depth of AI use within occupations.</strong> Our analysis found that very few occupations see AI use across most of their associated tasks: only approximately 4% of jobs used AI for at least 75% of tasks. However, more moderate use of AI is much more widespread: roughly 36% of jobs had some use of AI for at least 25% of their tasks.</p><p>As we predicted, there wasn’t evidence in this dataset of jobs being entirely automated: instead, AI was diffused across the many tasks in the economy, having stronger impacts for some groups of tasks than others.</p><p> The O*NET database provides the median U.S. salary for each of the occupations listed. We added this information to our analysis, allowing us to compare professions’ median salaries and the level of AI use in their corresponding tasks.</p><p>Interestingly, both low-paying and very-high-paying jobs had very low rates of AI use (these were generally jobs involving a large degree of manual dexterity, such as shampooers and obstetricians). It was specific occupations in the mid-to-high median salary ranges, like computer programmers and copywriters, who were—in our data—among the heaviest users of AI.</p><p><strong>Automation versus augmentation.</strong> We also looked in more detail at  the tasks were being performed—specifically, at which tasks involved “automation” (where AI directly performs tasks such as formatting a document) versus “augmentation” (where AI collaborates with a user to perform a task).</p><p>Overall, we saw a slight lean towards augmentation, with 57% of tasks being augmented and 43% of tasks being automated. That is, in just over half of cases, AI was not being used to replace people doing tasks, but instead worked  them, engaging in tasks like validation (e.g., double-checking the user’s work), learning (e.g., helping the user acquire new knowledge and skills), and task iteration (e.g., helping the user brainstorm or otherwise doing repeated, generative tasks).</p><p>Our study provides a unique glimpse into how AI is changing the labor market. But as with all studies it has important limitations. Some of these include:</p><ul><li>We can’t know for certain whether someone using Claude for a task was completing a task for work. Someone asking Claude for writing or editing advice  be doing so at work, but they could also be doing so for the novel they’re writing as a hobby.</li><li>Relatedly, we don’t know  the users were using the responses from Claude. Were they, for instance, copy-pasting code snippets? Were they fact-checking responses or accepting them uncritically? Some of what appears in our data to be automation could, in fact, be augmentation: for example, a user might ask Claude to write a full memo for them (which would appear as automation), but then edit it themselves afterwards (which would be augmentation).</li><li>We also only analyze data from <a href=\"http://claude.ai\">Claude.ai</a> Free and Pro plans, rather than API, Team, or Enterprise users. While Claude.ai data contains some non-work conversations, we used a language model to filter this data to only contain conversations relevant to an occupational task, which helps to mitigate this concern.</li><li>The sheer number of different tasks means it is possible that Clio classified some conversations incorrectly (please see the full paper, in particular Appendix B, for details on how we validated the analysis);</li><li>Claude can’t generate images (except indirectly via code), and so some creative uses won’t be referenced in the data;</li><li>Given that Claude is advertised for use as a state-of-the-art coding model, we might expect coding to be overrepresented as a use case. For that reason, we don’t argue that the uses in our dataset are a representative sample of AI use in general.</li></ul><h2>Conclusions and future research</h2><p>AI use is rapidly expanding, and models are becoming ever-more capable. The labor-market picture may look quite different within a relatively short time. For that reason, we’ll repeat many of the analyses above over time to help track the societal and economic changes that are likely to occur. We’ll regularly release the results and the associated datasets as part of the Anthropic Economic Index.</p><p>These kinds of longitudinal analyses can give us new insights into AI and the job market. For example, we’ll be able to monitor changes in the depth of AI use within occupations. If it remains the case that AI is used only for certain tasks, and only a few jobs use AI for the vast majority of their tasks, the future might be one where most current jobs evolve rather than disappear. We can also monitor the ratio of automation to augmentation, providing signals of areas where automation is becoming more prevalent.</p><p>Our research gives data on how AI is being used, but it doesn’t provide policy prescriptions. Answers to questions about how to prepare for AI’s impact on the labor market can’t come directly from research in isolation; instead, they’ll come from a combination of evidence, values, and experience from broad perspectives. We look forward to using our new methodology to shed more light on these issues.</p><p>Read <a href=\"https://assets.anthropic.com/m/2e23255f1e84ca97/original/Economic_Tasks_AI_Paper.pdf\">the full paper</a> for more details of our analyses and results.</p><h2>Open data and call for input</h2><p>The most important contribution of this paper, and of the Anthropic Economic Index, is its new methodology providing detailed data on the impacts of AI. We’re immediately openly sharing the dataset we used for the above analyses, and we plan to share further such datasets as they become available in the future.</p><p>The full dataset can be downloaded <a href=\"https://huggingface.co/datasets/Anthropic/EconomicIndex\">here</a>.</p><p>A form for researchers to provide feedback on our data and suggest new research directions is <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfDEdY-mT5lcXPaDSv-0Ci1rSXGlbIJierxkUbNB7_07-kddw/viewform?usp=dialog\">here</a>.</p><p>We appreciate the productive comments and discussion on early findings and drafts of the paper from Jonathon Hazell, Anders Humlum, Molly Kinder, Anton Korinek, Benjamin Krause, Michael Kremer, John List, Ethan Mollick, Lilach Mollick, Arjun Ramani, Will Rinehart, Robert Seamans, Michael Webb, and Chenzi Xu.</p><p>If you’re interested in working at Anthropic to research the effects of AI on the labor market, we encourage you to apply for our Societal Impacts <a href=\"https://boards.greenhouse.io/anthropic/jobs/4524032008\">Research Scientist</a> and <a href=\"https://boards.greenhouse.io/anthropic/jobs/4251453008\">Research Engineer</a> roles.</p>","contentLength":10759,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000529"},{"title":"Surnames from nicknames nobody has any more","url":"https://blog.plover.com/lang/etym/nickname-names.html","date":1739195678,"author":"JNRowe","guid":164,"unread":true,"content":"<p>English has a pattern of common patronymic names.  For example, \"John\nPeters\" and \"John Peterson\" are someone whose father was named\n\"Peter\".  (\"Peters\" should be understood as \"Peter's\".)  Similarly we\nhave John Williams and John Williamson, John Roberts and John\nRobertson, John Richards and John Richardson, John James and John\nJameson, John Johns and John Johnson, and so on.</p><p>Often Dad's name was a nickname.  For example, a common nickname for\n\"John\" is \"Jack\" and we have (less commonly) John Jacks and (more\ncommonly) John Jackson. John Bills and John Bilson, John Wills and\nJohn Wilson, and John Willis and John Willison are Bill, Will, and\nWille, all short for William.</p><p>\"Richard\" is \"Dick\", and we have John Dicks (or Dix) and John Dickson\n(or Dixon).  \"Nicholas\" is \"Nick\" and we have John Nicks (or Nix) and John\nNickson (or Nixon).</p><p>Sometimes the name has the diminutive suffix “-kin” inserted.  Wilkins\nis little Will's son, as is Wilkinson; Peterkins is little Peter's\nson.</p><p>These patterns are so common that if you find surnames that follow\nthem you can almost always infer a forename, although it may be one\nthat is no longer common, or that is spelled differently.  For\nexample, many people are named Pierce, Pearse, Pierson, or Pearson,\nwhich is from the name Pierre, Piers or Pierce, still used in English\nalthough much less common than in the past.  (It is from the same root\nas Peter.)  Perkins is little Pierre.  \nRobin used to be a nickname for\nRobert (it's “Robkin” with the difficult “-bk-” simplified to just\n“-b-”) and we have John Robins and John Robinson. </p><p>Sometimes, the pattern is there but the name is unclear because it is\na nickname that is now so uncommon that it is neatly forgotten.  The\nfathers of John Watts, Watson, and Watkins were called Wat, which used\nto be short for Walter.  John Hobbs, John Hobson, and Hobkins are\nnamed for Hob, which was short for Robert in the same way that Rob and\nBob are still.  (I had a neighbor who was called Hob, and told me his\nfamily claimed that it was short for Robert, but that he wasn't sure.\nI assured him that they were correct.)  “Daw”, an archaic nickname for\n“David”, gives us Dawes, Dawkins, and Dawson.</p><p>Back in September when I started this article I thought on John Gibbs\nand John Gibson.  Who's named \"Gib\", and why?  Is it archaic nickname?\nYes!  It was short for Gilbert.  Then I forgot about the draft article\nuntil today when I woke up wondering about John Simpson (and, I\nrealize now, John Simms and John Simkins). And it transpired \"Sim\" or\n\"Simme\" was once a common nickname for Simon.</p><p>I would welcome further examples.</p><p>Vicki Rosenzweig reminds me that another lost nickname for Richard is\n\"Hick\" (like \"Rick\" and \"Dick\") and from this we get Hicks, Hix,\nHickson, Hixon, Hickman, and (she informs me) \"Hickmott\".</p><p>\"-mott\" perhaps indicates a male in-law, according to the Dictionary\nof American Family Names via\n<a href=\"https://www.ancestry.com/name-origin?surname=hickmott\">Ancestrry.com</a>.\nThe only other clear example of this that I noticed was \"Willmott\".\nNote that Gaelic names like Dermott are not of this type; the\nresemblance is coincidental.</p><p>We get Hodge / Hodges / Hodgson / Hodgkins from \"Hodge\", an arcahic\nnickname for Roger.</p><p>Lucy Keer suggests that Bateson / Bates is from \"Bat\", an archair\nnickname for Bartholomew.</p><p>Christopher Taylor reminds me that in Atkins and Atkinson the \"At-\" is\nshort for \"Adam\", and   similarly in \"Addis\" and \"Addison\".  I know a\nguy named Atteson but I'm not sure it is the same thing.</p><p>Response to this article has been too much for me to keep up with.</p><ul><li>\"Dob\" is another old nickname for \"Robert\", giving us \"Dobson\";</li><li>\"Dodge\" another nickname for Roger, hence Dodgson, as in Louis Carrol's real name, Charles Dodgson;</li><li>\"Tibb\" is an old nickname for Theobald, giving surnames like \"Tibbs\" and \"Tibbets\";</li><li>\"Hud\" for \"Hugh\", giving us the Hudsons.</li></ul><p>I also asked Claude for suggestions:</p><blockquote><p>I'm going to paste a blog article that discusses common English\n  surnames that are patronymic versions of forenames that no longer\n  exist.  For example, the common surname \"Watson\" is from \"Wat\", and\n  \"Wat\" was once a common nickname for \"Walter\", but no longer used.\n  I would like you to list ten similar examples that are not mentioned\n  in the article.</p></blockquote><blockquote><p>Here are ten examples of common English surnames derived from obsolete\n  forenames or nicknames, different from those mentioned in the article:</p><ol><li>Higgins/Higgs - from \"Hick,\" a medieval pet form of Richard</li><li>Timms/Timson - from \"Tim,\" originally a medieval nickname for Timothy</li><li>Hodgson/Hodges - from \"Hodge,\" once a common nickname for Roger</li><li>Atkins/Atkinson - from \"At\" or \"Atte,\" medieval diminutive of Adam</li><li>Parkins/Parkinson - from \"Perkin,\" diminutive of Peter/Pierre</li><li>Judson/Judd - from \"Judd,\" medieval nickname for Jordan</li><li>Malkins/Malkinson - from \"Malk,\" diminutive of Malcolm</li><li>Larkins/Larkin - from \"Larry\" or \"Laurence,\" medieval form of Lawrence</li><li>Collins/Colson - from \"Col,\" medieval nickname for Nicholas</li><li>Huggins/Huggett - from \"Hug,\" medieval pet form of Hugh</li></ol></blockquote><p>Remember that some of these might be made up.  For example,\n<a href=\"https://en.wiktionary.org/wiki/Malkin#English\">Wiktionary claims that #7 is </a>,\nfrom , a nickname for Mary.  Who's right?  I don't know.  The\nonly way to settle this is with a cage match!  I hope.</p><p>I complained that #2 was not to spec because “Tim” is still common,\nand demanded a replacement:</p><blockquote><ol><li>Rankins/Rankin - from \"Ran,\" a medieval diminutive of Randolph</li></ol></blockquote><p>I also asked Claude if I had made any errors, and it said:</p><blockquote><p>The article claims that \"Robin\" comes from \"Robkin\" with \"bk\"\n  simplified to \"b\". This is incorrect.</p><p>\"Robin\" is actually a French\n  diminutive of Robert, coming from Old French Robin/Rovin.&nbsp;…</p></blockquote>","contentLength":5588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43000316"},{"title":"Why blog if nobody reads it?","url":"https://andysblog.uk/why-blog-if-nobody-reads-it/","date":1739123410,"author":"alexgiann","guid":163,"unread":true,"content":"<p>Back in the day, advertising legend Bill Bernbach said, “The most powerful element in advertising is the truth.”</p><p>Let’s tell the truth, then: Nobody reads your blog.</p><p>At least, not as many as you’d like. Maybe a handful, maybe none. You pour your thoughts into it, craft each sentence, pick the right image — then silence. No likes, no shares, no engagement.</p><p>There’s two lies we tell ourselves:</p><ol><li><em>If I write it, they will come.</em> They won’t. There are billions of blog posts out there. The internet is an infinite void, and your blog is a whisper in a hurricane.</li><li>If nobody reads it, it’s a waste of time.</li></ol><p>There is a hidden value in blogging.\nThere’s an old Zen saying: “Chop wood, carry water.” You do it not for the applause but because it needs doing.</p><p>Blogging forces clarity. It makes you structure your thoughts, sharpen your perspective. You stop writing fluff because — let’s be honest — you’re writing for yourself. And if you can’t keep yourself interested, nobody else stands a chance.</p><p>When you write, you think better. When you think better, you create better.</p><p>So, who’s there real audience?</p><p>You’re not just writing for today’s invisible audience. You’re writing for:</p><ul><li>Future you. Your posts become a time capsule of your evolving mind.</li><li>One right person. Maybe one day, someone stumbles across your words at exactly the right moment. And that changes something for them.</li><li>The work itself. Consistency beats virality. A hundred posts with depth will outlast a single viral hit.</li></ul><p>And there’s another thing. My other passion is street photography. Which is a bit like blogging.</p><p>You walk through the city, camera in hand. You see a scene — a moment of light, shadow, humanity. You capture it.</p><p>But that’s not why you did it. You did it because you saw something.</p><p>Like blogging. You write because you think, because you observe, because you need to put it somewhere.</p><p>If someone reads it? Bonus. If not? The work still got done.</p><p>And that’s the real point.</p>","contentLength":1978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42992159"},{"title":"Modern-Day Oracles or Bullshit Machines? How to thrive in a ChatGPT world","url":"https://thebullshitmachines.com/","date":1739089457,"author":"ctbergstrom","guid":162,"unread":true,"content":"<p>In a series of five- to ten-minute lessons, we will explain what these machines are, how they work, and how to thrive in a world where they are everywhere.</p><p>You will learn when these systems can save you a lot of time and effort. You will learn when they are likely to steer you wrong. And you will discover how to see through the hype to tell the difference. </p>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42989320"},{"title":"Jacksonpollock.org (2003)","url":"https://jacksonpollock.org/","date":1739049769,"author":"memalign","guid":161,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42986320"},{"title":"We are destroying software","url":"https://antirez.com/news/145","date":1739026105,"author":"antirez","guid":160,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42983275"},{"title":"Teen on Musk's DOGE team graduated from 'The Com'","url":"https://krebsonsecurity.com/2025/02/teen-on-musks-doge-team-graduated-from-the-com/","date":1739007264,"author":"mmsc","guid":159,"unread":true,"content":"<p> reported this week that a 19-year-old working for ‘s so-called <strong>Department of Government Efficiency</strong> (DOGE) was given access to sensitive US government systems even though his past association with cybercrime communities should have precluded him from gaining the necessary security clearances to do so. As today’s story explores, the DOGE teen is a former denizen of ‘,’ an archipelago of Discord and Telegram chat channels that function as a kind of distributed cybercriminal social network for facilitating instant collaboration.</p><p>Since President Trump’s second inauguration, Musk’s DOGE team has gained access to a truly staggering amount of personal and sensitive data on American citizens, moving quickly to seize control over databases at the , the <strong>Office of Personnel Management</strong>, the , and the <strong>Department of Health and Human Resources</strong>, among others.</p><p> first <a href=\"https://archive.ph/HCCA4\" target=\"_blank\" rel=\"noopener\">reported on Feb. 2</a> that one of the technologists on Musk’s crew is a 19-year-old high school graduate named , who reportedly goes by the nickname “Big Balls” online. One of the companies Coristine founded, , was set up in 2021, when he would have been around 16 years old.</p><p>“Tesla.Sexy LLC controls dozens of web domains, including at least two Russian-registered domains,”  reported. “One of those domains, which is still active, offers a service called Helfie, which is an AI bot for Discord servers targeting the Russian market. While the operation of a Russian website would not violate US sanctions preventing Americans doing business with Russian companies, it could potentially be a factor in a security clearance review.”</p><p>Mr. Coristine has not responded to requests for comment. In <a href=\"https://archive.ph/VAD48\" target=\"_blank\" rel=\"noopener\">a follow-up story</a> this week,  found that someone using a Telegram handle tied to Coristine solicited a DDoS-for-hire service in 2022, and that he worked for a short time at a company that specializes in protecting customers from DDoS attacks.</p><div><img aria-describedby=\"caption-attachment-70369\" decoding=\"async\" src=\"https://krebsonsecurity.com/wp-content/uploads/2025/02/coristine.png\" alt=\"\" width=\"637\" height=\"636\"><p>A profile photo from Coristine’s WhatsApp account.</p></div><p>Internet routing records <a href=\"https://bgp.he.net/net/23.133.104.0/24#_whois\" target=\"_blank\" rel=\"noopener\">show</a> that Coristine runs an Internet service provider called  (AS400495). Also known as “,” Packetware <a href=\"https://bgp.he.net/net/23.133.104.0/24#_dnsrecords\" target=\"_blank\" rel=\"noopener\">currently hosts</a> tesla[.]sexy and diamondcdn[.]com, among other domains.</p><p>DiamondCDN was advertised and claimed by someone who used the nickname “” on several Com-based Discord channels over the years. A review of chat logs from some of those channels show other members frequently referred to Rivage as “Edward.”</p><p>From late 2020 to late 2024, Rivage’s conversations would show up in multiple Com chat servers that are closely monitored by security companies. In November 2022, Rivage could be seen requesting recommendations for a reliable and powerful DDoS-for-hire service.</p><p>Rivage made that request in the cybercrime channel “,” a core Com hub where users could buy and sell attack services. <a href=\"https://www.bleepingcomputer.com/news/security/ddos-site-dstatcc-seized-and-two-suspects-arrested-in-germany/\" target=\"_blank\" rel=\"noopener\">Dstat’s website dstat[.]cc was seized</a> in 2024 as part of “Operation PowerOFF,” an international law enforcement action against DDoS services.</p><p>Coristine’s LinkedIn profile said that in 2022 he worked at an anti-DDoS company called , which  generously described as a “network monitoring firm known for hiring reformed blackhat hackers.”  wrote:</p><p>“At Path Network, Coristine worked as a systems engineer from April to June of 2022, according to his now-deleted LinkedIn résumé. Path has at times listed as employees , also known as Cosmo the God, a well-known former cybercriminal and member of the hacker group UGNazis, as well as Matthew Flannery, an Australian convicted hacker whom police allege was a member of the hacker group LulzSec. It’s unclear whether Coristine worked at Path concurrently with those hackers, and WIRED found no evidence that either Coristine or other Path employees engaged in illegal activity while at the company.”</p><p>The other founder of BackConnect Security LLC was , a Georgia man who <a href=\"https://krebsonsecurity.com/2020/01/ddos-mitigation-firm-founder-admits-to-ddos/\" target=\"_blank\" rel=\"noopener\">pleaded guilty in 2020</a> to paying a DDoS-for-hire service to launch attacks against others.</p><p>The aforementioned Path employee Eric Taylor <a href=\"https://krebsonsecurity.com/2017/02/men-who-sent-swat-team-heroin-to-my-home-sentenced/\" target=\"_blank\" rel=\"noopener\">pleaded guilty in 2017</a> to charges including <a href=\"http://krebsonsecurity.com/2013/03/the-world-has-no-room-for-cowards/\" target=\"_blank\" rel=\"noopener\">an attack on our home in 2013</a>. Taylor was among several men involved in making a false report to my local police department about a supposed hostage situation at our residence in Virginia. In response, a heavily-armed police force surrounded my home and put me in handcuffs at gunpoint before the police realized it was all a dangerous hoax known as “swatting.”</p><p>CosmoTheGod rocketed to Internet infamy in 2013 when he and a number of other hackers <a href=\"https://krebsonsecurity.com/2013/03/credit-reports-sold-for-cheap-in-the-underweb/\" target=\"_blank\" rel=\"noopener\">set up the Web site exposed[dot]su</a>, which “doxed” dozens of public officials and celebrities by publishing the address, Social Security numbers and other personal information on the former First Lady Michelle Obama, the then-director of the FBI and the U.S. attorney general, among others. The group also swatted many of the people they doxed.</p><p> noted that Coristine only worked at Path for a few months in 2022, but the story didn’t mention why his tenure was so short. A screenshot shared on the website <a href=\"https://www.pathtruths.com\" target=\"_blank\" rel=\"noopener\">pathtruths.com</a> includes a snippet of conversations in June 2022 between Path employees discussing Coristine’s firing.</p><p>According to that record, Path founder Marshal Webb dismissed Coristine for leaking internal documents to a competitor. Not long after Coristine’s termination, someone leaked an abundance of internal Path documents and conversations. Among other things, those chats revealed that one of Path’s technicians was a Canadian man named  who was convicted in 2017 of <a href=\"https://krebsonsecurity.com/2017/09/canadian-man-gets-9-months-detention-for-serial-swattings-bomb-threats/\" target=\"_blank\" rel=\"noopener\">perpetrating dozens of swatting attacks and fake bomb threats</a> — including at least two attempts against our home in 2014.</p><div><img aria-describedby=\"caption-attachment-70370\" decoding=\"async\" loading=\"lazy\" src=\"https://krebsonsecurity.com/wp-content/uploads/2025/02/pathchat.png\" alt=\"\" width=\"585\" height=\"808\"><p>A snippet of text from an internal Path chat room, wherein members discuss the reason for Coristine’s termination: Allegedly, leaking internal company information. Source: Pathtruths.com.</p></div><p>On May 11, 2024, Rivage posted on a Discord channel for a DDoS protection service that is chiefly marketed to members of The Com. Rivage expressed frustration with his time spent on Com-based communities, suggesting that its profitability had been oversold.</p><p>“I don’t think there’s a lot of money to be made in the com,” Rivage lamented. “I’m not buying Heztner [servers] to set up some com VPN.”</p><p>Rivage largely stopped posting messages on Com channels after that.  reports that Coristine subsequently spent three months last summer working at , Elon Musk’s brain implant startup.</p><p>The trouble with all this is that even if someone sincerely intends to exit The Com after years of consorting with cybercriminals, they are often still subject to personal attacks, harassment and hacking long after they have left the scene.</p><p>That’s because a huge part of Com culture involves harassing, swatting and hacking other members of the community. These internecine attacks are often for financial gain, but just as frequently they are perpetrated by cybercrime groups to exact retribution from or assert dominance over rival gangs.</p><p>Experts say it is extremely difficult for former members of violent street gangs to gain a security clearance needed to view sensitive or classified information held by the U.S. government. That’s because ex-gang members are highly susceptible to extortion and coercion from current members of the same gang, and that alone presents an unacceptable security risk for intelligence agencies.</p><p>And make no mistake: The Com is the English-language cybercriminal hacking equivalent of a violent street gang. KrebsOnSecurity has published numerous stories detailing how feuds within the community <a href=\"https://krebsonsecurity.com/2022/09/violence-as-a-service-brickings-firebombings-shootings-for-hire/\" target=\"_blank\" rel=\"noopener\">periodically spill over into real-world violence</a>.</p><p>When Coristine’s name surfaced in ‘s report this week, members of The Com immediately took notice.&nbsp;In the following segment from a February 5, 2025 chat in a Com-affiliated hosting provider, members criticized Rivage’s skills, and discussed harassing his family and notifying authorities about incriminating accusations that may or may not be true.</p><blockquote><p>2025-02-05 16:29:44 UTC vperked#0 they got this nigga on indiatimes man\n2025-02-05 16:29:46 UTC alexaloo#0 Their cropping is worse than AI could have done<p>\n2025-02-05 16:29:48 UTC hebeatsme#0 bro who is that</p>\n2025-02-05 16:29:53 UTC hebeatsme#0 yalla re talking about<p>\n2025-02-05 16:29:56 UTC xewdy#0 edward</p>\n2025-02-05 16:29:56 UTC .yarrb#0 rivagew<p>\n2025-02-05 16:29:57 UTC vperked#0 Rivarge</p>\n2025-02-05 16:29:57 UTC xewdy#0 diamondcdm<p>\n2025-02-05 16:29:59 UTC vperked#0 i cant spell it</p>\n2025-02-05 16:30:00 UTC hebeatsme#0 rivage<p>\n2025-02-05 16:30:08 UTC .yarrb#0 yes</p>\n2025-02-05 16:30:14 UTC hebeatsme#0 i have him added<p>\n2025-02-05 16:30:20 UTC hebeatsme#0 hes on discord still</p>\n2025-02-05 16:30:47 UTC .yarrb#0 hes focused on stroking zaddy elon<p>\n2025-02-05 16:30:47 UTC vperked#0 https://en.wikipedia.org/wiki/Edward_Coristine</p>\n2025-02-05 16:30:50 UTC vperked#0 no fucking way<p>\n2025-02-05 16:30:53 UTC vperked#0 they even made a wiki for him</p>\n2025-02-05 16:30:55 UTC vperked#0 LOOOL<p>\n2025-02-05 16:31:05 UTC hebeatsme#0 no way</p>\n2025-02-05 16:31:08 UTC hebeatsme#0 hes not a good dev either<p>\n2025-02-05 16:31:14 UTC hebeatsme#0 like????</p>\n2025-02-05 16:31:22 UTC hebeatsme#0 has to be fake<p>\n2025-02-05 16:31:24 UTC xewdy#0 and theyre saying ts</p>\n2025-02-05 16:31:29 UTC xewdy#0 like ok bro<p>\n2025-02-05 16:31:51 UTC .yarrb#0 now i wanna know what all the other devs are like…</p>\n2025-02-05 16:32:00 UTC vperked#0 “`Coristine used the moniker “bigballs” on LinkedIn and @Edwardbigballer on Twitter, according to The Daily Dot.[“`<p>\n2025-02-05 16:32:05 UTC vperked#0 LOL</p>\n2025-02-05 16:32:06 UTC hebeatsme#0 lmfaooo<p>\n2025-02-05 16:32:07 UTC vperked#0 bro</p>\n2025-02-05 16:32:10 UTC hebeatsme#0 bro<p>\n2025-02-05 16:32:17 UTC hebeatsme#0 has to be fake right</p>\n2025-02-05 16:32:22 UTC .yarrb#0 does it mention Rivage?<p>\n2025-02-05 16:32:23 UTC xewdy#0 He previously worked for NeuraLink, a brain computer interface company led by Elon Musk</p>\n2025-02-05 16:32:26 UTC xewdy#0 bro what<p>\n2025-02-05 16:32:27 UTC alexaloo#0 I think your current occupation gives you a good insight of what probably goes on</p>\n2025-02-05 16:32:29 UTC hebeatsme#0 bullshit man<p>\n2025-02-05 16:32:33 UTC xewdy#0 this nigga got hella secrets</p>\n2025-02-05 16:32:37 UTC hebeatsme#0 rivage couldnt print hello world<p>\n2025-02-05 16:32:42 UTC hebeatsme#0 if his life was on the line</p>\n2025-02-05 16:32:50 UTC xewdy#0 nigga worked for neuralink<p>\n2025-02-05 16:32:54 UTC hebeatsme#0 bullshit</p>\n2025-02-05 16:33:06 UTC Nashville Dispatch ##0000 ||@PD Ping||<p>\n2025-02-05 16:33:07 UTC hebeatsme#0 must have killed all those test pigs with some bugs</p>\n2025-02-05 16:33:24 UTC hebeatsme#0 ur telling me the rivage who failed to start a company<p>\n2025-02-05 16:33:28 UTC hebeatsme#0 https://cdn.camp</p>\n2025-02-05 16:33:32 UTC hebeatsme#0 who didnt pay for servers<p>\n2025-02-05 16:33:34 UTC hebeatsme#0 ?</p>\n2025-02-05 16:33:42 UTC hebeatsme#0 was too cheap<p>\n2025-02-05 16:33:44 UTC vperked#0 yes</p>\n2025-02-05 16:33:50 UTC hebeatsme#0 like??<p>\n2025-02-05 16:33:53 UTC hebeatsme#0 it aint adding up</p>\n2025-02-05 16:33:56 UTC alexaloo#0 He just needed to find his calling idiot.<p>\n2025-02-05 16:33:58 UTC alexaloo#0 He found it.</p>\n2025-02-05 16:33:59 UTC hebeatsme#0 bro<p>\n2025-02-05 16:34:01 UTC alexaloo#0 Cope in a river dude</p>\n2025-02-05 16:34:04 UTC hebeatsme#0 he cant make good money right<p>\n2025-02-05 16:34:08 UTC hebeatsme#0 doge is about efficiency</p>\n2025-02-05 16:34:11 UTC hebeatsme#0 he should make $1/he<p>\n2025-02-05 16:34:15 UTC hebeatsme#0 $1/hr</p>\n2025-02-05 16:34:25 UTC hebeatsme#0 and be whipped for better code<p>\n2025-02-05 16:34:26 UTC vperked#0 prolly makes more than us</p>\n2025-02-05 16:34:35 UTC vperked#0 with his dad too<p>\n2025-02-05 16:34:52 UTC hebeatsme#0 time to report him for fraud</p>\n2025-02-05 16:34:54 UTC hebeatsme#0 to donald trump<p>\n2025-02-05 16:35:04 UTC hebeatsme#0 rivage participated in sim swap hacks in 2018</p>\n2025-02-05 16:35:08 UTC hebeatsme#0 put that on his wiki<p>\n2025-02-05 16:35:10 UTC hebeatsme#0 thanks</p>\n2025-02-05 16:35:15 UTC hebeatsme#0 and in 2021<p>\n2025-02-05 16:35:17 UTC hebeatsme#0 thanks</p>\n2025-02-05 16:35:19 UTC chainofcommand#0 i dont think they’ll care tbh</p></blockquote><p>Given the speed with which Musk’s DOGE team was allowed access to such critical government databases, it strains credulity that Coristine could have been properly cleared beforehand. After all, he’d recently been dismissed from a job <em>for allegedly leaking internal company information to outsiders</em>.</p><p>According to the national security adjudication <a href=\"https://www.dni.gov/files/NCSC/documents/Regulations/SEAD-4-Adjudicative-Guidelines-U.pdf\" target=\"_blank\" rel=\"noopener\">guidelines</a> (PDF) released by the <strong>Director of National Intelligence</strong> (DNI), eligibility determinations take into account a person’s stability, trustworthiness, reliability, discretion, character, honesty, judgment, and ability to protect classified information.</p><p>The DNI policy further states that “eligibility for covered individuals shall be granted only when facts and circumstances indicate that eligibility is clearly consistent with the national security interests of the United States, and any doubt shall be resolved in favor of national security.”</p><p>On Thursday, 25-year-old DOGE staff member  resigned after being linked to a deleted social media account that advocated racism and eugenics. Elez resigned after <a href=\"https://www.wsj.com/tech/doge-staffer-resigns-over-racist-posts-d9f11a93\" target=\"_blank\" rel=\"noopener\">asked</a> the White House about his connection to the account.</p><p>“Just for the record, I was racist before it was cool,” the account posted in July. “You could not pay me to marry outside of my ethnicity,” the account wrote on X in September. “Normalize Indian hate,” the account wrote the same month, in reference to a post noting the prevalence of people from India in Silicon Valley.</p><p>Elez’s resignation came a day after the Department of Justice agreed to limit the number of DOGE employees who have access to federal payment systems. The DOJ said access would be limited to two people, Elez and , the CEO of a company called Cloud Software Group.</p><p>Earlier today, Musk <a href=\"https://thehill.com/business/5133430-elon-musk-doge-staffer-racist-posts/\" target=\"_blank\" rel=\"noopener\">said he planned to rehire Elez</a> after  and  reportedly endorsed the idea. Speaking at The White House today, Trump said he wasn’t concerned about the security of personal information and other data accessed by DOGE, adding that he was “very proud of the job that this group of young people” are doing.</p><p>A White House official <a href=\"https://www.reuters.com/world/us/staffer-with-elon-musks-doge-amplified-white-supremacists-online-2025-02-07/\" target=\"_blank\" rel=\"noopener\">told</a> on Wednesday that Musk and his engineers have appropriate security clearances and are operating in “full compliance with federal law, appropriate security clearances, and as employees of the relevant agencies, not as outside advisors or entities.”</p><p><a href=\"https://www.npr.org/2025/02/07/nx-s1-5288988/doge-elon-musk-staff-trump\" target=\"_blank\" rel=\"noopener\">reports</a> Trump added that his administration’s cost-cutting efforts would soon turn to the Education Department and the Pentagon, “where he suggested without evidence that there could be ‘trillions’ of dollars in wasted spending within the $6.75 trillion the federal government spent in fiscal year 2024.”</p><p>GOP leaders in the Republican-controlled House and Senate have largely shrugged about Musk’s ongoing efforts to seize control over federal databases, dismantle agencies mandated by Congress, freeze federal spending on a range of already-appropriated government programs, and threaten workers with layoffs.</p><p>Meanwhile, multiple parties have sued to stop DOGE’s activities. ABC News <a href=\"https://abcnews.go.com/US/judge-decide-block-doge-accessing-sensitive-labor-department/story?id=118575362\" target=\"_blank\" rel=\"noopener\">says</a> a federal judge was to rule today on whether DOGE should be blocked from accessing Department of Labor records, following a lawsuit alleging Musk’s team sought to illegally access highly sensitive data, including medical information, from the federal government.</p><p>At least 13 state attorneys general say they plan to file a lawsuit to stop DOGE from accessing federal payment systems containing Americans’ sensitive personal information, <a href=\"https://www.mychamplainvalley.com/news/local-news/vermont-joins-lawsuit-over-doge-access-to-government-payment-system/\" target=\"_blank\" rel=\"noopener\">reports</a>.</p><p>Reuters <a href=\"https://www.reuters.com/world/us/us-treasury-temporarily-agrees-limit-doge-access-amid-privacy-lawsuit-2025-02-06/\" target=\"_blank\" rel=\"noopener\">reported Thursday</a> that the U.S. Treasury Department had agreed not to give Musk’s team access to its payment systems while a judge is hearing arguments in a lawsuit by employee unions and retirees alleging Musk illegally searched those records.</p><p><a href=\"https://arstechnica.com/tech-policy/2025/02/lawsuit-tries-to-block-doges-access-to-student-loan-data/\" target=\"_blank\" rel=\"noopener\">writes</a> that The Department of Education (DoE) was sued Friday by a California student association demanding an “immediate stop” to DOGE’s “unlawfully” digging through student loan data to potentially dismantle the DoE.</p>","contentLength":15941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42981756"},{"title":"VSCode’s SSH agent is bananas","url":"https://fly.io/blog/vscode-ssh-wtf/","date":1738977932,"author":"zdyxry","guid":158,"unread":true,"content":"<p>We’re interested in getting integrated into the flow VSCode uses to do remote editing over SSH, because everybody is using VSCode now, and, in particular, they’re using forks of VSCode that generate code with LLMs. </p><div><p>”hallucination” is what we call it when LLMs get code wrong; “engineering” is what we call it when people do.</p></div><p>LLM-generated code is <a href=\"https://nicholas.carlini.com/writing/2024/how-i-use-ai.html\" title=\"\">useful in the general case</a> if you know what you’re doing. But it’s ultra-useful if you can close the loop between the LLM and the execution environment (with an “Agent” setup). There’s lots to say about this, but for the moment: it’s a semi-effective antidote to hallucination: the LLM generates the code, the agent scaffolding runs the code, the code generates errors, the agent feeds it back to the LLM, the process iterates. </p><p>So, obviously, the issue here is you don’t want this iterative development process happening on your development laptop, because LLMs have boundary issues, and they’ll iterate on your system configuration just as happily on the Git project you happen to be working in. A thing you’d really like to be able to do: run a closed-loop agent-y (“agentic”? is that what we say now) configuration for an LLM, on a clean-slate Linux instance that spins up instantly and that can’t screw you over in any way. You get where we’re going with this.</p><p>Anyways! I would like to register a concern.</p><p>Emacs hosts the spiritual forebearer of remote editing systems, a blob of hyper-useful Elisp called <a href=\"https://www.gnu.org/software/tramp/\" title=\"\">“Tramp”</a>. If you can hook Tramp up to any kind of interactive environment — usually, an SSH session — where it can run Bourne shell commands, it can extend Emacs to that environment.</p><p>So, VSCode has a feature like Tramp. Which, neat, right? You’d think, take Tramp, maybe simplify it a bit, switch out Elisp for Typescript.</p><p>Unlike Tramp, which lives off the land on the remote connection, VSCode mounts a full-scale invasion: it runs a Bash snippet stager that downloads an agent, including a binary installation of Node. </p><p>The agent runs over port-forwarded SSH. It establishes a WebSockets connection back to your running VSCode front-end. The underlying protocol on that connection can:</p><ul><li>Wander around the filesystem\n</li><li>Launch its own shell PTY processes\n</li></ul><p>In security-world, there’s a name for tools that work this way. I won’t say it out loud, because that’s not fair to VSCode, but let’s just say the name is murid in nature.</p><p>I would be a little nervous about letting people VSCode-remote-edit stuff on dev servers, and apoplectic if that happened during an incident on something in production. </p><p>It turns out we don’t have to care about any of this to get a custom connection to a Fly Machine working in VSCode, so none of this matters in any kind of deep way, but: we’ve decided to just be a blog again, so: we had to learn this, and now you do too.</p>","contentLength":2855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42979467"},{"title":"Obscure islands I find interesting","url":"https://amanvir.com/obscure-islands","date":1738967011,"author":"venusgirdle","guid":157,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42978199"},{"title":"Do-nothing scripting: the key to gradual automation (2019)","url":"https://blog.danslimmon.com/2019/07/15/do-nothing-scripting-the-key-to-gradual-automation/","date":1738957696,"author":"tehnub","guid":156,"unread":true,"content":"<p>Every ops team has some manual procedures that they haven’t gotten around to automating yet. <a href=\"https://landing.google.com/sre/sre-book/chapters/eliminating-toil/\">Toil</a> can never be totally eliminated.</p><p>Very often, the biggest toil center for a team at a growing company will be its procedure for modifying infrastructure or its procedure for provisioning user accounts. Partial instructions for the latter might look like this:</p><ol><li>Create an SSH key pair for the user.</li><li>Commit the public key to Git and push to master.</li><li>Wait for the build job to finish.</li><li>Find the user’s email address in the employee directory.</li><li>Send the user their private key via 1Password.</li></ol><p>This is a relatively short example. Sometimes there are 20 steps in the process. Sometimes there are branches and special cases to keep track of as you go. Over time, these procedures can become unmanageably large and complex.</p><p>Procedures like this are frustrating because they’re focus-intensive yet require very little thought. They demand our full attention, but our attention isn’t rewarded with interesting problems or satisfying solutions – just another checkbox checked. I have a word for a procedure like this: a .</p><p>We know that this procedure is ripe for automation. We can easily see how to automate any given step. And we know that a computer could carry out the instructions with far greater speed and accuracy than we can, and with less tendency toward <a href=\"https://risk-engineering.org/concept/Rasmussen-practical-drift\">practical drift</a>.</p><p>However, automating slogs sometimes feels like an all-or-nothing proposition. Sure, we could write a script to handle step 2, or step 5. But that wouldn’t  make the procedure any less cumbersome. It would lead to a proliferation of single-purpose scripts with different conventions and expectations, and you’d still have to follow a documented multi-step procedure for using those scripts.</p><p>This perception of futility is the problem we really need to solve in order to escape from these manual slogs. I’ve found an approach that works pretty reliably: .</p><p>Almost any slog can be turned into a . A do-nothing script is a script that encodes the instructions of a slog, encapsulating each step in a function. For the example procedure above, we could write the following do-nothing script:</p><div><pre title=\"\">import sys\n\ndef wait_for_enter():\n    raw_input(\"Press Enter to continue: \")\n\nclass CreateSSHKeypairStep(object):\n    def run(self, context):\n        print(\"Run:\")\n        print(\"   ssh-keygen -t rsa -f ~/{0}\".format(context[\"username\"]))\n        wait_for_enter()\n\nclass GitCommitStep(object):\n    def run(self, context):\n        print(\"Copy ~/new_key.pub into the `user_keys` Git repository, then run:\")\n        print(\"    git commit {0}\".format(context[\"username\"]))\n        print(\"    git push\")\n        wait_for_enter()\n\nclass WaitForBuildStep(object):\n    build_url = \"http://example.com/builds/user_keys\"\n    def run(self, context):\n        print(\"Wait for the build job at {0} to finish\".format(self.build_url))\n        wait_for_enter()\n\nclass RetrieveUserEmailStep(object):\n    dir_url = \"http://example.com/directory\"\n    def run(self, context):\n        print(\"Go to {0}\".format(self.dir_url))\n        print(\"Find the email address for user `{0}`\".format(context[\"username\"]))\n        context[\"email\"] = raw_input(\"Paste the email address and press enter: \")\n\nclass SendPrivateKeyStep(object):\n    def run(self, context):\n        print(\"Go to 1Password\")\n        print(\"Paste the contents of ~/new_key into a new document\")\n        print(\"Share the document with {0}\".format(context[\"email\"]))\n        wait_for_enter()\n\nif __name__ == \"__main__\":\n    context = {\"username\": sys.argv[1]}\n    procedure = [\n        CreateSSHKeypairStep(),\n        GitCommitStep(),\n        WaitForBuildStep(),\n        RetrieveUserEmailStep(),\n        SendPrivateKeyStep(),\n    ]\n    for step in procedure:\n        step.run(context)\n    print(\"Done.\")\n</pre></div><p>This script doesn’t actually  any of the steps of the procedure. That’s why it’s called a do-nothing script. It feeds the user a step at a time and waits for them to complete each step manually.</p><p>At first glance, it might not be obvious that this script provides value. Maybe it looks like all we’ve done is make the instructions harder to read. But the value of a do-nothing script is immense:</p><ul><li>It’s now much less likely that you’ll lose your place and skip a step. This makes it easier to maintain focus and power through the slog.</li><li>Each step of the procedure is now encapsulated in a function, which makes it possible to replace the text in any given step with code that performs the action automatically.</li><li>Over time, you’ll develop a library of useful steps, which will make future automation tasks more efficient.</li></ul><p>A do-nothing script doesn’t save your team any manual effort. It lowers the activation energy for automating tasks, which allows the team to eliminate toil over time.</p>","contentLength":4783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42976698"},{"title":"Asahi Linux lead developer Hector Martin resigns from Linux kernel","url":"https://lkml.org/lkml/2025/2/7/9","date":1738933323,"author":"Mond_","guid":155,"unread":true,"content":"I no longer have any faith left in the kernel development process orcommunity management approach.<p>Apple/ARM platform development will continue downstream. If I feel like</p>sending some patches upstream in the future myself for whatever subtree<p>I may, or I may not. Anyone who feels like fighting the upstreaming</p>fight themselves is welcome to do so.<p>Signed-off-by: Hector Martin &lt;marcan@marcan.st&gt;</p>--- 1 file changed, 1 deletion(-)<p>diff --git a/MAINTAINERS b/MAINTAINERS</p>index 1e930c7a58b13d8bbe6bf133ba7b36aa24c2b5e0..c9623439998709c9d6d6944cbd87e025356422da 100644+++ b/MAINTAINERS<p>@@ -2177,7 +2177,6 @@ F:\tsound/soc/codecs/cs42l84.*</p> F:\tsound/soc/codecs/ssm3515.c<p> ARM/APPLE MACHINE SUPPORT</p>-M:\tHector Martin &lt;marcan@marcan.st&gt;<p> M:\tSven Peter &lt;sven@svenpeter.dev&gt;</p> R:\tAlyssa Rosenzweig &lt;alyssa@rosenzweig.io&gt;---<p>base-commit: 40384c840ea1944d7c5a392e8975ed088ecf0b37</p>change-id: 20250207-rm-maint-af7cccc22871-- <p>Hector Martin &lt;marcan@marcan.st&gt;</p>","contentLength":928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42972062"}],"tags":["dev","hn"]}