{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HN","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"FFmpeg 8.0","url":"https://ffmpeg.org/index.html#pr8.0","date":1755876162,"author":"gyan","guid":211,"unread":true,"content":"<div><div><h2>\n        A complete, cross-platform solution to record, convert and stream audio and video.\n      </h2></div></div><div><h3>Converting  and  has never been so easy.\n    </h3><pre>$ ffmpeg -i input.mp4 output.avi</pre></div><h3>August 22nd, 2025, FFmpeg 8.0 </h3><p>\n  A new major release, <a href=\"https://ffmpeg.org/download.html#release_8.0\">FFmpeg 8.0 </a>,\n  is now available for download.\n  Thanks to several delays, and modernization of our entire infrastructure, this release ended up\n  being one of our largest releases to date. In short, its new features are:\n  </p><ul><li>Native decoders: , ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728</li><li>VVC decoder improvements: ,\n                                  ,\n                                  Palette Mode</li><li>Vulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)</li><li>Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5</li><li>Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5</li><li>Formats: MCC, G.728, Whip, APV</li><li>Filters: colordetect, pad_cuda, scale_d3d11, Whisper, and others</li></ul><p>\n  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.\n  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,\n  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.\n  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work\n  on any implementation of Vulkan 1.3.\n  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,\n  as enabling <a href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan\">Vulkan decoding</a> is sufficient to use them.\n  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).\n  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).\n  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,\n  to be merged soon and available with the next minor release.<p>\n  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with\n  more mainstream codecs not being planned for support.</p>\n  Depending on the hardware, these new codecs can provide very significant speedups, and open up\n  possibilities to work with them for situations like non-linear video editors and\n  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.\n  </p><p>\n  The project has recently started to modernize its infrastructure. Our mailing list servers have been\n  fully upgraded, and we have recently started to accept contributions via a new forge, available on\n  <a href=\"https://code.ffmpeg.org/\">code.ffmpeg.org</a>, running a Forgejo instance.\n  </p><p>\n    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.\n  </p><h3>September 30th, 2024, FFmpeg 7.1 </h3><p>\n    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,\n    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain\n    traction with broadcast standardization bodies.\n    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting\n    to be adopted by streaming websites, due to its extensive volume normalization metadata.<p>\n    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated\n    by recent phones and VR headsets.</p>\n    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an\n    external library.</p><p>\n    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode\n    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity\n    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,\n    and FFmpeg is aiming to have day-one support.\n  </p><p>\n    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally\n    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,\n    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10\n    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.\n    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also\n    laid the path for more advanced forms of negotiation.\n    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,\n    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.\n  </p><p>\n    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.\n  </p><h3>September 11th, 2024, Coverity</h3><p>\n  The number of issues FFmpeg has in <a href=\"https://scan.coverity.com/projects/ffmpeg\">Coverity (a static analyzer)</a> is now lower than it has been since 2016.\n  Our defect density is less than one 30th of the average in OSS with over a million code\n  lines. All this was possible thanks to a grant from the <a href=\"https://www.sovereigntechfund.de/\">Sovereign Tech Fund</a>.\n  </p><img src=\"https://ffmpeg.org/img/coverity-lifetime-2024-08.PNG\" alt=\"Coverity Lifetime Graph till 2024-08\"><h3>June 2nd, 2024, native xHE-AAC decoder</h3><p>\n  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround\n  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is\n  coming soon. Work is also ongoing to improve its stability and compatibility.\n  During the process we found several specification issues, which were then submitted back to the authors\n  for discussion and potential inclusion in a future errata.\n  </p><h3>May 13th, 2024, Sovereign Tech Fund</h3><p>\n  The FFmpeg community is excited to announce that Germany's\n  <a href=\"https://www.sovereigntechfund.de/tech/ffmpeg\">Sovereign Tech Fund</a>\n  has become its first governmental sponsor. Their support will help\n  sustain the maintainance of the FFmpeg project, a critical open-source\n  software multimedia component essential to bringing audio and video to\n  billions around the world everyday.\n  </p><h3>April 5th, 2024, FFmpeg 7.0 \"Dijkstra\"</h3><p>\n  This release is  backwards compatible, removing APIs deprecated before 6.0.\n  The biggest change for most library callers will be the removal of the old bitmask-based\n  channel layout API, replaced by the  API allowing such\n  features as custom channel ordering, or Ambisonics. Certain deprecated \n  CLI options were also removed, and a C11-compliant compiler is now required to build\n  the code.\n  </p><p>\n  As usual, there is also a number of new supported formats and codecs, new filters, APIs,\n  and countless smaller features and bugfixes. Compared to 6.1, the  repository\n  contains almost ∼2000 new commits by ∼100 authors, touching &gt;100000 lines in\n  ∼2000 files — thanks to everyone who contributed. See the\n  <a href=\"https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog;hb=n7.0\">Changelog</a>,\n  <a href=\"https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=doc/APIchanges;hb=n7.0\">APIchanges</a>,\n  and the git log for more comprehensive lists of changes.\n  </p><h3>January 3rd, 2024, native VVC decoder</h3><p>\n  The  library now contains a native VVC (Versatile Video Coding)\n  decoder, supporting a large subset of the codec's features. Further optimizations and\n  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,\n  Frank Plowman, Shaun Loo, and Wu Jianhua.\n  </p><h3>December 18th, 2023, IAMF support</h3><p>\n  The  library can now read and write <a href=\"https://aomediacodec.github.io/iamf/\">IAMF</a>\n  (Immersive Audio) files. The  CLI tool can configure IAMF structure with the new\n   option. IAMF support was written by James Almer.\n  </p><h3>December 12th, 2023, multi-threaded  CLI tool</h3><p>\n  Thanks to a major refactoring of the  command-line tool, all the major\n  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now\n  run in parallel. This should improve throughput and CPU utilization, decrease latency,\n  and open the way to other exciting new features.\n  </p><p>\n  Note that you should  expect significant performance improvements in cases\n  where almost all computational time is spent in a single component (typically video\n  encoding).\n  </p><h3>November 10th, 2023, FFmpeg 6.1 \"Heaviside\"</h3><ul><li>Playdate video decoder and demuxer</li><li>Extend VAAPI support for libva-win32 on Windows</li><li>afireqsrc audio source filter</li><li>ffmpeg CLI new option: -readrate_initial_burst</li><li>zoneplate video source filter</li><li>command support in the setpts and asetpts filters</li><li>Vulkan decode hwaccel, supporting H264, HEVC and AV1</li><li>Essential Video Coding parser, muxer and demuxer</li><li>Essential Video Coding frame merge bsf</li><li>Microsoft RLE video encoder</li><li>Raw AC-4 muxer and demuxer</li><li>Raw VVC bitstream parser, muxer and demuxer</li><li>Bitstream filter for editing metadata in VVC streams</li><li>Bitstream filter for converting VVC from MP4 to Annex B</li><li>scale_vt filter for videotoolbox</li><li>transpose_vt filter for videotoolbox</li><li>support for the P_SKIP hinting to speed up libx264 encoding</li><li>Support HEVC,VP9,AV1 codec in enhanced flv format</li><li>apsnr and asisdr audio filters</li><li>Support HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocol</li><li>ffmpeg CLI '-top' option deprecated in favor of the setfield filter</li><li>ffprobe XML output schema changed to account for multiple variable-fields elements within the same parent element</li><li>ffprobe -output_format option added as an alias of -of</li></ul><p>\n    This release had been overdue for at least half a year, but due to constant activity in the repository,\n    had to be delayed, and we were finally able to branch off the release recently, before some of the large\n    changes scheduled for 7.0 were merged.\n  </p><p>\n    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs\n    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).\n    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.<p>\n    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,\n    reducing overhead.</p>\n    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.<p>\n    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the\n    accurracy of variable frame rate video.\n  </p></p><p>\n    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick\n    to the new release schedule we announced at the start of this year.\n  </p><p>\n    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n  </p><h3>May 31st, 2023, Vulkan decoding</h3><p>\n    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.\n    This is the first vendor-generic and platform-generic decode acceleration API, enabling the\n    same code to be used on multiple platforms, with very minimal overhead.\n    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,\n    saturating all available decode engines the hardware exposes.\n  </p><p>\n    Those wishing to test the code can read our\n    <a href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan\">documentation page</a>.\n    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive\n    a VkImage to present or manipulate, documentation and examples are available in our source tree.\n    Currently, using the latest available git checkout of our\n    <a href=\"https://git.videolan.org/?p=ffmpeg.git;a=summary\">repository</a> is required.\n    The functionality will be included in stable branches with the release of version 6.1, due\n    to be released soon.\n  </p><p>\n    As this is also the first practical implementation of the specifications, bugs may be present,\n    particularly in drivers, and, although passing verification, the implementation itself.\n    New codecs, and encoding support are also being worked on, by both the Khronos organization\n    for standardizing, and us as implementing it, and giving feedback on improving.\n  </p><h3>February 28th, 2023, FFmpeg 6.0 \"Von Neumann\"</h3><p>\n    A new major release, <a href=\"https://ffmpeg.org/download.html#release_6.0\">FFmpeg 6.0 \"Von Neumann\"</a>,\n    is now available for download. This release has many new encoders and decoders, filters,\n    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major\n    releases will now bump the version of the ABI. We plan to have a new major release each\n    year. Another release-specific change is that deprecated APIs will be removed after 3\n    releases, upon the next major bump.\n    This means that releases will be done more often and will be more organized.\n  </p><p>\n    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.\n    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c\n    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,\n    and the ability to pass option values for filters from a file. There are quite a few new audio\n    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.\n    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT\n    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better\n    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V\n    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed\n    in the doc/APIchanges file in our tree.\n    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the\n    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.\n    Some highlights are:\n  </p><ul><li>Radiance HDR image support</li><li>ddagrab (Desktop Duplication) video capture filter</li><li>ffmpeg -shortest_buf_duration option</li><li>ffmpeg now requires threading to be built</li><li>ffmpeg now runs every muxer in a separate thread</li><li>Add new mode to cropdetect filter to detect crop-area based on motion vectors and edges</li><li>VAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li><li>WBMP (Wireless Application Protocol Bitmap) image format</li><li>Micronas SC-4 audio decoder</li><li>nvenc AV1 encoding support</li><li>MediaCodec decoder via NDKMediaCodec</li><li>QSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li><li>showcwt multimedia filter</li><li>WADY DPCM decoder and demuxer</li><li>ffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]</li><li>hstack_vaapi, vstack_vaapi and xstack_vaapi filters</li><li>XMD ADPCM decoder and demuxer</li><li>ffmpeg CLI new option: -fix_sub_duration_heartbeat</li><li>WavArc decoder and demuxer</li><li>CrystalHD decoders deprecated</li><li>filtergraph syntax in ffmpeg CLI now supports passing file contents as option values</li><li>hstack_qsv, vstack_qsv and xstack_qsv filters</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>July 22nd, 2022, FFmpeg 5.1 \"Riemann\"</h3><ul><li>add ipfs/ipns protocol support</li><li>dialogue enhance audio filter</li><li>dropped obsolete XvMC hwaccel</li><li>DFPWM audio encoder/decoder and raw muxer/demuxer</li><li>Vizrt Binary Image encoder/decoder</li><li>colorchart video source filter</li><li>PGS subtitle frame merge bitstream filter</li><li>added chromakey_cuda filter</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>January 17th, 2022, FFmpeg 5.0 \"Lorentz\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_5.0\">FFmpeg 5.0 \"Lorentz\"</a>, a new\n    major release, is now available! For this long-overdue release, a major effort\n    underwent to remove the old encode/decode APIs and replace them with an\n    N:M-based API, the entire libavresample library was removed, libswscale\n    has a new, easier to use AVframe-based API, the Vulkan code was much improved,\n    many new filters were added, including libplacebo integration, and finally,\n    DoVi support was added, including tonemapping and remuxing. The default\n    AAC encoder settings were also changed to improve quality.\n    Some of the changelog highlights:\n  </p><ul><li>ADPCM IMA Westwood encoder</li><li>ADPCM IMA Acorn Replay decoder</li><li>Argonaut Games CVG demuxer</li><li>audio and video segment filters</li><li>Apple Graphics (SMC) encoder</li><li>hsvkey and hsvhold video filters</li><li>adecorrelate audio filter</li><li>AV1 Low overhead bitstream format muxer</li><li>huesaturation video filter</li><li>colorspectrum source video filter</li><li>RTP packetizer for uncompressed video (RFC 4175)</li><li>VideoToolbox ProRes hwaccel</li><li>aspectralstats audio filter</li><li>adynamicsmooth audio filter</li><li>vflip_vulkan, hflip_vulkan and flip_vulkan filters</li><li>adynamicequalizer audio filter</li><li>yadif_videotoolbox filter</li><li>VideoToolbox ProRes encoder</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><p>\n    We have a new IRC home at Libera Chat\n    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at <a href=\"https://ffmpeg.org/contact.html#IRCChannels\">contact#IRCChannels</a></p><h3>April 8th, 2021, FFmpeg 4.4 \"Rao\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.4\">FFmpeg 4.4 \"Rao\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>AudioToolbox output device</li><li>VDPAU accelerated HEVC 10/12bit decoding</li><li>ADPCM IMA Ubisoft APM encoder</li><li>AV1 encoding support SVT-AV1</li><li>ADPCM Argonaut Games encoder</li><li>AV1 Low overhead bitstream format demuxer</li><li>MobiClip FastAudio decoder</li><li>AV1 decoder (Hardware acceleration used only)</li><li>Argonaut Games BRP demuxer</li><li>IPU decoder, parser and demuxer</li><li>Intel QSV-accelerated AV1 decoding</li><li>Argonaut Games Video decoder</li><li>libwavpack encoder removed</li><li>AVS3 video decoder via libuavs3d</li><li>VDPAU accelerated VP9 10/12bit decoding</li><li>afreqshift and aphaseshift filters</li><li>High Voltage Software ADPCM encoder</li><li>LEGO Racers ALP (.tun &amp; .pcm) muxer</li><li>DXVA2/D3D11VA hardware accelerated AV1 decoding</li><li>Microsoft Paint (MSP) version 2 decoder</li><li>Microsoft Paint (MSP) demuxer</li><li>AV1 monochrome encoding support via libaom &gt;= 2.0.1</li><li>asuperpass and asuperstop filter</li><li>Digital Pictures SGA demuxer and decoders</li><li>TTML subtitle encoder and muxer</li><li>RIST protocol via librist</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>June 15th, 2020, FFmpeg 4.3 \"4:3\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.3\">FFmpeg 4.3 \"4:3\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>Intel QSV-accelerated MJPEG decoding</li><li>Intel QSV-accelerated VP9 decoding</li><li>Support for TrueHD in mp4</li><li>Support AMD AMF encoder on Linux (via Vulkan)</li><li>support Sipro ACELP.KELVIN decoding</li><li>maskedmin and maskedmax filters</li><li>QSV-accelerated VP9 encoding</li><li>AV1 encoding support via librav1e</li><li>AV1 frame merge bitstream filter</li><li>MPEG-H 3D Audio support in mp4</li><li>Argonaut Games ADPCM decoder</li><li>Argonaut Games ASF demuxer</li><li>afirsrc audio filter source</li><li>Simon &amp; Schuster Interactive ADPCM decoder</li><li>High Voltage Software ADPCM decoder</li><li>LEGO Racers ALP (.tun &amp; .pcm) demuxer</li><li>AMQP 0-9-1 protocol (RabbitMQ)</li><li>avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filters</li><li>switch from AvxSynth to AviSynth+ on Linux</li><li>Expanded styling support for 3GPP Timed Text Subtitles (movtext)</li><li>Support for muxing pcm and pgs in m2ts</li><li>Cunning Developments ADPCM decoder</li><li>Pro Pinball Series Soundbank demuxer</li><li>pcm_rechunk bitstream filter</li><li>gradients source video filter</li><li>MediaFoundation encoder wrapper</li><li>Simon &amp; Schuster Interactive ADPCM encoder</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>October 5th, 2019, Bright Lights</h3><p>\n  FFmpeg has added a realtime bright flash removal filter to libavfilter.\n  </p><p>\n  Note that this filter is not FDA approved, nor are we medical professionals.\n  Nor has this filter been tested with anyone who has photosensitive epilepsy.\n  FFmpeg and its photosensitivity filter are not making any medical claims.\n  </p><p>\n  That said, this is a new video filter that may help photosensitive people\n  watch tv, play video games or even be used with a VR headset to block\n  out epiletic triggers such as filtered sunlight when they are outside.\n  Or you could use it against those annoying white flashes on your tv screen.\n  The filter fails on some input, such as the\n  <a href=\"https://www.youtube.com/watch?v=8L_9hXnUzRk\">Incredibles 2 Screen Slaver</a>\n  scene. It is not perfect. If you have other clips that you want this filter to\n  work better on, please report them to us on our <a href=\"http://trac.ffmpeg.org\">trac</a>.\n  </p><p>\n  We are not professionals. Please use this in your medical studies to\n  advance epilepsy research. If you decide to use this in a medical\n  setting, or make a hardware hdmi input output realtime tv filter,\n  or find another use for this, <a href=\"mailto:compn@ffmpeg.org\">please let me know</a>.\n  This filter was a feature request of mine\n  <a href=\"https://trac.ffmpeg.org/ticket/2104\">since 2013</a>.\n  </p><h3>August 5th, 2019, FFmpeg 4.2 \"Ada\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.2\">FFmpeg 4.2 \"Ada\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>AV1 decoding support through libdav1d</li><li>chromashift and rgbashift filters</li><li>truehd_core bitstream filter</li><li>libaribb24 based ARIB STD-B24 caption support (profiles A and C)</li><li>Support decoding of HEVC 4:4:4 content in nvdec and cuviddec</li><li>AV1 frame split bitstream filter</li><li>Support decoding of HEVC 4:4:4 content in vdpau</li><li>showspatial multimedia filter</li><li>mov muxer writes tracks with unspecified language instead of English by default</li><li>added support for using clang to compile CUDA kernels</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>November 6th, 2018, FFmpeg 4.1 \"al-Khwarizmi\"</h3><ul><li>aderivative and aintegral audio filters</li><li>pal75bars and pal100bars video filter sources</li><li>mbedTLS based TLS support</li><li>adeclick and adeclip filters</li><li>libtensorflow backend for DNN based filters like srcnn</li><li>VC1 decoder is now bit-exact</li><li>AVS2 video decoder via libdavs2</li><li>Brooktree ProSumer video decoder</li><li>MatchWare Screen Capture Codec decoder</li><li>WinCam Motion Video decoder</li><li>RemotelyAnywhere Screen Capture decoder</li><li>Support for AV1 in MP4 and Matroska/WebM</li><li>AVS2 video encoder via libxavs2</li><li>Block-Matching 3d (bm3d) denoising filter</li><li>audio denoiser as afftdn filter</li><li>S12M timecode decoding in h264</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>April 20th, 2018, FFmpeg 4.0 \"Wu\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.0\">FFmpeg 4.0 \"Wu\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams</li><li>Experimental MagicYUV encoder</li><li>Intel QSV-accelerated MJPEG encoding</li><li>native aptX and aptX HD encoder and decoder</li><li>NVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decoding</li><li>Intel QSV-accelerated overlay filter</li><li>VAAPI MJPEG and VP8 decoding</li><li>AMD AMF H.264 and HEVC encoders</li><li>support LibreSSL (via libtls)</li><li>Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.</li><li>hilbert audio filter source</li><li>Removed the ffserver program</li><li>Removed the ffmenc and ffmdec muxer and demuxer</li><li>VideoToolbox HEVC encoder and hwaccel</li><li>VAAPI-accelerated ProcAmp (color balance), denoise and sharpness filters</li><li>codec2 en/decoding via libcodec2</li><li>native SBC encoder and decoder</li><li>hapqa_extract bitstream filter</li><li>filter_units bitstream filter</li><li>AV1 Support through libaom</li><li>E-AC-3 dependent frames support</li><li>bitstream filter for extracting E-AC-3 core</li><li>Haivision SRT protocol via libsrt</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>October 15th, 2017, FFmpeg 3.4 \"Cantor\"</h3><ul><li>oscilloscope video filter</li><li>update cuvid/nvenc headers to Video Codec SDK 8.0.14</li><li>scale_cuda CUDA based video scale filter</li><li>librsvg support for svg rasterization</li><li>spec compliant VP9 muxing support in MP4</li><li>sofalizer filter switched to libmysofa</li><li>Gremlin Digital Video demuxer and decoder</li><li>superequalizer audio filter</li><li>additional frame format support for Interplay MVE movies</li><li>support for decoding through D3D11VA in ffmpeg</li><li>Dolby E decoder and SMPTE 337M demuxer</li><li>unpremultiply video filter</li><li>raw G.726 muxer and demuxer, left- and right-justified</li><li>NewTek NDI input/output device</li><li>VP9 tile threading support</li><li>V4L2 mem2mem HW assisted codecs</li><li>Rockchip MPP hardware decoding</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>April 13th, 2017, FFmpeg 3.3 \"Hilbert\"</h3><ul><li>PSD (Photoshop Document) decoder</li><li>FM Screen Capture decoder</li><li>DNxHR decoder fixes for HQX and high resolution videos</li><li>ClearVideo decoder (partial)</li><li>16.8 and 24.0 floating point PCM decoder</li><li>Intel QSV-accelerated VP8 video decoding</li><li>DNxHR 444 and HQX encoding</li><li>Quality improvements for the (M)JPEG encoder</li><li>VAAPI-accelerated MPEG-2 and VP8 encoding</li><li>abitscope multimedia filter</li><li>MPEG-7 Video Signature filter</li><li>add internal ebur128 library, remove external libebur128 dependency</li><li>Intel QSV video scaling and deinterlacing filters</li><li>Sample Dump eXchange demuxer</li><li>MIDI Sample Dump Standard demuxer</li><li>Scenarist Closed Captions demuxer and muxer</li><li>Support MOV with multiple sample description tables</li><li>Pro-MPEG CoP #3-R2 FEC protocol</li><li>Support for spherical videos</li><li>CrystalHD decoder moved to new decode API</li><li>configure now fails if autodetect-libraries are requested but not found</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>October 30th, 2016, Results: Summer Of Code 2016.</h3><p>\n    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.\n  </p><p>\n    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:\n  </p><h4>FFv1 (Mentor: Michael Niedermayer)</h4><p>\n    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.\n  </p><h4>Self test coverage (Mentor: Michael Niedermayer)</h4><p>\n    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.\n  </p><h4>MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)</h4><p>\n    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.\n  </p><h4>Tee muxer improvements (Mentor: Marton Balint)</h4><p>\n    Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.\n  </p><h4>TrueHD encoder (Mentor: Rostislav Pehlivanov)</h4><p>\n    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.\n  </p><h4>Motion interpolation filter (Mentor: Paul B Mahol)</h4><p>\n    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.\n  </p><p>\n    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!\n  </p><h3>September 24th, 2016, SDL1 support dropped.</h3><p>\n    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of\n    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device\n    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output\n    devices have been updated to support SDL2.\n  </p><h3>August 9th, 2016, FFmpeg 3.1.2 \"Laplace\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_3.1\">FFmpeg 3.1.2</a>, a new point release from the 3.1 release branch, is now available!\n    It fixes several bugs.\n  </p><p>\n    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.\n  </p><h3>July 10th, 2016, ffserver program being dropped</h3><p>\n    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.\n    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat\n    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has\n    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.\n    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs\n    and to contact us so we may point users to test and contribute to its development.\n  </p><h3>July 1st, 2016, FFmpeg 3.1.1 \"Laplace\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_3.1\">FFmpeg 3.1.1</a>, a new point release from the 3.1 release branch, is now available!\n    It mainly deals with a few ABI issues introduced in the previous release.\n  </p><p>\n    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to\n    upgrade unless they use current git master.\n  </p><h3>June 27th, 2016, FFmpeg 3.1 \"Laplace\"</h3><ul><li>DXVA2-accelerated HEVC Main10 decoding</li><li>loop video filter and aloop audio filter</li><li>Bob Weaver deinterlacing filter</li><li>protocol blacklisting API</li><li>VC-2 HQ RTP payload format (draft v1) depacketizer and packetizer</li><li>VP9 RTP payload format (draft v2) packetizer</li><li>AudioToolbox audio decoders</li><li>AudioToolbox audio encoders</li><li>coreimage filter (GPU based image filtering on OSX)</li><li>bitstream filter for extracting DTS core</li><li>hash and framehash muxers</li><li>VAAPI-accelerated format conversion and scaling</li><li>libnpp/CUDA-accelerated format conversion and scaling</li><li>Duck TrueMotion 2.0 Real Time decoder</li><li>Wideband Single-bit Data (WSD) demuxer</li><li>VAAPI-accelerated H.264/HEVC/MJPEG encoding</li><li>DTS Express (LBR) decoder</li><li>Generic OpenMAX IL encoder with support for Raspberry Pi</li><li>IFF ANIM demuxer &amp; decoder</li><li>Direct Stream Transfer (DST) decoder</li><li>OpenExr improvements (tile data and B44/B44A support)</li><li>BitJazz SheerVideo decoder</li><li>CUDA CUVID H264/HEVC decoder</li><li>10-bit depth support in native utvideo decoder</li><li>libutvideo wrapper removed</li><li>YUY2 Lossless Codec decoder</li><li>VideoToolbox H.264 encoder</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>March 16th, 2016, Google Summer of Code</h3><p>\n    FFmpeg has been accepted as a <a href=\"https://summerofcode.withgoogle.com/\">Google Summer of Code</a> open source organization. If you wish to\n    participate as a student see our <a href=\"https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2016\">project ideas page</a>.\n    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.\n    Good luck!\n  </p><h3>February 15th, 2016, FFmpeg 3.0 \"Einstein\"</h3><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>January 30, 2016, Removing support for two external AAC encoders</h3><p>\n    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and\n    libaacplus in FFmpeg master.\n  </p><p>\n    Even before marking our internal AAC encoder as\n    <a href=\"https://ffmpeg.org/index.html#aac_encoder_stable\">stable</a>, it was known that libvo-aacenc\n    was of an inferior quality compared to our native one for most samples.\n    However, the VisualOn encoder was used extensively by the Android Open\n    Source Project, and we would like to have a tested-and-true stable option\n    in our code base.\n  </p><p>\n    When first committed in 2011, libaacplus filled in the gap of encoding\n    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported\n    by any of the encoders in FFmpeg at that time.\n  </p><p>\n    The circumstances for both have changed. After the work spearheaded by\n    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC\n    encoder is ready to compete with much more mature encoders. The Fraunhofer\n    FDK AAC Codec Library for Android was added in 2012 as the fourth\n    supported external AAC encoder, and the one with the best quality and the\n    most features supported, including HE-AAC and HE-AACv2.\n  </p><p>\n    Therefore, we have decided that it is time to remove libvo-aacenc and\n    libaacplus. If you are currently using libvo-aacenc, prepare to transition\n    to the native encoder () when updating to the next version\n    of FFmpeg. In most cases it is as simple as merely swapping the encoder\n    name. If you are currently using libaacplus, start using FDK AAC\n    () with an appropriate  option\n    to select the exact AAC profile that fits your needs. In both cases, you\n    will enjoy an audible quality improvement and as well as fewer licensing\n    headaches.\n  </p><h3>January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10</h3><p>\n    We have made several new point releases ().\n    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.\n    Please see the changelog for each release for more details.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>December 5th, 2015, The native FFmpeg AAC encoder is now stable!</h3><p>\n    After seven years the native FFmpeg AAC encoder has had its experimental flag\n    removed and declared as ready for general use. The encoder is transparent\n    at 128kbps for most samples tested with artifacts only appearing in extreme\n    cases. Subjective quality tests put the encoder to be of equal or greater\n    quality than most of the other encoders available to the public.\n  </p><p>\n    Licensing has always been an issue with encoding AAC audio as most of the\n    encoders have had a license making FFmpeg unredistributable if compiled with\n    support for them. The fact that there now exists a fully open and truly\n    free AAC encoder integrated directly within the project means a lot to those\n    who wish to use accepted and widespread standards.\n  </p><p>\n    The majority of the work done to bring the encoder up to quality was started\n    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.\n    Both continued to work on the encoder with the latter joining as a developer\n    and mainainer, working on other parts of the project as well. Also, thanks\n    to <a href=\"http://d.hatena.ne.jp/kamedo2/\">Kamedo2</a> who does comparisons\n    and tests, the original authors and all past and current contributors to the\n    encoder. Users are suggested and encouraged to use the encoder and provide\n    feedback or breakage reports through our <a href=\"https://trac.ffmpeg.org/\">bug tracker</a>.\n  </p><p>\n    A big thank you note goes to our newest supporters: MediaHub and Telepoint.\n    Both companies have donated a dedicated server with free of charge internet\n    connectivity. Here is a little bit about them in their own words:\n  </p><ul><li><p><a href=\"http://www.telepoint.bg/en/\">Telepoint</a> is the biggest\n        carrier-neutral data center in Bulgaria. Located in the heart of Sofia\n        on a cross-road of many Bulgarian and International networks, the\n        facility is a fully featured Tier 3 data center that provides flexible\n        customer-oriented colocation solutions (ranging from a server to a\n        private collocation hall) and a high level of security.\n      </p></li><li><p>\n        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which\n        uses FFmpeg heavily since it started operating a year ago. <i>\"Donating\n        to help keep FFmpeg online is our way of giving back to the community\"\n        </i>.\n      </p></li></ul><p>\n    Thanks Telepoint and MediaHub for their support!\n  </p><h3>September 29th, 2015, GSoC 2015 results</h3><p>\n    FFmpeg participated to the latest edition of\n    the <a href=\"http://www.google-melange.com/gsoc/homepage/google/gsoc2015\">Google\n    Summer of Code</a> Project. FFmpeg got a total of 8 assigned\n    projects, and 7 of them were successful.\n  </p><p>We want to thank <a href=\"https://www.google.com\">Google</a>, the\n    participating students, and especially the mentors who joined this\n    effort. We're looking forward to participating in the next GSoC\n    edition!\n  </p><p>\n    Below you can find a brief description of the final outcome of\n    each single project.\n  </p><h4>Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George</h4><p>\n    Stephan Holljes's project for this session of Google Summer of Code was to\n    implement basic HTTP server features for libavformat, to complement the\n    already present HTTP client and RTMP and RTSP server code.\n  </p><p>\n    The first part of the project was to make the HTTP code capable of accepting\n    a single client; it was completed partly during the qualification period and\n    partly during the first week of the summer. Thanks to this work, it is now\n    possible to make a simple HTTP stream using the following commands:\n  </p><pre>    ffmpeg -i /dev/video0 -listen 1 -f matroska \\\n    -c:v libx264 -preset fast -tune zerolatency http://:8080\n    ffplay http://localhost:8080/\n  </pre><p>\n    The next part of the project was to extend the code to be able to accept\n    several clients, simultaneously or consecutively. Since libavformat did not\n    have an API for that kind of task, it was necessary to design one. This part\n    was mostly completed before the midterm and applied shortly afterwards.\n    Since the ffmpeg command-line tool is not ready to serve several clients,\n    the test ground for that new API is an example program serving hard-coded\n    content.\n  </p><p>\n    The last and most ambitious part of the project was to update ffserver to\n    make use of the new API. It would prove that the API is usable to implement\n    real HTTP servers, and expose the points where more control was needed. By\n    the end of the summer, a first working patch series was undergoing code\n    review.\n  </p><h4>Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek</h4><p>\n    Mariusz finished an API prepared by the FFmpeg community and implemented\n    Samba directory listing as qualification task.\n  </p><p>\n    During the program he extended the API with the possibility to\n    remove and rename files on remote servers. He completed the\n    implementation of these features for file, Samba, SFTP, and FTP\n    protocols.\n  </p><p>\n    At the end of the program, Mariusz provided a sketch of an\n    implementation for HTTP directory listening.\n  </p><h4>Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack</h4><p>\n    Mate was working on directshow input from digital video sources. He\n    got working input from ATSC input sources, with specifiable tuner.\n  </p><p>\n    The code has not been committed, but a patch of it was sent to the\n    ffmpeg-devel mailing list for future use.\n  </p><p>\n    The mentor plans on cleaning it up and committing it, at least for the\n    ATSC side of things. Mate and the mentor are still working trying to\n    finally figure out how to get DVB working.\n  </p><h4>Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale</h4><p>\n    Niklesh's project was to expand our support for 3GPP Timed Text\n    subtitles. This is the native subtitle format for mp4 containers, and\n    is interesting because it's usually the only subtitle format supported\n    by the stock playback applications on iOS and Android devices.\n  </p><p>\n    ffmpeg already had basic support for these subtitles which ignored all\n    formatting information - it just provided basic plain-text support.\n  </p><p>\n    Niklesh did work to add support on both the encode and decode side for\n    text formatting capabilities, such as font size/colour and effects like\n    bold/italics, highlighting, etc.\n  </p><p>\n    The main challenge here is that Timed Text handles formatting in a very\n    different way from most common subtitle formats. It uses a binary\n    encoding (based on mp4 boxes, naturally) and stores information\n    separately from the text itself. This requires additional work to track\n    which parts of the text formatting applies to, and explicitly dealing\n    with overlapping formatting (which other formats support but Timed\n    Text does not) so it requires breaking the overlapping sections into\n    separate non-overlapping ones with different formatting.\n  </p><p>\n    Finally, Niklesh had to be careful about not trusting any size\n    information in the subtitles - and that's no joke: the now infamous\n    Android stagefright bug was in code for parsing Timed Text subtitles.\n  </p><p>\n    All of Niklesh's work is committed and was released in ffmpeg 2.8.\n  </p><h4>libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla</h4><p>\n    Pedro Arthur has modularized the vertical and horizontal scalers.\n    To do this he designed and implemented a generic filter framework\n    and moved the existing scaler code into it. These changes now allow\n    easily adding removing, splitting or merging processing steps.\n    The implementation was benchmarked and several alternatives were\n    tried to avoid speed loss.\n  </p><p>\n    He also added gamma corrected scaling support.\n    An example to use gamma corrected scaling would be:\n  </p><pre>    ffmpeg -i input -vf scale=512:384:gamma=1 output\n  </pre><p>\n    Pedro has done impressive work considering the short time available,\n    and he is a FFmpeg committer now. He continues to contribute to\n    FFmpeg, and has fixed some bugs in libswscale after GSoC has\n    ended.\n  </p><h4>AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire</h4><p>\n    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main\n    prediction on the native AAC encoder. Of all those extensions, only\n    TNS was left in a less-than-usable state, but the implementation has\n    been pushed (disabled) anyway since it's a good basis for further\n    improvements.\n  </p><p>\n    PNS replaces noisy bands with a single scalefactor representing the\n    energy of that band, gaining in coding efficiency considerably, and\n    the quality improvements on low bitrates are impressive for such a\n    simple feature.\n  </p><p>\n    TNS still needs some polishing, but has the potential to reduce coding\n    artifacts by applying noise shaping in the temporal domain (something\n    that is a source of annoying, notable distortion on low-entropy\n    bands).\n  </p><p>\n    Intensity Stereo coding (I/S) can double coding efficiency by\n    exploiting strong correlation between stereo channels, most effective\n    on pop-style tracks that employ panned mixing. The technique is not as\n    effective on classic X-Y recordings though.\n  </p><p>\n    Finally, main prediction improves coding efficiency by exploiting\n    correlation among successive frames. While the gains have not been\n    huge at this point, Rostislav has remained active even after the GSoC,\n    and is polishing both TNS and main prediction, as well as looking for\n    further improvements to make.\n  </p><p>\n    In the process, the MIPS port of the encoder was broken a few times,\n    something he's also working to fix.\n  </p><h4>Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol</h4><p>\n    Donny Yang implemented basic keyframe only APNG encoder as the\n    qualification task. Later he wrote interframe compression via\n    various blend modes. The current implementation tries all blend\n    modes and picks one which takes the smallest amount of memory.\n  </p><p>\n    Special care was taken to make sure that the decoder plays\n    correctly all files found in the wild and that the encoder\n    produces files that can be played in browsers that support APNG.\n  </p><p>\n    During his work he was tasked to fix any encountered bug in the\n    decoder due to the fact that it doesn't match APNG\n    specifications. Thanks to this work, a long standing bug in the\n    PNG decoder has been fixed.\n  </p><p>\n    For latter work he plans to continue working on the encoder,\n    making it possible to select which blend modes will be used in the\n    encoding process. This could speed up encoding of APNG files.\n  </p><h3>September 9th, 2015, FFmpeg 2.8</h3><p>\n    We published release  as new major version.\n    It contains all features and bug fixes of the git master branch from September 8th. Please see\n    the \n    for a list of the most important changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use current git master.\n  </p><h3>August 1st, 2015, A message from the FFmpeg project</h3><p>\n    Dear multimedia community,\n  </p><p>\n    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has\n    come by surprise. He has worked tirelessly on the FFmpeg project for many\n    years and we must thank him for the work that he has done. We hope that in\n    the future he will continue to contribute to the project. In the coming\n    weeks, the FFmpeg project will be managed by the active contributors.\n  </p><p>\n    The last four years have not been easy for our multimedia community - both\n    contributors and users. We should now look to the future, try to find\n    solutions to these issues, and to have reconciliation between the forks,\n    which have split the community for so long.\n  </p><p>\n    Unfortunately, much of the disagreement has taken place in inappropriate\n    venues so far, which has made finding common ground and solutions\n    difficult. We aim to discuss this in our communities online over the coming\n    weeks, and in person at the <a href=\"https://www.videolan.org/videolan/events/vdd15/\">VideoLAN Developer\n    Days</a> in Paris in September: a neutral venue for the entire open source\n    multimedia community.\n  </p><h3>July 4th, 2015, FFmpeg needs a new host</h3><p> We have received more than 7 offers for hosting and servers, thanks a lot to everyone!</p><p>\n    After graciously hosting our projects (<a href=\"http://www.ffmpeg.org\">FFmpeg</a>, <a href=\"http://www.mplayerhq.hu\">MPlayer</a>\n    and <a href=\"http://rtmpdump.mplayerhq.hu\">rtmpdump</a>) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.\n  </p><p>\n    If you want to host an open source project, please let us know, either on <a href=\"http://ffmpeg.org/mailman/listinfo/ffmpeg-devel\">ffmpeg-devel</a>\n    mailing list or irc.freenode.net #ffmpeg-devel.\n  </p><p>\n    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, <a href=\"http://trac.ffmpeg.org\">trac</a>, <a href=\"http://samples.ffmpeg.org\">samples repo</a>, svn, etc.\n  </p><h3>March 16, 2015, FFmpeg 2.6.1</h3><p>\n    We have made a new major release ()\n    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>March 4, 2015, Google Summer of Code</h3><p>\n    FFmpeg has been accepted as a <a href=\"http://www.google-melange.com/gsoc/homepage/google/gsoc2015\">Google Summer of Code</a> Project. If you wish to\n    participate as a student see our <a href=\"https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2015\">project ideas page</a>.\n    You can already get in contact with mentors and start working on qualification tasks. Registration\n    at Google for students will open March 16th. Good luck!\n  </p><h3>March 1, 2015, Chemnitzer Linux-Tage</h3><p>\n    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage\n    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.\n  </p><p>\n    More information can be found <a href=\"https://chemnitzer.linux-tage.de/2015/en/\">here</a></p><p>\n    We demonstrate usage of FFmpeg, answer your questions and listen to\n    your problems and wishes. <strong>If you have media files that cannot be\n    processed correctly with FFmpeg, be sure to have a sample with you\n    so we can have a look!</strong></p><p>\n    For the first time in our CLT history, there will be an !\n    You can read the details <a href=\"https://chemnitzer.linux-tage.de/2015/de/programm/beitrag/209\">here</a>.\n    The workshop is targeted at FFmpeg beginners. First the basics of\n    multimedia will be covered. Thereafter you will learn how to use\n    that knowledge and the FFmpeg CLI tools to analyse and process media\n    files. The workshop is in German language only and prior registration\n    is necessary. The workshop will be on Saturday starting at 10 o'clock.\n  </p><p>\n    We are looking forward to meet you (again)!\n  </p><h3>December 5, 2014, FFmpeg 2.5</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from the 4th December.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>October 10, 2014, FFmpeg is in Debian unstable again</h3><p>\n    We wanted you to know there are\n    <a href=\"https://packages.debian.org/search?keywords=ffmpeg&amp;searchon=sourcenames&amp;suite=unstable&amp;section=main\">\n    FFmpeg packages in Debian unstable</a> again. <strong>A big thank-you\n    to Andreas Cadhalpun and all the people that made it possible.</strong> It has been anything but simple.\n  </p><p>\n    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't\n    migrate to Debian testing to be in the upcoming release codenamed jessie.\n    <a href=\"https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=763148\">Read the argumentation over at Debian.</a></p><p><strong>However things will come out in the end, we hope for your continued remarkable support!</strong></p><h3>October 8, 2014, FFmpeg secured a place in OPW!</h3><p>\n    Thanks to a generous 6K USD donation by Samsung (Open Source Group),\n    FFmpeg will be welcoming at least 1 \"Outreach Program for Women\" intern\n    to work with our community for an initial period starting December 2014\n    (through March 2015).\n  </p><p>\n    We all know FFmpeg is used by the industry, but even while there are\n    countless products building on our code, it is not at all common for\n    companies to step up and help us out when needed. So a big thank-you\n    to Samsung and the OPW program committee!\n  </p><p>\n    If you are thinking on participating in OPW as an intern, please take\n    a look at our <a href=\"https://trac.ffmpeg.org/wiki/SponsoringPrograms/OPW/2014-12\">OPW wiki page</a>\n    for some initial guidelines. The page is still a work in progress, but\n    there should be enough information there to get you started. If you, on\n    the other hand, are thinking on sponsoring work on FFmpeg through the\n    OPW program, please get in touch with us at opw@ffmpeg.org. With your\n    help, we might be able to secure some extra intern spots for this round!\n  </p><h3>September 15, 2014, FFmpeg 2.4</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from the 14th September.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8</h3><p>\n    We have made several new point releases ().\n    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.\n    Please see the changelog for more details.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>July 29, 2014, Help us out securing our spot in OPW</h3><p>\n    Following our previous post regarding our participation on this year's\n    OPW (Outreach Program for Women), we are now reaching out to our users\n    (both individuals and companies) to help us gather the needed money to\n    secure our spot in the program.\n    We need to put together 6K USD as a minimum but securing more funds would\n    help us towards getting more than one intern.<p>\n    You can donate by credit card using\n    </p><a href=\"https://co.clickandpledge.com/advanced/default.aspx?wid=56226\">\n    Click&amp;Pledge</a> and selecting the \"OPW\" option. If you would like to\n    donate by money transfer or by check, please get in touch by\n    <a href=\"mailto:opw@ffmpeg.org\">e-mail</a> and we will get back to you\n    with instructions.Thanks!\n  </p><h3>July 20, 2014, New website</h3><p>\n    The FFmpeg project is proud to announce a brand new version of the website\n    made by <a href=\"http://db0.fr\">db0</a>. While this was initially motivated\n    by the need for a larger menu, the whole website ended up being redesigned,\n    and most pages got reworked to ease navigation. We hope you'll enjoy\n    browsing it.\n  </p><h3>July 17, 2014, FFmpeg 2.3</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from the 16th July.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>July 3, 2014, FFmpeg and the Outreach Program For Women</h3><p>\n    FFmpeg has started the process to become an OPW includer organization for the\n    next round of the program, with internships starting December 9. The\n    <a href=\"https://gnome.org/opw/\">OPW</a> aims to \"Help women (cis and trans)\n    and genderqueer to get involved in free and open source software\". Part of the\n    process requires securing funds to support at least one internship (6K USD), so\n    if you were holding on your donation to FFmpeg, this is a great chance for you\n    to come forward, get in touch and help both the project and a great initiative!\n  </p><p>\n    We have set up an <a href=\"mailto:opw@ffmpeg.org\">email address</a> you can use\n    to contact us about donations and general inquires regarding our participation\n    in the program. Hope to hear from you soon!\n  </p><h3>June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14</h3><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><p>\n    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will\n    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a\n    different location closer to the city center.\n  </p><p>\n    We will have a shared booth with XBMC and VideoLAN.\n    <b>\n      If you have media files that cannot be processed correctly with\n      FFmpeg, be sure to have a sample with you so we can have a look!\n    </b></p><p>\n    More information about LinuxTag can be found <a href=\"http://www.linuxtag.org/2014/\">here</a></p><p>\n    We are looking forward to see you in Berlin!\n  </p><h3>April 18, 2014, OpenSSL Heartbeat bug</h3><p>\n    Our server hosting the Trac issue tracker was vulnerable to the attack\n    against OpenSSL known as \"heartbleed\". The OpenSSL software library\n    was updated on 7th of April, shortly after the vulnerability was publicly\n    disclosed. We have changed the private keys (and certificates) for all\n    FFmpeg servers. The details were sent to the mailing lists by\n    Alexander Strasser, who is part of the project server team. Here is a\n    link to the user mailing list\n    <a href=\"https://lists.ffmpeg.org/pipermail/ffmpeg-user/2014-April/020968.html\">archive</a>\n    .\n  </p><p>\n    We encourage you to read up on\n    <a href=\"https://www.schneier.com/blog/archives/2014/04/heartbleed.html\">\"OpenSSL heartbleed\"</a>.\n    <b>It is possible that login data for the issue tracker was exposed to\n      people exploiting this security hole. You might want to change your password\n      in the tracker and everywhere else you used that same password.</b></p><h3>April 11, 2014, FFmpeg 2.2.1</h3><p>\n    We have made a new point releases ().\n    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as\n    several other fixes.\n    See the git log for details.\n  </p><h3>March 24, 2014, FFmpeg 2.2</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from 1st March.\n    A partial list of new stuff is below:\n  </p><pre>    - HNM version 4 demuxer and video decoder\n    - Live HDS muxer\n    - setsar/setdar filters now support variables in ratio expressions\n    - elbg filter\n    - string validation in ffprobe\n    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)\n    - complete Voxware MetaSound decoder\n    - remove mp3_header_compress bitstream filter\n    - Windows resource files for shared libraries\n    - aeval filter\n    - stereoscopic 3d metadata handling\n    - WebP encoding via libwebp\n    - ATRAC3+ decoder\n    - VP8 in Ogg demuxing\n    - side &amp; metadata support in NUT\n    - framepack filter\n    - XYZ12 rawvideo support in NUT\n    - Exif metadata support in WebP decoder\n    - OpenGL device\n    - Use metadata_header_padding to control padding in ID3 tags (currently used in\n    MP3, AIFF, and OMA files), FLAC header, and the AVI \"junk\" block.\n    - Mirillis FIC video decoder\n    - Support DNx444\n    - libx265 encoder\n    - dejudder filter\n    - Autodetect VDA like all other hardware accelerations\n  </pre><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>February 3, 2014, Chemnitzer Linux-Tage</h3><p>\n    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'\n    in Chemnitz, Germany. The event will take place on 15th and 16th of March.\n  </p><p>\n    More information can be found <a href=\"http://chemnitzer.linux-tage.de/2014/en/info/\">here</a></p><p>\n    We invite you to visit us at our booth located in the Linux-Live area!\n    There we will demonstrate usage of FFmpeg, answer your questions and listen to\n    your problems and wishes.\n  </p><p><b>\n      If you have media files that cannot be processed correctly with\n      FFmpeg, be sure to have a sample with you so we can have a look!\n    </b></p><p>\n    We are looking forward to meet you (again)!\n  </p><h3>February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach</h3><p>\n    The server on which FFmpeg and MPlayer Trac issue trackers were\n    installed was compromised. The affected server was taken offline\n    and has been replaced and all software reinstalled.\n    FFmpeg Git, releases, FATE, web and mailinglists are on other servers\n    and were not affected. We believe that the original compromise happened\n    to a server, unrelated to FFmpeg and MPlayer, several months ago.\n    That server was used as a source to clone the VM that we recently moved\n    Trac to. It is not known if anyone used the backdoor that was found.\n  </p><p>\n    We recommend all users to change their passwords.\n    <b>Especially users who use a password on Trac that they also use\n      elsewhere, should change that password at least elsewhere.</b></p><h3>November 12, 2013, FFmpeg RFP in Debian</h3><p>\n    Since the splitting of Libav the Debian/Ubuntu maintainers have followed\n    the Libav fork. Many people have requested the packaging of ffmpeg in\n    Debian, as it is more feature-complete and in many cases less buggy.\n  </p><p><a href=\"http://cynic.cc/blog/\">Rogério Brito</a>, a Debian developer,\n    has proposed a Request For Package (RFP) in the Debian bug tracking\n    system.\n  </p><p>\n    Please let the Debian and Ubuntu developers know that you support packaging\n    of the real FFmpeg! See Debian <a href=\"http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=729203\">ticket #729203</a>\n    for more details.\n  </p><h3>October 28, 2013, FFmpeg 2.1</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from 28th October.\n    A partial list of new stuff is below:\n  </p><pre>    - aecho filter\n    - perspective filter ported from libmpcodecs\n    - ffprobe -show_programs option\n    - compand filter\n    - RTMP seek support\n    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate\n    even when used as an input option. Previous behavior can be restored with\n    the -noaccurate_seek option.\n    - ffmpeg -t option can now be used for inputs, to limit the duration of\n    data read from an input file\n    - incomplete Voxware MetaSound decoder\n    - read EXIF metadata from JPEG\n    - DVB teletext decoder\n    - phase filter ported from libmpcodecs\n    - w3fdif filter\n    - Opus support in Matroska\n    - FFV1 version 1.3 is stable and no longer experimental\n    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support\n    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be\n    more consistent with other muxers.\n    - adelay filter\n    - pullup filter ported from libmpcodecs\n    - ffprobe -read_intervals option\n    - Lossless and alpha support for WebP decoder\n    - Error Resilient AAC syntax (ER AAC LC) decoding\n    - Low Delay AAC (ER AAC LD) decoding\n    - mux chapters in ASF files\n    - SFTP protocol (via libssh)\n    - libx264: add ability to encode in YUVJ422P and YUVJ444P\n    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does\n    - make decoding alpha optional for prores, ffv1 and vp6 by setting\n    the skip_alpha flag.\n    - ladspa wrapper filter\n    - native VP9 decoder\n    - dpx parser\n    - max_error_rate parameter in ffmpeg\n    - PulseAudio output device\n    - ReplayGain scanner\n    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)\n    - Linux framebuffer output device\n    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4\n    - mergeplanes filter\n  </pre><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p>","contentLength":60817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44985730"},{"title":"AWS CEO says using AI to replace junior staff is 'Dumbest thing I've ever heard'","url":"https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/","date":1755780796,"author":"JustExAWS","guid":247,"unread":true,"content":"<p>Amazon Web Services CEO Matt Garman has suggested firing junior workers because AI can do their jobs is \"the dumbest thing I've ever heard.\"</p><p>Garman made that remark in <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=nfocTxMzOP4\">conversation</a> with AI investor Matthew Berman, during which he talked up AWS’s <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/18/aws_updated_kiro_pricing/\">Kiro AI-assisted coding tool</a> and said he's encountered business leaders who think AI tools \"can replace all of our junior people in our company.\"</p><p>That notion led to the “dumbest thing I've ever heard” quote, followed by a justification that junior staff are “probably the least expensive employees you have” and also the most engaged with AI tools.</p><p>“How's that going to work when ten years in the future you have no one that has learned anything,” he asked. “My view is you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it, just as much as you ever have.”</p><p>Naturally he thinks AI – and Kiro, natch – can help with that education.</p><p>Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization.</p><p>“It’s a silly metric,” he said, because while organizations can use AI to write “infinitely more lines of code” it could be bad code.</p><p>“Often times fewer lines of code is way better than more lines of code,” he observed. “So I'm never really sure why that's the exciting metric that people like to brag about.”</p><p>That said, he’s seen data that suggests over 80 percent of AWS’s developers use AI in some way.</p><p>“Sometimes it's writing unit tests, sometimes it's helping write documentation, sometimes it's writing code, sometimes it's kind of an agentic workflow” in which developers collaborate with AI agents.</p><p>Garman said usage of AI tools by AWS developers increases every week.</p><p>The CEO also offered some career advice for the AI age, suggesting that kids these days need to learn how to learn – and not just learn specific skills.</p><p>“I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?”</p><p>Garman thinks that approach is necessary because technological development is now so rapid it’s no longer sensible to expect that studying narrow skills can sustain a career for 30 years. He wants educators to instead teach “how do you think and how do you decompose problems”, and thinks kids who acquire those skills will thrive. ®</p>","contentLength":2582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44972151"},{"title":"Mark Zuckerberg freezes AI hiring amid bubble fears","url":"https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/","date":1755774248,"author":"pera","guid":246,"unread":true,"content":"<div data-test=\"article-body-text\"><p>Stock market volatility was largely prompted by a report from the Massachusetts Institute of Technology, which claimed that 95pc of companies were getting “zero return” on their AI investments.</p><p>A Meta spokesman sought to downplay the freeze, saying: “All that’s happening here is some basic organisational planning: creating a solid structure for our new superintelligence efforts after bringing people on board and undertaking yearly budgeting and planning exercises.”</p><p>It comes after the company has been offering top researchers at rival companies, including OpenAI and Google, enormous pay deals to join Meta Superintelligence Labs as Mr Zuckerberg seeks to dominate the field.</p><p>The company’s billionaire chief executive has become personally involved in developing cutting-edge AI after the disappointing release of its latest systems, personally messaging top researchers at Silicon Valley AI companies.</p><p>However, the division has been disrupted by repeated strategy overhauls, which led to the delayed release of its latest “Behemoth” AI model.</p><p>Mr Zuckerberg has said he wants to develop a “personal superintelligence” that acts as a permanent superhuman assistant and lives in smart glasses.</p><p>“We believe in putting this power in people’s hands to direct it towards what they value in their own lives,” he wrote last month.</p><p>“This is distinct from others in the industry who believe superintelligence should be directed centrally towards automating all valuable work, and then humanity will live on a dole of its output.”</p><p>Mr Zuckerberg recently told investors that he wanted “small, talent-dense teams” to be driving its AI work, rather than large groups of researchers.</p><p>Despite this, the company has said that the cost of paying staff will significantly increase in the coming years. Analysts at Morgan Stanley warned this week that the pay surge may “dilute shareholder value without any clear innovation gains”.</p><p>Concerns about AI progress have been amplified by the modest response to GPT-5, the much-anticipated new version of ChatGPT.</p><p>Sam Altman, OpenAI’s chief executive, has compared hype around AI to the <a href=\"https://www.telegraph.co.uk/business/2025/08/21/we-may-be-facing-dotcom-bubble-2/\">dotcom bubble</a> at the turn of the century.</p></div>","contentLength":2186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971273"},{"title":"Why are anime catgirls blocking my access to the Linux kernel?","url":"https://lock.cmpxchg8b.com/anubis.html","date":1755701685,"author":"taviso","guid":245,"unread":true,"content":"<p><em>Hey… quick question, why are anime catgirls blocking my access to\nthe Linux kernel?</em></p><section><p>I’ve started running into more sites recently that deploy <a href=\"https://github.com/TecharoHQ/anubis\">Anubis</a>, a sort of hybrid\nart project slash network countermeasure. The project “weighs the souls”\nof HTTP requests to help protect the web from AI crawlers.</p><p>If you’ve seen anime catgirl avatars when visiting a new website,\nthat’s Anubis.</p><p>I’m sympathetic to the cause – I host this blog on a single core\n128MB VPS, I can tell you some stories about aggressive crawlers!</p><p>Anubis recently started blocking how I access <a href=\"https://git.kernel.org/\">git.kernel.org</a> and <a href=\"https://lore.kernel.org\">lore.kernel.org</a>. Those sites host the\n<a href=\"https://en.wikipedia.org/wiki/LKML\">Linux Kernel Mailing\nList</a> archive and the kernel git repositories. As far as I know I do\nhave a soul, I just wasn’t using a desktop browser… so how exactly is my\nsoul being weighed?</p><blockquote><p>Note: Linux has Tux 🐧, OpenBSD has Puffy 🐡, SuSE has Geeko 🦎 and\nMicrosoft has Bob 🤓… nothing wrong with mascots! 😸</p></blockquote></section><section><p>The traditional solution to blocking nuisance crawlers is to use a\ncombination of <a href=\"https://en.wikipedia.org/wiki/Rate_limiting\">rate limiting</a> and\n<a href=\"https://en.wikipedia.org/wiki/Captcha\">CAPTCHAs</a>. The\nCAPTCHA forces vistors to solve a problem designed to be very difficult\nfor computers but trivial for humans. This isn’t perfect of course, we\ncan debate the accessibility tradeoffs and weaknesses, but conceptually\nthe idea makes some sense.</p><p>Anubis – confusingly – inverts this idea. It insists visitors solve a\nproblem trivial for computers, but impossible for humans. Visitors are\nasked to brute force a value that when appended to a challenge string,\ncauses it’s SHA-256 to begin with a few zero nibbles.</p><blockquote><div><pre><code></code></pre></div></blockquote><p>If that sounds familiar, it’s because it’s similar to how bitcoin\nmining works. Anubis is not literally mining cryptocurrency, but it\n similar in concept to other projects that do exactly that,\nperhaps most famously <a href=\"https://torrentfreak.com/the-pirate-bay-website-runs-a-cryptocurrency-miner-170916/\">Coinhive</a>\nand <a href=\"https://github.com/JSEcoin/website/blob/master/README.md\">JSECoin</a>.</p><p>So how do some useless SHA-256 operations prove you’re not a bot? The\nargument goes that this simply makes it too expensive to crawl your\nwebsite.</p><p>This… makes no sense to me. Almost by definition, an AI vendor will\nhave a datacenter full of compute capacity. It feels like this solution\nhas the problem backwards, effectively only limiting access to those\n resources or trying to conserve them.</p></section><section><p>Let’s assume the argument has some merit and math out the claims.</p><p>We can see that with the default Anubis configuration, a typical\nwebsite visitor will have to solve a challenge with a difficulty of\n4.</p><blockquote><div><pre><code></code></pre></div></blockquote><p>This means that a visitor must make the first 4 hex digits of the\nchallenge hash be zero, so 16 bits (4 digits, one nibble each).\nTherefore, you can expect to mine a suitable  within\n2^16 SHA-256 operations.</p><p>If every single github star on the anubis project represents a\nwebsite that has deployed Anubis, how much would the cloud services bill\nbe to mine enough tokens to crawl every single website?</p><p>At the time of writing, Anubis has 11,508 github stars.</p><p>The default configuration means mining one token gets you access for\n7 days <small>(although I think this expiration check is broken, see\nbelow)</small>, so we need 11,508 * 2^16 SHA-256 operations per week,\nhow expensive is that?</p><p>To get some numbers, I started an  vm on Google\nCompute Engine, and ran . This is what you get\nin the <a href=\"https://cloud.google.com/free/docs/free-cloud-features#compute\">free\ntier</a>.</p><pre><code>$ openssl speed sha256\nDoing sha256 for 3s on 16 size blocks: 6915549 sha256's in 3.00s\nDoing sha256 for 3s on 64 size blocks: 4631718 sha256's in 3.00s\nDoing sha256 for 3s on 256 size blocks: 393694 sha256's in 3.21s\nDoing sha256 for 3s on 1024 size blocks: 100123 sha256's in 3.00s\nDoing sha256 for 3s on 8192 size blocks: 13300 sha256's in 2.98s\nDoing sha256 for 3s on 16384 size blocks: 7137 sha256's in 2.99s\nversion: 3.0.17\nbuilt on: Tue Aug  5 07:09:41 2025 UTC\noptions: bn(64,64)\ncompiler: gcc -fPIC -pthread -m64 ...\nThe 'numbers' are in 1000s of bytes per second processed.\ntype             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes\nsha256           36882.93k    98809.98k    31397.40k    34175.32k    36561.61k    39107.90k</code></pre><p>It looks like we can test about 2^21 every second, perhaps a bit more\nif we used both SMT sibling cores. This amount of compute is simply too\ncheap to even be worth billing for.</p><p>So (11508 websites * 2^16 sha256 operations) / 2^21, that’s about 6\nminutes to mine enough tokens for every single Anubis deployment in the\nworld. That means the cost of unrestricted crawler access to the\ninternet for a week is approximately $0.</p><p>In fact, I don’t think we reach a single cent per month in compute\ncosts until several million sites have deployed Anubis.</p><p>I’m just not convinced this math works… this is \nnothing for a souless AI vendor with a monthly cloud services budget in\nthe 8 figures. However, the cost for real soul-owning humans with\nlimited access to compute is high – the Anubis forums are full of\ncomplaints like these:</p></section><section><p>Anubis cites <a href=\"https://en.wikipedia.org/wiki/Hashcash#Applications\">hashcash</a>\nas the primary inspiration for their design, an anti-spam solution from\nthe 90s that was never widely adopted.</p><p>The idea of “weighing souls” reminded me of another anti-spam\nsolution from the 90s… believe it or not, there was once a company that\nused poetry to block spam!</p><p><a href=\"https://web.archive.org/web/20030210085930/http://habeas.com/services/swe.htm\">Habeas</a>\nwould license short haikus to companies to embed in email headers. They\nwould then aggressively sue anyone who reproduced their poetry <a href=\"https://web.archive.org/web/20030207155559/http://www.habeas.com/faq/index.htm#5\">without\na license</a>. The idea was you can safely deliver any email with their\nheader, because it was too legally risky to use it in spam.</p><blockquote><p>winter into spring  brightly anticipated </p></blockquote></section><section><p>So you’re trying to read LKML, but catgirl says no… is there a\nsolution?</p><p>My issue is I don’t want to use a desktop browser to mine the\nrequired value, so how can I get the auth cookie?</p><p>If we look at the response with , we can see the\nchallenge in the HTTP headers:</p><pre><code>$ curl -I https://lore.kernel.org/\nHTTP/2 200\nserver: nginx\nset-cookie: techaro.lol-anubis-auth=; Path=/\nset-cookie: techaro.lol-anubis-cookie-test-if-you-block-this-anubis-wont-work=5d737f0600ff2dd; Path=/</code></pre><p>That <code>techaro.lol-anubis-cookie</code> is the challenge, here is\na quick C program to mine an acceptable token:</p><div><pre><code></code></pre></div><p>Let’s run that and see what it says…</p><pre><code>$ gcc -Ofast -march=native anubis-miner.c -lcrypto -o anubis-miner\n$ time ./anubis-miner 5d737f0600ff2dd\n47224\n\nreal    0m0.017s\nuser    0m0.016s\nsys     0m0.000s</code></pre><p>Looks okay, let’s verify that solution is correct:</p><pre><code>$ printf \"5d737f0600ff2dd%d\" 47224 | sha256sum\n000043f7c4392a781a04419a7cb503089ebcf3164e2b1d4258b3e6c15b8b07f1  -</code></pre><p>It seems valid, so now we can get a signed auth cookie by sending\nback the value we mined:</p><pre><code>$ curl -I --cookie \"techaro.lol-anubis-cookie-test-if-you-block-this-anubis-wont-work=5d737f0600ff2dd\" \\\n    'https://lore.kernel.org/.within.website/x/cmd/anubis/api/pass-challenge?response=000043f7c4392a781a04419a7\ncb503089ebcf3164e2b1d4258b3e6c15b8b07f1&amp;nonce=47224&amp;redir=/&amp;elapsedTime=0'\nHTTP/2 302\nserver: nginx\nlocation: /\nset-cookie: techaro.lol-anubis-auth=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJhY3Rpb24iO...OTYifQ...;</code></pre><p>Success, this cookie is now valid for 1 week of access. Let’s\nvalidate what it sent us, the actual schema is visible in the code:</p><blockquote><div><pre><code></code></pre></div></blockquote><p>We can examine this auth token and see what Anubis gave us…</p><pre><code>$ base64 -d &lt;&lt;&lt; eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9 | jq\n{\n  \"alg\": \"EdDSA\",\n  \"typ\": \"JWT\"\n}\n$ base64 -d &lt;&lt;&lt; eyJhY3Rpb24iO...OTYifQ== | jq\n{\n  \"action\": \"CHALLENGE\",\n  \"challenge\": \"5d737f0600ff2dd\",\n  \"exp\": 1756185722,\n  \"iat\": 1755580922,\n  \"method\": \"fast\",\n  \"nbf\": 1755580862,\n  \"policyRule\": \"dbf942088788cc96\"\n}</code></pre><p>It looks like  is the expiry date, so\n, which is…</p><pre><code>$ date --date @1756185722\nMon Aug 25 22:22:02 PDT 2025</code></pre><p>Yep, about 7 days from the date I requested it. You can now place\nthat into a cookie file for , etc.</p><blockquote><p>Interestingly, sending the same request the next day got me a new\nsigned cookie!?</p><p>This seems like a bug – exchanging a mined token for an auth cookie\nshould immediately remove the challenge from the store, or there is a <a href=\"https://en.wikipedia.org/wiki/Double-spending\">double spend</a>\nvulnerability.</p><p>This error benefits me, I have to mine less tokens, but I’ll open an\nissue 😇</p><p>Update: wow, <a href=\"https://github.com/TecharoHQ/anubis/pull/1003\">fixed</a> just a\nfew minutes after opening an issue by the maintainer!</p></blockquote><p>This dance to get access is just a minor annoyance for me, but I\nquestion how it proves I’m not a bot. These steps can be trivially and\ncheaply automated.</p><p>I think the end result is just an internet resource I need is a\nlittle harder to access, and we have to waste a small amount of\nenergy.</p></section><section><ul><li>I wrote this article with my own puny brain, I didn’t use any AI. I\nknow there are  (they’re \nemdashes!) – I have a habit of writing two consecutive dashes, which <a href=\"https://pandoc.org/demo/example33/7.1-typography.html\">pandoc</a>\nconverts into <a href=\"https://unicode-explorer.com/c/2013\">U+2013</a>.</li><li>This post is a bit critical of a small well-intentioned project, so\nI felt obliged to email the maintainer to discuss it before posting it\nonline. I didn’t hear back.</li></ul></section>","contentLength":8647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962529"},{"title":"Show HN: I was curious about spherical helix, ended up making this visualization","url":"https://visualrambling.space/moving-objects-in-3d/","date":1755698567,"author":"damarberlari","guid":174,"unread":true,"content":"<p>MOVING OBJECTS IN 3D SPACE</p><p>tap/click the right side of the screen to go forward →</p><h3>Have you ever wondered how to move objects along a spherical helix path?</h3><p>Okay… probably not, right?</p><p>But one morning, this question popped into my head.</p><p>It stuck with me long enough that I ended up diving into a few articles about it.</p><p>From there, it spiraled into lots of explorations, trying to figure out how to move objects in 3D space.</p><p>...to this complex, chaotic path.</p><p>All these explorations made me want to share what I learned with you.</p><p>I hope you enjoy this as much as I did.</p><p>A helix is a shape that loops around and around, like a spring.</p><h3>In a spherical helix, it loops around a sphere.</h3><h3>To move an object along a spherical helix path, we need to define its 3D coordinates to follow a helical pattern around a sphere.</h3><p>We'll get there! But first, let’s see how to position and move objects in 3D space.</p><p>In 3D space, we position objects by setting its coordinates along three axes: x,y, and z.</p><p>The x-axis typically represents horizontal movement—left or right.</p><p>The y-axis typically represents vertical movement—up or down.</p><p>The z-axis typically represents depth—forward or backward</p><p>To move an object in 3D space, we can use mathematical functions to set its position over time.</p><p>For example, this cube's x position is set to 10 * cos(πt/2), where t is time (in seconds).</p><p>The result? It oscillates from 10 to -10 along the x-axis every 2 seconds, following a cosine wave.</p><p>Similarly, setting the y position to 10 * cos(πt/2) makes the cube oscillates vertically, from 10 to -10 every 2 seconds.</p><p>We can create a two-dimensional path by setting the x and y positions to different functions.</p><p>For this circle, the x position is set to 10 * cos(πt/2).</p><p>The cube starts at x = 10, moves to -10 in 2 seconds, then back to 10, and so on.</p><p>Meanwhile, the y position is set to 10 * sin(πt/2).</p><p>The movement for x and y may look similar, but they are actually out of phase.</p><p>When x = 10, y = 0; when x = 0, y = 10; and so on.</p><p>Together, these two functions create a circular path for the cube.</p><p>Now we can get creative with functions to create even more complex paths.</p><p>For example, let's multiply the x function by 0.03 * t.</p><p>It would make the cube oscillates farther on the x-axis over time.</p><p>..and we will have a circular path whose radius grows over time.</p><h3>Okay, now it's time to talk about the spherical helix (finally!)</h3><h3>The spherical helix path is similar to the spiral we just made, but with some differences.</h3><h3>First, a spherical helix is three-dimensional.</h3><p>It has a z component that changes over time.</p><p>This cube's z position is set as 10 * cos(0.02 * πt).</p><p>It will start from z = 10 then slowly move to -10.</p><p>Second, unlike the previous spiral, the x and y positions don’t grow indefinitely.</p><p>They grow at first, then shrink halfway through.</p><p>This is because the x function is multiplied by another sine function: sin(0.02 * πt)</p><p>which makes the radius larger in the middle and smaller at the ends.</p><p>The same is also done to the y function.</p><h3>Together, these functions create a spherical helix.</h3><h3>By updating the cube’s position with these functions, it moves along a spherical helix path.</h3><p>In summary, we can move objects in 3D space by defining their x, y, z coordinates as functions of time.</p><p>These functions, which express x, y, z coordinates as a function of another variable (in this case, time), are called parametric equations.</p><p>Check out the Wikipedia article for more on parametric equations.</p><p>Now that we know this, we can get creative and move objects along any path we want!</p><p>...to this complex, chaotic path...</p><p>...which we know now isn't actually chaotic.</p><p>It's just a path defined by mathematical functions.</p><p>Thanks for sticking with me, I hope you enjoyed it!</p><p>visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.</p><p>If you like this, please consider following me on Twitter and sharing this with your friends.</p><p>I'm planning to write more articles like this, so stay tuned!</p><p>https://twitter.com/damarberlari</p>","contentLength":4018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962066"},{"title":"Copilot broke audit logs, but Microsoft won't tell customers","url":"https://pistachioapp.com/blog/copilot-broke-your-audit-log","date":1755649080,"author":"Sayrus","guid":244,"unread":true,"content":"<p>Like most tech companies, Microsoft is going all-in on AI. Their flagship AI product, Copilot (in all its various forms), allows people to utilize AI in their daily work to interact with Microsoft services and generally perform tasks. Unfortunately, this also creates a wide range of new security problems.</p><p>On July 4th, I came across a problem in M365 Copilot: Sometimes it would access a file and return the information, but the audit log would not reflect that. Upon testing further, I discovered that I could simply ask Copilot to behave in that manner, and it would. That made it possible to access a file without leaving a trace. Given the problems that creates, both for security and legal compliance, I immediately reported it to Microsoft through their MSRC portal.</p><p>Helpfully, Microsoft provides <a target=\"_blank\" rel=\"noreferrer\" href=\"https://msrc.microsoft.com/blog/2023/07/what-to-expect-when-reporting-vulnerabilities-to-microsoft/\">a clear guide on what to expect</a> when reporting vulnerabilities to them. Less helpfully, they didn’t follow that guide at all. The entire process has been a mess. And while they did fix the issue, classifying this issue as an ‘important’ vulnerability, they also decided not to notify customers or publicize that this happened. What that means is that your audit log is wrong, and Microsoft doesn’t plan on telling you that.</p><p>This post is split into three parts. The first part explains the Copilot vulnerability and the problems it can cause. The second part outlines how Microsoft handled the case. And the third part discusses Microsoft’s decision not to publish this information, and why I consider that to be a huge disservice to Microsoft’s customers.</p><h2>The Vulnerability: Copilot and Audit Logging</h2><p>The vulnerability here is extremely simple. Normally, if you ask M365 Copilot to summarize a file for you, it will give you a summary and the audit log will show that Copilot accessed that file on your behalf.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-1\" data-discover=\"true\">[1]</a></p><p>That’s good. Audit logs are important. Imagine someone downloaded a bunch of files before leaving your company to start a competitor; you’d want some record of that, and it would be bad if the person could use Copilot to go undetected.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-2\" data-discover=\"true\">[2]</a> Or maybe your company has sensitive personal data, and you need a strict log of who accessed those files for legal and compliance purposes; again, you’d need to know about access that occurred via Copilot. That’s just two examples. Organizations rely on having an accurate audit log.</p><p>But what happens if you ask Copilot to not provide you with a link to the file it summarized? Well, in that case, the audit log is empty.</p><p>Just like that, your audit log is wrong. For a malicious insider, avoiding detection is as simple as asking Copilot.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-3\" data-discover=\"true\">[3]</a></p><p>You might be thinking, “Yikes, but I guess not too many people figured that out, so it’s probably fine.” Unfortunately, you’d be wrong. When I found this, I wasn’t searching for ways to break the audit log. Instead, I was simply trying to trigger the audit log so I could test functionality we are developing at Pistachio, and I noticed it was unreliable. In other words, this can happen by chance.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-4\" data-discover=\"true\">[4]</a> So if your organization has M365 Copilot licenses, your audit log is probably wrong.</p><p> It turns out that <a target=\"_blank\" rel=\"noreferrer\" href=\"https://x.com/mbrg0\">Michael Bargury</a>, the CTO at <a target=\"_blank\" rel=\"noreferrer\" href=\"https://zenity.io/\">Zenity</a>, found this a year ago and disclosed it, and Microsoft still didn’t fix it (hence my report). He gave a really good talk on the topic, amongst other bad AI stuff. The relevant part is <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.youtube.com/embed/FH6P288i2PE?start=643\">here</a>.</p><p>I had never reported a vulnerability to Microsoft before, and my initial reaction to the process was fairly positive. The fact that I could submit something already felt unusually friendly by Microsoft’s standards. And like I mentioned, they even had a guide on what to expect.</p><p>Unfortunately, nothing went according to plan. On July 7th my report’s status was changed to “reproducing”, but when I went to provide more evidence on July 10th the functionality had changed. That isn’t Microsoft’s policy; they’re meant to reproduce, then move to “develop” when they start working on a fix. Seeing the functionality change while still in “reproducing” made me think Microsoft was going to get back to me and claim they couldn’t reproduce the issue, when actually they had simply fixed it based on my report.</p><p>So I asked MSRC what was happening, and instead of responding with a simple explanation, they changed the status of the report to “develop” and said nothing. Up until that point I thought Microsoft was going to follow a process, and coordinate with me if they had to deviate from that. Instead, it felt like the process was less a reflection of what was really happening, and more akin to the Domino’s Pizza Tracker for security researchers. The statuses aren’t real.</p><p>On August 2nd, Microsoft informed me that a full fix would be released on August 17th, that I would be free to disclose as of August 18th. I then asked when a CVE number would be issued, and I was told:</p><blockquote><p>CVEs are given to fixes deployed in security releases when customers need to take action to stay protected. In this case, the mitigation will be automatically pushed to Copilot, where users do not need to manually update the product and a CVE will not be assigned.</p></blockquote><p>That is not Microsoft’s policy at all, which I pointed out to them by <a target=\"_blank\" rel=\"noreferrer\" href=\"https://msrc.microsoft.com/blog/2024/06/toward-greater-transparency-unveiling-cloud-service-cves/\">linking to their own policy</a>. MSRC then wrote back, “I understand you may not have full visibility into how MSRC approaches these cases”, as if I was the wrong one. They then explained that the vulnerability is classified as “important”, not “critical”, and that is why they will not issue a CVE.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-5\" data-discover=\"true\">[5]</a> Of course, they had not told me that they had classified the vulnerability at all prior to that point.</p><h2>Microsoft’s Decision to Say Nothing</h2><p>If Microsoft isn’t issuing a CVE for this vulnerability, how are they going to inform customers about it? The answer is that they’re not going to. On a call on August 14th, Microsoft told me that they had no plans to disclose this.<a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log#footnote-6\" data-discover=\"true\">[6]</a></p><p>I strongly feel that is wrong. It might be okay to move on silently if this was some esoteric exploit, but the reality is that it is so easy that it basically happens by accident. If you work at an organization that used Copilot prior to August 18th, there is a very real chance that your audit log is incomplete.</p><p>Do organizations not need to know that? What about companies that are subject to HIPAA and are relying on Microsoft’s audit log to satisfy some of the <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.law.cornell.edu/cfr/text/45/164.312\">technical safeguard requirements</a>? Do they not get to know, despite Microsoft claiming M365 Copilot can be <a target=\"_blank\" rel=\"noreferrer\" href=\"https://learn.microsoft.com/en-us/copilot/microsoft-365/enterprise-data-protection\">HIPAA compliant</a>? There are almost certainly other regulated entities with similar requirements, and they also won’t be told.</p><p>There are so many cases in which organizations rely on audit logs to detect, investigate, and respond to incidents. There are lawsuits where audit logs are used as important evidence. The US government even made an <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.cisa.gov/news-events/news/when-tech-vendors-make-important-logging-info-available-free-everyone-wins\">issue out of Microsoft charging more for audit logs</a>, with a US senator <a target=\"_blank\" rel=\"noreferrer\" href=\"https://www.cybersecuritydive.com/news/microsoft-free-security-logs-Outlook-email-hack-backlash/688388/\">shaming Microsoft</a> and referring to audit logging as an essential security feature.</p><p>And now Microsoft is saying that even though the audit log was very plausibly wrong for any customer using Copilot, no one needs to know? This raises serious questions about what other problems Microsoft chooses to silently sweep under the rug.</p>","contentLength":7188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44957454"},{"title":"AGENTS.md – Open format for guiding coding agents","url":"https://agents.md/","date":1755648903,"author":"ghuntley","guid":243,"unread":true,"content":"<code><div>- Use  to jump to a package instead of scanning with .</div><div>- Run  to add the package to your workspace so Vite, ESLint, and TypeScript can see it.</div><div>- Use  to spin up a new React + Vite package with TypeScript checks ready.</div><div>- Check the name field inside each package's package.json to confirm the right name—skip the top-level one.</div><div>- Find the CI plan in the .github/workflows folder.</div><div>- Run  to run every check defined for that package.</div><div>- From the package root you can just call . The commit should pass all tests before you merge.</div><div>- To focus on one step, add the Vitest pattern: .</div><div>- Fix any test or type errors until the whole suite is green.</div><div>- After moving files or changing imports, run  to be sure ESLint and TypeScript rules still pass.</div><div>- Add or update tests for the code you change, even if nobody asked.</div><div>- Title format: [&lt;project_name&gt;] &lt;Title&gt;</div><div>- Always run  and  before committing.</div></code>","contentLength":872,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44957443"},{"title":"Anna's Archive: An Update from the Team","url":"https://annas-archive.org/blog/an-update-from-the-team.html","date":1755534708,"author":"jerheinze","guid":242,"unread":true,"content":"<p>\n    annas-archive.li/blog, 2025-08-17\n  </p><p>We are still alive and kicking. In recent weeks we’ve seen increased attacks on our mission. We are taking steps to harden our infrastructure and operational security. The work of securing humanity’s legacy is worth fighting for.</p><p>Since we started in 2022, we have liberated tens of millions of books, scientific articles, magazines, newspapers, and more. These are now forever protected from destruction by natural disasters, wars, budget cuts, and other catastrophes, thanks to everyone who helps with torrenting.</p><p>Anna’s Archive itself has organized some of the largest scrapes: we acquired tens of millions of files from IA Controlled Digital Lending, HathiTrust, DuXiu, and many more.</p><p>We have also scraped and published the largest book metadata collections in history: WorldCat, Google Books, and others. With this we’ll be able to identify which books are still missing from our collections, and prioritize saving the rarest ones.</p><p>Much thanks to all of our volunteers for making these projects happen.</p><p>We’ve forged some incredible partnerships. We’ve partnered with two LibGen forks, STC/Nexus, Z-Library. We’ve secured tens of millions additional files through these partnerships. And they are helping the mission by mirroring our files.</p><p>Unfortunately we have seen the disappearance of one of the LibGen forks. We don’t have further information about what happened there, but are saddened by this development.</p><p>There is a new entrant: WeLib. They appear to have mirrored most of our collection, and use a fork of our codebase. We have copied some of their user interface improvements, and are grateful for that push. Sadly, we are not seeing them share any new collections, nor share their codebase improvements. Since they haven’t shown commitment to contributing back to the ecosystem, we advise extreme caution. <em>We recommend not using them.</em></p><p>In the meantime, we have some exciting projects in the works. We have hundreds of terabytes in new collections sitting on our servers, waiting to be processed. If you’re at all interested in helping out, feel free to check out our Volunteering and Donate pages. We run all of this on a minimal budget, so any help is greatly appreciated.</p>","contentLength":2239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44942501"},{"title":"Good system design","url":"https://www.seangoedecke.com/good-system-design/","date":1755329926,"author":"dondraper36","guid":241,"unread":true,"content":"<p>I see a lot of bad system design advice. One classic is the LinkedIn-optimized “bet you never heard of ” style of post, presumably aimed at people who are new to the industry. Another is the Twitter-optimized “you’re a terrible engineer if you ever store booleans in a database” clever trick. Even good system design advice can be kind of bad. I love <em>Designing Data-Intensive Applications</em>, but I don’t think it’s particularly useful for most system design problems engineers will run into.</p><p>What is system design? In my view, if software design is how you assemble lines of code, system design is how you assemble . The primitives of software design are variables, functions, classes, and so on. The primitives of system design are app servers, databases, caches, queues, event buses, proxies, and so on. </p><p>This post is my attempt to write down, in broad strokes, everything I know about good system design. A lot of the concrete judgment calls do come down to experience, which I can’t convey in this post. But I’m trying to write down what I can.</p><p>What does good system design look like? I’ve written before that it <a href=\"https://www.seangoedecke.com/great-software-design\">looks underwhelming</a>. In practice, it looks like nothing going wrong for a long time. You can tell that you’re in the presence of good design if you have thoughts like “huh, this ended up being easier than I expected”, or “I never have to think about this part of the system, it’s fine”. Paradoxically, good design is self-effacing: bad design is often more impressive than good. I’m always suspicious of impressive-looking systems. If a system has distributed-consensus mechanisms, many different forms of event-driven communication, CQRS, and other clever tricks, I wonder if there’s some fundamental bad decision that’s being compensated for (or if the system is just straightforwardly over-designed).</p><p>I’m often alone on this. Engineers look at complex systems with many interesting parts and think “wow, a lot of system design is happening here!” In fact, a complex system usually reflects an absence of good design. I say “usually” because sometimes you do need complex systems. I’ve worked on many systems that earned their complexity. However, a complex system that works always evolves from a simple system that works. Beginning from scratch with a complex system is a really bad idea.</p><p>The hard part about software design is state. If you’re storing any kind of information for any amount of time, you have a lot of tricky decisions to make about how you save, store and serve it. If you’re not storing information, your app is “stateless”. As a non-trivial example, GitHub has an internal API that takes a PDF file and returns a HTML rendering of it. That’s a real stateless service. Anything that writes to a database is stateful.</p><p>You should try and minimize the amount of stateful components in any system. (In a sense this is trivially true, because you should try to minimize the amount of  components in a system, but stateful components are particularly dangerous.) The reason you should do this is that <strong>stateful components can get into a bad state</strong>. Our stateless PDF-rendering service will safely run forever, as long as you’re doing broadly sensible things: e.g. running it in a restartable container so that if anything goes wrong it can be automatically killed and restored to working order. A stateful service can’t be automatically repaired like this. If your database gets a bad entry in it (for instance, an entry with a format that triggers a crash in your application), you have to manually go in and fix it up. If your database runs out of room, you have to figure out some way to prune unneeded data or expand it.</p><p>What this means in practice is having one service that knows about the state - i.e. it talks to a database - and other services that do stateless things. Avoid having five different services all write to the same table. Instead, have four of them send API requests (or emit events) to the first service, and keep the writing logic in that one service. If you can, it’s worth doing this for the read logic as well, although I’m less absolutist about this. It’s  better for services to do a quick read of the  table than to make a 2x slower HTTP request to an internal sessions service.</p><p>Since managing state is the most important part of system design, the most important component is usually where that state lives: the database. I’ve spent most of my time working with SQL databases (MySQL and PostgreSQL), so that’s what I’m going to talk about.</p><p>If you need to store something in a database, the first thing to do is define a table with the schema you need. Schema design should be flexible, because once you have thousands or millions of records, it can be an enormous pain to change the schema. However, if you make it too flexible (e.g. by sticking everything in a “value” JSON column, or using “keys” and “values” tables to track arbitrary data) you load a ton of complexity into the application code (and likely buy some very awkward performance constraints). Drawing the line here is a judgment call and depends on specifics, but in general I aim to have my tables be human-readable: you should be able to go through the database schema and get a rough idea of what the application is storing and why.</p><p>If you expect your table to ever be more than a few rows, you should put indexes on it. Try to make your indexes match the most common queries you’re sending (e.g. if you query by  and , create an index with those two fields). Indexes work like nested dictionaries, so make sure to put the highest-cardinality fields first (otherwise each index lookup will have to scan all users of  to find the one with the right ). Don’t index on every single thing you can think of, since each index adds write overhead.</p><p>Accessing the database is often the bottleneck in high-traffic applications. This is true even when the compute side of things is relatively inefficient (e.g. Ruby on Rails running on a preforking server like Unicorn). That’s because complex applications need to make a  of database calls - hundreds and hundreds for every single request, often sequentially (because you don’t know if you need to check whether a user is part of an organization until after you’ve confirmed they’re not abusive, and so on). How can you avoid getting bottlenecked?</p><p>When querying the database, . It’s almost always more efficient to get the database to do the work than to do it yourself. For instance, if you need data from multiple tables,  them instead of making separate queries and stitching them together in-memory. Particularly if you’re using an ORM, beware accidentally making queries in an inner loop. That’s an easy way to turn a <code>select id, name from table</code> to a  and a hundred <code>select name from table where id = ?</code>.</p><p>Every so often you do want to break queries apart. It doesn’t happen often, but I’ve run into queries that were ugly enough that it was easier on the database to split them up than to try to run them as a single query. I’m sure it’s always possible to construct indexes and hints such that the database can do it better, but the occasional tactical query-split is a tool worth having in your toolbox.</p><p>Send as many read queries as you can to database replicas. A typical database setup will have one write node and a bunch of read-replicas. The more you can avoid reading from the write node, the better - that write node is already busy enough doing all the writes. The exception is when you really, really can’t tolerate any replication lag (since read-replicas are always running at least a handful of ms behind the write node). But in most cases replication lag can be worked around with simple tricks: for instance, when you update a record but need to use it right after, you can fill in the updated details in-memory instead of immediately re-reading after a write.</p><p>Beware spikes of queries (particularly write queries, and  transactions). Once a database gets overloaded, it gets slow, which makes it more overloaded. Transactions and writes are good at overloading databases, because they require a lot of database work for each query. If you’re designing a service that might generate massive query spikes (e.g. some kind of bulk-import API), consider throttling your queries.</p><h3>Slow operations, fast operations</h3><p>A service has to do some things fast. If a user is interacting with something (say, an API or a web page), they should see a response within a few hundred ms. But a service has to do other things that are slow. Some operations just take a long time (converting a very large PDF to HTML, for instance). The general pattern for this is splitting out <strong>the minimum amount of work needed to do something useful for the user</strong> and doing the rest of the work in the background. In the PDF-to-HTML example, you might render the first page to HTML immediately and queue up the rest in a background job.</p><p>What’s a background job? It’s worth answering this in detail, because “background jobs” are a core system design primitive. Every tech company will have some kind of system for running background jobs. There will be two main components: a collection of queues, e.g. in Redis, and a job runner service that will pick up items from the queues and execute them. You enqueue a background job by putting an item like  on the queue. It’s also possible to schedule background jobs to run at a set time (which is useful for periodic cleanups or summary rollups). Background jobs should be your first choice for slow operations, because they’re typically such a well-trodden path.</p><p>Sometimes you want to roll your own queue system. For instance, if you want to enqueue a job to run in a month, you probably shouldn’t put an item on the Redis queue. Redis persistence is typically not guaranteed over that period of time (and even if it is, you likely want to be able to query for those far-future enqueued jobs in a way that would be tricky with the Redis job queue). In this case, I typically create a database table for the pending operation with columns for each param plus a  column. I then use a daily job to check for these items with , and either delete them or mark them as complete once the job has finished.</p><p>Sometimes an operation is slow because it needs to do an expensive (i.e. slow) task that’s the same between users. For instance, if you’re calculating how much to charge a user in a billing service, you might need to do an API call to look up the current prices. If you’re charging users per-use (like OpenAI does per-token), that could (a) be unacceptably slow and (b) cause a lot of traffic for whatever service is serving the prices. The classic solution here is : only looking up the prices every five minutes, and storing the value in the meantime. It’s easiest to cache in-memory, but using some fast external key-value store like Redis or Memcached is also popular (since it means you can share one cache across a bunch of app servers).</p><p>The typical pattern is that junior engineers learn about caching and want to cache , while senior engineers want to cache as little as possible. Why is that? It comes down to the first point I made about the danger of statefulness. A cache is a source of state. It can get weird data in it, or get out-of-sync with the actual truth, or cause mysterious bugs by serving stale data, and so on. You should never cache something without first making a serious effort to speed it up. For instance, it’s silly to cache an expensive SQL query that isn’t covered by a database index. You should just add the database index!</p><p>I use caching a lot. One useful caching trick to have in the toolbox is using a scheduled job and a document storage like S3 or Azure Blob Storage as a large-scale persistent cache. If you need to cache the result of a  expensive operation (say, a weekly usage report for a large customer), you might not be able to fit the result in Redis or Memcached. Instead, stick a timestamped blob of the results in your document storage and serve the file directly from there. Like the database-backed long-term queue I mentioned above, this is an example of using the caching  without using a specific cache technology.</p><p>As well as some kind of caching infrastructure and background job system, tech companies will typically have an . The most common implementation of this is Kafka. An event hub is just a queue - like the one for background jobs - but instead of putting “run this job with these params” on the queue, you put “this thing happened” on the queue. One classic example is firing off a “new account created” event for each new account, and then having multiple services consume that event and take some action: a “send a welcome email” service, a “scan for abuse” service, a “set up per-account infrastructure” service, and so on.</p><p>You shouldn’t overuse events. Much of the time it’s better to just have one service make an API request to another service: all the logs are in the same place, it’s easier to reason about, and you can immediately see what the other service responded with. Events are good for when the code sending the event doesn’t necessarily care what the consumers do with the event, or when the events are high-volume and not particularly time-sensitive (e.g. abuse scanning on each new Twitter post).</p><p>When you need data to flow from one place to a lot of other places, there are two options. The simplest is to . This is how most websites work: you have a server that owns some data, and when a user wants it they make a request (via their browser) to the server to pull that data down to them. The problem here is that users might do a lot of pulling down the same data - e.g. refreshing their email inbox to see if they have any new emails, which will pull down and reload the entire web application instead of just the data about the emails.</p><p>The alternative is to . Instead of allowing users to ask for the data, you allow them to register as clients, and then when the data changes, the server pushes the data down to each client. This is how GMail works: you don’t have to refresh the page to get new emails, because they’ll just appear when they arrive.</p><p>If we’re talking about background services instead of users with web browsers, it’s easy to see why pushing can be a good idea. Even in a very large system, you might only have a hundred or so services that need the same data. For data that doesn’t change much, it’s much easier to make a hundred HTTP requests (or RPC, or whatever) whenever the data changes than to serve up the same data a thousand times a second.</p><p>Suppose you did need to serve up-to-date data to a million clients (like GMail, does). Should those clients be pushing or pulling? It depends. Either way, you won’t be able to run it all from a single server, so you’ll need to farm it out to other components of the system. If you’re pushing, that will likely mean sticking each push on an event queue and having a horde of event processors each pulling from the queue and sending out your pushes. If you’re pulling, that will mean standing up a bunch (say, a hundred) of fast read-replica cache servers that will sit in front of your main application and handle all the read traffic.</p><p>When you’re designing a system, there are lots of different ways users can interact with it or data can flow through it. It can get a bit overwhelming. The trick is to mainly focus on the “hot paths”: the part of the system that is most critically important, and the part of the system that is going to handle the most data. For instance, in a metered billing system, those pieces might be the part that decides whether or not a customer gets charged, and the part that needs to hook into all user actions on the platform to identify how much to charge.</p><p>Hot paths are important because they have fewer possible solutions than other design areas. There are a thousand ways you can build a billing settings page and they’ll all mainly work. But there might be only a handful of ways that you can sensibly consume the firehose of user actions. Hot paths also go wrong more spectacularly. You have to really screw up a settings page to take down the entire product, but any code you write that’s triggered on all user actions can easily cause huge problems.</p><p>How do you know if you’ve got problems? One thing I’ve learned from my most paranoid colleagues is to log aggressively during unhappy paths. If you’re writing a function that checks a bunch of conditions to see if a user-facing endpoint should respond 422, you should log out the condition that was hit. If you’re writing billing code, you should log every decision made (e.g. “we’re not billing for this event because of X”). Many engineers don’t do this because it adds a bunch of logging boilerplate and makes it hard to write beautifully elegant code, but you should do it anyway. You’ll be happy you did when an important customer is complaining that they’re getting a 422 - even if that customer did something wrong, you still need to figure out  for them.</p><p>You should also have basic observability into the operational parts of the system. That means CPU/memory on the hosts or containers, queue sizes, average time per-request or per-job, and so on. For user-facing metrics like time per-request, you also need to watch the p95 and p99 (i.e. how slow your slowest requests are). Even one or two very slow requests are scary, because they’re disproportionately from your largest and most important users. If you’re just looking at averages, it’s easy to miss the fact that some users are finding your service unusable.</p><h3>Killswitches, retries, and failing gracefully</h3><p>I wrote a <a href=\"https://www.seangoedecke.com/killswitches\">whole post</a> about killswitches that I won’t repeat here, but the gist is that you should think carefully about what happens when the system fails badly.</p><p>Retries are not a magic bullet. You need to make sure you’re not putting extra load on other services by blindly retrying failed requests. If you can, put high-volume API calls inside a “circuit breaker”: if you get too many 5xx responses in a row, stop sending requests for a while to let the service recover. You also need to make sure you’re not retrying write events that may or may not have succeeded (for instance, if you send a “bill this user” request and get back a 5xx,  if the user has been billed or not). The classic solution to this is to use an “idempotency key”, which is a special UUID in the request that the other service uses to avoid re-running old requests: every time they do something, they save the idempotency key, and if they get another request with the same key, they silently ignore it. </p><p>It’s also important to decide what happens when part of your system fails. For instance, say you have some rate limiting code that checks a Redis bucket to see if a user has made too many requests in the current window. What happens when that Redis bucket is unavailable? You have two options: fail  and let the request through, or fail  and block the request with a 429.</p><p>Whether you should fail open or closed depends on the specific feature. In my view, a rate limiting system should almost always fail open. That means that a problem with the rate limiting code isn’t necessarily a big user-facing incident. However, auth should (obviously) always fail closed: it’s better to deny a user access to their own data than to give a user access to some other user’s data. There are a lot of cases where it’s not clear what the right behavior is. It’s often a difficult tradeoff.</p><p>There are some topics I’m deliberately not covering here. For instance, whether or when to split your monolith out into different services, when to use containers or VMs, tracing, good API design. Partly this is because I don’t think it matters that much (in my experience, monoliths are fine), or because I think it’s too obvious to talk about (you should use tracing), or because I just don’t have the time (API design is complicated).</p><p>The main point I’m trying to make is what I said at the start of this post: good system design is not about clever tricks, it’s about knowing how to use boring, well-tested components in the right place. I’m not a plumber, but I imagine good plumbing is similar: if you’re doing something too exciting, you’re probably going to end up with crap all over yourself.</p><p>Especially at large tech companies, where these components already exist off the shelf (i.e. your company already has some kind of event bus, caching service, etc), good system design is going to look like nothing. There are very, very few areas where you want to do the kind of system design you could talk about at a conference. They do exist! I have seen hand-rolled data structures make features possible that wouldn’t have been possible otherwise. But I’ve only seen that happen once or twice in ten years. I see boring system design every single day.</p>","contentLength":21077,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44921137"},{"title":"Steve Wozniak: Life to me was never about accomplishment, but about happiness","url":"https://yro.slashdot.org/comments.pl?sid=23765914&cid=65583466","date":1755195582,"author":"MilnerRoute","guid":240,"unread":true,"content":"<p>The sooner all the animals are extinct, the sooner we'll find their money.\n- Ed Bluestone</p>","contentLength":89,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44903803"},{"title":"Streaming services are driving viewers back to piracy","url":"https://www.theguardian.com/film/2025/aug/14/cant-pay-wont-pay-impoverished-streaming-services-are-driving-viewers-back-to-piracy","date":1755190591,"author":"nemoniac","guid":239,"unread":true,"content":"<p>ith a trip to Florence booked, all I want is to rewatch <a href=\"https://www.theguardian.com/tv-and-radio/2016/apr/04/medici-drama-premiere-cannes-tv-festival-dustin-hoffman-renaissance\" data-link-name=\"in body link\">Medici</a>. The 2016 historical drama series tells of the rise of the powerful Florentine banking dynasty, and with it, the story of the Renaissance. Until recently, I could simply have gone to Netflix and found it there, alongside a wide array of award-winning and obscure titles. But when I Google the show in 2025, the Netflix link only takes me to a blank page. I don’t see it on HBO Max, Disney+, Apple TV+, or any of the smaller streaming platforms. On Amazon Prime I am required to buy each of the three seasons or 24 episodes separately, whereupon they would be stored in a library subject to <a href=\"https://www.theguardian.com/media/article/2024/may/14/my-whole-library-is-wiped-out-what-it-means-to-own-movies-and-tv-in-the-age-of-streaming-services?utm_source=chatgpt.com\" data-link-name=\"in body link\">overnight deletion</a>. Raised in the land of The Pirate Bay, the Swedish torrent index, I feel, for the first time in a decade, a nostalgia for the high seas of digital piracy. And I am not alone.</p><figure data-spacefinder-role=\"inline\" data-spacefinder-type=\"model.dotcomrendering.pageElements.DisclaimerBlockElement\"></figure><p>For my teenage self in the 00s, torrenting was the norm. Need the new Coldplay album on your iPod? The <a href=\"https://www.theguardian.com/technology/pirate-bay\" data-link-name=\"in body link\" data-component=\"auto-linked-tag\">Pirate Bay</a>. The 1968 adaptation of Romeo and Juliet? The Pirate Bay. Whatever you needed was accessible with just a couple of clicks. But as smartphones proliferated, so did Spotify, the music streaming platform that is also headquartered in Sweden. The same Scandinavian country had become a hub of illegal torrenting and simultaneously conjured forth its solution.</p><p>“Spotify would never have seen the light of day without The Pirate Bay,” Per Sundin, the then managing director of Universal Music Sweden, <a href=\"https://www.svd.se/a/2ca5d142-d3a9-3d76-8a3c-d86946e3111c/piratkrigets-forlorare\" data-link-name=\"in body link\">reflected in 2011</a>. But music torrenting died out as we all either listened with ads or paid for the subscription. And when Netflix launched in Sweden in late 2012, open talk of torrenting moving images also stopped. Most of the big shows and a great collection of award-winning films could all be found for just 79 SEK (£6) a month. Meanwhile, the three founders of The Pirate Bay were <a href=\"https://www.theguardian.com/technology/2015/jun/02/last-remaining-pirate-bay-founder-freed-from-jail-fredrik-neij\" data-link-name=\"in body link\">arrested and eventually jailed</a>. Pirating faded into the history books as far as I was concerned.</p><p>A decade and a half on from <a href=\"https://en.wikipedia.org/wiki/The_Pirate_Bay_trial\" data-link-name=\"in body link\">the Pirate Bay trial</a>, the winds have begun to shift. On an unusually warm summer’s day, I sit with fellow film critics by the old city harbour, once a haven for merchants and, rumour has it, smugglers. Cold  in hand (that’s what they call pints up here), they start venting about the “<a href=\"https://www.theguardian.com/commentisfree/2023/mar/11/users-advertisers-we-are-all-trapped-in-the-enshittification-of-the-internet\" data-link-name=\"in body link\">enshittification</a>” of streaming – enshittification being the process by which platforms degrade their services and ultimately die in the pursuit of profit. Netflix now costs upwards of 199 SEK (£15), and you need more and more subscriptions to watch the same shows you used to find in one place. Most platforms now offer plans that, despite the fee, force advertisements on subscribers. Regional restrictions often compel users to use VPNs to access the full selection of available content. The average European household now spends close to €700 (£600) a year on three or more VOD subscriptions. People pay more and get less.</p><p>A fellow film critic confides anonymously: “I never stopped pirating, and my partner also does it if he doesn’t find the precise edition he is looking for on DVD.” While some people never abandoned piracy, others admit they have recently returned – this time turning to unofficial streaming platforms. One commonly used app is legal but can, through community add-ons, channel illicit streams. “Downloading is too difficult. I don’t know where to start,” says one film viewer. “The shady streams might bombard me with ads, but at least I don’t have to worry about getting hacked or caught.”</p><p>“Piracy is not a pricing issue,” Gabe Newell, the co-founder of Valve, the company behind the world’s largest PC gaming platform, Steam, <a href=\"https://www.gamesradar.com/gabe-newell-piracy-issue-service-not-price/\" data-link-name=\"in body link\">observed in 2011</a>. “It’s a service issue.” Today, the crisis in streaming makes this clearer than ever. With titles scattered, prices on the rise, and <a href=\"https://www.forbes.com/sites/barrycollins/2020/05/12/the-netflix-lottery-how-your-browser-choice-affects-streaming-quality/\" data-link-name=\"in body link\">bitrates throttled depending on your browser</a>, it is little wonder some viewers are raising the jolly roger again. Studios carve out fiefdoms, build walls and levy tolls for those who wish to visit. The result is artificial scarcity in a digital world that promised abundance.</p><p>Whether piracy today is rebellion or resignation is almost irrelevant; the sails are hoisted either way. As the streaming landscape fractures into feudal territories, more viewers are turning to the high seas. The Medici understood the value linked to access. A client could travel from Rome to London and still draw on their credit, thanks to a network built on trust and interoperability. If today’s studios want to survive the storm, they may need to rediscover that truth.</p>","contentLength":4557,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44902797"},{"title":"Gemma 3 270M: Compact model for hyper-efficient AI","url":"https://developers.googleblog.com/en/introducing-gemma-3-270m/","date":1755187716,"author":"meetpateltech","guid":238,"unread":true,"content":"<img src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3-270M_Wagtail_RD2-V02.original.jpg\" alt=\"Gemma 3 270M\"><div><p data-block-key=\"8637c\">The last few months have been an exciting time for the Gemma family of open models. We introduced <a href=\"https://blog.google/technology/developers/gemma-3/\">Gemma 3</a> and <a href=\"https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/\">Gemma 3 QAT</a>, delivering state-of-the-art performance for single cloud and desktop accelerators. Then, we announced the full release of <a href=\"https://developers.googleblog.com/en/introducing-gemma-3n/\">Gemma 3n</a>, a mobile-first architecture bringing powerful, real-time multimodal AI directly to edge devices. Our goal has been to provide useful tools for developers to build with AI, and we continue to be <a href=\"https://www.youtube.com/watch?v=Fx6IuEggeac\">amazed</a> by the vibrant <a href=\"https://deepmind.google/models/gemma/gemmaverse/\">Gemmaverse</a> you are helping create, celebrating together as downloads surpassed 200 million last week.</p><p data-block-key=\"6eq2f\">Today, we're adding a new, highly specialized tool to the Gemma 3 toolkit: <a href=\"https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune\">Gemma 3 270M</a>, a compact, 270-million parameter model designed from the ground up for task-specific fine-tuning with strong instruction-following and text structuring capabilities already trained in.</p></div><div><h2 data-block-key=\"v6pc7\">Core capabilities of Gemma 3 270M</h2><ul><li data-block-key=\"4ki9m\"><b>Compact and capable architecture:</b> Our new model has a total of 270 million parameters: 170 million embedding parameters due to a large vocabulary size and 100 million for our transformer blocks. Thanks to the large vocabulary of 256k tokens, the model can handle specific and rare tokens, making it a strong base model to be further fine-tuned in specific domains and languages.</li></ul><ul><li data-block-key=\"394ff\"><b>Extreme energy efficiency:</b> A key advantage of Gemma 3 270M is its low power consumption. Internal tests on a Pixel 9 Pro SoC show the INT4-quantized model used just 0.75% of the battery for 25 conversations, making it our most power-efficient Gemma model.</li></ul><ul><li data-block-key=\"5c37m\"> An instruction-tuned model is released alongside a pre-trained checkpoint. While this model is not designed for complex conversational use cases, it’s a strong model that follows general instructions right out of the box.</li></ul><p data-block-key=\"cr9ib\">In engineering, success is defined by efficiency, not just raw power. You wouldn't use a sledgehammer to hang a picture frame. The same principle applies to building with AI.</p><p data-block-key=\"2mh3o\">Gemma 3 270M embodies this \"right tool for the job\" philosophy. It's a high-quality foundation model that follows instructions well out of the box, and its true power is unlocked through fine-tuning. Once specialized, it can execute tasks like text classification and data extraction with remarkable accuracy, speed, and cost-effectiveness. By starting with a compact, capable model, you can build production systems that are lean, fast, and dramatically cheaper to operate.</p><h2 data-block-key=\"o52yv\">A real-world blueprint for success</h2><p data-block-key=\"9ef55\">The power of this approach has already delivered incredible results in the real world. A perfect example is <a href=\"https://deepmind.google/models/gemma/gemmaverse/adaptiveml/\">the work done by Adaptive ML with SK Telecom.</a> Facing the challenge of nuanced, multilingual content moderation, they chose to specialize. Instead of using a massive, general-purpose model, Adaptive ML fine-tuned a Gemma 3 4B model. The results were stunning: the specialized Gemma model not only met but exceeded the performance of much larger proprietary models on its specific task.</p><p data-block-key=\"8htle\">Gemma 3 270M is designed to let developers take this approach even further, unlocking even greater efficiency for well-defined tasks. It's the perfect starting point for creating a fleet of small, specialized models, each an expert at its own task.</p></div><div><div>Gemma 3 270M used to power a Bedtime Story Generator web app using Transformers.js. The model’s size and performance make it suitable for offline, web-based, creative tasks. (Credit: Joshua (@xenovacom on X) from the Hugging Face team)</div></div><div><h2 data-block-key=\"0mwz8\">When to choose Gemma 3 270M</h2><p data-block-key=\"88i9k\">Gemma 3 270M inherits the advanced architecture and robust pre-training of the Gemma 3 collection, providing a solid foundation for your custom applications.</p><p data-block-key=\"5p4a2\">Here’s when it’s the perfect choice:</p><ul><li data-block-key=\"dp1oc\"><b>You have a high-volume, well-defined task.</b> Ideal for functions like sentiment analysis, entity extraction, query routing, unstructured to structured text processing, creative writing, and compliance checks.</li></ul><ul><li data-block-key=\"fb2n2\"><b>You need to make every millisecond and micro-cent count.</b> Drastically reduce, or eliminate, your inference costs in production and deliver faster responses to your users. A fine-tuned 270M model can run on lightweight, inexpensive infrastructure or directly on-device.</li></ul><ul><li data-block-key=\"7oe6a\"><b>You need to iterate and deploy quickly.</b> The small size of Gemma 3 270M allows for rapid fine-tuning experiments, helping you find the perfect configuration for your use case in hours, not days.</li></ul><ul><li data-block-key=\"e03e8\"><b>You need to ensure user privacy.</b> Because the model can run entirely on-device, you can build applications that handle sensitive information without ever sending data to the cloud.</li></ul><ul><li data-block-key=\"1jppn\"><b>You want a fleet of specialized task models.</b> Build and deploy multiple custom models, each expertly trained for a different task, without breaking your budget.</li></ul><h2 data-block-key=\"12vtd\">Get started with fine-tuning</h2><p data-block-key=\"1jvma\">We want to make it as easy as possible to turn Gemma 3 270M into your own custom solution. It’s built on the same architecture as the rest of the Gemma 3 models, with recipes and tools to get you started quickly. You can find our guide on <a href=\"https://ai.google.dev/gemma/docs/core/huggingface_text_full_finetune\">full fine-tuning</a> using Gemma 3 270M as part of the Gemma docs.</p><p data-block-key=\"1fc66\">The Gemmaverse is built on the idea that innovation comes in all sizes. With Gemma 3 270M, we’re empowering developers to build smarter, faster, and more efficient AI solutions. We can’t wait to see the specialized models you create.</p></div>","contentLength":5190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44902148"},{"title":"Why LLMs can't really build software","url":"https://zed.dev/blog/why-llms-cant-build-software","date":1755177969,"author":"srid","guid":237,"unread":true,"content":"<p>One of the things I have spent a lot of time doing is interviewing software engineers. This is obviously a hard task, and I don’t claim to have a magic solution; but it’s given me some time to reflect on what effective software engineers actually do.</p><p>When you watch someone who knows what they are doing, you'll see them looping over the following steps:</p><ol><li>Build a mental model of the requirements</li><li>Write code that (hopefully?!) does that</li><li>Build a mental model of what the code actually does</li><li>Identify the differences, and update the code (or the requirements).</li></ol><p>There are lots of different ways to do these things, but the distinguishing factor of effective engineers is their ability to build and maintain clear mental models.</p><p>To be fair, LLMs are quite good at writing code. They're also reasonably good at updating code when you identify the problem to fix. They can also do all the things that real software engineers do: read the code, write and run tests, add logging, and (presumably) use a debugger.</p><p>But what they cannot do is maintain clear mental models.</p><p>LLMs get endlessly confused: they assume the code they wrote actually works; when test fail, they are left guessing as to whether to fix the code or the tests; and when it gets frustrating, they just delete the whole lot and start over.</p><p>This is exactly the opposite of what I am looking for.</p><p>Software engineers test their work as they go. When tests fail, they can check in with their mental model to decide whether to fix the code or the tests, or just to gather more data before making a decision. When they get frustrated, they can reach for help by talking things through. And although sometimes they do delete it all and start over, they do so with a clearer understanding of the problem.</p><p>Will this change as models become more capable? Perhaps?? But I think it's going to require a change in how models are built and optimized. Software engineering requires models that can do more than just generate code.</p><p>When a person runs into a problem, they are able to temporarily stash the full context, focus on resolving the issue, and then pop their mental stack to get back to the problem in hand. They are also able to zoom out and focus on the big picture, allowing the details to temporarily disappear, diving into small pieces as necessary. We don't just keep adding more words to our context window, because it would drive us mad.</p><p>Even if it wasn't just too much context to deal with, we know that current generative models suffer from several issues that directly impact their ability to maintain clear mental models:</p><ul></ul><p>These are hopefully not insurmountable problems, and work is being done on adding <a href=\"https://research.ibm.com/blog/memory-augmented-LLMs\">memory</a> to let them perform similar mental tricks to us.\nUnfortunately, for now, they cannot (<a href=\"https://machinelearning.apple.com/research/illusion-of-thinking\">beyond a certain complexity</a>) actually understand what is going on.</p><p>They cannot build software because they cannot maintain two similar \"mental models\", identify the differences, and figure out whether or not to update the code or the requirements.</p><p>Clearly LLMs are useful to software engineers. They can quickly generate code, and they are excellent at synthesizing requirements and documentation. For some tasks this is enough: the requirements are clear enough, and the problems are simple enough, that they can one-shot the whole thing.</p><p>That said, for anything non-trivial, they are not capable of maintaining enough context accurately enough to iterate to a working solution. You, the software engineer, are responsible for ensuring that the requirements are clear, and that the code actually does what it purports to do.</p><p>At Zed we believe in a world where people and agents can collaborate together to build software. But, we firmly believe that (at least for now) you are in the drivers seat, and the LLM is just another tool to reach for.</p><div><h3>Looking for a better editor?</h3><p>If you're passionate about the topics we cover on our blog, please consider <a href=\"https://zed.dev/jobs\">joining our team</a> to help us ship the future of software development.</p></div>","contentLength":3962,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44900116"},{"title":"Nginx introduces native support for ACME protocol","url":"https://blog.nginx.org/blog/native-support-for-acme-protocol","date":1755099715,"author":"phickey","guid":236,"unread":true,"content":"<p>We are very excited to announce the preview release of <a href=\"https://github.com/nginx/nginx-acme\" target=\"_blank\" rel=\"noreferrer noopener\">ACME support in NGINX</a>. The implementation introduces a new module  that provides built-in directives for requesting, installing, and renewing certificates directly from NGINX configuration. The ACME support leverages our <a href=\"https://github.com/nginx/ngx-rust\" target=\"_blank\" rel=\"noreferrer noopener\">NGINX-Rust SDK</a>&nbsp;and is available as a Rust-based dynamic module&nbsp;for both NGINX Open Source users as well as enterprise NGINX One customers using NGINX Plus.</p><p>NGINX’s native support for ACME brings a variety&nbsp;of benefits that simplify and enhance the overall SSL/TLS certificate management process. Being able to <a href=\"https://nginx.org/en/docs/http/ngx_http_acme_module.html\" data-type=\"link\" data-id=\"https://nginx.org/en/docs/http/ngx_http_acme_module.html\" target=\"_blank\" rel=\"noreferrer noopener\">configure ACME directly using NGINX directives</a> drastically reduces manual errors and eliminates much of the ongoing overhead traditionally associated with managing SSL/TLS certificates. It also reduces reliance on external tools like Certbot, creating a more secure and streamlined workflow with fewer vulnerabilities and a smaller attack surface. Additionally, unlike existing external tools which can be prone to platform-specific limitations, a native implementation ensures greater portability and platform independence, making it a versatile and reliable solution for modern, evolving web infrastructures.&nbsp;</p><p>The <a href=\"https://www.rfc-editor.org/rfc/rfc8555.html\" data-type=\"link\" data-id=\"https://www.rfc-editor.org/rfc/rfc8555.html\" target=\"_blank\" rel=\"noreferrer noopener\">ACME protocol</a> (Automated Certificate Management Environment) is a communications protocol primarily designed to automate the process of issuing, validating, renewing, and revoking digital security certificates (e.g., SSL/TLS certificates). It allows clients to interact with a Certificate Authority (CA) without requiring manual intervention, simplifying the deployment of secure websites and other services that rely on HTTPS.&nbsp;</p><p>The ACME protocol was initially developed by the <strong>Internet Security Research Group (ISRG)</strong> as part of the  initiative in late 2015, offering free, automated SSL/TLS certificates. Before ACME, obtaining TLS certificates was often a manual, costly, and error-prone process. ACME revolutionized this by providing open-source, automated workflows for certificate management.&nbsp;</p><p><a href=\"https://datatracker.ietf.org/doc/html/rfc8555/\" target=\"_blank\" rel=\"noreferrer noopener\">ACMEv2</a> is an updated version of the original ACME protocol. It added support for new challenges, expanded authentication methods, wildcard certificates, and other enhancements to improve flexibility and security.&nbsp;</p><p>The ACME workflow with NGINX can be broken into 4 steps:</p><ol><li>Setting up the ACME Server</li><li>Certificate Issue and Renewal</li></ol><h3>Setting up the ACME Server</h3><p>To enable ACME functionality, the first (and the only mandatory step) is to specify the directory URL of the ACME server.&nbsp;&nbsp;</p><p>Additional information regarding how to contact the client in case of certificate-related issues or where to store module data can also be provided, as shown.&nbsp;</p><pre><code> {&nbsp;\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://acme-v02.api.letsencrypt.org/directory;&nbsp;\n&nbsp;&nbsp;&nbsp; # &nbsp;&nbsp; admin@example.test;&nbsp;\n&nbsp;&nbsp;&nbsp; &nbsp; /var/cache/nginx/acme-letsencrypt;&nbsp;\n\n&nbsp;&nbsp;&nbsp; accept_terms_of_service;&nbsp;\n}</code></pre><p>The implementation also provides an optional directive  to store certificates, private keys, and challenge data for all the configured certificate issuers. The zone has a default size of 256K, which can be increased as required.&nbsp;</p><pre><code> zone=acme_shared:1M;&nbsp;</code></pre><p>The current preview implementation supports HTTP-01 challenges to verify the client’s domain ownership. It requires defining a listener on port 80 in the nginx configuration to process ACME HTTP-01 challenges:&nbsp;</p><pre><code> {&nbsp;\n&nbsp;&nbsp;&nbsp; # listener on port 80 is required to process ACME HTTP-01 challenges&nbsp;\n&nbsp;&nbsp;&nbsp; ;&nbsp;\n\n&nbsp;&nbsp;&nbsp;  / {&nbsp;\n        #Serve a basic 404 response while listening for challenges&nbsp;\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;;&nbsp;\n&nbsp;&nbsp;&nbsp; }&nbsp;\n}</code></pre><p>Support for other challenges (TLS-ALPN, DNS -01) is planned in <a href=\"https://github.com/nginx/nginx-acme/issues\" target=\"_blank\" rel=\"noreferrer noopener\">future</a>.&nbsp;&nbsp;</p><h3>Certificate Issuance and Renewal</h3><p>Use the  directive in the respective server block in your NGINX configuration to automate the issuance/renewal of TLS certificates. The directive requires the list of identifiers(domains) for which the certificates need to be dynamically issued. The list of identifiers can be defined using the <a href=\"https://nginx.org/en/docs/http/server_names.html\" target=\"_blank\" rel=\"noreferrer noopener\"></a> directive.&nbsp;&nbsp;</p><p>The snippet below shows how to configure the server block for issuing/renewing SSL certificate for “.example.domain” domain using the previously defined  ACME certificate issuer.&nbsp;</p><pre><code> {&nbsp;\n\n&nbsp;&nbsp;&nbsp;  443 ssl;&nbsp;\n\n&nbsp;&nbsp;&nbsp; &nbsp; .example.com;&nbsp;\n\n&nbsp;&nbsp;&nbsp; <strong> letsencrypt;&nbsp;</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $acme_certificate;&nbsp;\n&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; $acme_certificate_key;&nbsp;\n&nbsp;&nbsp;&nbsp;  max=2;&nbsp;\n}</code></pre><p>Note that not all values accepted by  directive are valid identifiers. Wildcards are not supported in this initial implementation. Regular expressions are not supported.&nbsp;</p><p>Use the  and  variables in the module to pass the SSL certificate and key information for the associated domain.&nbsp;&nbsp;</p><p>The rapid rise of HTTPS adoption globally has been driven largely by ACME protocol, making secure web connections a standard expectation. ACME modernizes the way TLS/SSL certificates are issued, renewed, and managed by automating the entire process, eliminating manual effort and reducing costs associated with certificate lifecycle management. Beyond the web, the growth of IoT devices and edge computing positions ACME to play a critical role in automating security for APIs, devices, and edge compute infrastructures.&nbsp;</p><p>NGINX’s native support for ACME underscores the protocol’s importance for the future of web security, automation, and scalability. ACME is expected to remain the backbone of certificate automation across the internet and beyond for foreseeable future. With security becoming a baseline for web standards, we’ll continue seeing requirements for evolving deployment models and security needs, pushing improvements in ACME.&nbsp;&nbsp;</p><p>Looking ahead, we are committed to evolving our implementation to align with the needs of our users and customers, meeting them where they are today and building capabilities for where they are headed in the future.&nbsp;</p><p><a href=\"https://github.com/nginx/nginx-acme\" target=\"_blank\" rel=\"noreferrer noopener\">Get started</a> with the native ACME implementation in NGINX today. If you are an open source user, pre-built packages are available <a href=\"https://nginx.org/en/linux_packages.html\" target=\"_blank\" rel=\"noreferrer noopener\">here</a>. If you are an enterprise NGINX One customer using NGINX Plus, pre-built packages are available as a F5 supported <a href=\"https://docs.nginx.com/nginx/admin-guide/dynamic-modules/dynamic-modules/\" target=\"_blank\" rel=\"noreferrer noopener\">dynamic module</a>. For more information on the module, refer to the <a href=\"https://nginx.org/en/docs/http/ngx_http_acme_module.html\" target=\"_blank\" rel=\"noreferrer noopener\">NGINX Docs</a>.&nbsp;</p><p>As always, your feedback is invaluable in shaping the future development of NGINX. If you have suggestions, encounter issues, or want to request additional features, please share them through <a href=\"https://github.com/nginx/nginx-acme/issues\" target=\"_blank\" rel=\"noreferrer noopener\">GitHub Issues</a>. We can’t wait for you to try it out.</p>","contentLength":6305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44889941"},{"title":"FFmpeg 8.0 adds Whisper support","url":"https://code.ffmpeg.org/FFmpeg/FFmpeg/commit/13ce36fef98a3f4e6d8360c24d6b8434cbb8869b","date":1755080375,"author":"rilawa","guid":235,"unread":true,"content":"<div> with  and </div>","contentLength":11,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44886647"},{"title":"Claude says “You're absolutely right!” about everything","url":"https://github.com/anthropics/claude-code/issues/3382","date":1755068375,"author":"pr337h4m","guid":234,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44885398"},{"title":"VC-backed company just killed my EU trademark for a small OSS project","url":"https://news.ycombinator.com/item?id=44883634","date":1755047676,"author":"marcjschmidt","guid":233,"unread":true,"content":"I run a small open-source project Deepkit (Trademark 017875717) I've been building for many years. It's not huge, just a few thousand users compared to the big OSS names, but to me it was worth protecting, so I trademarked the name in the EU and US a few years back. I had hoped to be protected from other corporations this way and live peacefully.<p>A $160M-funded company named Deepki (Trademark 1751952) came along and filed for cancellation at EUIPO since they needed the trademark now after getting lots of funding. They won. Now my trademark is gone.</p><p>The frustrating part? The EU actually does allow open-source (even free projects) to have trademarks, but you have to prove \"genuine use\" in the EU for the goods/services your trademark covers. Which seems to force you in collecting user sensitive data otherwise you are entirely unable to prove that you have actual users in the EU. I generally try to collect as little information as possible (also because I don't care where my users are coming from). I had google analytics running for some time on the main page (not documentation), but most of the time it didn't work and it seems most of my users block it anyway.</p><p>Here's what I gave the EUIPO and why they said no:</p><p>- Google Analytics for my site with a full country breakdown from 2018–2023. A few hundred to ~1,800 EU visitors per year per country. They said that’s \"too small\" to count as real commercial exploitation for my Class 9 software. Also, they said they couldn’t tell which goods those visits were actually for.</p><p>- npmjs + GitHub stats - hundreds of thousands of downloads and thousands of stars. Rejected because there's no location data, so they couldn't confirm if the usage was in the EU. In some cases, they said the timeframes weren't even clear.</p><p>- They basically kept repeating that they couldn't clearly link any of the usage to the specific goods/services my trademark was registered for.</p><p>&gt;Conclusion: It follows from the above that the EUTM proprietor has not proven genuine use of the contested mark for any of the goods and services for which it is registered. As a result, the application for revocation is wholly successful and the contested European Union trade mark must be revoked in its entirety. According to Article 62(1) EUTMR, the revocation will take effect from the date of the application for revocation, that is, as of 18/03/2024.</p><p>&gt;COSTS: According to Article 109(1) EUTMR, the losing party in cancellation proceedings must bear fees and costs incurred by the other party.</p><p>They even admitted there's no strict minimum for usage, and free software can count, but in their eyes my EU traffic was too low and not clearly tied to the trademarked goods.</p><p>I also have the US trademark for the name. This same company tried to register in the US around 2022 (Trademark #79379273) and got blocked because it was too similar (decision made by USPTO). But a few months ago they somehow got it registered there too (Trademark #7789522), not sure how they did that now.</p><p>Now I'm sitting here wondering:</p><p>- Is it even worth getting a second opinion and appealing in the EU? I mean the project is very small.</p><p>- Should I fight the US registration?</p><p>- Or should I just walk away from trademarks altogether for my open-source projects. I lost so much money because of this already.</p><p>- And for OSS projects in general, is there even a practical, privacy-friendly way to prove EU usage without generating revenue?</p><p>- Is it even worth holding the trademark if proving EU usage is this brittle for OSS? If the trademark can be deleted just like that even after spending a few thousands dollars on lawyers. Probably a skill issue, but still, damn.</p><p>It sucks to lose the name I've been building for years to a corporation with $160M behind them, especially when this is just a side project I do in my spare time, and to them I'm a nobody. If nothing else, maybe my case can be a cautionary tale for other OSS maintainers.</p>","contentLength":3929,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44883634"},{"title":"Claude Sonnet 4 now supports 1M tokens of context","url":"https://www.anthropic.com/news/1m-context","date":1755014543,"author":"adocomplete","guid":232,"unread":true,"content":"<p>Claude Sonnet 4 now supports up to 1 million tokens of context on the Anthropic API—a 5x increase that lets you process entire codebases with over 75,000 lines of code or dozens of research papers in a single request.</p><p>Long context support for Sonnet 4 is now in public beta on the Anthropic API and in Amazon Bedrock, with Google Cloud’s Vertex AI coming soon.</p><h2>Longer context, more use cases</h2><p>With longer context, developers can run more comprehensive and data-intensive use cases with Claude, including:</p><ul><li><strong>Large-scale code analysis: </strong>Load entire codebases including source files, tests, and documentation. Claude can understand project architecture, identify cross-file dependencies, and suggest improvements that account for the complete system design.</li><li> Process extensive document sets like legal contracts, research papers, or technical specifications. Analyze relationships across hundreds of documents while maintaining full context.</li><li>Build agents that maintain context across hundreds of tool calls and multi-step workflows. Include complete API documentation, tool definitions, and interaction histories without losing coherence.</li></ul><p>To account for increased computational requirements, <a href=\"https://www.anthropic.com/pricing#api\">pricing</a> adjusts for prompts over 200K tokens:</p><p>When combined with <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching\">prompt caching</a>, users can reduce latency and costs for Claude Sonnet 4 with long context. The 1M context window can also be used with <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/batch-processing\">batch processing</a> for an additional 50% cost savings.</p><h2>Customer spotlight: Bolt.new</h2><p>Bolt.new transforms web development by integrating Claude into their browser-based development platform.</p><p>“Claude Sonnet 4 remains our go-to model for code generation workflows, consistently outperforming other leading models in production. With the 1M context window, developers can now work on significantly larger projects while maintaining the high accuracy we need for real-world coding,\" said Eric Simons, CEO and Co-founder of Bolt.new.</p><h2>Customer spotlight: iGent AI</h2><p>London-based iGent AI is advancing the field of software development with Maestro, an AI partner that transforms conversations into executable code.</p><p>\"What was once impossible is now reality: Claude Sonnet 4 with 1M token context has supercharged autonomous capabilities in Maestro, our software engineering agent at iGent AI. This leap unlocks true production-scale engineering—multi-day sessions on real-world codebases—establishing a new paradigm in agentic software engineering,\" said Sean Ward, CEO and Co-founder of iGent AI.</p><p>Long context support for Sonnet 4 is now in public beta on the Anthropic API for customers with Tier 4 and custom rate limits, with broader availability rolling out over the coming weeks. Long context is also available in Amazon Bedrock, and is coming soon to Google Cloud's Vertex AI. We’re also exploring how to bring long context to other Claude products.</p><p>To learn more about Sonnet 4 and the 1M context window, see our <a href=\"https://docs.anthropic.com/en/docs/build-with-claude/context-windows#1m-token-context-window\">documentation</a> and <a href=\"https://www.anthropic.com/pricing#api\">pricing page</a>.</p>","contentLength":2919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44878147"},{"title":"Wikipedia loses challenge against Online Safety Act","url":"https://www.bbc.com/news/articles/cjr11qqvvwlo","date":1754930006,"author":"phlummox","guid":231,"unread":true,"content":"<div data-component=\"text-block\"><p>Wikipedia has lost a legal challenge to new Online Safety Act rules which it says could threaten the human rights and safety of its volunteer editors.</p><p>The Wikimedia Foundation - the non-profit which supports the online encyclopaedia - wanted a judicial review of regulations which could mean Wikipedia has to verify the identities of its users.</p><p>But it said despite the loss, <a target=\"_blank\" href=\"https://www.judiciary.uk/wp-content/uploads/2025/08/Wikimedia-Foundation-and-another-v-Secretary-of-State-for-Science-Innovation-and-Technology.pdf\">the judgement</a> \"emphasized the responsibility of Ofcom and the UK government to ensure Wikipedia is protected\".</p><p>The government told the BBC it welcomed the High Court's judgment, \"which will help us continue our work implementing the Online Safety Act to create a safer online world for everyone\".</p></div><div data-component=\"text-block\"><p>Judicial reviews challenge the lawfulness of the way in which a decision has been made by a public body.</p><p>In this case the Wikimedia Foundation and a Wikipedia editor tried to challenge the way in which the government decided to make regulations covering which sites should be classed \"Category 1\" under the Online Safety Act - the strictest rules sites must follow.</p><p>It argued the rules were logically flawed and too broad, meaning a policy intended to impose extra rules on large social media companies would instead apply to Wikipedia.</p><p>In particular the foundation is concerned the extra duties required - if Wikipedia was classed as Category 1 - would mean it would have to verify the identity of its contributors, undermining their privacy and safety.</p><p>The only way it could avoid being classed as Category 1 would be to cut the number of people in the UK who could access the online encyclopaedia by about three-quarters, or disable key functions on the site. </p><p>The government's lawyers argued that ministers had  considered whether Wikipedia should be exempt from the regulations but had reasonably rejected the idea.</p></div><div data-component=\"text-block\"><p>In the end, the court rejected Wikimedia's arguments.</p><p>But Phil Bradley-Schmieg, Lead Counsel at the Wikimedia Foundation, said the judgment did not give Ofcom and the Secretary of State, in Mr Justice Johnson's words, \"a green light to implement a regime that would significantly impede Wikipedia's operations\".</p><p>And the judgement makes it clear other legal challenges could be possible. </p><p>Wikimedia could potentially challenge Ofcom's decision making if the regulator did ultimately decide to classify the site as Category 1.</p><p>And if the effect of making Wikipedia Category 1 meant it could not continue to operate, then other legal challenges could follow.</p><p>\"Wikipedia has been caught in the stricter regulations due to its size and user created content even though it argues (convincingly) that it differs significantly from other user-to-user platforms,\" said Mona Schroedel, data protection litigation specialist at law firm Freeths.</p><p>\"The court's decision has left the door open for Wikipedia to be exempt from the stricter rules upon review.\"</p><p>The communications regulator Ofcom, which will enforce the act, told the BBC: \"We note the court's judgment and will continue to progress our work in relation to categorised services and the associated extra online safety rules for those companies.\"</p></div>","contentLength":3070,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44866208"},{"title":"GitHub is no longer independent at Microsoft after CEO resignation","url":"https://www.theverge.com/news/757461/microsoft-github-thomas-dohmke-resignation-coreai-team-transition","date":1754927268,"author":"Handy-Man","guid":230,"unread":true,"content":"<div><p>Microsoft is moving GitHub even more closely into its CoreAI team, following the resignation of GitHub CEO Thomas Dohmke today. After nearly four years as CEO, Dohmke is leaving GitHub to “become a startup founder again,” and pursue opportunities outside of Microsoft and GitHub.</p></div><div><p>GitHub has operated as a separate company ever <a href=\"https://www.theverge.com/2018/10/26/17954714/microsoft-github-deal-acquisition-complete\">since Microsoft acquired it</a> in 2018 for $7.5 billion, but Dohmke’s departure is part of a big shakeup to the way GitHub operates. Microsoft isn’t replacing Dohmke’s CEO position, and the rest of GitHub’s leadership team will now report more directly to Microsoft’s CoreAI team.</p></div><div><p>“GitHub and its leadership team will continue its mission as part of Microsoft’s CoreAI organization, with more details shared soon,” says Dohmke in <a href=\"https://github.blog/news-insights/company-news/goodbye-github/\">a memo</a> to GitHub employees today. “I’ll be staying through the end of 2025 to help guide the transition and am leaving with a deep sense of pride in everything we’ve built as a remote-first organization spread around the world.”</p></div><div><p>Microsoft’s CoreAI team is a new engineering group led by former Meta executive Jay Parikh. It includes Microsoft’s platform and tools division and Dev Div teams, with a focus on building an AI platform and tools for both Microsoft and its customers.</p></div><div><p>Today’s change means GitHub no longer has a single leader, or CEO, and responsibility for GitHub will align more closely to the CoreAI leadership team. GitHub’s reporting structure originally changed in 2021 when former CEO Nat Friedman stepped down, and Dohmke reported up to Julia Liuson, head of Microsoft’s developer division. Liuson then started reporting to Parikh earlier this year with the formation of the CoreAI team.</p></div><div><p>Jay Parikh, head of CoreAI, described his vision of an AI agent factory <a href=\"https://www.theverge.com/notepad-microsoft-newsletter/672598/microsoft-ai-agent-factory-jay-parikh-interview\">in an interview with </a>earlier this year, and how he is convincing the developer division of Microsoft to adopt AI. “Just like how Bill [Gates] had this idea of Microsoft being a bunch of software developers building a bunch of software, I want our platform, for any enterprise or any organization, to be able to be the thing they turn into their own agent factory,” said Parikh.</p></div><div><p>Dohmke only just <a href=\"https://www.theverge.com/decoder-podcast-with-nilay-patel/720075/github-ceo-thomas-dohmke-ai-coding-copilot-openai-interview\">appeared on last week</a>, discussing Copilot, vibe coding, and what’s next for AI. Dohmke was thinking a lot about the competition and GitHub’s role in the future of software development, and now he’s about to leave to potentially create some more competition for Microsoft’s AI efforts.</p></div><div><p><em>: GitHub was already part of CoreAI, but its leadership will no longer be under a single CEO.</em></p></div><div><ul></ul></div>","contentLength":2550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44865560"},{"title":"Claude Code is all you need","url":"https://dwyer.co.za/static/claude-code-is-all-you-need.html","date":1754921026,"author":"sixhobbits","guid":229,"unread":true,"content":"<a href=\"https://dwyer.co.za/static/claude-code-is-all-you-need.html#build-things\"><h2>You can just build things</h2></a><p>These are only a few things I've built with Claude Code since using it. Most of them are experimental and I've read some reports of it not doing as well on massive real-world code bases, but from what I've seen I'd be surprised it wasn't still useful in those contexts given enough guidance. I'm still surprised by how much better it is as a tool when given a lot of context and input. Here are a few other toy projects I've had it spit out - all things that I've wanted to build for months or years but never found the time. Now you can do stuff like this in a few minutes or hours instead of days or weeks.</p><h3>Building a HackerNews comment ranker plugin</h3><p>I've often been annoyed by comments on HackerNews that are not at all about the article they're commenting on. \"Bitcoin adopts a new FlibbityGippity Protocol and can now handle 2.3 transactions per day\" and someone will comment that all Crypto projects are scams or something. Note that I don't care about the quality of the comment, or whether or not I agree with it, but I'd wanted a visual way to skip over the 'noise' comments that aren't actually about the article at all.</p><p>I tried to build this before but got distracted by more important stuff, so I figured I'd start over with Claude Code.</p><img src=\"https://dwyer.co.za/static/img/hn-plugin-demo.png\" alt=\"HackerNews comment ranking plugin demo\"><p>It took a few tries before it could actually display the badges correctly within HN's (pretty simple) HTML structure, but after a few rounds of 'no try again' or 'add more debugging so I can paste the errors to you', it created almost exactly what I had envisioned.</p><p>I was surprised by how good it looked (much better than my normal hacky frontends), and the details it had added unprompted (like the really nice settings page, even with a nod to the HN orange theme).</p><img src=\"https://dwyer.co.za/static/img/hn-plugin-settings.png\" alt=\"HackerNews plugin settings page\"><p>The actual ranking (which I'm using OpenAI for, not Anthropic) is not that good. It could probably be improved with a better prompt and some more examples of what I think is a '1' comment or a '5' comment, but it works and looks at least directionally accurate so far.</p><h3>Building Poster Maker - A Minimal Canva Replacement</h3><p>AI is getting good at graphic design, and I knew people who were using it to generate basic posters. They liked that the AI could choose good background images, and generally make things look nice with well-sized fonts etc, but they were frustrated that the AI was still only 80% good at generating images of text, and often had spelling errors or other artifacts.</p><p>I was going to tell them to use Canva or Slides.new or another alternative. I tried them out so I could do a quick tutorial on how to use them and realised they were all kinda bad. Either enshittified to death, or lacking the basic AI features, or too complicated for non-technical people to use.</p><p>This was the project that felt a bit more like engineering and less like vibe coding than the others. I knew what I wanted: a really simple interface to combine images and text and get an A4 PDF out. I'd tried to build something like this before and looked at different PDF creation libraries, HTML→PDF flows, and seen that it's not the easiest problem to solve.</p><p>Last time I solved a similar problem (in 2018) <a href=\"https://www.codementor.io/@garethdwyer/create-pdf-files-from-templates-with-python-and-google-scripts-p63kal1vb\" target=\"_blank\">I ended up hacking in Google Docs</a> to create A4 PDFs, but that was more of a templating problem and Google Docs isn't great for layout stuff.</p><p>So I built <a href=\"https://posters.dwyer.co.za\" target=\"_blank\">posters.dwyer.co.za</a> - it lets you generate the background image with AI (I used Claude Code to build everything, but I told it to use GPT for image generation as that's what I'd used before and I think it's better? I don't even know if Anthropic has image generation APIs to be honest and it seemed easier to just use what I knew).</p><img src=\"https://dwyer.co.za/static/img/poster-maker.png\" alt=\"Poster Maker interface\"><div><p> Another small snag, it seems like OpenAI blocks Anthropic crawlers so Claude couldn't go read the OpenAI API docs and figure out how to image gen. I had to save the file locally and tell it to reference that.</p></div><p>This project took a few hours of back and forth. I was really impressed with some of Claude's UI knowledge (it one shotted the font selection when I told it what I wanted) and also saw the limitations in other aspects (it kept overlaying elements in a very un-userfriendly way, sidebar would hide and show and move everything around, it clearly has no idea what it's like to be a human and use something like this).</p><p>But after telling it exactly where to put elements and what they should do, I got more or less exactly what I had envisioned. I was surprised at how well the PDF export worked after the 6th or 7th attempt of blank files or cut off files - now it seems really great at giving me a PDF that is exactly like the preview version which for anyone not in tech seems like a really basic piece of functionality and anyone who has actually tried to do it before knows is like the XKCD bird problem:</p><div><a href=\"https://xkcd.com/1425/\"><img src=\"https://imgs.xkcd.com/comics/tasks.png\" alt=\"XKCD: Tasks\"></a><p>XKCD #1425: Why seemingly simple tasks can be surprisingly hard for computers</p></div><h3>Doing admin with Claude Code</h3><p>This isn't really a project I built, but I'm using Claude Code more and more to do non-coding related tasks. I needed to upload bank statements for my accountant, but my (shitty) South African banks don't name the files well. I can download each month from the web app, but it calls them all \"Unknown (5)\" or whatever with no extension so it's a pain to go and name them correctly.</p><p>I asked Claude to rename all the files and I could go do something else while it churned away, reading the files and figuring out the correct names.</p><p>I then took it a step further and told it to merge them all into a single CSV file (which also involved extracting random header tabs off the badly formatted XLSX files that my bank provides), and classifying all expenses into broad and specific categories. I told it a few things like the roles of specific people in the team and I think it one-shotted that too. I'm not going to fire my bookkeepers yet, but if I were a bookkeeper I'd definitely make sure to be upskilling with AI tooling right now.</p><img src=\"https://dwyer.co.za/static/img/bank-statements.png\" alt=\"Bank statements renaming tool\"><h4>Using Claude Code as my Text Editor</h4><p>I'm a die-hard vanilla vim user for all writing, coding, configuration and anything else that fits. I've tried nearly every IDE and text editor out there, and I was certainly happy to have a real IDE when I was pushing production Java for AWS, but vim is what I've always come back to.</p><p>Switching to Claude Code has opened a lot of new design possibilities. Before (did I mention I suck at front end coding), I was restricted to whatever output was produced by static site generators or pandoc templates. Now I can just tell Claude to write an article (like the one you're currently reading) and give it some pointers regarding how I want it to look, and it can generate any custom HTML and CSS and JavaScript I want on the fly.</p><img src=\"https://dwyer.co.za/static/img/this-article.png\" alt=\"This article being written\"><p>I wrote this entire article in the Claude Code interactive window. The TUI flash (which I've read is a problem with the underlying library that's hard to fix) is really annoying, but it's a really nice writing flow to type stream of consciousness stuff into an editor, mixing text I want in the article, and instructions to Claude, and having it fix up the typos, do the formatting, and build the UX on the fly.</p><p>Nearly every word, choice of phrase, and the overall structure is still manually written by me, a human. I'm still on the fence about whether I'm just stuck in the old way by preferring to hand-craft my words, or if models are generally not good at writing.</p><p>When I read answers to questions I've asked LLMs, or the long research-style reports they create, the writing style is pretty good and I've probably read more LLM-generated words than human-generated words in the last few months.</p><p>But whenever I try to get them to produce the output I want to produce, they fail hard unless I spend as much effort on the prompt as I would have on writing the output myself.</p><p>Simon Willison calls them <a href=\"https://simonwillison.net/2023/Apr/2/calculator-for-words/\" target=\"_blank\">'word calculators'</a> and this is still mainly how I think of them. Great at moving content around (if you want a summary of this now very long article, an LLM will probably do a great job) but pretty useless at generating new stuff.</p><p>Maybe us writers will be around for a while still - let's see, and lfg.</p>","contentLength":8018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44864185"},{"title":"I tried every todo app and ended up with a .txt file","url":"https://www.al3rez.com/todo-txt-journey","date":1754920742,"author":"al3rez","guid":228,"unread":true,"content":"<p>I’ve tried them all. Notion, Todoist, Things 3, OmniFocus, Asana, Trello, Any.do, TickTick. I even built my own todo app once (spoiler: I never finished it). After years of productivity app hopping, I’m back to where I started: a plain text file called .</p><p>I’m not alone in this. Jeff Huang wrote about his <a href=\"https://jeffhuang.com/productivity_text_file/\">“never-ending .txt file”</a> that he’s used for over 14 years. Reading his post validated everything I’d discovered on my own.</p><p>My productivity journey started like everyone else’s. I’d devour blog posts about getting things done or spot a cool app and think “this is it, this will finally organize me.” I’d burn hours building the perfect system, creating categories, tags, projects, labels. Setting it up felt like work.</p><p>Then reality hits. The app wants $9.99/month. The sync breaks. The company sells out and dies. Or worse - I waste more time managing the system than working.</p><h2>What Actually Happened With Each App</h2><p>: Built an entire life operating system. Spent three weeks perfecting it. Used it for two days. Now it’s a graveyard of abandoned databases.</p><p>: Great until I realized I was gaming the points system instead of doing actual work. Turns out completing “drink water” 8 times a day doesn’t make you productive.</p><p>: Beautiful. Expensive. Tricked me into thinking I had my life together. But I kept forgetting to check it.</p><p>: Turned my todo list into a board with columns. Realized I’m not a startup. I’m just one person trying to remember to buy milk.</p><p>: So powerful I needed a manual to use it. Spent more time learning OmniFocus than finishing my actual projects.</p><p>One day my phone died and I couldn’t check my tasks. I grabbed a sticky note and scribbled:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>And you know what? I crushed all four things. No tags, no priorities, no due dates. Just four things written down.</p><h2>My Current System: One Text File</h2><p>Now I run everything through a single text file. That’s it. Here’s what it looks like:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>Every night, I check tomorrow’s calendar. I dump everything into the next day’s section. Scheduled items get times in front. Sub-bullets hold notes or reminders. Finished tasks? I delete them or add what happened. Still on the list? Not done yet. That’s it.</p><p>This transforms into a living document throughout the day. I scribble notes right next to tasks as I work:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>Every few days I start fresh with a new date. The old sections stay. They transform into my journal. I search back to find when I did something, who I met, what we decided. Todo list and work log in one file.</p><p>: The file sits on my desktop. It stares at me every time I open my laptop. No app to launch, no subscription to manage.</p><p>: My keyboard shortcut launches my todo.txt in a floating window. Doesn’t matter what I’m doing, my todos are one key press away. No switching between apps, no waiting for things to load, just boom - there’s my list.</p><p><strong>AI helps but isn’t needed</strong>: With Cursor/Claude Code or Neovim + Supermaven, I can write my entire day’s schedule in 5 minutes. The AI completes my sentences, predicts meeting times, memorizes how I write tasks. But if all these AI companies disappear tomorrow, my system still works. It’s just a text file. The AI makes it faster, not required.</p><p>: Adding a task burns 2 seconds. No clicking through menus or selecting projects.</p><p>: Cmd+F and I find anything instantly. “When did I last call the dentist?” Search for “dentist”. Done.</p><p>: No company can kill it. No updates can destroy it. No algorithm decides what I should see.</p><p>: I can’t hide behind fancy features. Either I did the thing or I didn’t.</p><p>: A text file is the most basic thing a computer can read. It’ll work after every software update, every company shutdown, every app that stops working. Text files from 20 years ago still open perfectly. Try that with your Notion workspace.</p><p>Productivity isn’t about finding the perfect app. It’s about:</p><ol><li>Dumping things onto paper so your brain can forget them</li><li>Checking the list regularly</li></ol><p>That’s it. Everything else is procrastination dressed up as organization.</p><h2>What About [Insert Feature Here]?</h2><p>“But what about reminders?” - I use my calendar for time-specific stuff.</p><p>“But what about projects?” - I add a note like  if I need to.</p><p>“But what about collaboration?” - I use work tools for work. This is for my life.</p><p>“But what about mobile?” - The file syncs through Dropbox. Any text editor works.</p><p>I’m more productive now than when I had all those fancy apps. Turns out the best productivity system is the one you actually use. And I use this one because there’s nothing to figure out. It’s just a list.</p><p>Ready to ditch the productivity app hamster wheel? Do this:</p><ol><li>Create a file called </li><li>Write down what you need to do tomorrow</li><li>Start a new date section when needed</li></ol><p>Give it a week. Simple beats sophisticated every time.</p><p>And if it doesn’t work? Well, there’s always another shiny new app launching next week.</p>","contentLength":4875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44864134"},{"title":"Fight Chat Control","url":"https://fightchatcontrol.eu/","date":1754844634,"author":"tokai","guid":227,"unread":true,"content":"<div><p data-i18n=\"overview.mass_surveillance.description\">Every private message, photo, and file scanned automatically: no suspicion required, no exceptions*, even encrypted communications.</p></div><div><p data-i18n=\"overview.breaking_encryption.description\">Weakening or breaking end-to-end encryption exposes everyone's communications—including sensitive financial, medical, and private data—to hackers, criminals, and hostile actors.</p></div><div><p data-i18n=\"overview.fundamental_rights.description\">Undermines your fundamental rights to privacy and data protection, as guaranteed by Articles 7 and 8 of the EU Charter—rights considered core to European democratic values.</p></div><div><p data-i18n=\"overview.false_positives.description\">Automated scanners routinely misidentify innocent content, such as vacation photos or private jokes, as illegal, putting ordinary people at risk of false accusations and damaging investigations.</p></div><div><h3 data-i18n=\"overview.ineffective_protection.title\">Ineffective Child Protection</h3><p data-i18n=\"overview.ineffective_protection.description\">Child protection experts and organisations, including the UN, warn that mass surveillance fails to prevent abuse and actually makes children less safe—by weakening security for everyone and diverting resources from proven protective measures.</p></div><div><p data-i18n=\"overview.global_precedent.description\">Creates a dangerous global precedent enabling authoritarian governments, citing EU policy, to roll out intrusive surveillance at home, undermining privacy and free expression worldwide.</p></div>","contentLength":1137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44856426"},{"title":"Debian 13 “Trixie”","url":"https://www.debian.org/News/2025/20250809","date":1754763525,"author":"ducktective","guid":226,"unread":true,"content":"<p>After 2 years, 1 month, and 30 days of development, the Debian\nproject is proud to present its new stable version 13 (code name ).</p><p>Debian 13  ships with several desktop environments, such as:\n</p><ul></ul><p>This release contains over  new packages for a total count of\n packages, while over  packages have been removed\nas .  packages were updated in this release.\nThe overall disk usage for  is , and\nis made up of  lines of code.</p><p>Thanks to our translators who have made the -pages for\n available in multiple languages.</p>\nThe manpages-l10n project has contributed many improved and new translations\nfor manual pages. Especially Romanian and Polish translations are greatly\nenhanced since bookworm.\nAll architectures other than i386 now use a 64-bit time_t ABI, supporting dates\nbeyond 2038.\nDebian contributors have made significant progress towards ensuring package\nbuilds produce byte-for-byte reproducible results. You can check the status for\npackages installed on your system using the new package debian-repro-status, or\nvisit <a href=\"https://reproduce.debian.net/\">reproduce.debian.net</a> for\nDebian's overall statistics for trixie and newer.\n<p>Debian 13  includes numerous updated software packages\n(over 63% of all packages from the previous release), such as:\n</p><ul><li>Exim (default email server) 4.98</li><li>GNU Compiler Collection 14.2</li><li>Linux kernel 6.12 LTS series</li><li>LLVM/Clang toolchain 19 (default), 17 and 18 available</li></ul><p>\nWith this broad selection of packages and its traditional wide\narchitecture support, Debian once again stays true to its goal of being\n<q>The Universal Operating System</q>. It is suitable for many different\nuse cases: from desktop systems to netbooks; from development servers to\ncluster systems; and for database, web, and storage servers. At the same\ntime, additional quality assurance efforts like automatic installation and\nupgrade tests for all packages in Debian's archive ensure that \nfulfills the high expectations that users have of a stable Debian release.\n</p><p>\nThis release for the first time officially supports the riscv64\narchitecture, allowing users to run Debian on 64-bit RISC-V hardware and\nbenefit from all Debian 13 features. A total of seven architectures are officially supported for :\n</p><ul><li>ARMv7 (EABI hard-float ABI, armhf),</li><li>64-bit little-endian PowerPC (ppc64el),</li><li>64-bit little-endian RISC-V (riscv64),</li></ul><p>\ni386 is no longer supported as a regular architecture: there is no official\nkernel and no Debian installer for i386 systems. The i386 architecture is now\nonly intended to be used on a 64-bit (amd64) CPU. Users running i386 systems\nshould not upgrade to trixie. Instead, Debian recommends either reinstalling\nthem as amd64, where possible, or retiring the hardware.\n</p><p> will be the last release for the armel architecture. See <a href=\"https://www.debian.org/releases/trixie/release-notes/issues.html#last-release-for-armel\">5.1.3.\nLast release for armel</a> in the release notes for more information on our ARM\nEABI support.\n</p><p>The Debian Cloud team publishes  for several cloud computing\nservices:\n</p><ul><li>Amazon EC2 (amd64 and arm64),</li><li>OpenStack (generic) (amd64, arm64, ppc64el),</li><li>PlainVM (amd64, arm64, ppc64el),</li><li>GenericCloud (arm64, amd64),</li><li>NoCloud (amd64, arm64, ppc64el)</li></ul><p>\nThe genericcloud image should be able to run in any virtualised environment,\nand there is also a nocloud image which is useful for testing the build process.\n</p><p>Cloud images provide automation hooks via ``cloud-init`` and prioritize\nfast instance startup using specifically optimized kernel packages and\ngrub configurations.\n</p><p>\nIf you simply want to try Debian 13  without installing it,\nyou can use one of the available <a href=\"https://www.debian.org/CD/live/\">live images</a>\nwhich load and run the complete operating system in a read-only state via your\ncomputer's memory.\n</p><p>\nThese live images are provided for the  architecture and are\navailable for DVDs, USB sticks, and netboot setups.\nThe user can choose among different desktop\nenvironments to try: GNOME, KDE Plasma, Cinnamon, MATE, LXDE, LXQt, and Xfce.\nDebian Live  has a standard live image, so it is also possible\nto try a base Debian system without any of the graphical user interfaces.\n</p><p>\nShould you enjoy the operating system you have the option of installing\nfrom the live image onto your computer's hard disk. The live image\nincludes the Calamares independent installer as well as the standard Debian\nInstaller. More information is available in the\n<a href=\"https://www.debian.org/releases/trixie/releasenotes\">release notes</a> and the\n<a href=\"https://www.debian.org/CD/live/\">live install images</a> sections of the Debian\nwebsite.\nMulti-architecture Debian  container images are also available on\n<a href=\"https://hub.docker.com/_/debian\">Docker Hub</a>. In addition to the\nstandard images, a  variant is available to reduce disk usage.\nThe Debian Installer and Debian Live Images can now be booted using\n on supported UEFI and U-Boot firmware.\n</p><p>\nTo install Debian 13  directly onto your computer's storage\ndevice you can choose from a variety of installation media types to\n<a href=\"https://www.debian.org/download\">Download</a>\nsuch as: Blu-ray Disc, DVD, CD, USB stick, or via a network connection.\nSee the <a href=\"https://www.debian.org/releases/trixie/installmanual\">Installation Guide</a>\nfor more details.\n</p><p>\nDebian can now be installed in 78 languages, with most of them available\nin both text-based and graphical user interfaces.\n</p><p>\nThe installation images may be downloaded right now via\n<a href=\"https://www.debian.org/CD/torrent-cd/\">bittorrent</a> (the recommended method),\n<a href=\"https://www.debian.org/CD/jigdo-cd/#which\">jigdo</a>, or\n<a href=\"https://www.debian.org/CD/http-ftp/\">HTTP</a>; see\n<a href=\"https://www.debian.org/CD/\">Debian on CDs</a> for further information. \nwill soon be available on physical DVD, CD-ROM, and Blu-ray Discs from\nnumerous <a href=\"https://www.debian.org/CD/vendors\">vendors</a> too.\n</p><p>\nUpgrades to Debian 13  from the previous release, Debian 12\n, are automatically handled by the APT package\nmanagement tool for most configurations.\n</p><p>Before upgrading your system, it is strongly recommended that you make a\nfull backup, or at least back up any data or configuration information you\ncan't afford to lose. The upgrade tools and process are quite reliable, but\na hardware failure in the middle of an upgrade could result in a severely\ndamaged system.</p><p>The main things you'll want to back up are the contents of /etc,\n/var/lib/dpkg, /var/lib/apt/extended_states and the output of:\n</p><code>$ dpkg --get-selections '*' # (the quotes are important)</code><p>We welcome any information from users related to the upgrade from\n to . Please share information by filing a bug in the\n<a href=\"https://www.debian.org/releases/trixie/release-notes/about.en.html#upgrade-reports\">Debian\nbug tracking system</a> using the  package with your results.\n</p><p>\nThere has been a lot of development on the Debian Installer since its previous\nofficial release with Debian 12, resulting in improved hardware support and\nsome very useful new features such as\n</p><ul><li>Improved hardware and software support for speech synthesis</li><li>Initial and restricted support for rescuing Debian installed to a btrfs subvolume</li><li>Changed default unit from MB to GB when partitioning disks</li><li>Disabled cdrom sources if installation medium is not a real CD (USB stick, SD card, ISO file), because APT cannot use it after the installation</li><li>Plus support for secure boot with systemd-boot</li></ul><p>It is advisable to remove bookworm-backports entries from APT source-list\nfiles before the upgrade; after the upgrade consider adding .\n</p><p>\nIf your APT configuration also involves pinning or ,\nit is likely to require adjustments to allow the upgrade of packages to the new\nstable release. Please consider\n<a href=\"https://www.debian.org/releases/trixie/release-notes/upgrading.html#disabling-apt-pinning\">disabling APT pinning</a>.\n</p><p>\nUnder some circumstances, issues might arise during the upgrade process, or\nwhile running .\n</p><p>\nFor instance, the TLS support in the OpenLDAP client  and server \nis now provided by OpenSSL instead of GnuTLS. This affects the available\nconfiguration options, as well as their behavior. If no TLS CA\ncertificates are specified, the system default trust store will now be loaded automatically. If you do not want\nthe default CAs to be used, you must configure\nthe trusted CAs explicitly. For more information about LDAP client configuration,\nsee the <a href=\"https://manpages.debian.org/trixie/ldap.conf.5\">ldap.conf.5</a>\nman page.\n</p><p>\nAs always, Debian systems may be upgraded painlessly, in place,\nwithout any forced downtime, but it is strongly recommended to read\nthe <a href=\"https://www.debian.org/releases/trixie/releasenotes\">release notes</a> as\nwell as the <a href=\"https://www.debian.org/releases/trixie/installmanual\">installation\nguide</a> for possible issues, and for detailed instructions on\ninstalling and upgrading. The release notes will be further improved and\ntranslated to additional languages in the weeks after the release.\n</p><p>\nDebian is a free operating system, developed by\nthousands of volunteers from all over the world who collaborate via the\nInternet. The Debian project's key strengths are its volunteer base, its\ndedication to the Debian Social Contract and Free Software, and its\ncommitment to provide the best operating system possible. This new\nrelease is another important step in that direction.\n</p><p>\nFor further information, please visit the Debian web pages at\n<a href=\"https://www.debian.org/\">https://www.debian.org/</a> or send mail to\n&lt;press@debian.org&gt;.\n</p>","contentLength":8336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44848782"},{"title":"Show HN: The current sky at your approximate location, as a CSS gradient","url":"https://sky.dlazaro.ca/","date":1754745916,"author":"dlazaro","guid":225,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44846281"},{"title":"I want everything local – Building my offline AI workspace","url":"https://instavm.io/blog/building-my-offline-ai-workspace","date":1754677145,"author":"mkagenius","guid":224,"unread":true,"content":"<blockquote><p>I want everything local — no cloud, no remote code execution.</p></blockquote><p>That’s what a friend said. That one-line requirement, albeit simple, would need multiple things to work in tandem to make it happen.</p><p>What does a mainstream LLM (Large Language Model) chat app like ChatGPT or Claude provide at a high level?</p><ul><li>Ability to use chat with a cloud hosted LLM,</li><li>Ability to run code generated by them mostly on their cloud infra, sometimes locally via shell,</li><li>Ability to access the internet for new content or services.</li></ul><p>With so many LLMs being open source / open weights, shouldn't it be possible to do all that locally? But just local LLM is not enough, we need a truely isolated environment to run code as well.</p><p>So, LLM for chat, Docker to containerize code execution, and finally a browser access of some sort for content.</p><p>We wanted a system where:</p><ul><li>LLMs run completely </li><li><strong>Code executes inside a lightweight VM</strong>, not on the host machine</li><li>Bonus:  for automation and internet access</li></ul><p>The idea was to perform tasks which require privacy to be executed completely locally, starting from planning via LLM to code execution inside a container. For instance, if you wanted to edit your photos or videos, how could you do it without giving your data to OpenAI/Google/Anthropic? Though they take security seriously (more than many), it's just a matter of one slip leading to your private data being compromised, a case in point being the early days of ChatGPT when user chats were <a href=\"https://www.bloomberg.com/news/articles/2023-03-21/openai-shut-down-chatgpt-to-fix-bug-exposing-user-chat-titles\">accessible</a> from another's account!</p><blockquote><p>💡 We ran this entirely on Apple Silicon, using  for isolation.</p></blockquote><h3>🛠️  Our Attempt at a Mac App</h3><p>We started with zealous ambition: make it feel native. We tried using , hoping it could help generate a Mac app. But it appears to be meant more for iOS app development — and getting it to work for MacOS was painful, to say the least.</p><p>Even with help from the \"world's best\" LLMs, things didn't go quite as smoothly as we had expected. They hallucinated steps, missed platform-specific quirks, and often left us worse off.</p><p>Then we tried wrapping a  app inside Electron. It took us longer than we'd like to admit. As of this writing, it looks like there's just no (clean) way to do it.</p><p>So, we gave up on the Mac app. The local web version of  was good enough — simple, configurable, and didn't fight back.</p><p>We thought  provided multiple LLM support out-of-the-box, as their landing page shows a drop-down of models. But, no.\nSo, we had to look for examples on how to go about it, and  appeared to be the popular choice.\nFinally we had a dropdown for model selection. We decided not to restrict the set to just local models, as smaller local models are not quite there just yet. Users can get familiar with the tool and its capabilities, and later as small local models become better, they can just switch to being completely local.\n<img src=\"https://instavm.io/blog-images/Pasted%20image%2020250729225206.png\"></p><p>Our use-case also required us to have models that support tool-calling. While some models do, Ollama has not implemented the tool support for them. For instance:</p><pre><code>responseBody: '{\"error\":\"registry.ollama.ai/library/deepseek-r1:8b does not support tools\"}',\n</code></pre><p>And to add to the confusion, Ollama has decided to put this model under tool calling category on their site. Understandably, with the fast-moving AI landscape, it can be difficult for community driven projects to keep up.</p><p>At the moment, essential information like whether a model has tool-support or not, pricing per token, for various models are so fickle. A model's official page mentions tool-support but then tools like Ollama take a while to implement them. Anyway, we shouldn't complain - it's open source, we could've contributed.</p><p>After the UI was MVP-level sorted, we moved on to the isolated VM part. Recently Apple released a tool called 'Container'. Yes, that's right. So, we checked it out and it seemed better than Docker as it provided one isolated VM per container - a perfect fit for running AI generated code.\nSo, we deployed a Jupyter server in the VM, exposed it as MCP (Model Context Protocol) tool, and made it available at <code>http://coderunner.local:8222/mcp</code>.</p><p>The advantage of MCPing vs a exposing an API is that existing tools that work with MCPs can use this right away. For instance, Claude Desktop and Gemini CLI can start executing AI-generated code with a simple config.</p><pre><code>\"mcpServers\": {\n    \"coderunner\": {\n      \"httpUrl\": \"http://coderunner.local:8222/mcp\"\n    }\n}\n</code></pre><p>As you can see below, Claude figured out it should use the tool  exposed from our isolated VM via the MCP endpoint.\n<img src=\"https://instavm.io/blog-images/Pasted%20image%2020250730135012.png\">\nAside - if you want to just use the  bit as an MCP to execute code with your existing tools, the code for  is <a href=\"https://github.com/instavm/coderunner\">public</a>.</p><blockquote><p>A tangent - if you're planning to work with Apple  and building VM images using it, have an abundance of patience. The build keeps failing with  error or just hangs without any output. To continue, you should  all container processes and restart the  tool. Then remove the  image so that the next  process fetches a fresh one.\nAnd repeat the three steps till it successfully works; this can take hours. We are excited to see Apple container mature as it moves beyond its early stages.</p></blockquote><p>Back to our app, we tested the  on a task to  and it worked!</p><p>After the coderunner was verified to be working, we decided to add the support of a headless browser. The main reason was to allow the app to look for new/updated tools/information online, for example, browsing github to find installation instruction for some tool it doesn't yet know about. Another reason was laying the foundation for .\nWe chose Playwright for the task. We deployed it in the same container and exposed it as an MCP tool. Here is one task we asked it to do -</p><p>With this our basic set up was ready: <strong>Local LLM + Sandboxed arbitrary code execution + Headless browser</strong> for latest information.</p><h2>What It Can Do (Examples)</h2><ol><li><strong>Generate and render charts</strong> from CSV using plain English</li><li> (via ) — e.g., “cut between 0:10 and 1:00”</li><li> — resize, crop, convert formats</li><li><strong>Install tools from GitHub</strong> in a containerized space</li><li> to fetch pages and summarize content etc.</li></ol><p>We mapped a volume from\n (host)\nto\n (container)</p><p>So files edited/generated stay in a safe shared space, <strong>but code never touches the host system</strong>.</p><ul><li>Currently <strong>only works on Apple Silicon</strong> (macOS 26 is optional)</li><li>Needs better UI for managing tools and output streaming</li><li>Headless browser is classified as bot by various sites</li></ul><p>This is more than a just an experiment. It's a philosophy shift <strong>bringing compute and agency back to your machine</strong>. No cloud dependency. No privacy tradeoffs. While the best models will probably be always with the giants, we hope that we will still have local tools which can get our day-to-day work done with the privacy we deserve.</p><blockquote><p>We didn't just imagine it. We built it. And now, you can use it too.</p><p>Check out  on <a href=\"https://github.com/instavm/coderunner-ui\">Github</a> to get started, and let us know what you think. We welcome feedback, issues and contributions.</p></blockquote>","contentLength":6809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44840013"},{"title":"Ultrathin business card runs a fluid simulation","url":"https://github.com/Nicholas-L-Johnson/flip-card","date":1754653264,"author":"wompapumpum","guid":223,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44835879"},{"title":"Vibechart","url":"https://www.vibechart.net/","date":1754602605,"author":"datadrivenangel","guid":222,"unread":true,"content":"<div>\n                To chart based on what you want to see instead of what is true, beautiful, or useful.\n            </div>","contentLength":115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44830684"},{"title":"GPT-5","url":"https://openai.com/gpt-5/","date":1754586021,"author":"rd","guid":221,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44826997"},{"title":"Emailing a one-time code is worse than passwords","url":"https://blog.danielh.cc/blog/passwords","date":1754533165,"author":"max__dev","guid":220,"unread":true,"content":"<div><p>Too many services have been using the following login method:</p><ul><li>Enter an email address or phone number</li><li>The website will send a 6-digit code</li><li>Use the 6-digit code to log in</li></ul><p>This is terrible for account security:</p><ul><li>An attacker can simply send your email address to a legitimate service, and prompt for a 6-digit code. You can't know for sure if the code is supposed to be entered in the right place. Password managers (a usual defense against phishing) can't help you either.</li><li>In fact, this attack method has been successfully used in the wild: Microsoft's login for Minecraft accounts use this login method, and <a href=\"https://www.reddit.com/r/hypixel/comments/1l7h2be/account_verification_scam_help_me_if_you_can/\" target=\"_blank\" rel=\"noreferrer\">many</a><a href=\"https://www.reddit.com/r/Minecraft/comments/1ell43o/microsoft_account_discord_verification_scam_and/\" target=\"_blank\" rel=\"noreferrer\">accounts</a><a href=\"https://www.youtube.com/watch?v=QXHGFXq28oQ\" target=\"_blank\" rel=\"noreferrer\">have</a><a href=\"https://learn.microsoft.com/en-us/answers/questions/5493482/i-fell-to-a-phishing-scam-on-discord-how-i-get-my\" target=\"_blank\" rel=\"noreferrer\">been</a><a href=\"https://www.youtube.com/watch?v=qWwU9uK2b3I\" target=\"_blank\" rel=\"noreferrer\">stolen</a> already.</li></ul></div>","contentLength":634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44819917"}],"tags":["dev","hn"]}