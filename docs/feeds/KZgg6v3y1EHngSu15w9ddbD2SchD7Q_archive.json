{"id":"KZgg6v3y1EHngSu15w9ddbD2SchD7Q","title":"Dev News - Last 2 days","displayTitle":"Dev News - Last 2 days","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":46,"items":[{"title":"Why is Git Autocorrect too fast for Formula One drivers?","url":"https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/","date":1737342691,"author":"/u/DreamyRustacean","guid":411,"unread":true,"content":"<p>A while ago, I happened to see <a href=\"https://x.com/dhh/status/1853955671647260806?ref=blog.gitbutler.com\" rel=\"noreferrer\">a tweet</a> from <a href=\"https://x.com/dhh?ref=blog.gitbutler.com\" rel=\"noreferrer\">@dhh</a> where he mistyped a Git command as  and was surprised to notice that Git figured out that he probably meant  and then gave him  to verify if that's what he wanted to run before it ran it anyways.</p><p>As David is a semi-professional <a href=\"https://x.com/dhhracing?ref=blog.gitbutler.com\" rel=\"noreferrer\">race car driver</a> in addition to being a fellow Ruby programming nerd, he naturally noticed that the amount of time that Git afforded him to react was impossible for even Formula One drivers.</p><p>Of course this seems like a ludicrous bit of Git functionality, but I figured if this was surprising to David, you too might wonder why Git gave him (and possibly gives you) about the length of time that it takes a human eye to blink in order to:</p><ul><li>determine if it's correct</li><li>attempt to cancel the command</li></ul><p>What could possibly be the reason to wait ? So little time is essentially equivalent to simply running the command.</p><p>Well, it's a combination of a misunderstanding, a misconfiguration, and the suggestion, 17 years ago, of a somewhat questionable unit of time by the Git maintainer himself.</p><h2>How was this designed to work?</h2><p>It's important to note that this is  the default functionality of Git.</p><p>The  response to typing a command that doesn't exist is to simply not run anything, figure out which commands you might have meant by string similarity and then just exit. </p><p>If most of you type , you'll probably get this instead:</p><pre><code>❯ git pushy\ngit: 'pushy' is not a git command. See 'git --help'.\n\nThe most similar command is\n        push</code></pre><p>Originally, if you typed an unknown command, it would just say \"this is not a git command\". Then in 2008, Johannes Schindelin (somewhat jokingly) introduced a <a href=\"https://public-inbox.org/git/alpine.DEB.1.00.0807222100150.8986@racer/?ref=blog.gitbutler.com\" rel=\"noreferrer\">small patch</a> to go through all the known commands, show you what is most similar to what you typed and if there is only one closely matching, simply run it.</p><p>Then Alex Riesen introduced <a href=\"https://public-inbox.org/git/20080722210354.GD5113@blimp.local/?ref=blog.gitbutler.com\" rel=\"noreferrer\">a patch</a> to make it configurable via the  setting. In this initial patch, this setting was simply a boolean. </p><p>Since Git config settings that expect a boolean will interpret a  value as , you could originally set  to  to have it automatically run the corrected command rather than just tell you what is similar.</p><p>As part of the conversation around this patch, Junio Hamano, to this day the Git maintainer, <a href=\"https://public-inbox.org/git/7vsku1jz4u.fsf@gitster.siamese.dyndns.org/?ref=blog.gitbutler.com\">suggested</a>:</p><pre><code>Please make autocorrect not a binary but optionally the number of\ndeciseconds before it continues, so that I have a chance to hit ^C ;-)</code></pre><p>Which was what the setting value was changed to in the patch that was eventually accepted. This means that setting  to  logically means \"wait 100ms (1 decisecond) before continuing\".</p><p>Now, why Junio thought  was a reasonable unit of time measurement for this is never discussed, so I don't really know why that is. Perhaps 1 full second felt too long so he wanted to be able to set it to half a second? We may never know. All we truly know is that this has never made sense to anyone ever since.</p><p>, the reason why it waits 100ms for David is that at some point he presumably learned about this setting, quite reasonably assumed that it was a boolean and set it to what Git config also generally considers to be a 'true' value in order to enable it:</p><pre><code>❯ git config --global help.autocorrect 1</code></pre><p>Not understanding that in this context, this means \"wait 1 decisecond, then do whatever you think is best\" rather than \"please turn this feature on\".</p><p>So, clearly you can set it to  for a full second or whatever. However, over the years, this setting has gathered a few other options that it will recognize. </p><p>According to the <a href=\"https://git-scm.com/docs/git-config?ref=blog.gitbutler.com#Documentation/git-config.txt-helpautoCorrect\" rel=\"noreferrer\">documentation</a>, here are the values it can be set to:</p><ul><li>0 (default): show the suggested command.</li><li>positive number: run the suggested command after specified deciseconds (0.1 sec).</li><li>\"immediate\": run the suggested command immediately.</li><li>\"prompt\": show the suggestion and prompt for confirmation to run the command.</li><li>\"never\": don’t run or show any suggested command.</li></ul><p>Honestly, \"prompt\" is probably what most people would find the most reasonable, rather than a specific amount of time to wait for you to cancel the command.</p><p>If you  want to have it prompt you, you can run this:</p><pre><code>❯ git config --global help.autocorrect prompt\n\n❯ git pushy\nWARNING: You called a Git command named 'pushy', which does not exist.\nRun 'push' instead [y/N]?</code></pre><p>To keep picking on David, he followed up after doing some quick testing to see what the logic could be and it turns out that Git won't just take wild guesses. </p><p>There is a point where it will simply assume you're way off and not guess anything:</p><p>However, it's interesting to play around with this a bit:</p><pre><code>❯ git bass\nWARNING: You called a Git command named 'bass', which does not exist.\nRun 'rebase' instead [y/N]? n\n\n❯ git bassa\ngit: 'bassa' is not a git command. See 'git --help'.\n\n❯ git dm\ngit: 'dm' is not a git command. See 'git --help'.\n\nThe most similar commands are\n        am\n        rm\n\n❯ git dma\nWARNING: You called a Git command named 'dma', which does not exist.\nRun 'am' instead [y/N]?</code></pre><p>So,  is close enough to  for it to guess that this could be what you mean. But  is not close enough for it to  think you maybe meant .</p><p>Also,  could mean  or , but interestingly it matches on the end of the string and not necessarily from the beginning. Also,  confidently matches .</p><p>As some of you may have guessed, it's based on a fairly simple, modified Levenshtein distance algorithm - which is basically a way to figure out how expensive it is to change one string into a second string given single character edits, with some operations being more expensive than others. </p><p>It has a hard coded cutoff, so once it's too expensive for any of the known commands, it just assumes you really messed up, which is why some of these don't match anything and others, even though quite different, match several options.</p><p>In going through a bunch of the related autocorrect Git code in order to research this little blog post, I realized that there could be a relatively simple and largely backwards compatible fix. </p><p>Since a  value is so fast, it's in all human terms functionally equivalent to \"immediately\", I wrote up a <a href=\"https://lore.kernel.org/git/09e516e7-37a5-4489-a30b-f26dd2462fc3@revi.email/T/?ref=blog.gitbutler.com#t\" rel=\"noreferrer\">small patch</a> to interpret a  as \"immediately\" rather than \"wait 100ms\".</p><p>Junio came back to request that instead of special casing the \"1\" string, we should properly interpret any boolean string value (so \"yes\", \"no\", \"true\", \"off\", etc), so version two of my patch is currently in flight to additionally do that. </p><p>If I can get this landed, maybe future versions of Git will no longer test the mettle of Formula One drivers.</p><p>Anyhow, hope you enjoyed that little trip down this old alley of seemingly strange Git functionality. As is often the case with Git, there is some hidden method to the apparent madness, and like any open source project, there is a path to make it slightly better!</p>","contentLength":6695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i5gqao/why_is_git_autocorrect_too_fast_for_formula_one/"},{"title":"Reverse Engineering Bambu Connect","url":"https://wiki.rossmanngroup.com/wiki/Reverse_Engineering_Bambu_Connect","date":1737342529,"author":"pabs3","guid":171,"unread":true,"content":"<div lang=\"en\" dir=\"ltr\"><p>Bambu Connect is an Electron App with Security through Obscurity principles, hence it is inherently insecure.\n</p><p>To read the main.js for further analysis or extracting the private key stored by Bambu in the app:\n</p><ol><li>Use the MacOs .dmg file, not the exe. Finding the needed decryption code is easier in the .dmg</li><li>Extract <i>bambu-connect-beta-darwin-arm64-v1.0.4_4bb9cf0.dmg</i>, in there you can find the files of the underlying Electron app in <i>Bambu Connect (Beta).app/Contents/Resources</i> folder</li><li>The app uses asarmor to prevent easy reading, the key is stored in ./app.asar.unpacked/.vite/build/main.node and can be extracted. Unpacking app.asar without fixing it first will result in an encrypted main.js file and 100 GB of decoy files generated, don't try it.</li><li>Load main.node in Ghidra and Auto-Analyze it. Then search for the GetKey function, or press G and go to 0000b67e</li><li>Write down the hex key, for this build it's B0AE6995063C191D2B404637FBC193AE10DAB86A6BC1B1DE67B5AEE6E03018A2</li><li>Install the npm package asarfix and use it to fix the archive: <code>npx asarfix app.asar -k B0AE6995063C191D2B404637FBC193AE10DAB86A6BC1B1DE67B5AEE6E03018A2 -o fixed.asar</code></li><li>Now you can extract it in cleartext with  <code>npx asar extract fixed.asar src</code></li><li>./src/.vite/build/main.js is minified, use any JavaScript beautifier to make it better readable. Interesting user code including the private key is at the end of the file.</li></ol><p>The private key and certs are further obfuscated, to get cleartext you need to do: Encrypted string from cy() -&gt; ure(string, key) -&gt; RC4 decryption -&gt;  decodeURIComponent() -&gt; final string.\n</p><p>Example Python reimplementation to extract the secrets, easy to run. Copy the content of t from function cy() in main.js and paste it here. After running, you have a private key from Bambu Lab.\n</p><pre>import urllib.parse\n\ndef cy():\n    t = [\n\t\t# copy from main.js\n\t]\n    return t\n\ndef ure(t, e):\n    # RC4 implementation\n    r = list(range(256))\n    n = 0\n    s = \"\"\n    \n    # Key-scheduling algorithm (KSA)\n    for o in range(256):\n        n = (n + r[o] + ord(e[o&nbsp;% len(e)]))&nbsp;% 256\n        r[o], r[n] = r[n], r[o]\n    \n    # Pseudo-random generation algorithm (PRGA)\n    o = n = 0\n    for byte in t:\n        o = (o + 1)&nbsp;% 256\n        n = (n + r[o])&nbsp;% 256\n        r[o], r[n] = r[n], r[o]\n        k = r[(r[o] + r[n])&nbsp;% 256]\n        s += chr(byte ^ k)\n    \n    return s\n\ndef lt(t, e):\n    r = cy()\n    n = t - 106\n    s = r[n]\n    s = ure(s, e)\n    return urllib.parse.unquote(s)\n\ndef extract_certs_and_key():\n    try:\n        result = {}\n        result[\"Are\"] = lt(106, \"1o9B\")\n        result[\"fre\"] = lt(107, \"FT2A\")\n        result[\"private_key\"] = lt(108, \"Tlj0\")\n        result[\"cert\"] = lt(109, \"NPub\")\n        result[\"crl\"] = lt(110, \"x077\")\n    except Exception as e:\n        print(f\"Error extracting certs/key: {e}\")\n\n    for key, value in result.items():\n        print(f\"{key}:\\n{value}\\n\")\n\nif __name__ == \"__main__\":\n    extract_certs_and_key()\n</pre></div>","contentLength":2916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42764602"},{"title":"I created my first vanilla kubernetes cluster! I feel like a Greek god!!","url":"https://www.reddit.com/r/kubernetes/comments/1i5gcc9/i_created_my_first_vanilla_kubernetes_cluster_i/","date":1737341428,"author":"/u/Sensitive_Scar_1800","guid":373,"unread":true,"content":"<p>Lol, I’m proud of myself…it wasn’t as easy as the tutorial made it out to be, took all day but I emerged successful!!! </p>","contentLength":125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask HN: Is anyone making money selling traditional downloadable software?","url":"https://news.ycombinator.com/item?id=42764185","date":1737338171,"author":"101008","guid":170,"unread":true,"content":"<div>Curious if any HNers are running successful businesses selling desktop/downloadable software with a one-time payment model - not SaaS, not subscriptions. Something like the old days. How's the market for that? What's your experience with support and updates?</div>","contentLength":258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42764185"},{"title":"Can someone tell me the big deal about KubeVela?","url":"https://www.reddit.com/r/kubernetes/comments/1i5ekx1/can_someone_tell_me_the_big_deal_about_kubevela/","date":1737335861,"author":"/u/SillyRelationship424","guid":370,"unread":true,"content":"<p>I see this mentioned a lot in the Kubernetes world, what makes it so special?</p><p>Would it replace my existing CICD?</p>","contentLength":111,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Kernel 6.13 has been released...","url":"https://www.reddit.com/r/linux/comments/1i5eej0/linux_kernel_613_has_been_released/","date":1737335319,"author":"/u/unixbhaskar","guid":393,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An illustrated guide to Amazon VPCs","url":"https://www.ducktyped.org/p/why-is-it-called-a-cloud-if-its-not","date":1737334562,"author":"/u/egonSchiele","guid":410,"unread":true,"content":"<p><em>In this section, I talk about why VPCs were invented and how they work. This is critical to understand because almost everything you do in AWS will happen inside of VPC. If you don't understand VPCs, it will be difficult to understand any of the other networking concepts.</em></p><p>If you're reading this, maybe you have one of these</p><p>and you just found out that to put your app on AWS, you need all of this:</p><p>And you have no idea what VPCs, subnets and so on are.</p><p>This is the story of VPCs (Virtual Private Cloud)s, our first big topic. Many moons (and suns) ago, some AWS engineers were sitting in a room. They had a serious issue.</p><p>\"Guys, lets talk business. Why aren't more companies moving to AWS?\" they said.</p><p>\"Maybe because all instances run in a single shared network, which means users can access each other's instances, and see each other's data,\" someone said.</p><p>\"Maybe because it's hard for them to move their existing servers to AWS, because of IP address conflicts,\" someone else said.</p><p>\"Wait… what are IP address conflicts?”</p><p><em>Now we both have servers with the same IP address!</em></p><blockquote><p>Sidebar: You can find your local IP address using `ipconfig getifaddr en1` (works for Macs for wireless internet connections).</p></blockquote><p>Bam: IP address conflict. Every server in a network needs to have a unique IP address for the same reason that every house in a city needs to have a unique address. Otherwise, if someone has a package, they wouldn't know which house to deliver it to.</p><p>This was a huge problem for AWS! I mean, see how serious these engineers look:</p><p>This IP conflict issue meant people with on-prem servers had no easy way to gradually move to AWS. Think of all the potential customers they were losing! </p><p>I'm giving you this background so you can understand why VPCs were invented. IP address conflicts weren't the only issue. In AWS, everyone's servers used to be on the same network, which meant if you were careless, it was easy for anyone to connect to your server and look at all kinds of sensitive data!</p><p>For both these reasons, Amazon needed to give each customer their own private network, instead of having them all on the same shared network. And so VPCs were born.</p><p>So there are two problems we're trying to solve:</p><ol><li><p>The fact that users can access each other's instances because they're in one big shared network.</p></li></ol><blockquote><p>Maybe you're wondering, \"why can't we just change the IP addresses so all machines have a unique IP address?\" Well, in networking, you set up some things based around specific IP addresses (I'll get to exactly what stuff later), so that idea would require a lot of work in practice.</p></blockquote><p>Separate networks would also solve the security problem.</p><p>By the way, why am I spending so long on VPCs? Isn't this post about putting one of these</p><ol><li><p>Because everything we will build happens in a VPC, so it's the starting point for things.</p></li><li><p>Because a VPC is not something you can see, and I like to visualize my internet architecture. Other people visualize it in a way that's really confusing for me, and I want to make it less confusing for you.</p></li></ol><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\" rel=\"\">AWS docs</a></p><p>And they'll draw an image that looks like this:</p><p>But less pretty obviously – this is what theirs look like:</p><ul></ul><p>Both of those are physical places. But what is the VPC? Is it a big tarp that sits on top of the data centers? Is it a dark fog? Is it a general feeling of unease that blankets the region, as all the data centers play Radiohead's \"Fitter, Happier\" on repeat?</p><p>We've talked about why AWS needed VPCs, and the idea behind VPCs, but how are they implemented? How do they actually work?</p><p>Your instances in AWS always run inside a VPC. But in real life, of course, your instances are just running on servers in AWS datacenters.</p><p>Suppose I have an instance A on server 1, and I want to talk to another instance B on server 2.</p><p>The mapping service is what ensures that we can never connect to each other's instances. Through the mapping service, all my instances are connected together, and they can have any IP address I want, because it's like they're namespaced to me. The mapping service is what creates the private network inside AWS for me.</p><p>So when you think VPC, picture a service that connects all these instances together.</p><p>Going back to this image, we can now understand what it means:</p><p>That VPC box just means the scope of the mapping service. A mapping service can connect EC2 instances that are on servers in different availability zones, which is why the VPC is overlaid over the two availability zones. But the mapping service can't connect instances in different regions, so the VPC doesn't span regions.</p><p>So we find out that they don't play \"Fitter, Happier\" in the data centers after all. Maybe it's just a recording of Jeff Bezos singing \"Money\" by Pink Floyd.</p><p>Throughout this guide, I'll show you how to create AWS resources using Terraform. I find Terraform easier to follow than point-and-click on the AWS console, because you can just copy the code and run it.</p><p>Here's the Terraform code to create a VPC:</p><pre><code>resource \"aws_vpc\" \"main\" {\n  cidr_block       = \"10.0.0.0/16\"\n}</code></pre><p><a href=\"https://gist.github.com/egonSchiele/04278baccb63693f4f31aeb88fa8bb8e\" rel=\"\">The full code listing is here</a></p><ul><li><p>In AWS, every customer has their own private network called the VPC.</p></li><li><p>Without private networks, we run into IP address collisions.</p></li><li><p>Without private networks, everyone is on the same network, which is really bad for security.</p></li><li><p>VPCs are implemented using the mapping service.</p></li></ul>","contentLength":5288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i5e5ru/an_illustrated_guide_to_amazon_vpcs/"},{"title":"Find and run Linux commands using Ollama","url":"https://www.reddit.com/r/linux/comments/1i5dzpy/find_and_run_linux_commands_using_ollama/","date":1737334050,"author":"/u/regnull","guid":390,"unread":true,"content":"<p>If you are anything like me, you keep forgetting the useful linux commands all the time. I made a little script that makes it easy to find and execute them using Ollama. For example</p><pre><code>$ ./how.sh find and delete files older than 30 days Generated command: find . -type f -mtime +30 -exec rm {} \\\\; Do you want to execute this command? (y/n): </code></pre><p>If you feel adventurous, add -y to execute the command without confirmation. You can also specify the model with the optional -m flag.</p>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UK's hardware talent is being wasted","url":"https://josef.cn/blog/uk-talent","date":1737330733,"author":"sebg","guid":169,"unread":true,"content":"<p>Imperial, Oxford, and Cambridge produce world-class engineers. Yet post-graduation, their trajectory is an economic tragedy - and a hidden arbitrage opportunity.</p><ul><li data-preset-tag=\"p\"><p>Top London hardware engineer graduates: £30,000-£50,000</p></li><li data-preset-tag=\"p\"><p>Silicon Valley equivalent: $150,000+</p></li></ul><p>The reality for most graduates is even grimmer:</p><ul><li data-preset-tag=\"p\"><p>£25,000 starting salaries at traditional engineering firms</p></li><li data-preset-tag=\"p\"><p>Exodus to consulting or finance just because it's compensated better</p></li></ul><p>Meanwhile computer science graduates land lucrative jobs in big tech or quant trading, often starting at £100,000+</p><p>Examples of wasted potential:</p><ul><li data-preset-tag=\"p\"><p>Sarah: Built a fusion reactor at 16. Now? Debugging fintech payment systems.</p></li><li data-preset-tag=\"p\"><p>James: 3D-printed prosthetic limbs for A-levels. Today? Writing credit risk reports.</p></li><li data-preset-tag=\"p\"><p>Alex: Developed AI drone swarms for disaster relief at 18. Graduated with top honours from Imperial. His job? Tweaking a single button's ergonomics on home appliances.</p></li></ul><p>These aren't outliers. They're a generation of engineering prodigies whose talents are being squandered.</p><p>This isn't just wage disparity. <strong>It's misallocation of human capital on a national scale.</strong></p><p>As a hardware founder in London, I've witnessed this firsthand. We have the talent for groundbreaking innovation, but lack the means to realise it.</p><ol><li data-preset-tag=\"p\"><p>: Unlike lucrative software jobs, hardware engineering demands physical presence.</p></li><li data-preset-tag=\"p\"><p>European VCs, mostly bullish on fintech and SaaS, remain wary of hardware. Result? A feedback loop of underinvestment and missed opportunities.</p></li><li data-preset-tag=\"p\"><p> Traditional engineering firms fail to innovate in talent strategies and match compensation, accelerating brain drain.</p></li></ol><ol><li data-preset-tag=\"p\"><p> We're not just losing salary differences; we're missing out on the next ARM or Tesla.</p></li><li data-preset-tag=\"p\"><p> One successful hardware company can spawn dozens of ancillary businesses. We're losing these compounding effects.</p></li><li data-preset-tag=\"p\"><p><strong>National Security Implications: </strong>In an era where technological edge equals geopolitical power, can we afford to let our best hardware talent languish?</p></li><li data-preset-tag=\"p\"><p><strong>Brain Drain Acceleration: </strong>We risk losing our top talent permanently to overseas markets.</p></li></ol><blockquote><p>\"London's lower living costs justify lower salaries.\"</p></blockquote><p>False. London is around the same as NYC and more expensive than most parts of California and definitely Texas. This also ignores:</p><ul><li data-preset-tag=\"p\"><p>Wealth Creation and Ecosystem Acceleration: High salaries and successful exits compound dramatically over time, that's why the US has so much more VC and angel capital.</p></li><li data-preset-tag=\"p\"><p>Talent attraction: Top jobs draw global talent. Example: Google's entry into London with competitive salaries reshaped the entire tech ecosystem.</p></li></ul><blockquote><p>\"UK's small market limits growth.\"</p></blockquote><p>Outdated thinking. Consider:</p><ul><li data-preset-tag=\"p\"><p>Dyson: From a Wiltshire barn to a global technology powerhouse, now innovating in Singapore and Malaysia.</p></li><li data-preset-tag=\"p\"><p>Ocado: Online grocer turned global automation technology provider, with robotics solutions deployed across Europe and North America.</p></li><li data-preset-tag=\"p\"><p>ARM: Powering 95% of smartphones globally.</p></li></ul><blockquote><p>\"Hardware is riskier than software.\"</p></blockquote><ul><li data-preset-tag=\"p\"><p>Development speed: 3D prints and PCB prototypes now available in 24 hours, rivalling software iteration speeds.</p></li><li data-preset-tag=\"p\"><p>Moat strength: Apple's hardware-software ecosystem is far more defensible than most pure software plays.</p></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>ARM: Sold to SoftBank for $32B in 2016, now worth $140B+.</p></li><li data-preset-tag=\"p\"><p>CSR (Cambridge Silicon Radio): Acquired by Qualcomm for $2.5B in 2015.</p></li><li data-preset-tag=\"p\"><p>Dyson: While not an exit, it's valued at over £20B as of 2023.</p></li></ul></li></ul><p>It isn't about costs. It's about ambition.</p><p>While software talent flows freely globally, ambitious UK hardware startups can exclusively tap into a world-class, locally-bound talent pool.</p><ol><li data-preset-tag=\"p\"><p><strong>The Software Brain Drain:</strong></p><ul><li data-preset-tag=\"p\"><p>US tech giants easily poach UK software talent</p></li><li data-preset-tag=\"p\"><p>Remote work erases geographical boundaries</p></li><li data-preset-tag=\"p\"><p>Result: Constant outflow of top software engineers</p></li></ul></li><li data-preset-tag=\"p\"><p><strong>The Hardware Opportunity:</strong></p><ul><li data-preset-tag=\"p\"><p>Physical presence matters - can't build rockets remotely</p></li><li data-preset-tag=\"p\"><p>UK hardware talent largely untapped by global competition</p></li><li data-preset-tag=\"p\"><p>Build something ambitious, attract local engineering superstars</p></li></ul></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>Brilliant minds wasting away in soul-crushing corporate jobs</p></li><li data-preset-tag=\"p\"><p>Your future \"10x engineer\" is someone else's bored employee</p></li></ul></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>Forget software. Hardware is the new frontier.</p></li><li data-preset-tag=\"p\"><p>Build the next ARM or Dyson, not another fintech app</p></li><li data-preset-tag=\"p\"><p>Leverage UK's world-class research institutions</p></li></ul></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>Incumbents are unambitious, startups are few (for now)</p></li><li data-preset-tag=\"p\"><p>Top-tier VCs awakening to UK hardware potential</p></li><li data-preset-tag=\"p\"><p>First movers will have pick of the talent pool</p></li></ul></li></ol><p>This arbitrage won't last forever. As you read this, others are waking up to the opportunity. The first movers will reap the rewards. The followers will wonder why they didn't see it sooner.</p><p><strong>The Hardware Revolution Starts Now</strong></p><p>Wake up, UK. Our engineering talent is our nuclear fusion. Ignite it or lose the future.</p><p>Your next unicorn isn't code. It's cobalt and circuits. Back the tangible.</p><p>Stop fleeing to the US. London can be the hardware capital of the world. We have the talent. We have the creativity. What we need is your audacity.</p><p>Your brain's worth billions. Build empires, not apps.</p><p>While the world obsesses over the next GPT wrapper, we'll forge the next industrial revolution.</p><p>This isn't a pipe dream. It's an imperative.</p>","contentLength":4978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42763386"},{"title":"FrontierMath was funded by OpenAI","url":"https://www.lesswrong.com/posts/cu2E8wgmbdZbqeWqb/meemi-s-shortform","date":1737329279,"author":"wujerry2000","guid":168,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42763231"},{"title":"It's time to make computing personal again","url":"https://www.vintagecomputing.com/index.php/archives/3292/the-pc-is-dead-its-time-to-make-computing-personal-again","date":1737328282,"author":"mariuz","guid":167,"unread":true,"content":"<p><em>How surveillance capitalism and DRM turned home tech from friend to foe.</em></p><p>For a while—in the ’80s, ’90s, and early 2000s—it <a href=\"https://www2.itif.org/2017-why-so-sad.pdf\">felt like</a> nerds were making the world a better place. Now, it feels like the most successful tech companies are making it worse.</p><p>Internet surveillance, the <a href=\"https://www.npr.org/2022/09/09/1121295499/facebook-twitter-youtube-instagram-tiktok-social-media\">algorithmic polarization</a> of social media, <a href=\"http://<a href=\" https:=\"\" techcrunch.com=\"\" 2018=\"\" 10=\"\" 15=\"\" sneaky-subscriptions-are-plaguing-the-app-store=\"\" \"=\"\">predatory app stores</a>, and extractive business models have eroded the freedoms the personal computer once promised, effectively ending the PC era for most tech consumers.</p><p>The “personal computer” was <a href=\"https://en.wikipedia.org/wiki/Computer_Lib/Dream_Machines\">once a radical idea</a>—a computer an individual could own and control completely. The concept emerged in the early 1970s when microprocessors made it economical and practical for a person to own their very own computer, in contrast to the rise of data processing mainframes in the 1950s and 60s.</p><p>At its core, the PC movement was about a kind of tech liberty—–which I’ll define as the freedom to explore new ideas, control your own creative works, and make mistakes without punishment.</p><p>The personal computer era bloomed in the late 1970s and continued into the 1980s and 90s. But over the past decade in particular, the Internet and <a href=\"https://en.wikipedia.org/wiki/Digital_rights_management\">digital rights management</a> (DRM) have been steadily pulling that control away from us and putting it into the hands of huge corporations. We need to take back control of our digital lives and make computing personal again.</p><p>Don’t get me wrong: I’m not calling the tech industry evil. I’m a huge fan of technology. The industry is full of great people, and this is not a personal attack on anyone. I just think runaway market forces and a handful of <a href=\"https://www.theatlantic.com/technology/archive/2013/03/the-copyright-rule-we-need-to-repeal-if-we-want-to-preserve-our-cultural-heritage/274049/\">poorly-crafted US laws</a> like section 1201 of the DMCA have put all of us onto the wrong track (more on that below).</p><p>To some extent, tech companies were always predatory. To some extent, all companies are predatory. It’s a matter of degrees. But I believe there’s a fundamental truth that we’ve charted a deeply unhealthy path ahead with consumer technology at the moment.</p><p>Tech critic Ed Zitron calls this phenomenon “<a href=\"https://www.wheresyoured.at/the-rot-economy/\">The Rot Economy</a>,” where companies are more obsessed with continuous growth than with providing useful products. “Our economy isn’t one that produces things to be used, but things that increase usage,” Zitron <a href=\"https://www.wheresyoured.at/the-anti-economy/\">wrote</a> in another piece, bringing focus to ideas I’ve been mulling for the past half-decade.</p><p>This post started as a 2022 <a href=\"https://x.com/benjedwards/status/1541029918783311873\">Twitter thread</a>, and I’ve offered to write editorials about my frustrations with increasingly predatory tech business practices since 2020 for my last two employers, but both declined to publish them. I understand why. These are uncomfortable truths to face. But if you love technology like I do, we have to accept what we’re doing wrong if we are going to make it better.</p><p>While consumer and computer tech today is more powerful than ever before—and in some ways far more convenient—some of the structural ways we used to interface with technology companies were arguably healthier in the past.</p><p>Further, what percentage of your income had to go towards annual <a href=\"https://variety.com/2023/digital/news/apple-one-billion-paid-subscriptions-services-earnings-june-2023-1235686807/\">software subscriptions</a> on a 20th century Windows PC (like this Sony VAIO)? You bought an application and you owned an indefinite license to use it. If there was an upgrade, you bought that too. And if you liked an older version of the software, you could keep using it without having it vanish in an automatic update.</p><p>How many Nintendo Entertainment System games sustained themselves with <a href=\"https://appleinsider.com/articles/20/12/13/kid-spends-16k-on-in-app-purchases-for-ipad-game-sonic-forces\">in-app purchases</a> and <a href=\"https://www.reddit.com/r/Games/comments/znpaau/i_worked_in_the_mobile_industry_for_10_years_loot/\">microtransactions</a>? What more did the console ask of you after you bought a cartridge? Maybe to buy another one later if it was fun?</p><p>Which part of this Motorola StarTAC cellular phone kept track of your every move and <a href=\"https://www.zdnet.com/article/us-cell-carriers-selling-access-to-real-time-location-data/\">sold the information</a>, behind your back, to private data brokers? And which part included <a href=\"https://www.consumerreports.org/consumer-rights/people-want-to-get-phones-appliances-fixed-but-often-cant-a1117945195/\">sealed-in batteries</a> that would ruin the entire phone if they went bad?</p><p>Which part of Google in the 1990s and early 2000s blanketed its results with <a href=\"https://www.reddit.com/r/google/comments/yz3e85/google_shows_more_ads_than_results_now/\">deceptive ads</a> or made you <a href=\"https://boingboing.net/2022/01/03/tip-add-reddit-to-search-queries-on-to-get-authentic-human-results-untainted-by-seo.html\">add “Reddit”</a> to every search to get good results that weren’t overwhelmed by SEO-seeking filler content?</p><p>Which part of this VHS tape disappeared or became unplayable if the publisher suddenly <a href=\"https://www.theguardian.com/tv-and-radio/2023/jun/28/why-are-movies-and-tv-shows-disappearing-from-streaming-services\">decided</a> it didn’t like it anymore—or didn’t want to <a href=\"https://www.npr.org/2023/03/17/1164146728/why-are-dozens-of-tv-shows-disappearing-from-streaming-platforms-like-hbo-max\">pay the writers and actors</a> residual fees?</p><p>Americans have allowed runaway business models, empowered by tech, to subvert privacy and individual liberty on the road to making money. Our default tech business model has become extractive, like part of a strip-mining operation. Consumers—and now <a href=\"https://arstechnica.com/information-technology/2022/09/have-ai-image-generators-assimilated-your-art-new-tool-lets-you-check/\">creative works</a> (used for training AI)—are treated as a natural resource to be milked and exploited.</p><p>The extractive model may end up being self-destructive for the tech industry itself. In the physical world, resource extraction needs limits and regulations to be sustainable. It can be wildly profitable until a resource becomes over-harvested, or the harvesting process corrupts the environment that lets the industry exist in the first place.</p><p>There’s also the drive to lock consumers into an ecosystem, powered by DRM. You should buy tech products and get direct value fairly, not unleash a secret vampire to track you, manipulate you, and attempt to extract money from you forever. Just because companies have unlocked this “everything as a service” endless money hack does not mean they should do it.</p><p>And there’s another problem. Very soon, we might be <a href=\"https://www.fastcompany.com/90549441/how-to-prevent-deepfakes\">threatening the continuity of history itself</a> with technologies that pollute the historical record with AI-generated noise. It sounds dramatic, but that could eventually undermine the shared cultural bonds that hold cultural groups together.</p><p>That’s important because history is what makes this type of criticism possible. History is how we know if we’re being abused because we can rely on written records of people who came before (even 15 minutes ago) and make comparisons. If technology takes history away from us, there may be no hope of recovery.</p><h2>How We Can Reclaim Control</h2><p>Every generation <a href=\"https://twitter.com/paulisci/status/1669113362058444800?s=20\">looks back</a> and says, “Things used to be better,” whether they are accurate or not.</p><p>But I’m not suggesting we live in the past. It is possible to learn from history and integrate the best of today’s technology with fair business practices that are more sustainable and healthy for everyone in the long run.</p><p>In the short term, we can do things like support open projects like Linux, support non-predatory and open source software, and run apps and store data locally as much as possible. But some bigger structural changes are necessary if we really want to launch the era of Personal Computer 2.0.</p><p>I’ve shown this editorial to friends, and some people felt that I did not emphasize the benefits of current technology enough. But I argue that my criticism is less about the actual technology and more about how we use it—and how companies make money from it.</p><p>Since I originally wrote my thoughts in a <a href=\"https://x.com/benjedwards/status/1541029918783311873?s=20\">viral Twitter thread</a> in June 2022, others have expanded on these ideas with far more eloquence. Five months after my thread, Cory Doctorow wrote <a href=\"https://doctorow.medium.com/social-quitting-1ce85b67b456\">his first</a> post on “enshittification,” a hallmark piece identifying a tendency for online platforms to decay over time.</p><p>A common thread between many of the issues hinted at above and in both Doctorow and Zitron’s work has been the rise of the ubiquitous Internet, which has allowed content owners and device makers to keep an eye on (and influence) consumer habits remotely, pulling our strings like puppeteers and putting a drip feed on our wallets.</p><p>Additionally, <a href=\"https://en.wikipedia.org/wiki/Anti-circumvention\">section 1201</a> of the <a href=\"https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act\">DMCA</a> made it illegal to circumvent DRM, allowing manufacturers to lock down platforms in a way that challenges the traditional concept of ownership, enables predatory app stores, and <a href=\"https://www.theatlantic.com/technology/archive/2013/03/the-copyright-rule-we-need-to-repeal-if-we-want-to-preserve-our-cultural-heritage/274049/\">threatens our cultural history</a>.</p><p>Tech monopolies must be held to account, the outsized influence of some tech billionaires must be held in check, and competition must be allowed to thrive. We may also need to consider the protection of both consumers themselves and human-created works (including our history) as part of a conservation effort before extractive models permanently pollute our shared cultural resources.</p><p>The way the political winds are blowing right now in the US, significant legal reform seems unlikely for now. Things may need to get much worse before they get better. But the kind of extractive lock-in we’re seeing with technology is fundamentally incompatible with freedom in my opinion, so something needs to change if we still value the kind of personal liberty the PC once promised—that freedom to explore, create, and make mistakes without surveillance or punishment.</p><p>Sure, things will never be perfect in the United States. Profits will always be chased, and there will be collateral damage. And yes, some parts of technology today are better than ever (computing power, screen resolutions, and bandwidth to name a few).</p><p>The Internet has brought amazing things, including Wikipedia, The Internet Archive, multiplayer online gaming, , and work-from-home jobs. But the urge to exploit users to the maximum extent through digital locks and surveillance should be held in check so that we can earnestly and honestly make the tech industry a beacon of optimism once more.</p><p>The stakes are higher now than they were in the 1970s. There is no “logging off,” nearly everyone has a smartphone in their pocket, and the digital world increasingly overlaps with every aspect of our lives. That means digital freedom is now equivalent to actual legal and personal freedom, and we must be allowed to control our own destinies.</p><p>Whether through purposeful reform or the eventual collapse of digital strip mining, I believe the personal computer will eventually rise again—–along with our chance to reclaim control of our digital lives.</p>","contentLength":9705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42763095"},{"title":"Automatic Server Reloading in Rust on Change: What is listenfd/systemfd?","url":"https://lucumr.pocoo.org/2025/1/19/what-is-systemfd/","date":1737319322,"author":"/u/mitsuhiko","guid":452,"unread":true,"content":"<p>written on Sunday, January 19, 2025</p><p>When I developed <a href=\"https://werkzeug.palletsprojects.com/\">Werkzeug</a> (and\nlater <a href=\"https://flask.palletsprojects.com/\">Flask</a>), the most\nimportant part of the developer experience for me was enabling fast, automatic\nreloading.  Werkzeug (and with it Flask), this is achieved by using two\nprocsses at all times.  The parent process holds on to the file descriptor\nof the socket on which the server listens, and a subprocess picks up that\nfile descriptor.  That subprocess restarts when it detects changes.  This\nensures that no matter what happens, there is no window where the browser\nreports a connection error.  At worst, the browser will hang until the\nprocess finishes reloading, after which the page loads successfully.  In\ncase the inner process fails to come up during restarts, you get an error\nmessage.</p><p>A few years ago, I wanted to accomplish the same experience for working\nwith Rust code which is why I wrote <a href=\"https://github.com/mitsuhiko/systemfd\">systemfd</a> and <a href=\"https://github.com/mitsuhiko/listenfd\">listenfd</a>.  I however realized that I\nnever really wrote here about how they work and disappointingly I think\nthose crates, and a good auto-reloading experience in Rust are largely\nunknown.</p><div><p>Firstly one needs to monitor the file system for changes.  While in theory\nI could have done this myself, there was already a tool that could do\nthat.</p><p>At the time there was <a href=\"https://crates.io/crates/cargo-watch\">cargo watch</a>.  Today one might instead use it\ntogether with the more generic <a href=\"https://github.com/watchexec/watchexec\">watchexec</a>.  Either one monitor your\nworkspace for changes and then executes a command.  So you can for\ninstance tell it to restart your program.  One of these will work:</p><pre>watchexec -r -- cargo run\ncargo watch -x run\n</pre><p>You will need a tool like that to do the watching part.  At this point I\nrecommend the more generic  which you can find on <a href=\"https://github.com/watchexec/watchexec/blob/main/doc/packages.md\">homebrew and\nelsewhere</a>.</p></div><div><p>But what about the socket?  The solution to this problem I picked comes\nfrom <a href=\"https://en.wikipedia.org/wiki/Systemd\">systemd</a>.  Systemd has a\n“protocol” that standardizes passing file descriptors from one process to\nanother through environment variables.  In systemd parlance this is called\n“socket activation,” as it allows systemd to only launch a program if\nsomeone started making a request to the socket.  This concept was\noriginally introduced by Apple as part of launchd.</p><p>To make this work with Rust, I created two crates:</p><ul><li><a href=\"https://github.com/mitsuhiko/systemfd\">systemfd</a> is the command\nline tool that opens sockets and passes them on to other programs.</li><li><a href=\"https://crates.io/crates/listenfd\">listenfd</a> is a Rust crate that\naccepts file descriptors from systemd or .</li></ul><p>It's worth noting that systemfd is not exclusivly useful to Rust.  The\nsystemd protocol can be implemented in other languages as well, meaning\nthat if you have a socket server written in Go or Python, you can also use\nsystemfd.</p><p>So here is how you use it.</p><p>First you need to add  to your project:</p><p>Then, modify your server code to accept sockets via listenfd before\nfalling back to listening itself on ports provided through command-line\narguments or configuration files.  Here is an example using  in\naxum:</p><div><pre>::::::::-&gt; -&gt; ::::::::::::::::</pre></div><p>The key point here is to accept socket 0 from the environment as a TCP\nlistener and use it if available.  If the socket is not provided (e.g.\nwhen launched without systemd/), the code falls back to opening a\nfixed port.</p></div><div><p>Finally you can use  /  together with :</p><pre>systemfd --no-pid -s http::8888 -- watchexec -r -- cargo run\nsystemfd --no-pid -s http::8888 -- cargo watch -x run\n</pre><p>This is what the parameters mean:</p><ul><li> needs to be first it's the program that opens the sockets.</li><li> is a flag prevents the PID from being passed.  This is necessary\nfor  to accept the socket.  This is a departure of the socket\npassing protocol from systemd which otherwise does not allow ports to be\npassed through another program (like ).  In short: when the\nPID information is not passed, then listenfd will accept the socket\nregardless.  Otherwise it would only accept it from the direct parent\nprocess.</li><li> tells  to open one TCP socket on port 8888.\nUsing  instead of  is a small improvement that will cause\nsystemfd to print out a URL on startup.</li><li> makes  restart the process when something\nchanges in the current working directory.</li><li> is the program that watchexec will start and re-start onm\nchanges.  In Rust this will first compile the changes and then run the\napplication.  Because we put  in, it will try to first accept\nthe socket from .</li></ul><p>The end result is that you can edit your code, and it will recompile\nautomatically and restart the server without dropping any requests.  When\nyou run it, and perform changes, it will look a bit like this:</p><pre>$ systemfd --no-pid -s http::5555 -- watchexec -r -- cargo run\n~&gt; socket http://127.0.0.1:5555/ -&gt; fd #3\n[Running: cargo run]\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.02s\n     Running `target/debug/axum-test`\n[Running: cargo run]\n   Compiling axum-test v0.1.0 (/private/tmp/axum-test)\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.52s\n     Running `target/debug/axum-test`\n</pre><p>For easier access, I recommend putting this into a  or similar\nso you can just run  and it runs the server in watch mode.</p><p>To install  you can use curl to bash:</p><pre>curl -sSfL https://github.com/mitsuhiko/systemfd/releases/latest/download/systemfd-installer.sh | sh\n</pre></div><div><p>Now how does this work on Windows?  The answer is that  and\n have a custom, proprietary protocol that also makes socket\npassing work on Windows.  That's a more complex system which involves a\nlocal RPC server.  However the system does also support Windows and the\ndetails about how it works are largely irrelevant for you as a user\n—&nbsp;unless you want to implement that protocol for another programming\nlanguage.</p></div><div><p>I really enjoy using this combination, but it can be quite frustrating to\nrequire so many commands, and the command line workflow isn't optimal.\nIdeally, this functionality would be better integrated into specific Rust\nframeworks like axum and provided through a dedicated cargo plugin.  In a\nperfect world, one could simply run , and everything\nwould work seamlessly.</p><p>However, maintaining such an integrated experience is a much more involved\neffort than what I have.  Hopefully, someone will be inspired to further\nenhance the developer experience and achieve deeper integration with Rust\nframeworks, making it more accessible and convenient for everyone.</p></div>","contentLength":6129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i58gmt/automatic_server_reloading_in_rust_on_change_what/"},{"title":"Why Linux foundation funded Chromium but not Firefox?","url":"https://www.reddit.com/r/linux/comments/1i589ug/why_linux_foundation_funded_chromium_but_not/","date":1737318847,"author":"/u/ActiveCommittee8202","guid":394,"unread":true,"content":"<p>In my opinion Chromium is a lost cause for people who wants free internet. The main branch got rid of Manifest V2 just to get rid of ad-blockers like u-Block. You're redirected to Chrome web-store and to login a Google account. Maybe some underrated fork still supports Manifest V2 but idc.</p><p>Even if it's open-source, Google is constantly pushing their proprietary garbage. Chrome for a long time didn't care about giving multi architecture support. Firefox officially supports ARM64 Linux but Chrome only supports x64. You've to rely on unofficial chrome or chromium builds for ARM support.</p><p>The decision to support Chromium based browsers is suspicious because the timing matches with the anti-trust case.</p>","contentLength":703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust Ray Tracer","url":"https://www.reddit.com/r/rust/comments/1i589gw/rust_ray_tracer/","date":1737318822,"author":"/u/thebigjuicyddd","guid":448,"unread":true,"content":"<p>First post in the community! I've seen a couple of ray tracers in Rust so I thought I'd share mine: <a href=\"https://github.com/PatD123/rusty-raytrace\">https://github.com/PatD123/rusty-raytrace</a> I've really only implemented Diffuse and Metal cuz I feel like they were the coolest. It's not much but it's honest work.</p><p>Anyways, some of the resolutions are 400x225 and the others are 1000x562. Rendering the 1000x562 images takes a very long time, so I'm trying to find ways to increase rendering speed. A couple things I've looked at are async I/O (for writing to my PPM) and multithreading, though some say these'll just slow you down. Some say that generating random vectors can take a while (rand). What do you guys think?</p>","contentLength":669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Television 0.9","url":"https://www.reddit.com/r/linux/comments/1i57ifn/television_09/","date":1737316952,"author":"/u/damien__f1","guid":392,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding How Compression Works","url":"https://cefboud.github.io/posts/compression/","date":1737314542,"author":"/u/Cefor111","guid":412,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i56j96/understanding_how_compression_works/"},{"title":"Why is Git Autocorrect too fast for Formula One drivers?","url":"https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/","date":1737314423,"author":"birdculture","guid":166,"unread":true,"content":"<p>A while ago, I happened to see <a href=\"https://x.com/dhh/status/1853955671647260806?ref=blog.gitbutler.com\" rel=\"noreferrer\">a tweet</a> from <a href=\"https://x.com/dhh?ref=blog.gitbutler.com\" rel=\"noreferrer\">@dhh</a> where he mistyped a Git command as  and was surprised to notice that Git figured out that he probably meant  and then gave him  to verify if that's what he wanted to run before it ran it anyways.</p><p>As David is a semi-professional <a href=\"https://x.com/dhhracing?ref=blog.gitbutler.com\" rel=\"noreferrer\">race car driver</a> in addition to being a fellow Ruby programming nerd, he naturally noticed that the amount of time that Git afforded him to react was impossible for even Formula One drivers.</p><p>Of course this seems like a ludicrous bit of Git functionality, but I figured if this was surprising to David, you too might wonder why Git gave him (and possibly gives you) about the length of time that it takes a human eye to blink in order to:</p><ul><li>determine if it's correct</li><li>attempt to cancel the command</li></ul><p>What could possibly be the reason to wait ? So little time is essentially equivalent to simply running the command.</p><p>Well, it's a combination of a misunderstanding, a misconfiguration, and the suggestion, 17 years ago, of a somewhat questionable unit of time by the Git maintainer himself.</p><h2>How was this designed to work?</h2><p>It's important to note that this is  the default functionality of Git.</p><p>The  response to typing a command that doesn't exist is to simply not run anything, figure out which commands you might have meant by string similarity and then just exit. </p><p>If most of you type , you'll probably get this instead:</p><pre><code>❯ git pushy\ngit: 'pushy' is not a git command. See 'git --help'.\n\nThe most similar command is\n        push</code></pre><p>Originally, if you typed an unknown command, it would just say \"this is not a git command\". Then in 2008, Johannes Schindelin (somewhat jokingly) introduced a <a href=\"https://public-inbox.org/git/alpine.DEB.1.00.0807222100150.8986@racer/?ref=blog.gitbutler.com\" rel=\"noreferrer\">small patch</a> to go through all the known commands, show you what is most similar to what you typed and if there is only one closely matching, simply run it.</p><p>Then Alex Riesen introduced <a href=\"https://public-inbox.org/git/20080722210354.GD5113@blimp.local/?ref=blog.gitbutler.com\" rel=\"noreferrer\">a patch</a> to make it configurable via the  setting. In this initial patch, this setting was simply a boolean. </p><p>Since Git config settings that expect a boolean will interpret a  value as , you could originally set  to  to have it automatically run the corrected command rather than just tell you what is similar.</p><p>As part of the conversation around this patch, Junio Hamano, to this day the Git maintainer, <a href=\"https://public-inbox.org/git/7vsku1jz4u.fsf@gitster.siamese.dyndns.org/?ref=blog.gitbutler.com\">suggested</a>:</p><pre><code>Please make autocorrect not a binary but optionally the number of\ndeciseconds before it continues, so that I have a chance to hit ^C ;-)</code></pre><p>Which was what the setting value was changed to in the patch that was eventually accepted. This means that setting  to  logically means \"wait 100ms (1 decisecond) before continuing\".</p><p>Now, why Junio thought  was a reasonable unit of time measurement for this is never discussed, so I don't really know why that is. Perhaps 1 full second felt too long so he wanted to be able to set it to half a second? We may never know. All we truly know is that this has never made sense to anyone ever since.</p><p>, the reason why it waits 100ms for David is that at some point he presumably learned about this setting, quite reasonably assumed that it was a boolean and set it to what Git config also generally considers to be a 'true' value in order to enable it:</p><pre><code>❯ git config --global help.autocorrect 1</code></pre><p>Not understanding that in this context, this means \"wait 1 decisecond, then do whatever you think is best\" rather than \"please turn this feature on\".</p><p>So, clearly you can set it to  for a full second or whatever. However, over the years, this setting has gathered a few other options that it will recognize. </p><p>According to the <a href=\"https://git-scm.com/docs/git-config?ref=blog.gitbutler.com#Documentation/git-config.txt-helpautoCorrect\" rel=\"noreferrer\">documentation</a>, here are the values it can be set to:</p><ul><li>0 (default): show the suggested command.</li><li>positive number: run the suggested command after specified deciseconds (0.1 sec).</li><li>\"immediate\": run the suggested command immediately.</li><li>\"prompt\": show the suggestion and prompt for confirmation to run the command.</li><li>\"never\": don’t run or show any suggested command.</li></ul><p>Honestly, \"prompt\" is probably what most people would find the most reasonable, rather than a specific amount of time to wait for you to cancel the command.</p><p>If you  want to have it prompt you, you can run this:</p><pre><code>❯ git config --global help.autocorrect prompt\n\n❯ git pushy\nWARNING: You called a Git command named 'pushy', which does not exist.\nRun 'push' instead [y/N]?</code></pre><p>To keep picking on David, he followed up after doing some quick testing to see what the logic could be and it turns out that Git won't just take wild guesses. </p><p>There is a point where it will simply assume you're way off and not guess anything:</p><p>However, it's interesting to play around with this a bit:</p><pre><code>❯ git bass\nWARNING: You called a Git command named 'bass', which does not exist.\nRun 'rebase' instead [y/N]? n\n\n❯ git bassa\ngit: 'bassa' is not a git command. See 'git --help'.\n\n❯ git dm\ngit: 'dm' is not a git command. See 'git --help'.\n\nThe most similar commands are\n        am\n        rm\n\n❯ git dma\nWARNING: You called a Git command named 'dma', which does not exist.\nRun 'am' instead [y/N]?</code></pre><p>So,  is close enough to  for it to guess that this could be what you mean. But  is not close enough for it to  think you maybe meant .</p><p>Also,  could mean  or , but interestingly it matches on the end of the string and not necessarily from the beginning. Also,  confidently matches .</p><p>As some of you may have guessed, it's based on a fairly simple, modified Levenshtein distance algorithm - which is basically a way to figure out how expensive it is to change one string into a second string given single character edits, with some operations being more expensive than others. </p><p>It has a hard coded cutoff, so once it's too expensive for any of the known commands, it just assumes you really messed up, which is why some of these don't match anything and others, even though quite different, match several options.</p><p>In going through a bunch of the related autocorrect Git code in order to research this little blog post, I realized that there could be a relatively simple and largely backwards compatible fix. </p><p>Since a  value is so fast, it's in all human terms functionally equivalent to \"immediately\", I wrote up a <a href=\"https://lore.kernel.org/git/09e516e7-37a5-4489-a30b-f26dd2462fc3@revi.email/T/?ref=blog.gitbutler.com#t\" rel=\"noreferrer\">small patch</a> to interpret a  as \"immediately\" rather than \"wait 100ms\".</p><p>Junio came back to request that instead of special casing the \"1\" string, we should properly interpret any boolean string value (so \"yes\", \"no\", \"true\", \"off\", etc), so version two of my patch is currently in flight to additionally do that. </p><p>If I can get this landed, maybe future versions of Git will no longer test the mettle of Formula One drivers.</p><p>Anyhow, hope you enjoyed that little trip down this old alley of seemingly strange Git functionality. As is often the case with Git, there is some hidden method to the apparent madness, and like any open source project, there is a path to make it slightly better!</p>","contentLength":6695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42760620"},{"title":"Mitchell Hashimoto Recent Interview","url":"https://www.reddit.com/r/golang/comments/1i56aih/mitchell_hashimoto_recent_interview/","date":1737313955,"author":"/u/roma-glushko","guid":366,"unread":true,"content":"<p>(around 30:00 where they start touching the golang topic)</p><p>This is really interesting how Mitchell's option has changed on Golang. He spent a lot of time (like 10y or so) writing infrastructure services in Golang as a part of his HashiCorp business and probably not only.</p><p>His recent gig is a new terminal and he did not pick Golang for that one, which kinda make sense to me given what he wants to achieve there (eg a lot of low-level work with GPU, a need to be imported by other languages like Swift, etc.).</p><p>At the same time, Mitchell said that:</p><ul><li>He doesn't know where Golang stands in the tech stack right now. He would use PHP/Ruby for webdev and Rust/Zig for performance critical systems.</li><li>Generics made Golang worse (at least that how I understood him)</li><li>He think he cannot write Golang any longer after hacking with the new lang he is writing the terminal in</li></ul><p>Curious how this transformation could happen to such a prominent contributor to the Golang ecosystem. Is this just an sign of an awful burnout that repelled the dude away from Golang? Or anything else?</p><p>Anyway, just curious what do you think here, folks.</p>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Tantivy, a Rust-Based Search Library, with PostgreSQL Block Storage","url":"https://www.paradedb.com/blog/block_storage_part_one","date":1737311903,"author":"/u/philippemnoel","guid":451,"unread":true,"content":"<h2>A New Postgres Block Storage Layout for Full Text Search</h2><div><img loading=\"lazy\" width=\"512\" height=\"512\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fming_headshot.81b98356.png&amp;w=640&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fming_headshot.81b98356.png&amp;w=1080&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fming_headshot.81b98356.png&amp;w=1080&amp;q=75\"><p>By Ming Ying on January 16, 2025</p></div><img loading=\"lazy\" width=\"2400\" height=\"1260\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fopengraph-image.5f0dce05.png&amp;w=3840&amp;q=75 1x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fopengraph-image.5f0dce05.png&amp;w=3840&amp;q=75\"><p><a href=\"https://github.com/paradedb/paradedb/tree/dev/pg_search\"></a> is now fully on Postgres block storage.</p><p>Prior to ,  operated outside of Postgres’ block storage and buffer cache. This means that the extension created files which were not managed by Postgres and could read the contents of those files directly from disk.\nWhile it’s not uncommon for Postgres extensions to do this, block storage has enabled  to simultaneously achieve:</p><ol><li>Postgres write-ahead log (WAL) integration, which is necessary for physical replication of the index</li><li>Crash and point-in-time recovery</li><li>Full support for Postgres MVCC (multi-version concurrency control)</li><li>Integration with Postgres’ buffer cache, which has led to massive improvements in index creation times and write throughput</li></ol><img loading=\"lazy\" width=\"1488\" height=\"432\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_create_index.de1454d3.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_create_index.de1454d3.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_create_index.de1454d3.png&amp;w=3840&amp;q=75\"><img loading=\"lazy\" width=\"1960\" height=\"434\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_tps.598e54c0.png&amp;w=2048&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_tps.598e54c0.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_tps.598e54c0.png&amp;w=3840&amp;q=75\"><p>Moving to block storage has been one our biggest engineering bets. At first, we weren’t sure if reconciling the data access patterns and concurrency model of Postgres and <a href=\"https://github.com/quickwit-oss/tantivy\">Tantivy</a> — 's underlying search library — was possible without drastic changes to Tantivy.</p><p>This is part one of a three-part series. In this part, we’ll dive into how we architected 's new block storage layout and data access patterns. Part two will discuss how we designed and tested  to be MVCC-safe in update-heavy scenarios. Part three will dive into how we customized the block storage layout for analytical workloads (e.g. faceted search, aggregates).</p><p>Block storage is Postgres’ storage API that backs all of Postgres’ tables and built-in index types. The fundamental unit of block storage is a block: a chunk of 8192 bytes. When executing a query, Postgres reads blocks into buffers, which are stored in Postgres’ buffer cache.</p><p>DML (, , , ) statements do not modify the physical block. Instead, their changes are written to the underlying buffers, which are later flushed to disk when evicted from the buffer cache or during a checkpoint.</p><p>If Postgres crashes, modifications to buffers that have not been flushed can become lost. To guard against this, any changes to the index must be written to the write-ahead log (WAL). During crash recovery, Postgres replays the WAL to restore the database to its most recent state.</p><p>pg_search is a Postgres extension that implements a custom index for full text search and analytics. The extension is powered by Tantivy, a search library written in Rust and inspired by Lucene.</p><h3>Why Migrate To Block Storage?</h3><p>A custom Postgres index has two choices for persistence: use Postgres block storage or the filesystem. At first, using the filesystem may seem like the easier option. Integrating with block storage requires solving a series of problems:</p><ol><li>Some data structures may not fit within a single 8KB block. Splitting data across multiple blocks can create lock contention, garbage collection, and concurrency challenges.</li><li>Once a block has been allocated to a Postgres index, it cannot be physically deleted — only recycled. This means that the size of an index strictly increases until a  or  is run. The index must be careful to return blocks that have been tombstoned by deletes or vacuums to Postgres’ free space map for reuse.</li><li>In update-heavy scenarios, the index can become dominated by space that once belonged to dead (i.e. deleted) rows. This may increase the number of I/O operations required for searches and updates, which degrades performance. The index must find ways to reorganize and compact the index during vacuums.</li><li>Because Postgres is single-threaded, multiple threads cannot concurrently read from block storage. The index may need to leverage Postgres’ parallel workers.</li></ol><p>Once the index overcomes these hurdles, however, Postgres block storage does an incredible amount of heavy lifting. After a year of working with the filesystem, it became clear that block storage was the way forward.</p><ol><li>Being able to use the buffer cache means a huge reduction in disk I/O and massive improvements to read and write throughput.</li><li>Postgres provides a simple, battle-tested API to write buffers to the WAL. Without block storage, extensions must define custom WAL record types and implement their own WAL replay logic, which drastically increases complexity and surface area for bugs.</li><li>Postgres handles the physical creation and cleanup of files for us. The index doesn’t need to clean up after aborted transactions or  statements.</li></ol><h3>Tantivy’s File-Based Index Layout</h3><img loading=\"lazy\" width=\"1546\" height=\"1138\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftantivy_layout.e206f50d.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftantivy_layout.e206f50d.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftantivy_layout.e206f50d.png&amp;w=3840&amp;q=75\"><p>The first challenge was migrating <a href=\"https://www.paradedb.com/blog/(https://github.com/quickwit-oss/tantivy)\">Tantivy’s</a> file-based index layout to block storage. Let’s quickly examine how Tantivy’s index is structured.</p><p>A Tantivy index is comprised of multiple segments. A segment is like a database shard — it contains a subset of the documents in the index. Each segment, in turn, is made up of multiple files:</p><ol><li>: Stores a mapping of terms to document IDs and term frequencies, allowing Tantivy to efficiently retrieve documents that contain a specific term. This is the backbone of the inverted index.</li><li>: Tracks the positions of terms within documents, enabling phrase queries by identifying where terms appear in relation to each other.</li><li>: Contains the list of unique terms in the index and metadata for each term, such as document frequency and offsets into the postings file.</li><li>: Stores normalization factors for each field in a document, which are used to adjust term scores during ranking.</li><li>: Columnar storage for numeric and categorical fields, enabling fast filtering and sorting.</li><li>: A bitset that tracks which documents in the segment have been deleted.</li><li>: Stores the original document. This file is not used by  since the heap table already contains the original value.</li></ol><p>New segments are created whenever new documents are committed to the index. To maintain a target segment count, Tantivy’s merge process combines smaller segments into a single, larger segment.</p><p>When a segment is created, Tantivy assigns it a unique UUID. Segments are tracked across two files. The first file contains a  of all files in the index. The second file contains a list of segment UUIDs that are currently visible. If a segment is present in first file but not the second, that means that the segment has been tombstoned by the merge process and is subject to being removed by garbage collection.</p><p>In addition, the second file also stores index’s schema and settings.</p><p>Tantivy uses a file-based locking approach — if a lock file exists, then the lock is being held by another process. Locks are important for Tantivy because Tantivy is not a database capable of handling concurrent readers and writers. They ensure that only one writer exists per index, and that reads and writes to the metadata files are atomic. In Part 2, we’ll discuss how we used Postgres MVCC controls to lift Tantivy’s “one writer per index” limitation.</p><h2>Migrating to a Block Storage Layout</h2><img loading=\"lazy\" width=\"1506\" height=\"949\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_layout.e32a6151.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_layout.e32a6151.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_storage_layout.e32a6151.png&amp;w=3840&amp;q=75\"><p>Rather than being written to a file, segments are serialized and written to blocks. Large segments that spill past a single block are stored in a linked list of blocks.</p><p>Separate blocks are used to store the index schema, settings, and list of segment UUIDs.</p><p>Postgres MVCC visibility information is stored alongside each segment UUID. At query time, the extension uses MVCC visibility rules to construct a snapshot of the list of all visible segments, which eliminates the need for a second list of visible segments.</p><p>Tantivy’s lock files are no longer needed since Postgres provides buffer-level, interprocess locking mechanisms.</p><h3>Challenge 1: Large Files Can Spill Over a Single Block</h3><img loading=\"lazy\" width=\"1632\" height=\"772\" decoding=\"async\" data-nimg=\"1\" srcset=\"/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_linked_list.1fcd2db4.png&amp;w=1920&amp;q=75 1x, /_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_linked_list.1fcd2db4.png&amp;w=3840&amp;q=75 2x\" src=\"https://www.paradedb.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fblock_linked_list.1fcd2db4.png&amp;w=3840&amp;q=75\"><p>A segment file can exceed an 8KB block. To accommodate these files, we implement a linked list over block storage, where each block is a node.</p><p>The linked list starts with a header block that contains a bitpacked representation of all subsequent block numbers. This structure enables O(1) lookups by directly mapping the starting offset of any byte range to its position in the list.</p><p>After the header block, the next block stores the file’s serialized data. Once the block becomes full, a new block is allocated. In Postgres, every block has an area reserved for metadata called special data. The current block's special data section is updated to store the block number of the newly allocated block, forming the linked list.</p><h3>Challenge 2: Blocks Cannot Be Memory Mapped</h3><p>Tantivy’s data access patterns for fast fields assume that the underlying file can be memory mapped, which means that Tantivy can leverage zero-copy access for the entire fast field. This is not the case for block storage — the buffer cache can only provide a pointer to the contents of a single block. If a fast field spans multiple blocks, each block must be copied into memory, introducing significant overhead.</p><p>To address this problem, we <a href=\"https://github.com/paradedb/tantivy/commit/b42a45dd4aa29ce880864c95f2b6e69ad26cdc06\">modified Tantivy</a> to defer dereferencing large slices of bytes up front. Instead, bytes are dereferenced lazily and\ncached in memory to avoid re-reading blocks that have been previously accessed.</p><h3>Challenge 3: The Segment Count Explodes in Update-Heavy Scenarios</h3><p>Because segments are immutable, every DML statement in Tantivy creates at least one new segment. Having too many segments can degrade performance because there is a cost to opening a segment reader, searching over the segment, and merging the results with other segments. While the ideal number of segments depends on the dataset and the underlying hardware, having more than a hundred segments is generally not optimal.</p><p>If a table experiences a high volume of updates, the number of segments quickly explodes. To address this, we introduce a step called  , which looks for merge opportunities after an  completes.</p><p>It is critical that only one merge process runs concurrently. If two merge processes run at the same time, they could both see the same segments, merge them together, and create duplicate segments. To guard against this, every merge process atomically writes its transaction ID to a metadata block. Subsequent merge attempts first read this transaction ID, and are only allowed to proceed if the effects of that transaction ID are MVCC-visible.</p><p>The following parts of this blog post series will dive into some more exciting challenges we faced with block storage, with a focus on concurrency and analytical performance.</p><ol><li><a href=\"https://github.com/zombodb/zombodb\">ZomboDB</a> and <a href=\"https://github.com/CrunchyData/pg_parquet\">pg_parquet</a> are examples of Postgres extensions that directly interact with the filesystem.</li><li>Fortunately this ended up not being the case.</li><li>This will be covered in detail in Parts 2 and 3.</li></ol>","contentLength":10322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1i55ggv/integrating_tantivy_a_rustbased_search_library/"},{"title":"MicroPie - An ultra-micro Python web framework that gets out of your way","url":"https://patx.github.io/micropie/","date":1737311631,"author":"/u/Miserable_Ear3789","guid":408,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i55clr/micropie_an_ultramicro_python_web_framework_that/"},{"title":"Using your Apple device as an access card in unsupported systems","url":"https://github.com/kormax/apple-device-as-access-card","date":1737309702,"author":"ValentineC","guid":165,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42759557"},{"title":"What version of Go did this feature come in?","url":"https://www.reddit.com/r/golang/comments/1i54h8p/what_version_of_go_did_this_feature_come_in/","date":1737309525,"author":"/u/lickety-split1800","guid":363,"unread":true,"content":"<div><p>It had been a while since I had coded in Go, and I noticed that interface{} or any can be used in comparisons without type assertions. I thought I had made a typo, then did a double take as it worked without an error.</p><pre><code>package main import ( \"fmt\" ) func main() { var data any data = true whatType(data) // prints \"bool true\" data = \"true\" whatType(data) // prints \"string true\" } func whatType(data any) { if data == true { fmt.Println(\"bool true\") } if data == \"true\" { fmt.Println(\"string true\") } } </code></pre></div>   submitted by   <a href=\"https://www.reddit.com/user/lickety-split1800\"> /u/lickety-split1800 </a>","contentLength":540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TikTok says it is restoring service for U.S. users","url":"https://www.nbcnews.com/tech/tech-news/tiktok-says-restoring-service-us-users-rcna188320","date":1737308564,"author":"Leary","guid":164,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42759336"},{"title":"Potential CERN Debian proposal","url":"https://www.reddit.com/r/linux/comments/1i536fz/potential_cern_debian_proposal/","date":1737306275,"author":"/u/pedersenk","guid":389,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/pedersenk\"> /u/pedersenk </a>","contentLength":32,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Node.js 'Type Stripping' for TypeScript Now Enabled by Default","url":"https://developers.slashdot.org/story/25/01/19/0335202/nodejs-type-stripping-for-typescript-now-enabled-by-default?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1737304440,"author":"EditorDavid","guid":229,"unread":true,"content":"The JavaScript runtime Node.js can execute TypeScript (Microsoft's JavaScript-derived language with static typing). \nBut now it can do it even better, explains Marco Ippolito of the Node.js steering committee:\n\nIn August 2024 Node.js introduced a new experimental feature, Type Stripping, aimed at addressing a longstanding challenge in the Node.js ecosystem: running TypeScript with no configuration. Enabled by default in Node.js v23.6.0, this feature is on its way to becoming stable. \n\n\nTypeScript has reached incredible levels of popularity and has been the most requested feature in all the latest Node.js surveys. Unlike other alternatives such as CoffeeScript or Flow, which never gained similar traction, TypeScript has become a cornerstone of modern development. While it has been supported in Node.js for some time through loaders, they relied heavily on configuration and user libraries. This reliance led to inconsistencies between different loaders, making them difficult to use interchangeably. The developer experience suffered due to these inconsistencies and the extra setup required... The goal is to make development faster and simpler, eliminating the overhead of configuration while maintaining the flexibility that developers expect... \n\nTypeScript is not just a language, it also relies on a toolchain to implement its features. The primary tool for this purpose is tsc, the TypeScript compiler CLI... Type checking is tightly coupled to the implementation of tsc, as there is no formal specification for how the language's type system should behave. This lack of a specification means that the behavior of tsc is effectively the definition of TypeScript's type system. tsc does not follow semantic versioning, so even minor updates can introduce changes to type checking that may break existing code. Transpilation, on the other hand, is a more stable process. It involves converting TypeScript code into JavaScript by removing types, transforming certain syntax constructs, and optionally \"downleveling\" the JavaScript to allow modern syntax to execute on older JavaScript engines. Unlike type checking, transpilation is less likely to change in breaking ways across versions of tsc. The likelihood of breaking changes is further reduced when we only consider the minimum transpilation needed to make the TypeScript code executable — and exclude downleveling of new JavaScript features not yet available in the JavaScript engine but available in TypeScript... \n\nNode.js, before enabling it by default, introduced --experimental-strip-types. This mode allows running TypeScript files by simply stripping inline types without performing type checking or any other code transformation. This minimal technique is known as Type Stripping. By excluding type checking and traditional transpilation, the more unstable aspects of TypeScript, Node.js reduces the risk of instability and mostly sidesteps the need to track minor TypeScript updates. Moreover, this solution does not require any configuration in order to execute code... Node.js eliminates the need for source maps by replacing the removed syntax with blank spaces, ensuring that the original locations of the code and structure remain intact. It is transparent — the code that runs is the code the author wrote, minus the types... \n\n\"As this experimental feature evolves, the Node.js team will continue collaborating with the TypeScript team and the community to refine its behavior and reduce friction. You can check the roadmap for practical next steps...\"","contentLength":3543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The surprising struggle to get a Unix Epoch time from a UTC string in C or C++","url":"https://berthub.eu/articles/posts/how-to-get-a-unix-epoch-from-a-utc-date-time-string/","date":1737303011,"author":"PascalW","guid":163,"unread":true,"content":"<p>So how hard could it be. As input we have something like <code>Fri, 17 Jan 2025 06:07:07</code> in UTC, and we’d like to turn this into 1737094027, the notional (but not actual) number of seconds that have passed since 1970-01-01 00:00:00 UTC.</p><p>Trying to figure this out led me to discover many ‘surprise features’ and otherwise unexpected behaviour of POSIX time handling functions as implemented in various C libraries &amp; the languages that build on them. There are many good things in the world of C and UNIX, but time handling is not one of them.</p><p>There is a narrow path of useful behaviour however. But first some context.</p><blockquote><p>The tl;dr: <strong>as long as you never call</strong>, you can use  to parse a UTC time string. Do not use %z or %Z. Pass the  calculated by  to the pre-standard function  ( on Windows) to get the correct UNIX epoch timestamp for your UTC time string. Do read on for solutions for if you do use locales. C++ has better support, which could also help you from C.</p></blockquote><p>Time is difficult enough by itself, even if we ignore leap seconds and <a href=\"https://en.wikipedia.org/wiki/Barycentric_Dynamical_Time\">general relativity</a>. When we add human behaviour and politics, it all becomes exceptionally challenging. Timestamps as used by human beings range from impossible to imprecise.</p><p>For example in Amsterdam, “the 30th of March 2025, 02:20” does not exist as a time:</p><pre tabindex=\"0\"><code>$ TZ=Europe/Amsterdam date -d '20250330 01:59:59'\nSun Mar 30 01:59:59 AM CET 2025\n$ TZ=Europe/Amsterdam date -d '20250330 02:30:00'\ndate: invalid date ‘20250330 02:30:00’\n</code></pre><p>This at least is clear. Because of daylight saving time we go straight from 01:59:59 to 03:00:00. Our tooling rightfully refuses to parse “02:30:00” since that timestamp never exists in Amsterdam on that day.</p><p>But “the 27th of October 2024, 02:30” is harder to interpret. The second after 02:59:59, it is 02:00:00 again. This means we have  points in time locals would call ‘02:00’. And here already we can see our tooling starting to make arbitrary choices:</p><pre tabindex=\"0\"><code>$ TZ=Europe/Amsterdam date -d '20241027 01:59:59' +\"%Y-%m-%d %H:%M:%S %s %z\"\n2024-10-27 01:59:59 1729987199 +0200\n$ TZ=Europe/Amsterdam date -d '20241027 02:00:00' +\"%Y-%m-%d %H:%M:%S %s %z\"\n2024-10-27 02:00:00 1729990800 +0100\n</code></pre><p>Apparently, when asked to interpret 02:00:00, my copy of GNU date picks the  time this happens. As far as I can tell, this is because I’m running the command in January. In April it would likely have picked the first 02:00:00 instance. Wild eh?</p><p>The only useful way anyone should ever be specifying a point of time is of course as a number of seconds after or before a known ’epoch’. For POSIX/Unix this is 1970-01-01 00:00:00 UTC, for GPS this is 1980-01-06 00:00:00 UTC, for Galileo (‘EU GPS’) 21st of August 1999, 23:59:47 UTC, for BeiDou 2006-01-01 00:00:00 UTC. GPS, Galileo and BeiDou wisely ignore leap seconds, leaving these as things for human beings to worry about.</p><p>But, our preference for the POSIX/Unix “time_t” is well founded. There is never any ambiguity, except during leap seconds, which might never happen again.</p><p>However, human beings have a hard time parsing 1737214750, so we do need to convert to and from timestamps that include messy things like ‘months’. To this end, UNIX offered us , holding the ‘broken-down time’:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>The standards say that  contains  these fields. There could be more. Now, this struct is of course wildly overdetermined. Day of the week and day of the year follow from the rest, for example. The meanings of ,  and  are badly specified and also badly understood, and vary based on how the struct is used.</p><blockquote><p>Interestingly, the Soviet GLONASS satellite navigation system is not based on an epoch timestamp.  They took the  approach based on ‘Moscow wall clock time’, including leap seconds. This reportedly causes lots of problems. <a href=\"https://en.wikipedia.org/wiki/Slava_Ukraini\">And they deserve them</a>.</p></blockquote><p>One major role of  is as input to , which as  of its work turns a ‘broken-down time <strong>according to your local TZ</strong>’ into a UNIX epoch timestamp. However, it also does many other things!</p><blockquote><p>Note that at least the Linux glibc manpage for  is pretty vague. The <a href=\"https://pubs.opengroup.org/onlinepubs/9799919799/functions/mktime.html\">IEEE Std 1003.1-2024</a> specification offers a lot more (discouraging) words.</p></blockquote><p>It is important to understand that  does absolutely nothing with  or . Its inputs are defined to exclusively be: , , , , , , and .  can be negative, which means that  should figure out if daylight saving time is active at the specified time.</p><p>As noted, time is difficult. If you for example want to adjust a date by a week, you could add 604800 seconds to a  timestamp. However, if that adjustment crosses a daylight savings time boundary, your 2PM appointment might suddenly turn into a 1PM or 3PM appointment next week. Humans do not expect this.</p><p> not only returns a , it also  the  you passed it. And, at least as of 2024 <a href=\"https://pubs.opengroup.org/onlinepubs/9799919799/functions/mktime.html\">there are rules for how it should do so</a>. This means that to get the date that a human being would identify as “the same time next week” you can take the current time, add 7 to , and call  again. Although you just created a date like ‘March 35’,  will fix that up for you.</p><p>Now, if we do this, we find that it doesn’t work:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>Here in the Europe/Amsterdam timezone this prints:</p><pre tabindex=\"0\"><code>original:        Fri Mar 28 14:00:00 2025\nmktime adjusted: Fri Apr  4 15:00:00 2025\n</code></pre><p>What happened, why did our appointment shift by an hour?  is what happened.  may not be perfect, but it does force you to make up your mind. Is the time it is looking at daylight savings time or not? Or, at your choice, do you want  to take a stab at figuring that out for you? The latter option is what we initially chose by setting  to -1.</p><p>When we then ran , it discovered we were initially not in daylight saving time, so it set  to 0. When we ran  for the second time, this DST setting remained in place, even though the new intended time  happen in DST. The fix is to reset  to -1 before the second call.</p><p>Now,  will interpret whatever you pass it as “local time”. This in means you should set your timezone to UTC before calling  to process a UTC time. Changing the timezone for your whole application however might have side effects if you have other threads running. But, you could do that if you have no other threads.</p><p>There is a non-standard/pre-standard function that is very widely available that makes the UTC situation a lot better. From IEEE Std 1003.1-2024: “A future version of this standard is expected to add a  function that is similar to , except that the tm structure pointed to by timeptr contains a broken-down time in Coordinated Universal Time”.</p><ol><li>When using  on local times, set  to -1, which is almost always what ‘human beings’ expect. There is a chance you randomly get back one of two instances of ‘02:30’ (or equivalent) during DST fall back.</li><li>Make sure to zero the rest of  before filling it out, just to be sure.</li><li>Be aware that  does surgery on your , and that this may have side effects. At least reset  before reuse.</li><li>No matter what you do with  or ,  will use your current timezone. If you want it to interpret your  as UTC, you actually need to set the TZ environment variable to UTC. This will however mess with any other threads doing time operations.</li><li>Just use  or  instead</li></ol><p>But, how do we get our time string into a ?</p><p>Now, it would be lovely if we could feed <code>Fri, 17 Jan 2025 06:07:07 GMT</code> into  and get a sensible  in return. The <a href=\"https://man7.org/linux/man-pages/man3/strptime.3.html\">Linux glibc  manpage</a> has some airy words about how the %z and %Z timezone format specifiers might or likely might not do something. You just can’t tell:</p><blockquote><p>“For reasons of symmetry, glibc tries to support for strptime() the same format characters as for strftime(3).  (In , the corresponding fields are parsed, but no field in tm is changed.)</p></blockquote><p>Now, our goal is to convert a UTC time string into a UNIX epoch timestamp. Many people justifiably harbor some expectations that  with %Z (to parse ‘GMT’) followed by  could make that happen.</p><p>Above we learned however that  does not look at the  or  fields at all, so there really is nothing that  can achieve there, even if it did the right thing. Oh, and it also does not do the right thing.</p><p>Because of this  has no more specified behaviour for ‘%z’, which was one day supposed to do something with “+0200” style offset identifiers, which would have been lovely. But you can’t count on ‘%z’ doing anything useful. <a href=\"https://en.wiktionary.org/wiki/nasal_demon\">Nasal demons</a> might ensue.</p><p>There is however some wording on , but it is very limited. If your locale is known to have a DST timezone identifier (‘CEST’) that is different from the regular time zone (‘CET’), and if ‘%Z’ sees any of these two, it will set  to the right value for you. <a href=\"https://www.redhat.com/en/blog/brief-history-mktime\">Except possibly if you live in Ireland</a>. In general, it is also not useful to try to parse a string like ‘EST’, as it has no well defined meaning anyhow, except perhaps locally.</p><p>Luckily, because we discovered  above, we can just ignore %z and %Z as we don’t need them anyhow.</p><p>Now, what I didn’t know is that unless they specifically ask for it, C and C++ programs will stick to the “C” locale, which effectively is American English. This means that out of the box all LC_TIME etc environment variables are ignored. For our purposes of parsing time strings found in data, this is usually great, since these are almost exclusively in English.</p><p>However, if you are in a C or C++ program that  call , thus asking for a possibly non-C locale, suddenly your program might only work for Dutch time strings. And these are quite rare.</p><p>Now, you might ponder changing the locale to “C” before calling  and then changing it back, but sadly  is not safe to call in multithreaded programs (except before launching threads). Also you might confuse the output of other threads even if this was safe.</p><p>So in general, if you need to parse specific time strings and you want to use , do make sure your program is on the locale you expect it to be. And although  exists, in which you can specify the locale to use for formatting time, the equivalent  is not officially available.</p><p>Alternatively, it is not that hard to parse a string like  and fill out a  and then let  do the actual hard work of calculating a UNIX epoch timestamp.</p><p>C++ iostreams are not much loved, but they did have a better think about locales than C/POSIX did. In C++ you can set the locale per iostream. <a href=\"https://github.com/berthubert/utchelper\">Here is a C++ helper that you can call from C</a> if you want to parse arbitrary UTC time strings in programs that do set the locale:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>This incidentally also shows how to do error handling for , which will helpfully return -1, , if you ask it to look at 31st of December 1969 23:59. The trick is to employ  as a sentinel to see if anything was processed or not. This tells you there was no error.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>C++20 and beyond contain a luxurious timezone database. This is not yet available on all compilers, but luckily the <a href=\"https://howardhinnant.github.io/date/tz.html\">pre-standardized version</a> is available for standalone use. Sometimes we get lucky because someone sacrifices a few years of their life to give us some truly excellent code that we really don’t deserve, and Howard Hinnant clearly delivered.</p><p>Here is a glorious example:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>This picks “the first Monday of May 2016, 9AM local time in New York”, and then seamlessly converts this to two other timezones:</p><pre tabindex=\"0\"><code>The New York meeting is 2016-05-02 09:00:00 EDT\nThe London   meeting is 2016-05-02 14:00:00 BST\nThe Sydney   meeting is 2016-05-02 23:00:00 AEST\n</code></pre><p>Being truly luxurious, the tz library supports using not just your operating system’s time zone databases, which might lack crucial leap second detail, but can also source the IANA tzdb directly. This allows you to faithfully calculate the actual duration of a plane flight in 1978 that not only passed through a DST change, but also an actual leap second. I don’t swoon easily, <a href=\"https://howardhinnant.github.io/date/tz.html#Examples\">but I’m swooning</a>.</p>","contentLength":11638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42758257"},{"title":"Build a tiny CA for your homelab with a Raspberry Pi","url":"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/","date":1737301851,"author":"timkq","guid":162,"unread":true,"content":"<time datetime=\"{updatedAt}\">Updated on: January 19, 2025</time><p> In this tutorial, we're going to build a tiny, standalone, online Certificate Authority (CA) that will mint TLS certificates and is secured with a YubiKey. It will be an internal ACME server on our local network (ACME is the same protocol used by <a href=\"https://letsencrypt.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Let's Encrypt</a>). The YubiKey will securely store the CA private keys and sign certificates, acting as a cheap alternative to a Hardware Security Module (HSM). We'll also use an open-source True Random Number Generator, called <a href=\"https://www.crowdsupply.com/13-37/infinite-noise-trng/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Infinite Noise TRNG</a>, to spice up the Linux entropy pool.</p><h3>Why would I want a Certificate Authority in my homelab?!<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#why-would-i-want-a-certificate-authority-in-my-homelab\"></a></h3><ul><li>Because end-to-end TLS is great and you should easily be able to run TLS wherever you need it. Especially in your homelab. Internal networks are no longer perceived as a safe zone where unencrypted traffic is okay. But you need certificates.</li><li>Because the ACME protocol (used by Let's Encrypt) can easily be deployed internally, so you can automate renewal and never have to think about your certificates.</li><li>Because maybe you've done the 'self-signed certificate' rigmarole with OpenSSL a dozen times already. Might as well formalize things and get your devices to trust a CA that you can use wherever you need it.</li><li>Because setting up a simple CA is a great learning experience.</li></ul><h3>Basic OS &amp; Networking Setup<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#basic-os--networking-setup\"></a></h3><ul><li>Fire up the Raspberry Pi, plug it into your network, and find its initial IP address.\nYou can run <code>arp -na | grep -e \"b8:27:eb\" -e \"dc:a6:32\" -e \"e4:5f:01\"</code> to discover Raspberry Pi devices on the local network.</li><li>Login via SSH (username and password will be ), and change the password.</li><li>Set the hostname via <code>hostnamectl set-hostname tinyca</code></li><li>Set the timezone using <code>timedatectl set-timezone America/Los_Angeles</code> (or whatever your timezone is; <code>timedatectl list-timezones</code> will list them all)</li><li>Be sure NTP is working. Check status with — make sure \"NTP Service\" is \"active\". If not, you can add some NTP servers to <code>/etc/systemd/timesyncd.conf</code> and run <code>systemctl restart systemd-timesyncd</code>.</li><li>You'll need the machine to have a DNS name (for me it's ) and/or a static IP on your network.</li></ul><p>Now that you have good time synchronization and a stable hostname, we can proceed.</p><h3>Install prerequisite: <a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#install-prerequisite-ykman\"></a></h3><p>Now, insert your YubiKey. Let's install the  (and dependency ) and make sure you can connect to the YubiKey:</p><pre><section><code><pre>$ sudo apt update\n$ sudo apt install -y yubikey-manager\n$ ykman info\nDevice type: YubiKey 5 NFC\nSerial number: 13910388\nFirmware version: 5.2.7\nForm factor: Keychain (USB-A)\nEnabled USB interfaces: OTP+FIDO+CCID\nNFC interface is enabled.\n</pre></code></section></pre><p>You'll need Go in order to build the  server.</p><pre><section><code><pre>$ cd\n$ curl -LO https://go.dev/dl/go1.20.1.linux-arm64.tar.gz\n$ sudo tar -C /usr/local -xzf go1.20.1.linux-arm64.tar.gz\n$ echo \"export PATH=\\$PATH:/usr/local/go/bin\" &gt;&gt; .profile\n$ source .profile\n$ go version\ngo version go1.20.1 linux/arm64\n</pre></code></section></pre><h3>Build and install  and <a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#build-and-install-step-ca-and-step\"></a></h3><p>You'll need to install both  (the CA server software) and  (the command used to configure and control ).</p><pre><section><code><pre>$ curl -LO https://github.com/smallstep/certificates/releases/download/v0.23.2/step-ca_0.23.2.tar.gz\n$ mkdir step-ca\n$ tar -xvzf step-ca_0.23.2.tar.gz -C step-ca\n$ cd step-ca\n</pre></code></section></pre><p>Now build . This will take some time on a Raspberry Pi, so be patient:</p><pre><section><code><pre>$ sudo apt-get install -y libpcsclite-dev gcc make pkg-config\n$ make bootstrap\n$ make build GOFLAGS=\"\"\n....\nBuild Complete!\n$ sudo cp bin/step-ca /usr/local/bin\n$ sudo setcap CAP_NET_BIND_SERVICE=+eip /usr/local/bin/step-ca\n$ step-ca version\nSmallstep CA/0.23.2 (linux/arm64)\nRelease Date: 2023-02-16 22:25 UTC\n</pre></code></section></pre><pre><section><code><pre>$ cd\n$ curl -LO https://github.com/smallstep/cli/releases/download/v0.23.2/step_linux_0.23.2_arm64.tar.gz\n$ tar xvzf step_linux_0.23.2_arm64.tar.gz\n$ sudo cp step_0.23.2/bin/step /usr/local/bin\n$ step version\nSmallstep CLI/0.23.2 (linux/arm64)\nRelease Date: 2023-02-07T00:53:54Z\n</pre></code></section></pre><h3>Optional, but 🔥: Set up the outboard random number generator<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#optional-but--set-up-the-outboard-random-number-generator\"></a></h3><p><a href=\"https://www.crowdsupply.com/13-37/infinite-noise-trng\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Infinite Noise TRNG</a> is an open-source USB True Random Number Generator. It uses a \"modular entropy multiplier\" architecture to generate a  of random data quickly. For this setup, a daemon will continuously feed entropy into Linux's system entropy pool by writing to .</p><blockquote><p><strong>But will this lovely new entropy generator actually be used by the CA?</strong> I needed to answer two questions here:</p><ol><li>How does the CA generate random numbers? I had to dig around a little to confirm this.  uses Go's  for all of its key generation, and  uses  as its random data source on Linux systems.</li><li>Does the entropy created via writing to  actually affects what is read from ? It does—because Linux has only one entropy pool, shared by  and .</li></ol><p>We also need to confirm that the outboard TRNG is actually generating high quality noise. We'll do that in a minute.\nYou'll need to <a href=\"https://github.com/13-37-org/infnoise/releases\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">compile the driver from source</a>, because there's no pre-built  package available.</p></blockquote><pre><section><code><pre>$ curl -LO https://github.com/leetronics/infnoise/archive/refs/tags/0.3.3.tar.gz\n$ tar xvzf 0.3.3.tar.gz\n$ cd infnoise-0.3.3/software\n$ sudo apt-get install -y libftdi-dev libusb-dev\n$ make -f Makefile.linux\n$ sudo make -f Makefile.linux install\ninstall -d /usr/local/sbin\ninstall -m 0755 infnoise /usr/local/sbin/\ninstall -d /usr/local/lib/udev/rules.d/\ninstall -m 0644 init_scripts/75-infnoise.rules /usr/local/lib/udev/rules.d/\ninstall -d /usr/local/lib/systemd/system\ninstall -m 0644 init_scripts/infnoise.service /usr/local/lib/systemd/system\n$ infnoise --version\nGIT VERSION -\nGIT COMMIT  -\nGIT DATE    -\n</pre></code></section></pre><p>Now, plug in the TRNG and restart your system.</p><p>After a restart, you should see that the driver has started up. It will start and stop based on whether the TRNG is present.</p><pre><section><code><pre>$ systemctl status infnoise\n● infnoise.service - Wayward Geek InfNoise TRNG driver\n     Loaded: loaded (/usr/local/lib/systemd/system/infnoise.service; disabled; preset: enabled)\n     Active: active (running) since Thu 2023-02-16 14:43:02 PST; 1min 44s ago\n    Process: 655 ExecStart=/usr/local/sbin/infnoise --dev-random --daemon --pidfile /var/run/infnoise.pid (code=e&gt;\n   Main PID: 661 (infnoise)\n      Tasks: 1 (limit: 2082)\n     Memory: 700.0K\n        CPU: 162ms\n     CGroup: /system.slice/infnoise.service\n             └─661 /usr/local/sbin/infnoise --dev-random --daemon --pidfile /var/run/infnoise.pid\n\nFeb 16 14:43:02 tinyca systemd[1]: Starting Wayward Geek InfNoise TRNG driver...\nFeb 16 14:43:02 tinyca systemd[1]: Started Wayward Geek InfNoise TRNG driver.\n</pre></code></section></pre><p>Finally, let's run a health check to make sure the TRNG is ready for use:</p><pre><section><code><pre>$ infnoise --debug --no-output\nGenerated 1048576 bits.  OK to use data.  Estimated entropy per bit: 0.878415, estimated K: 1.838354\nnum1s:50.466260%, even misfires:0.119403%, odd misfires:0.156459%\n^C\n</pre></code></section></pre><p>Entropy is written to  by  every second. You're all set on randomness! Now that you have more than enough entropy, you're ready to generate your CA keys.</p><h2>Part 2: Creating Your PKI<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#part-2-creating-your-pki\"></a></h2><p>Now you'll create your root and intermediate CA certificates and keys, and store them securely on the YubiKey.</p><p>Ideally, your Raspberry Pi should be kept offline for this section. Disconnect the Ethernet cable, and connect directly to the device via HDMI and a keyboard.</p><h3>Prepare a USB thumb drive for storing the private keys<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#prepare-a-usb-thumb-drive-for-storing-the-private-keys\"></a></h3><p>You can't just have your CA private keys live  on the YubiKey. You'll want at least one backup of them, in case the YubiKey breaks!</p><p>Insert a USB thumb drive. You'll generate the keys directly on this drive, so that they never touch the Pi's microSD card. First, find the device name of your USB drive:</p><pre><section><code><pre>$ sudo fdisk -l\n...\nDisk /dev/sda: 14.91 GiB, 16005464064 bytes, 31260672 sectors\nDisk model: Cruzer Fit\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\n...\n</pre></code></section></pre><p>In this case, the drive is . Let's initialize it with a single  partition:</p><pre><section><code><pre>$ sudo fdisk /dev/sda\nWelcome to fdisk (util-linux 2.36).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\nCommand (m for help): n\nPartition type\n   p   primary (0 primary, 0 extended, 4 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (1-4, default 1):\nFirst sector (2048-31260671, default 2048):\nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-31260671, default 31260671):\nCreated a new partition 1 of type 'Linux' and of size 14.9 GiB.\nCommand (m for help): w\nThe partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n$ sudo mkfs.ext4 /dev/sda1 -v\nmke2fs 1.45.6 (20-Mar-2020)\nfs_types for mke2fs.conf resolution: 'ext4'\nFilesystem label=\nOS type: Linux\n...\nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done\n</pre></code></section></pre><h3>Generate your PKI on the thumb drive<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#generate-your-pki-on-the-thumb-drive\"></a></h3><p>Great, now you're ready to create your Public Key Infrastructure (PKI). Specifically, you'll be creating CA keys and certificates.</p><blockquote><ul><li>Tiny CA has a root CA key and certificate, and an intermediate CA key and certificate.</li><li>The root CA key signs the Intermediate CA certificate.</li><li>The root CA certificate is self-signed (signed with the root CA key)</li><li>The intermediate CA key will sign all of your TLS certificates.</li><li>By default,  issues certificates with a 24-hour lifetime. I hope this default will compel you to <a href=\"https://smallstep.com/docs/step-ca/renewal\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">set up automated renewal</a> on your clients. And you can always increase the TLS certificate duration in the CA configuration, if you want something a bit more relaxed.</li><li>If a device is configured to trust your root CA, it will trust certificates you create with .</li><li>You can throw away the root CA key if you never need another intermediate.</li></ul></blockquote><pre><section><code><pre>$ sudo mount /dev/sda1 /mnt\n$ cd /mnt\n$ sudo mkdir ca\n$ sudo chown ubuntu:ubuntu ca\n$ export STEPPATH=/mnt/ca\n$ step ca init --pki --name=\"Tiny\" --deployment-type standalone\n✔ What do you want your password to be? [leave empty and we'll generate one]: ...\nGenerating root certificate...\nall done!\nGenerating intermediate certificate...\nall done!\n✔ Root certificate: /mnt/ca/certs/root_ca.crt\n✔ Root private key: /mnt/ca/secrets/root_ca_key\n✔ Root fingerprint: d6b3b9ef79a42aeeabcd5580b2b516458ddb25d1af4ea7ff0845e624ec1bb609\n✔ Intermediate certificate: /mnt/ca/certs/intermediate_ca.crt\n✔ Intermediate private key: /mnt/ca/secrets/intermediate_ca_key\nFEEDBACK 😍 🍻\n      The step utility is not instrumented for usage statistics. It does not\n      phone home. But your feedback is extremely valuable. Any information you\n      can provide regarding how you’re using `step` helps. Please send us a\n      sentence or two, good or bad: feedback@smallstep.com or join\n      https://github.com/smallstep/certificates/discussions.\n</pre></code></section></pre><p>Don't forget to give your CA a cute name! It will appear on all of your certificates. Hold onto your root fingerprint, too; you'll need it to bootstrap your clients later.</p><h3>Import the CA into the YubiKey<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#import-the-ca-into-the-yubikey\"></a></h3><p>Now, let's import our PKI to the YubiKey.</p><pre><section><code><pre>$ sudo systemctl enable pcscd\n$ sudo systemctl start pcscd\n$ ykman piv certificates import 9a /mnt/ca/certs/root_ca.crt\nSuccessfully imported a new certificate.\n$ ykman piv keys import 9a /mnt/ca/secrets/root_ca_key\nEnter PEM pass phrase: ...\nSuccessfully imported a new private key.\n$ ykman piv certificates import 9c /mnt/ca/certs/intermediate_ca.crt\nSuccessfully imported a new certificate.\n$ ykman piv keys import 9c /mnt/ca/secrets/intermediate_ca_key\nEnter PEM pass phrase: ...\nSuccessfully imported a new private key.\n$ ykman piv info\nPIV version: 5.2.7\nPIN tries remaining: 3\nCHUID:\t3019d4e739da739ced39ce739d836858210842108421c84210c3eb34104610300df33f7fd273e44f17361ce7c4350832303330303130313e00fe00\nCCC: \tNo data available.\nSlot 9a:\n\tAlgorithm:\tECCP256\n\tSubject DN:\tCN=Tiny CA Root CA\n\tIssuer DN:\tCN=Tiny CA Root CA\n\tSerial:\t\t280998571002718115143415195266043025218\n\tFingerprint:\td6b3b9ef79a42aeeabcd5580b2b516458ddb25d1af4ea7ff0845e624ec1bb609\n\tNot before:\t2020-12-08 20:12:15\n\tNot after:\t2030-12-08 20:12:15\nSlot 9c:\n\tAlgorithm:\tECCP256\n\tSubject DN:\tCN=Tiny CA Intermediate CA\n\tIssuer DN:\tCN=Tiny CA Root CA\n\tSerial:\t\t38398140468675846143165983044297636289\n\tFingerprint:\tfa21279c114ef44be899cb41e830b920faa6ce2c0ec5bc4f1c9310194e5837d2\n\tNot before:\t2020-12-08 20:12:15\n\tNot after:\t2030-12-08 20:12:15\n</pre></code></section></pre><p>OK! Now you'll copy out the CA certificate files, leave the private keys on the USB stick, and continue creating your CA.</p><pre><section><code><pre>$ sudo cp /mnt/ca/certs/intermediate_ca.crt /mnt/ca/certs/root_ca.crt /root\n$ cd\n$ sudo umount /mnt\n</pre></code></section></pre><p>Finally, reconnect your CA to your local network to continue the setup.</p><h2>Part 3: Configuring Your CA<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#part-3-configuring-your-ca\"></a></h2><p>You're going to re-run  now, but <em>you're not going to use the certificates or keys that it generates</em>.\nYou're just doing this to create the configuration files.\nThe password you choose when prompted will be your <em>admin provisioner password</em>.\nAnyone with this password will be able to administer your CA and get any certificate from it,\nusing the  subcommand.</p><p>Don't use your root CA password for your provisioner,\nbut pick something strong and store it somewhere safe.</p><pre><section><code><pre>$ sudo useradd step\n$ sudo passwd -l step\n$ sudo mkdir /etc/step-ca\n$ export STEPPATH=/etc/step-ca\n$ sudo --preserve-env step ca init --name=\"Tiny CA\" \\\n    --dns=\"tinyca.internal,10.20.30.42\" --address=\":443\" \\\n    --provisioner=\"you@example.com\" \\\n    --deployment-type standalone \\\n    --remote-management\nChoose a password for your CA keys and first provisioner.\n✔ [leave empty and we'll generate one]:\n\nGenerating root certificate... done!\nGenerating intermediate certificate... done!\n\n✔ Root certificate: /etc/step-ca/certs/root_ca.crt\n✔ Root private key: /etc/step-ca/secrets/root_ca_key\n✔ Root fingerprint: 60440dc6ef5b923810b22f85a907f307badb58314c5fdc2231a3c1a892d6c275\n✔ Intermediate certificate: /etc/step-ca/certs/intermediate_ca.crt\n✔ Intermediate private key: /etc/step-ca/secrets/intermediate_ca_key\n✔ Database folder: /etc/step-ca/db\n✔ Default configuration: /etc/step-ca/config/defaults.json\n✔ Certificate Authority configuration: /etc/step-ca/config/ca.json\n✔ Admin provisioner: you@example.com (JWK)\n✔ Super admin subject: step\n\nYour PKI is ready to go. To generate certificates for individual services see 'step help ca'.\n</pre></code></section></pre><p>Next, let's get your certificates in place.</p><pre><section><code><pre>$ sudo mv /root/root_ca.crt /root/intermediate_ca.crt /etc/step-ca/certs\n$ sudo rm -rf /etc/step-ca/secrets\n</pre></code></section></pre><p>Next, you'll need to configure  to use your YubiKey to sign certificates, using the intermediate key on the YubiKey. Notice that the default YubiKey PIN () is shown here, too.</p><blockquote><p>You should change your YubiKey PIN, PUK, and management key if you haven't already! <a href=\"https://developers.yubico.com/PIV/Guides/Device_setup.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Learn how in this guide.</a>\nNow edit the file <code>/etc/step-ca/config/ca.json</code>. You'll want the top of the file to look like this:</p></blockquote><pre><section><code><pre>\n...\n</pre></code></section></pre><p>Now you'll start up the CA and make sure it's running properly:</p><pre><section><code><pre>$ sudo chown -R step:step /etc/step-ca\n$ sudo -u step step-ca /etc/step-ca/config/ca.json\n2020/12/08 14:17:06 Serving HTTPS on :443 ...\n</pre></code></section></pre><p>In another window, you'll generate a test certificate for localhost.\nThis is where you'll need the CA fingerprint, which is displayed when you start up the CA. Run:</p><pre><section><code><pre>$ step ca bootstrap --ca-url \"https://tinyca.internal\" --fingerprint d6b3b9ef79a42aeeabcd5580b2b516458ddb25d1af4ea7ff0845e624ec1bb609\nThe root certificate has been saved in /home/ubuntu/.step/certs/root_ca.crt.\nYour configuration has been saved in /home/ubuntu/.step/config/defaults.json.\n$ step ca certificate \"localhost\" localhost.crt localhost.key\n✔ Provisioner: you@example.com (JWK) [kid: izgi9tn1YWbVnY_rmIUKzE-Dn-XIuKz-_J1dnnKeDRA]\n✔ Please enter the password to decrypt the provisioner key:\n✔ CA: https://tinyca.internal:443\n✔ Certificate: localhost.crt\n✔ Private Key: localhost.key\n$ step certificate inspect localhost.crt --short\nX.509v3 TLS Certificate (ECDSA P-256) [Serial: 2903...3061]\n  Subject:     localhost\n  Issuer:      Tiny Intermediate CA\n  Provisioner: you@example.com [ID: izgi...eDRA]\n  Valid from:  2023-02-16T23:03:52Z\n          to:  2023-02-17T23:04:52Z\n</pre></code></section></pre><p>Great! You just signed your first X.509 TLS leaf certificate using the YubiKey and .</p><p>When you ask the CA to issue a leaf certificate for a TLS endpoint, you'll get a certificate file and a (locally-generated) private key file.  The certificate file will contain both the intermediate CA certificate and the leaf certificate you requested. This way, a device which trusts your root CA can verify the chain of trust from the root to the intermediate, and from the intermediate to the leaf.</p><p>Finally, you'll add an ACME provisioner, which will turn your Tiny CA into a tiny Let's Encrypt!</p><pre><section><code><pre>$ step ca provisioner add acme --type acme --admin-name step\nNo admin credentials found. You must login to execute admin commands.\n✔ Provisioner: you@example.com (JWK) [kid: izgi9tn1YWbVnY_rmIUKzE-Dn-XIuKz-_J1dnnKeDRA]\nPlease enter the password to decrypt the provisioner key:\n</pre></code></section></pre><p>Sign in with your admin password, and your new new ACME provisioner will be created.</p><p>You can now shut down the  process you started in the other terminal window.</p><h3>Configure  to start the CA<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#configure-systemd-to-start-the-ca\"></a></h3><p>In this section you'll set up a systemd service for  so it starts when the system starts up.\nYou'll also configure systemd to stop the CA when the YubiKey is removed, and restart it when the YubiKey is reinserted.\nFirst, you need to tell <a href=\"https://wiki.archlinux.org/index.php/Udev\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">udev</a> about your YubiKey by adding some udev rules, which will help make the YubiKey visible to systemd as a device.</p><pre><section><code><pre>$ sudo tee /etc/udev/rules.d/75-yubikey.rules &gt; /dev/null &lt;&lt; EOF\nACTION==\"add\", SUBSYSTEM==\"usb\", ENV{PRODUCT}==\"1050/407/*\", TAG+=\"systemd\", SYMLINK+=\"yubikey\"\nACTION==\"remove\", SUBSYSTEM==\"usb\", ENV{PRODUCT}==\"1050/407/*\", TAG+=\"systemd\"\nEOF\n$ sudo udevadm control --reload-rules\n</pre></code></section></pre><p>Here, the format of the  value is . Yubico's vendor ID is , and  is the product ID for the YubiKey 5 NFC. If you're using a different YubiKey, <a href=\"https://devicehunt.com/view/type/usb/vendor/1050\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">you can find your model number here</a>.</p><ul><li>run on system startup, when the YubiKey is inserted</li><li>stop when the YubiKey is removed</li><li>start again when the YubiKey is reinserted</li></ul><pre><section><code><pre>$ sudo tee /etc/systemd/system/step-ca.service &gt; /dev/null &lt;&lt; EOF\n[Unit]\nDescription=step-ca\nBindsTo=dev-yubikey.device\nAfter=dev-yubikey.device\n[Service]\nUser=step\nGroup=step\nExecStart=/bin/sh -c '/usr/local/bin/step-ca /etc/step-ca/config/ca.json'\nType=simple\nRestart=on-failure\nRestartSec=10\n[Install]\nWantedBy=multi-user.target\nEOF\n$ sudo mkdir /etc/systemd/system/dev-yubikey.device.wants\n$ sudo ln -s /etc/systemd/system/step-ca.service /etc/systemd/system/dev-yubikey.device.wants/\n$ sudo systemctl daemon-reload\n$ sudo systemctl enable step-ca\n</pre></code></section></pre><p>Now insert the YubiKey and the service should start:</p><pre><section><code><pre>$ sudo systemctl status step-ca\n● step-ca.service - step-ca\n     Loaded: loaded (/etc/systemd/system/step-ca.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2020-12-08 14:27:02 PST; 3s ago\n   Main PID: 3269 (sh)\n      Tasks: 9 (limit: 2099)\n     CGroup: /system.slice/step-ca.service\n             ├─3269 /bin/sh -c /usr/local/bin/step-ca /etc/step-ca/config/ca.json\n             └─3270 /usr/local/bin/step-ca /etc/step-ca/config/ca.json\nDec 08 14:27:02 tinyca systemd[1]: Started step-ca.\nDec 08 14:27:02 tinyca sh[3270]: 2020/12/08 14:27:02 Serving HTTPS on :443 ...\n</pre></code></section></pre><p>Now restart your system and ensure that the CA starts up automatically.</p><p>Test out removing the YubiKey, and you should see that the CA stops.</p><p>Reinsert it, and the CA should start up again.</p><h3>Finally, turn on the firewall and disable SSH access<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#finally-turn-on-the-firewall-and-disable-ssh-access\"></a></h3><p>Your tiny CA will be most secure without any SSH access at all. The only open port will be 443, for the CA. For maintenance, you'll need to plug in a keyboard and a display.</p><pre><section><code><pre>$ sudo tee /etc/ufw/applications.d/step-ca-server &gt; /dev/null &lt;&lt; EOF\n[step-ca]\ntitle=Smallstep CA\ndescription=step-ca is an online X.509 and SSH Certificate Authority\nports=443/tcp\nEOF\n$ sudo ufw allow step-ca\n$ sudo ufw enable\nCommand may disrupt existing ssh connections. Proceed with operation (y|n)? y\nFirewall is active and enabled on system startup\n</pre></code></section></pre><p>You did it! Your CA is up and running.</p><h4>Bootstrapping a new device into your PKI<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#bootstrapping-a-new-device-into-your-pki\"></a></h4><p>When you run <a href=\"https://smallstep.com/docs/step-cli/reference/ca/bootstrap\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"></a> (as above) on a new device,\nthe root certificate  is downloaded from the CA.\nIf you run <code>step ca bootstrap --install --ca-url=https://your.ca --fingerprint=your-ca-fingerprint</code>,\nit will install the root certificate into your device's trust store.</p><p>You can also use the  command for easy installation of your root CA certificate (<a href=\"https://smallstep.com/docs/step-cli/reference/certificate/install\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"></a>),\nfor ACME enrollment (<code>step ca certificate example.com example.crt example.key --provisioner acme</code>)\nand for renewal of any certificate that hasn't yet expired (<code>step ca renew example.crt example.key</code>).</p><p>For mobile devices, you can usually install a certificate by sending it to yourself via Bluetooth or AirDrop, or as an email attachment. Make sure the certificate isn't just installed, but actually trusted by the device. This usually involves a couple of confirmation steps on the device.</p><h4>Automating certificate renewal<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#automating-certificate-renewal\"></a></h4><p>Because certificates from your CA have a 24-hour lifetime, you'll want to renew them every 16ish hours.\nOur <a href=\"https://smallstep.com/docs/step-ca/renewal\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">renewal documentation</a> has a few options\nfor setting up renewal on your clients.</p><p>Now that you have an internal CA, here's a few useful resources:</p><ul><li><a href=\"https://smallstep.com/hello-mtls\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Hello mTLS</a> shows you how to get mutual TLS authentication configured for several common services and programming languages, using the  command.</li><li>There's also a lot to learn about the different provisioners you can add to your CA to suit your workflows.\nSee <a href=\"https://smallstep.com/docs/step-ca/configuration\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Configuring </a>.</li><li>Bonus: Want to use SSH certificates?\nYou can turn your tiny CA into an SSH CA, and use certificates and single sign-on for your SSH hosts.\nWe have a <a href=\"https://smallstep.com/blog/diy-single-sign-on-for-ssh/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">blog post</a> and <a href=\"https://www.youtube.com/watch?v=ZhxLRlcNUM4\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">video walk-through</a> that describes how to set it up.</li></ul><div><div><p>Carl Tashian (<a href=\"https://tashian.com\">Website</a>, <a href=\"https://www.linkedin.com/in/tashian/\">LinkedIn</a>) is an engineer, writer, exec coach, and startup all-rounder. He's currently an Offroad Engineer at Smallstep. He co-founded and built the engineering team at Trove, and he wrote the code that opens your Zipcar. He lives in San Francisco with his wife Siobhan and he loves to play the modular synthesizer 🎛️🎚️</p></div></div>","contentLength":21763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42758070"},{"title":"Crates doing protocol programming well","url":"https://www.reddit.com/r/rust/comments/1i51chr/crates_doing_protocol_programming_well/","date":1737301584,"author":"/u/mjaakkola","guid":449,"unread":true,"content":"<p>I've been programming with Rust a couple years now but haven't created a pattern that I feel comfortable for protocol programming. Protocol programming includes decode/encode (can this be done with declarative manner), state-machines and abstracting the stack with links (traits vs. generics vs. ipc) so there are multiple aspects to think about. </p><p>I wanted to ping community about what crates do feel are having nice clean patters of doing these things. Especially, in the non-async world. Obviously, the pattern cleaness may be impacted zero copying and other performance requirements, but right now I'm looking clean good examples from the existing crates so no need for anyone to create examples. </p>","contentLength":699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Waku: A CLI Project Generation Tool","url":"https://www.reddit.com/r/golang/comments/1i50hu2/waku_a_cli_project_generation_tool/","date":1737299303,"author":"/u/caffeinestack","guid":361,"unread":true,"content":"<p>I’m excited (and kinda nervous) to share , my first real project in Go! 🎉</p><p>Waku is a project generation CLI tool designed to simplify setting up new projects and improve Developer Experience (DX). Think of it as a smarter copy-paste tool! It works with public/private repos or local directories as template sources.</p><p>Currently, there’s just one official style available, but the vision is to add more to cover most (and even some uncommon) use cases—like a Go microservice, a React front-end, or a full-stack project with CI/CD baked in.</p><p>I created Waku because I got tired of setting up projects from scratch over and over. The goal is to make project setup painless, so you can skip the boring boilerplate and get straight to coding.</p><p>I’ve been using Waku myself and iterating on it for over half a year now, and so far, the feedback from developers has been pretty positive.</p><p>It’s still early days (), but that’s the vision for Waku moving forward.</p><p>Would love your to hear your thoughts, ideas, or contributions! Thank you.</p>","contentLength":1030,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"All Lisp Indentation Schemes Are Ugly","url":"https://aartaka.me/lisp-indent.html","date":1737291608,"author":"/u/aartaka","guid":413,"unread":true,"content":"<a href=\"https://aartaka.me/about.html\">By Artyom Bologov</a><p>\nOnce you get used to Lisp, you stop noticing the parentheses and rely on indentation instead.\nThat's partially why there are several alternative syntaxes based solely on indentation.\n<a href=\"https://aartaka.me/wisp.html\">Like Wisp</a>,\n<a href=\"https://srfi.schemers.org/srfi-110\">or sweet expressions</a>.\nBut then, the question stands: how to indent the code, actually?\nEspecially so—in Lispy syntax.\n\n</p><section><p>\nA solution that will likely satisfy a proponent of any indentation style:\n\"Just put it all on one line lol.\"\nOf course, lines are not infinitely readable and there's a column cap,\n(whether natural or enforced.)\nSo this line (adapted from\n<a href=\"https://github.com/aartaka/cl-blc\">cl-blc</a>,)\nwhile devoid of indentation problems, is unreadable:\n\n</p><figure><pre lang=\"lisp\">(list (tree-transform-if predicate transformer (first tree) depth) (tree-transform-if predicate transformer (second tree) depth))\n</pre><figcaption>Absurdly long line of a Lisp code</figcaption></figure><p>\nThat's the problem statement: some forms need multiple lines and indentation.\nBut what kind of indentation?\n\n</p></section><section><p>\nThere's an established style of indentation: align the function arguments on the same column:\n\n</p><figure><pre lang=\"lisp\">(list (tree-transform-if predicate transformer (first tree) depth)\n      (tree-transform-if predicate transformer (second tree) depth))\n(inc! d (/ (* (mtx:get x i k)\n              (mtx:get x j k))\n           (1+ (* (vec:get dl l)\n                  (vec:get eval k)))))\n(let ((s (if (&lt; j i) j i))\n      (l (if (&lt; j i) i j)))\n  (+ (* s 1/2 (- (* 2 d-size) (1+ s)))\n     l\n     (- s)))\n</pre><figcaption>Examples of function-like indentation</figcaption></figure><p>\nThis style if useful in reflecting the code structure: just look at what's indented and what's outdented.\nIt works especially well for short function/macro/form names, like  or .\nNot so well for long ones:\n\n</p><figure><pre lang=\"lisp\">(tree-transform-if predicate\n                   transformer\n                   (second tree)\n                   depth)\n</pre><figcaption>A problematic function indentation</figcaption></figure><p>\nNineteen!\nNineteen spaces of indentation!\nIt's getting unruly.\nSuch an indent, when used in deeply nested code, makes it too wide and unreadable.\nIf you add the strict one-per-line alignment of arguments, it's also painfully long line-wise.\nLet's handle the verticality first:\n\n</p></section><section><p>\nNo sane Lisper would write a  with every keyword on its own line:\n\n</p><figure><pre lang=\"lisp\">(loop for\n      i\n      below\n      10\n      collect\n      i)\n</pre></figure><p>\n(Some pretty-printers do that too (I'm looking at you, ECL!), but that's a topic for another day.)\n\n</p><p>\nWe don't have to put every argument on its own line.\nThat's the intuition behind the space-filling indent:\n\n</p><figure><pre lang=\"lisp\">(loop for i below 10\n      collect i)\n</pre></figure><p>\nOne can go as far as splitting the argument list in arbitrary places.\nRegardless of semantics.\n(Looking at you, SBCL!)\nPutting some keyword arguments on the first line, and then some on the second/third/etc.\nThis utilizes the space efficiently enough to be used.\nBut what if one's stuck really deep in nesting levels?\n\n</p></section><section><p>\nNow what I'm about to suggest is likely not to your taste:\n\n</p><figure><pre lang=\"lisp\">(tree-transform-if\n predicate transformer (second tree) depth)\n</pre><figcaption>My indentation style suited for deeply nested code</figcaption></figure><p>\nThis style of indentation\n(putting function name on one line, and arguments on the other)\nwas frowned upon more than once in my practice:\n\n</p><ul><li> It messes up with nesting identification: one space is not enough.\n</li><li> It makes the code too vertical.\n</li><li> And it certainly isn't idiomatic.\n</li></ul><p>\nBut what this style achieves is perfect indentation control.\nYou only get one space of indentation per form.\nComplex algorithms are easier to read when written in this style.\n\n</p><p>\nAnd!\nThis style also plays well with most indentation tools, even the simplest ones.\nI had this situation more than once:\n\n</p><ul><li> Writing a  macro in Scheme;\n</li><li> And realizing that Emacs/Geiser indentation functions think that this macro is a procedure.\n</li></ul><p>\nIndenting all the arguments of the /-like macro in this sick style helps:\n\n</p><figure><pre lang=\"scheme\">;; From\n(mtx:with-column (uab-col uab index-ab)\n                 (mtx:set!\n                  ppab 0 index-ab\n                  (blas:dot hi-hi-eval uab-col)))\n;; To\n(mtx:with-column\n (uab-col uab index-ab)\n (mtx:set!\n  ppab 0 index-ab\n  (blas:dot hi-hi-eval uab-col)))\n</pre><figcaption>Sick indent helps you to manage macros</figcaption></figure></section><section><p>\nI worked on a deeply nested corporate codebase functions.\nIn Clojure.\nAnd I realized why threading macros exist.\nTo make the logic more sequential and readable, true.\nBut also to tame excessive nesting!\n\n</p><p>\nUnfortunately, threading/arrow macros don't always work.\nCommon Lisp in particular is extremely unfriendly to threading macros.\nArrows imply a consistent thread-first or thread-last functions.\nBut CL's standard lib is too inconsistent for that to work.\nSo we're left with picking an indentation style we don't necessarily like.\n\n</p></section><section><p>\nI often prefer the macro-like indentation, because I sometimes write deeply nested code.\nBut I see the value in all the other indentation schemes!\n\n</p></section>","contentLength":4702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i4xwk5/all_lisp_indentation_schemes_are_ugly/"},{"title":"I just added an interactive console to my 3D viewer","url":"https://www.reddit.com/r/linux/comments/1i4x9f4/i_just_added_an_interactive_console_to_my_3d/","date":1737289225,"author":"/u/GloWondub","guid":391,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Too high memory usage on kubernetes cluster/nodepools","url":"https://www.reddit.com/r/kubernetes/comments/1i4x2x7/too_high_memory_usage_on_kubernetes/","date":1737288514,"author":"/u/sehrian3000","guid":368,"unread":true,"content":"<p>Hi everyone, I've an kubernetes (v1.28.10) cluster on azure and memory usage is too high. As soon as I create another user nodepool, it starts with too high memory usage. I couldn't find the reason but something is wrong. what could be the reason and how can I identify it? Thank you!</p><p>Here's my top pods. memory usage just don't addup :/ </p><p>|| || |NAMESPACE|NAME|CPU(cores)|MEMORY(bytes)| |default|cert-manager-c49c7c678-ff559|1m|29Mi| |default|cert-manager-cainjector-55b59f869b-trg4v|1m|21Mi| |default|cert-manager-webhook-6d7f5ffb54-k5cxk|1m|14Mi| |default|api-74dfd7f777-lrrp6|1m|291Mi| |default|ingress-ingress-nginx-controller-7668877844-2v4ln|1m|51Mi| |default|interface-68766f484c-z2zgt|0m|3Mi| |kube-system|azure-cns-fpwmf|1m|18Mi| |kube-system|azure-cns-qzh5m|1m|28Mi| |kube-system|azure-ip-masq-agent-7rhg9|1m|5Mi| |kube-system|azure-ip-masq-agent-pbbw7|1m|4Mi| |kube-system|cloud-node-manager-46rkq|1m|14Mi| |kube-system|cloud-node-manager-h6jzj|1m|13Mi| |kube-system|coredns-589b555d5b-6rw4h|2m|17Mi| |kube-system|coredns-589b555d5b-gvzlb|2m|16Mi| |kube-system|csi-azuredisk-node-248gk|3m|21Mi| |kube-system|csi-azuredisk-node-882cn|2m|25Mi| |kube-system|csi-azurefile-node-6zhvx|2m|30Mi| |kube-system|csi-azurefile-node-tm9lm|2m|25Mi| |kube-system|konnectivity-agent-6d64b499c7-9tds2|2m|10Mi| |kube-system|konnectivity-agent-6d64b499c7-fmggb|2m|9Mi| |kube-system|kube-proxy-bk7pt|1m|36Mi| |kube-system|kube-proxy-fw59w|1m|35Mi| |kube-system|metrics-server-5cf84656d6-7jxhq|4m|28Mi| |kube-system|metrics-server-5cf84656d6-92244|3m|24Mi|</p>","contentLength":1545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Fuzzing Book","url":"https://www.fuzzingbook.org/","date":1737287838,"author":"chautumn","guid":161,"unread":true,"content":"<p>This book is written by <em>Andreas Zeller, Rahul Gopinath, Marcel Böhme, Gordon Fraser, and Christian Holler</em>.  All of us are long-standing experts in software testing and test generation; and we have written or contributed to some of the most important test generators and fuzzers on the planet.  As an example, if you are reading this in a Firefox, Chrome, or Edge Web browser, you can do so safely partly because of us, as <em>the very techniques listed in this book have found more than 2,600 bugs in their JavaScript interpreters so far.</em>  We are happy to share our expertise and making it accessible to the public.</p>","contentLength":612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42756286"},{"title":"What are some lesser-known Rust books worth reading?","url":"https://www.reddit.com/r/rust/comments/1i4wv95/what_are_some_lesserknown_rust_books_worth_reading/","date":1737287666,"author":"/u/EightLines_03","guid":453,"unread":true,"content":"<p>What are your favorite hidden gems? Bonus points for saying why you think they're worth reading in addition to the standard works.</p>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go is a Well-Designed Language, Actually","url":"https://mattjhall.co.uk/posts/go-is-well-designed-actually.html","date":1737283705,"author":"/u/finallyanonymous","guid":364,"unread":true,"content":"<blockquote><p>An ancient programmer proverb</p></blockquote><p>In many ways 2009 decided my future career. I was thirteen and had just scored my first goal in a competitive football match - a lovely one-two with the winger finished by a powerful strike into the top left corner. Sadly the talent scouts were missing that day. Whilst I was dreaming of Wembley, Go was announced to the world.</p><p>Go soon attracted a large following. People loved how simple it was, how optimised it was for web services and the tooling like . Every action has an equal and opposite reaction however, and so it was with Go. People hated how simple it was, how it was just for noddy REST APIs and the overzealous tooling.</p><blockquote><p>And so they didn't. They didn't design a language. It sorta just \"happened\".</p></blockquote><p>To me, a design is a plan or specification for something that fulfils a goal. For example, the goal of the BBC News website might be to inform users of the most relevant things that are going on in the world. The way they do that is by writing news articles, ordering them based on location and importance. The nuclear missile speeding towards me trumps the cat stuck up a tree.</p><p>It follows that a design can be evaluated on how well it achieves the design goal.</p><p>Go was designed at Google, where Russ Cox, Rob Pike, Ken Thompson and many others were working. Google was using Java and C++ at the time which the designers of Go felt were performant but hard to use. The compilers were slow, tooling was finicky and the languages had been designed at least a decade before. Cloud computing - large numbers of multicore servers working together - was becoming widespread.</p><p>They decided to design their own language and prioritised making it work at scale - in computing and man power. Rob Pike explains in <a href=\"https://go.dev/talks/2012/splash.article\">Go at Google</a>:</p><blockquote><p>The hardware is big and the software is big. There are many millions of lines of software, with servers mostly in C++ and lots of Java and Python for the other pieces. Thousands of engineers work on the code.</p></blockquote><p>Elsewhere Pike talks about the thousands of engineers he was targeting, in his usual modest, subtle way:</p><blockquote><p>The key point here is our programmers are Googlers, they’re not researchers. They’re not capable of understanding a brilliant language.</p></blockquote><p>Top tip: if you're designing something, try to avoid belittling and patronising the people you are designing for.</p><p>Notwithstanding this quote, we get a pretty reasonable design goal: the language should make writing and maintaining large, concurrent server code easy; even across thousands of developers of differing skill levels.</p><p>Let's look at some complaints people have about Go and evaluate them against the design goal.</p><p>Go's filesystem API is often criticised for being geared towards Unix. Windows doesn't have file permissions in the way that Unix does, so Go just returns some made up ones. Furthermore Go takes a pretty simplistic approach to paths. An OS has a path separator and paths themselves are Go's  type - just a slice of bytes with no real checking or constraints.</p><p>This can be explained by the design goal. Go was designed for use at Google where their servers <a href=\"https://workspace.google.com/learn-more/security/security-whitepaper/page-4.html\">are all Linux</a>, like most servers. If you're designing a language that is aimed at servers, writing the filesystem API to be Unix-centric is not such a bad idea.</p><h3>No Operator or Function Overloading</h3><p>In Go, unlike Java, functions and methods only have one definition (once build tags and target are specified). Operators are implemented in the compiler, unlike C++, and so cannot be overloaded either.  In the  package, to add a  to a  you need to use the  method. If you want to add two days you can't just call <code>Add(0 /*years*/, 0 /*months*/, 2 /*days*/)</code>, you need to use .</p><p>To some this may seem inelegant but it is simpler. If you see a function call in Go you know there is one definition you need to check. If you see an operator you know it is for a built in type and will do something sensible, not <a href=\"https://github.com/zhuowei/nft_ptr?tab=readme-ov-file\">mint an NFT</a>.</p><p>It's fair to say that the current trend in programming languages is towards terseness. No wonder programmers hate Go's  style of error handling.</p><p>However, again, this was a deliberate choice:</p><blockquote><p>Although in contrast Go makes it more verbose to check errors, the explicit design keeps the flow of control straightforward—literally.</p></blockquote><p>Obvious control flow makes code more readable. Although languages with exceptions might be quicker to write, the code they produce is not as simple, and control flow is hidden.</p><p>Go is often criticised for being a bit of a throw-back by avoiding features like exceptions. Someone once asked the designers \"Why did you choose to ignore any research about type systems since the 1970s?\". Similar arguments has been <a href=\"https://www.bluxte.net/musings/2018/04/10/go-good-bad-ugly/#go-ignored-advances-in-modern-language-design\">repeated elsewhere</a>.</p><p>Firstly Rob Pike sees your snootiness, and doesn't care about it:</p><blockquote><p>Go was designed to address the problems faced in software development at Google, which led to a language that is not a breakthrough research language but is nonetheless an excellent tool for engineering large software projects.</p></blockquote><p>But secondly designing errors as explicit values has been a trend-(re)setter. Go, Rust and Zig have all chosen to use this approach. Swift, even though it uses exceptions, requires you to mark functions that can error in their signature.</p><p>Go doesn't play nicely with other languages. If you want to call a C function - for example to bind to SQLite - then you have to go through CGo. <a href=\"https://dave.cheney.net/2016/01/18/cgo-is-not-go\">CGo is not Go</a> and has a performance overhead. As goroutines - which have their own stack set up by the Go runtime - are the unit of execution, Go has to do some acrobatics to have the stack in the way C expects. This can be costly.</p><p>Go's FFI is not helped by it having its own compiler, linker and debugger. Much in the Go ecosystem is custom.</p><p>When we consider the design goal we can find justification. Server software has to be concurrent, and so goroutines were chosen. That necessarily makes calling C code more complicated, but the trade-off does at least fit with Go being used for concurrent systems where  talk to each other, not processes.</p><p>These decisions also gave Go a leg-up on tooling. The compiler being specific to Go means it can focus on compiling just Go as quickly as possible. The debugger can understand goroutines and all of Go's built in types.</p><p>That's subjective. For my part, I like it. The Go code I've looked at and worked on has generally been easy to read and understand. The lack of bells and whistles forces me to just write the damn code rather than building abstractions that don't hold their weight. I've also taught Go to a large group of graduates just out of university with success.</p><p>That doesn't mean I don't see its downsides. I've been sat in a call with a customer who was affected by a bug that we couldn't trace due to not checking an error. Easily prevented by linters; painful when you haven't turned them on. For ages Go didn't have generics which made it annoying to write generic data structures. Every time I receive a bug report on Windows I have to pause to think about whether Go has lulled me into a false sense of security.</p><p>Ultimately these are issues that have resulted from a trade-off that was deliberately made in the design process. You can say you don't like Go, or that it is a bad fit for a certain application, or it doesn't give you the things you need. Heck, you can even say you hate it. But don't say it wasn't (well) designed.</p>","contentLength":7301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1i4vwjt/go_is_a_welldesigned_language_actually/"},{"title":"I wrote a DHCP Server in Go (personal project)","url":"https://www.reddit.com/r/golang/comments/1i4vroz/i_wrote_a_dhcp_server_in_go_personal_project/","date":1737283125,"author":"/u/umegbewe","guid":365,"unread":true,"content":"<p>Hey everyone, i wanted to share a project i have been working on. Early 2024, I came across DHCP when i tried building a network boot server that enables anyone to boot popular OS'es across the internet.</p><p>I just found the protocol fascinating, dug into the RFC's and found ended up implemented a server i named </p><ul><li>Instead of storing every free IP in a slice, I used a bitmap to track used/free addresses, searching in a ring-like fashion. This scales better for large address pools.</li><li>Boltdb for lease persistence (future work to swap out for other db's like redis, postgres, mysql etc)</li><li>The server exposes metrics, including active/free lease counts, so you can plug it into your existing monitoring stack.</li></ul><p>If you’re interested in DHCP, network booting, or just want to explore lease-allocation algorithms in Go, check it out on GitHub <a href=\"https://github.com/umegbewe/dhcpd\">github.com/umegbewe/dhcpd</a></p><p>I’d love any feedback, ideas, or contributions especially if you’ve dealt with DHCP servers or large IP pools before. Thanks for reading, and let me know what you think!</p>","contentLength":1026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trying to find oss project relating to this and learning the logic behind","url":"https://www.reddit.com/r/kubernetes/comments/1i4vm8y/trying_to_find_oss_project_relating_to_this_and/","date":1737282463,"author":"/u/syedsadath17","guid":369,"unread":true,"content":"<p>It says it creates virtualization layer on top of aws bare metal to manage the cost . I'm trying to learn what's the steps behind this to too . As I'm curious as developer and want to gain expertize in finops</p><p>Someone more experienced can help ? </p>","contentLength":244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Baremetal Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1i4v29d/baremetal_kubernetes/","date":1737279999,"author":"/u/blgdmbrl","guid":372,"unread":true,"content":"<p>I'm currently exploring bare-metal Kubernetes and Bare Metal as a Service (BMAAS) solutions. I've come across container-based operating systems like RancherOS and Kubernetes-optimized OSs such as Talos Linux, Flatcar Linux, and RHEL. Each of these seems to support different Kubernetes flavors (e.g., Rancher Kubernetes, Tanzu, upstream Kubernetes).</p><p>I've also noticed other bare-metal Kubernetes options like OpenShift, EKS Anywhere, and Anthos. What are your experiences with these platforms? Are there specific ones you recommend exploring further for a better understanding or practical use cases?</p><p>As for BMAAS, I’ve looked into MAAS and Tinkerbell so far. Are there other tools or solutions worth considering in this space?</p><p>Thanks in advance for sharing your insights!</p>","contentLength":771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"metaheuRUSTics v0.2.0 Released! (under MIT License this time)","url":"https://www.reddit.com/r/rust/comments/1i4u8hx/metaheurustics_v020_released_under_mit_license/","date":1737276266,"author":"/u/aryashah2k","guid":450,"unread":true,"content":"<p>After a good response and a month of reading papers and implementing them, metaheuRUSTics v0.2.0 is here, there are two new algorithms: an improvement/variant of Grey Wolf and the Firefly algorithm, along with two new test functions: greiwank and beal</p><p>The error handling has improved along with plots and I have added great benchmark scripts to evaluate all the algorithms present in this package</p><p>I would love it if this community can give inputs, contribute to the code and collaborate together in the spirit of open source.</p><p>There have been lots of papers that I want to implement as algorithms in this library and if any veteran or fellow rookie wishes to learn and contribute to this project at the same time, nothing else will make me happier!</p><p><a href=\"https://www.reddit.com/r/rust/comments/1hdetdq/metaheurustics/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button\">Prev. Reddit Post(v0.1.0)</a> p.s v0.1.0 was released under GPL license but I pledge to continue development and release code under the MIT License. If you end up using this in your research, do cite this package (while the paper is still being written)</p><pre><code>Shah, A. S. (2025). MetaheuRUSTics: A comprehensive collection of metaheuristic optimization algorithms implemented in Rust. https://github.com/aryashah2k/metaheuRUSTics </code></pre>","contentLength":1163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UNLINK vs DEL - A deep dive into how it works internally in Redis.","url":"https://www.pankajtanwar.in/blog/unlink-vs-del-a-deep-dive-into-how-it-works-internally-in-redis","date":1737268352,"author":"/u/the2ndfloorguy","guid":409,"unread":true,"content":"<p>A couple of days back, I found myself debating the differences between <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis\">Redis</a>'  and  commands with my friend <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://www.linkedin.com/in/sarthak-dalabehera\">Sarthak</a> in a social media <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://www.linkedin.com/posts/pankajtanwarbanna_in-redis-whenever-a-key-is-deleted-or-expired-activity-7280448080246710272-I4FY/\">comment section</a>. An interesting take; I come across that the majority of the people seemed to believe is <em>\" is a blocking command. while  is non-blocking - so  is better\"</em>. I don't fully agree with this characterisation.</p><p>It's somewhat true - but it's not the full story. As an engineer, it's my moral duty to unnecessarily dive into the rabbit hole and dig into the <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis\">Redis codebase</a> to see the actual implementation. Let's see what's actually happening under the hood.</p><p>DEL vs UNLINK - the only difference is the way they free the value (freeing the key is straightforward). Respectfully, It will be completely wrong to just say one is blocking and another is not.</p><p>UNLINK is a smart command: it's not always non-blocking/async. It calculates the deallocation cost of an object, and if it is very small (cost of freeing &lt; 64), it will just do what DEL is supposed to do and free the object ASAP. Otherwise, the object is sent to the background queue for processing.</p><p>For my internet friends who aren't familiar with <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis\">Redis</a> - it's a super popular, distributed in-memory key-value database. As the name suggests, both  and  commands do the same thing - they remove the keys from Redis.</p><p>A quick Google search tells you why <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://redis.io/docs/latest/commands/unlink/\">UNLINK</a> was introduced in Redis 4.0, if we already had DEL.  is very similar to  but it performs the memory reclaiming in a different thread - so it's not blocking other operations, while DEL is. In simple terms, UNLINK just unlinks the key from the keyspace, and actual key removal happens later, asynchronously - so it's faster.</p><blockquote><p>Fun fact - In redis 6.0, a new configuration  was introduced - so if this is set to true, your DEL command runs like UNLINK.</p></blockquote><p>But how it's implemented internally?</p><p>It doesn't take a genius to find the very famous <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis/blob/unstable/src/db.c\"></a> in redis codebase, and head over to <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis/blob/unstable/src/db.c#L928\"></a> method - thanks to Github symbol search.</p><div><pre><code><div>client c</div><div>cserverlazyfree_lazy_user_del</div></code></pre></div><p>Not surprised - It just proxies the  method with the lazy flag which is sent as the value of  redis server config. I'm sure for , they are gonna just set it to always true - neat.</p><div><pre><code><div>client c lazy</div><div>cdbcargvj KEY_DELETED</div><div> deleted   lazy cdbcargvj</div><div>cdbcargvj</div><div>ccdbcargvj</div><div>NOTIFY_GENERIC</div><div>cargvjcdbid</div><div>cnumdel</div></code></pre></div><p>There is too much to go through - but this beautiful ternary operator got my attention - the lazy flag decides which method to call. Now, we will go deeper into  and come back to  in UNLINK code analysis.</p><blockquote><p>Hold on - Redis using  as variable name? Where are all those clean code evangelists lecturing me about meaningful variable names? never mind.</p></blockquote><p>Let's get into . Let me guess - it shouldn't be hard deleting a key from a hash table. Ah, not really. My immature high-level language lover mind took garbage collectors for granted. For a language like C, it's very very interesting and important to pay attention to how memory gets released.</p><div><pre><code><div>redisDb db robj key</div><div>db key DB_FLAG_KEY_DELETED</div></code></pre></div><p>Ah, it's a proxy to  with  argument set as 0. Cool. Let's go to  .</p><div><pre><code><div>redisDb db robj key async flags</div><div> slot keyptr</div><div>    dictEntry de dbkeys slot keyptrplinktable</div><div>db slot valtypeval</div><div>dbhexpires val</div><div> need to incr to retain val </div><div>keyvaldbidflags</div><div>dbkeyvaltype</div><div> greater than  so freeObjAsync doesnt work </div><div>keyde dbid</div><div>dbkeys slot de</div><div> the key because it is shared with the main dictionary</div><div>dbexpires slot keyptr</div><div>dbkeys slot de plink table</div></code></pre></div><p>Ahh I too found it overwhelming !! But I did find a couple of things really interesting here.</p><p>The method <code>int slot = getKeySlot(key-&gt;ptr);</code> calculates the slot for the given key. Redis uses a hash slot mechanism to distribute keys across nodes in a cluster. Shamelessly plugging <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://x.com/the2ndfloorguy/status/1873751525752762779\">my tweet</a> that explains the mechanism in a little detail.</p><p>Redis moved away from the traditional way of removing the key long back. <code>kvstoreDictTwoPhaseUnlinkFind</code> method finds the key in the main dictionary using the slot number and the key. This is the first phase - it does not delete the key instead it sets up  and . In the second phase - key will be safely removed. <code>kvstoreDictTwoPhaseUnlinkFree</code> is what I'm referring to here. This immediately releases the memory.</p><p>The  code block does a bunch of really nice things, like scheduling the memory to be freed up asynchronously. And also setting up the value of entry in main dict to NULL to mark it for deletion. We will dive into it in the UNLINK section.</p><h3><a href=\"https://www.pankajtanwar.in/blog/unlink-vs-del-a-deep-dive-into-how-it-works-internally-in-redis#4-delete-expiration-data\" aria-hidden=\"true\" tabindex=\"-1\"></a>4. Delete expiration data</h3><p>Interesting thing to note is redis maintains 2 dictionary - the main key dictionary and expiration dictionary. So, as a key is deleted - it needs to be removed from the expiration dict as well - manually. This is what -  method does.</p><p>Well - so what have you learnt here? The normal DEL command implementation is quite straight forward. Synchronously remove the key and release the memory using the two-phase linking mechanism. This is done in both the main key dictionary and expiration dictionary. And if there is an inner reference - recursive deletion comes to the rescue.</p><p>As we already covered the core logic and the code is generic with the  flag being passed down, lets' dive straight into how async delete is actually happening.</p><div><pre><code><div>keyde dbid</div><div>dbkeys slot de</div></code></pre></div><p>Wohoo, jump to .</p><div><pre><code><div>robj key robj obj dbid</div><div>    size_t free_effort keyobjdbid</div><div> possible This rarely happens however sometimes the implementation</div><div> of parts of the Redis core may call  to protect</div><div> objects then call </div><div>free_effort  LAZYFREE_THRESHOLD  objrefcount </div><div>lazyfree_objects</div><div>lazyfreeFreeObjectobj</div></code></pre></div><p>Damn, that's smart - it <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis/blob/unstable/src/lazyfree.c#L129\">calculates</a> the cost of deleting the object. Once it gets how expensive it is (free_effort indicates efforts in terms of CPU, time and memory usage may be?) - it decides to either free the memory immediately or delay it for later once it has enough CPU cycles to do it.</p><p> is set as 64. And the check  ensures that object is no-longer referenced anywhere else otherwise it might get stuck in removing it's references recursively.</p><p> is just an atomic increment counter operation on global variable  that has the number of objects currently being freezed lazily. It helps redis in scheduling the operations in the background.</p><p> is responsible for creating a job in background IO (BIO) to lazily free the object.  is a callback function defined <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/redis/redis/blob/unstable/src/lazyfree.c#L13\">on line 13 of this file</a>.</p><h2><a href=\"https://www.pankajtanwar.in/blog/unlink-vs-del-a-deep-dive-into-how-it-works-internally-in-redis#how-effort-for-deleting-the-key-is-calculated\" aria-hidden=\"true\" tabindex=\"-1\"></a>How effort for deleting the key is calculated?</h2><p>Redis' codebase says - the return value is not always the actual number of allocations the object is composed of, but a number proportional to it.</p><ul><li>for strings, it is always 1 as it does not require multiple allocations to free - so constant effort</li><li>for list objects - it is the number of elements in the quicklist.</li><li>for set objects - it is the number of elements in the hash table as set is backed by hashtable.</li><li>for sorted set objects - it is the length of skiplist</li><li>for hash objects - it is the number of elements in the hash table</li><li>for stream objects - it is the number of RAX nodes + consumer groups + entries in pending entry list (PEL)</li><li>for module - the process of calculating the value in this case seems a bit complicated for me to understand. It uses .</li></ul><h2><a href=\"https://www.pankajtanwar.in/blog/unlink-vs-del-a-deep-dive-into-how-it-works-internally-in-redis#the-magic-number-lazyfree_threshold--64\" aria-hidden=\"true\" tabindex=\"-1\"></a>The magic number LAZYFREE_THRESHOLD = 64</h2><p>Although, there is no clear explanation of  as 64, so it seems a bit arbitrary. A couple of my internet stranger friends and chatGPT says this number was chosen by  redis developers post a massive benchmarking. The consideration was trade-off such as performance vs blocking, memory management and avoiding overhead.</p><p>Now that I’ve wrapped up my ramblings, I invite your comments on it. If you find any technical inaccuracies, let me know, please. I'm active on X (twitter) as&nbsp;<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://twitter.com/the2ndfloorguy\">@the2ndfloorguy</a>&nbsp;and if you are interested in what an unfunny &amp; strange programmer will do next, see you there!</p><h4><a href=\"https://www.pankajtanwar.in/blog/unlink-vs-del-a-deep-dive-into-how-it-works-internally-in-redis#references-that-ive-used\" aria-hidden=\"true\" tabindex=\"-1\"></a>References, that I've used.</h4>","contentLength":7653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1i4sdkx/unlink_vs_del_a_deep_dive_into_how_it_works/"},{"title":"Haskell: A Great Procedural Language","url":"https://entropicthoughts.com/haskell-procedural-programming","date":1737265818,"author":"kqr","guid":160,"unread":true,"content":"<p>\nEffectful computations in Haskell are first class values. This means we can\nstore them in variables or data structures for later use. There is a Haskell\nfunction\n</p><div><pre> (, ) </pre></div><p>\nwhich, when given two integers as arguments, picks a random integer between\nthem. We can put calls to this function into a list, like so:\n</p><div><pre> [ randomRIO(1, 6), randomRIO(1, 6) ]\n</pre></div><p>\nThis is a list of two calls to . What surprises non-Haskellers is\nthat when this list is created, no random numbers are generated. Coming from\nother programming languages, we are used to side effects (such as random\ngeneration) being executed directly when the side effectful function is\ncalled.</p><p>\nWe can add more random generation to the list:\n</p><div><pre> some_dice  [ randomRIO(1, 6) ]\n</pre></div><p>\nand still no random numbers will be generated. We can go ahead and manipulate\nthis list in all sorts of ways, and  no random numbers would be\ngenerated.\n</p><p>\nTo be clear, the  function could well be called, and when\nit is called it returns a value of type . It’s just that this value . If anything, we can think of it as a set of instructions for\neventually, somehow, getting an integer. It’s not an actual integer. It’s an\nobject encapsulating a side effect. When this side effect object executes, it\nwill produce a random integer, but the object itself just describes the\ncomputation, it is not an integer.\n</p><p>\nIn other words, in Haskell, it is not enough to call a side effectful function\nto execute its side effects. When we call the side effectful function, it\nproduces an object encapsulating the side effect, and this object can be\nexecuted in the future to produce the result of the side effect.</p><p>\nThe common way we teach beginners to do execute side effect objects is by\ncalling them from a  block, using the special  assignment operator to\nextract their result. As a first approximation, we can think of the following\ncode as the way to force side effects to execute.\n</p><div><pre>\n  side  randomRIO(1, 6)\n  printf  side\n</pre></div><p>\nWe can imagine that the  arrow executes the side effect object returned by\n and captures the value it produces. Similarly, the side effect\nobject returned by  gets executed, but we don’t capture the result; we\ndon’t care about the value produced by it, we only care about the side effect\nitself.\n</p><p>\nThe lie-to-children here is that we pretend the  block is magical and that\nwhen it executes, it also executes side effects of functions called in it. This\nmental model will take the beginner a long way, but at some point, one will want\nto break free of it. That is when Haskell starts to really shine as a procedural\nlanguage.\n</p><p>\nThis article features another lie-to-children: it will have type signatures\nspecialised to  and . All the functions I mention are more generic\nthan I’m letting on.\n</p><ul><li>Anywhere this article says  it will work with any type of side effect\n(like , , , etc.)</li><li>Anywhere this article says  it probably also works with other\ncollection/container types (like , , , , etc.)</li></ul><p>\nThe reason this article uses more direct type signatures is to hopefully be\nreadable also to someone who does not use Haskell.</p>","contentLength":3056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42754098"},{"title":"HA Storage solution for \"Low Resource\" Cluster","url":"https://www.reddit.com/r/kubernetes/comments/1i4rbr6/ha_storage_solution_for_low_resource_cluster/","date":1737264381,"author":"/u/InsuranceOne446","guid":371,"unread":true,"content":"<p>Hello Everyone, I have spent the last 3 Months very slowly but surely learning k8s and have now set up a (terraform backed) talos cluster with: </p><p>3 Ctrl 3 Worker Nodes. The Setup is structured like this: 3x N100 Mini PC with 16GB RAM and 512GB SSD each.<p> (each PC does have 2 1Gig Nics, Talos Nodes run in Proxmox with 4Gigs of Ram for the Control Nodes and 8Gigs of Ram for the workers)</p></p><p>I then went about learning about GitOps and have my Cluster completely provisioned by FluxCD as of now.</p><p>I initially set up rook-ceph for my pves but It seems to be too much for my little system. rook-ceph is hogging about 11Gigs of Ram which leaves not much place for other resource hungry pods like Jellyfin.</p><p>I have tought about going the Longhorn Route as it seems to be more friendly on the memory but have not heard a lot of good things about Longhorn in comparison to ceph.</p><p>Any Advice would be greatly appreciated.</p>","contentLength":900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wish this handy \"robustio\" package was exported, it used by \"go clean -modcache\"","url":"https://pkg.go.dev/cmd/go/internal/robustio","date":1737263463,"author":"/u/wolfeidau","guid":362,"unread":true,"content":"<section><p>Package robustio wraps I/O functions that are prone to failure on Windows,\ntransparently retrying errors up to an arbitrary timeout.\n</p><p>Errors are classified heuristically and retries are bounded, so the functions\nin this package do not completely eliminate spurious errors. However, they do\nsignificantly reduce the rate of failure in practice.\n</p><p>If so, the error will likely wrap one of:\nThe functions in this package do not completely eliminate spurious errors,\nbut substantially reduce their rate of occurrence in practice.\n</p></section><section><div><p>IsEphemeralError reports whether err is one of the errors that the functions\nin this package attempt to mitigate.\n</p><p>Errors considered ephemeral include:\n</p><ul><li>syscall.ERROR_ACCESS_DENIED</li><li>syscall.ERROR_FILE_NOT_FOUND</li><li>internal/syscall/windows.ERROR_SHARING_VIOLATION</li></ul><p>This set may be expanded in the future; programs must not rely on the\nnon-ephemerality of any given error.\n</p></div><div><p>ReadFile is like os.ReadFile, but on Windows retries errors that may\noccur if the file is concurrently replaced.\n</p><p>(See golang.org/issue/31247 and golang.org/issue/32188.)\n</p></div><div><p>RemoveAll is like os.RemoveAll, but on Windows retries errors that may occur\nif an executable file in the directory has recently been executed.\n</p><p>(See golang.org/issue/19491.)\n</p></div><div><p>Rename is like os.Rename, but on Windows retries errors that may occur if the\nfile is concurrently read or overwritten.\n</p><p>(See golang.org/issue/31247 and golang.org/issue/32188.)\n</p></div></section>","contentLength":1405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1i4r29m/i_wish_this_handy_robustio_package_was_exported/"},{"title":"TikTok goes dark in the US","url":"https://techcrunch.com/2025/01/18/tiktok-goes-dark-in-the-u-s/","date":1737258709,"author":"mfiguiere","guid":201,"unread":true,"content":"<p>TikTok went dark in the U.S. on Saturday night as a result of a federal law that bans the popular short-form video app for millions of Americans. However, <a href=\"https://techcrunch.com/2025/01/19/tiktok-says-is-restoring-service-in-the-us/\">the company began restoring service</a> by midday Sunday.</p><p>TikTok users began receiving a message about the ban around 10:30 p.m. Eastern on Saturday evening, and the app also disappeared from the Apple and Google Play app stores. As of Sunday morning, some users in the U.S.  could still access TikTok via the web.</p><p>“Sorry, TikTok isn’t available right now,” the company’s message reads. “A law banning TikTok has been enacted in the U.S. Unfortunately, that means you can’t use TikTok for now.”</p><p>The message also suggested this might only be a temporary disappearance. TikTok credited President-elect Donald Trump for indicating “he will work with us on a solution to reinstate TikTok once he takes office,” with users urged to “stay tuned!”</p><p>The company warned earlier this week the app’s disappearance was imminent, <a href=\"https://techcrunch.com/2025/01/18/tiktok-says-it-will-go-dark-sunday-unless-biden-offers-definitive-statement/\">saying Friday that it would “go dark”</a> unless President Joe Biden’s administration made a “definitive statement” that it wouldn’t enforce the ban.</p><p>The Supreme Court issued a ruling <a href=\"https://techcrunch.com/2025/01/17/supreme-court-upholds-tiktok-ban/\">upholding the law Friday</a>; and the Biden administration seemed inclined to<a href=\"https://techcrunch.com/2025/01/18/tiktok-says-it-will-go-dark-sunday-unless-biden-offers-definitive-statement/\"> leave the app’s fate in the hands of the next president</a>. White House Press Secretary Karine Jean-Pierre noted that with the law taking effect right before Trump’s inauguration on Monday, “actions to implement the law simply must fall to the next Administration.” Deputy Attorney General Lisa Monaco issued a similar statement that “the next phase of this effort — implementing and ensuring compliance with the law after it goes into effect on January 19 — will be a process that plays out over time.”</p><p>TikTok, however, suggested this was not enough assurance for “critical service providers” to continue listing or hosting the app in the United States unless the Biden administration made the aforementioned “definitive statement.” Jean-Pierre called TikTok’s response “a stunt” and claimed there’s “no reason for TikTok or other companies to take actions in the next few days before the Trump administration takes office on Monday.”</p><p>As for the app’s long-term prospects, Trump has said he plans to “negotiate a resolution” that would presumably involve a sale or other concessions from ByteDance, which has repeatedly said it’s not interested in selling yet <a href=\"https://techcrunch.com/2025/01/17/tiktok-ceo-responds-to-trump-thanks-him-for-trying-to-solve-us-ban/\">seems optimistic about its prospects under Trump</a>. </p><p>Trump <a rel=\"nofollow\" href=\"https://www.nbcnews.com/politics/donald-trump/trump-likely-give-tiktok-90-day-extension-avoid-ban-rcna188258\">reiterated to NBC News</a> on Saturday that he will “most likely” give TikTok a 90-day reprieve from the ban once he takes office Monday.</p><p>“I think that would be, certainly, an option that we look at. The 90-day extension is something that will be most likely done, because it’s appropriate. You know, it’s appropriate. We have to look at it carefully. It’s a very big situation,” Trump told the outlet.</p><p>There was even a report suggesting that the Chinese government was <a href=\"https://techcrunch.com/2025/01/17/tiktok-ceo-responds-to-trump-thanks-him-for-trying-to-solve-us-ban/\">considering a sale to Elon Musk</a> as part of a broader deal with the Trump administration. A TikTok spokesperson called that report “pure fiction.”</p><p><em>This post has been updated to reflect that TikTok is restoring service. It was previously updated to reflect Trump’s statements Sunday morning, as well as the additional apps that have been blocked by the law.</em></p>","contentLength":3346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42753396"},{"title":"Show HN: Interactive systemd – a better way to work with systemd units","url":"https://isd-project.github.io/isd/","date":1737217323,"author":"kai-tub","guid":155,"unread":true,"content":"<p>I created a TUI for systemd/systemctl called isd (interactive systemd).</p><p>It provides a fuzzy search for units, auto-refreshing previews, smart sudo handling, and a fully customizable, keyboard-focused interface for power users and newcomers alike.</p><p>It is a more powerful (but heavier) version of sysz, which was the inspiration for the project.</p><p>This should be a huge timesaver for anybody who frequently interacts with or edits systemd units/services. And if not, please let me know why! :)</p>","contentLength":485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42749402"},{"title":"Apple Intelligence rolled back after doing dumb stuff...","url":"https://www.youtube.com/watch?v=7rXgVsIGvGQ","date":1737213770,"author":"Fireship","guid":356,"unread":true,"content":"<article>Apple recently rolled back its news summary feature on iOS powered by Apple Intelligence because it was hallucinating misinformation. Let's examine recent failures and controversies in the tech world. \n\n#tech #ai #thecodereport \n\n💬 Chat with Me on Discord\n\nhttps://discord.gg/fireship\n\n🔗 Resources\n\nTech Trends in 2025 https://youtu.be/v4H2fTgHGuc\nApple Intelligence Announcement https://youtu.be/ek2yOqAIYuU\n\n📚 Chapters\n\n🔥 Get More Content - Upgrade to PRO\n\nUpgrade at https://fireship.io/pro\nUse code YT25 for 25% off PRO access \n\n🎨 My Editor Settings\n\n- Atom One Dark \n- vscode-icons\n- Fira Code Font\n\n🔖 Topics Covered\n\n- Apple Intelligence failures\n- Progress of Artificial Super Intelligence\n- Elon feud with Asmongold \n- Zuckerberg lies on Joe Rogan\n- Software engineering trends</article>","contentLength":803,"flags":null,"enclosureUrl":"https://www.youtube.com/v/7rXgVsIGvGQ?version=3","enclosureMime":"","commentsUrl":null}]}