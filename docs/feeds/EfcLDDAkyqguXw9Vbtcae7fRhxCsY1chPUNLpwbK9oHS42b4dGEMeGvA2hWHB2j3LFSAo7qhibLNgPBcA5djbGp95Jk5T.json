{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Lifelong Learning: 88+ Resources I Don't Regret as a Senior Software Engineer","url":"https://thetshaped.dev/p/lifelong-learning-88-plus-resources-i-do-not-regret-as-senior-software-engineer","date":1737954912,"author":"/u/pepincho","guid":502,"unread":true,"content":"<p>The Internet is flooded with content, materials, and resources.</p><p>Knowing the most useful newsletters, books, courses, creators, and tools is hard.</p><p>In this article, I want to share some incredible resources that I‚Äôve found valuable in my experience and day-to-day job.</p><p>Each resource on this list taught me something new and helped me learn and grow as an engineer.</p><p>As a Senior Software Engineer, I need to keep up with the industry trends, updates, libraries, tools, vulnerabilities, etc, so that I can apply this knowledge at my job.</p><p>Here are most of the newsletters I read every week.</p><p>I use these four newsletters to stay up-to-date with the Web and JavaScript world.</p><p><strong>I‚Äôd suggest avoiding reading all these newsletter at once because you‚Äôll feel overwhelmed.</strong></p><p>Depending on your current needs, priorities, and career aspirations, you might want to choose a few of them and come to the rest when needed.</p><p>Books are a great way to learn and grow as an individual.</p><p>We can learn from a lot of people even though they‚Äôre not in front of us.</p><p>We can learn from their mistakes and see what lessons they have learned.</p><p>However, I‚Äôve found that if I read a book and don‚Äôt apply my knowledge from it as soon as possible, the value from reading the book drops drastically.</p><p><strong>Think in advance how reading a particular book will help you in your day-to-day tasks, job, and personal life. If you can‚Äôt apply the knowledge immediately, postpone reading it.</strong></p><p>Each book has taught me something or sparked a new idea and way of thinking.</p><p>It‚Äôs not necessary to apply everything from each book but rather look for the things that most suits you at the moment and adapt them to your lifestyle.</p><p>A great mistake I made in the past was to try to apply everything on 100%.</p><p><strong>The true wisdom comes when you find the 10-20% of the book to apply at the moment.</strong></p><p>At each phase of our lives we need different things, so think twice before applying anything directly. Be conscious.</p><ul><li><p>Don‚Äôt try to read and follow everything at once.</p></li><li><p>Think in advance how reading a particular book will help you in your day-to-day tasks, job, and personal life.</p></li><li><p>If you can‚Äôt apply the knowledge immediately, postpone reading the book or newsletter.</p></li><li><p>As you might see, there‚Äôre no courses. My preferred way is to read and apply what I read immediately in my day-to-day job or side-projects.</p></li></ul><p>That's all for today. I hope this was helpful.</p><p>What are the 1-2 resources you‚Äôve found life changing? Share them in the comments üëÄ üëá</p><p>Become a better React Software Engineer. Join 17,400+ engineers who are improving their skills every week.</p><p>I share daily practical tips to level up your skills and become a better engineer.</p><p><em>Thank you for being a great supporter, reader, and for your help in growing to 17.6K+ subscribers this week üôè</em></p><p><em>You can also hit the like ‚ù§Ô∏è button at the bottom to help support me or share this with a friend. It helps me a lot! üôè</em></p>","contentLength":2890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ib0lvg/lifelong_learning_88_resources_i_dont_regret_as_a/"},{"title":"DeepSeek R1 API First Look: How This Open-Source Model Outperforms OpenAI","url":"https://www.kaishira.com/2025/01/26/deepseek-r1-api-first-look-how-this-open-source-model-outperforms-openai/","date":1737922386,"author":"/u/haberveriyo","guid":504,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iaoyh2/deepseek_r1_api_first_look_how_this_opensource/"},{"title":"How long is a second in JS? | Why Some Are Longer Than Others","url":"https://docs.timetime.in/blog/how-long-is-a-second-in-js","date":1737912975,"author":"/u/iagolast","guid":505,"unread":true,"content":"<p>When you ask the question \"How long is a second in JavaScript?\", it seems like a straightforward query. However, the answer reveals layers of complexity intertwined with history, science, and the foundations of modern computing. Let‚Äôs dive deep to understand how humanity has measured time and how it connects to JavaScript‚Äôs timekeeping.</p><h3>The Early Days of Timekeeping<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#the-early-days-of-timekeeping\" aria-label=\"Direct link to The Early Days of Timekeeping\" title=\"Direct link to The Early Days of Timekeeping\">‚Äã</a></h3><p>Time measurement began with observing natural periodic phenomena. Early civilizations looked to the moon's phases and the apparent movement of the sun to divide time into manageable units. These observations gave rise to the concepts of days, months, and years.</p><p>As human needs evolved, a more granular division of time became necessary. The ancient Egyptians and Babylonians divided the day into 24 hours, likely influenced by their base-12 numbering system. Later, hours were divided into 60 minutes and minutes into 60 seconds, creating the framework we still use today.</p><p>While these early methods sufficed for centuries, they lacked precision. By the mid-20th century, advancements in technology demanded a more accurate measurement of time. The invention of the atomic clock in the 1950s marked a revolutionary step. These clocks measure time based on the vibrations of cesium atoms, offering unparalleled precision.</p><p>As a result, the definition of a second was updated to:</p><blockquote><p>\"The duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the cesium-133 atom.\"</p></blockquote><p>This definition formed the foundation for what we now call international atomic time (TAI)</p><p><a href=\"https://en.wikipedia.org/wiki/Universal_Time\" target=\"_blank\" rel=\"noopener noreferrer\">Universal Time</a> is based on the Earth's rotation. It‚Äôs intuitive and tied to our everyday experience of day and night. However, the Earth's rotation is not perfectly consistent due to various factors such as gravitational interactions and geophysical processes. As a result, we could say that Universal Time is imprecise and variable.</p><p><a href=\"https://en.wikipedia.org/wiki/International_Atomic_Time\" target=\"_blank\" rel=\"noopener noreferrer\">Atomic Time</a>, derived from atomic clocks, is based on physical phenomena and is both precise and consistent. It serves as the backbone for modern timekeeping systems.</p><p>In the 1970s, <a href=\"https://en.wikipedia.org/wiki/Coordinated_Universal_Time\" target=\"_blank\" rel=\"noopener noreferrer\">Coordinated Universal Time (UTC</a>) was introduced to reconcile the discrepancies between Universal Time and Atomic Time. UTC aligns with Universal Time but incorporates adjustments, such as the addition or removal of leap seconds, to compensate for the Earth's irregular rotation.</p><p>Leap seconds are occasional one-second adjustments added to UTC to keep it in sync with Universal Time. These adjustments ensure that UTC remains within 0.9 seconds of Universal Time. However, they introduce complications for systems that rely on precise time calculations.</p><p>Here‚Äôs the improved article in English, incorporating the original tables:</p><h3>ECMAScript and POSIX Time<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#ecmascript-and-posix-time\" aria-label=\"Direct link to ECMAScript and POSIX Time\" title=\"Direct link to ECMAScript and POSIX Time\">‚Äã</a></h3><p>This approach treats every day as having exactly  (24 hours), ignoring both leap seconds and astronomical variations in the Earth's rotation. While this simplifies calculations and suffices for most applications, it introduces limitations in scenarios where precise timekeeping is critical.</p><p>Although POSIX and UTC appear similar, . Here are the key differences:</p><ol><li><ul><li>UTC includes leap seconds to adjust official time to the Earth's rotation.</li><li>POSIX does not count leap seconds, meaning that <strong>POSIX days always have 86,400 seconds</strong>, even when UTC days have 86,401 or 86,399 seconds due to a leap second.</li></ul></li><li><p><strong>Simplified Interoperability</strong>:</p><ul><li>POSIX assumes all seconds are of equal duration. This makes calculating time differences straightforward but causes discrepancies when leap seconds occur.</li><li>For example, during a leap second (e.g., ), POSIX simply ignores the additional second, making timestamps near the leap second ambiguous or inaccurate.</li></ul></li></ol><h3>Step Adjustment: The Traditional Solution<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#step-adjustment-the-traditional-solution\" aria-label=\"Direct link to Step Adjustment: The Traditional Solution\" title=\"Direct link to Step Adjustment: The Traditional Solution\">‚Äã</a></h3><p>Historically, systems that needed to account for leap seconds would use a technique called . This method can be described as just ignoring the leap second, causing the clock to jump forward or backward by one second at the exact moment of the leap second.</p><p>Lets see a table to understand this:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>As you can see from the both TAI and UTC are strictly increasing, but POSIX (step) has a jump in time at the leap second. This means that 2 different UTC seconds are mapped to the same POSIX second. This can cause issues in some applications. The biggest problem is that some apps can go \"back in time\" when a leap second is added.</p><p>Lets say that a payment is requested at , and approved during the next second at . Our logs will show something like:</p><div><div><pre tabindex=\"0\"><code></code></pre></div></div><p>This is a problem because the payment was approved before it was requested. This is why most system use a different approach to handle leap seconds.</p><h3>Smearing: The Practical Solution<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#smearing-the-practical-solution\" aria-label=\"Direct link to Smearing: The Practical Solution\" title=\"Direct link to Smearing: The Practical Solution\">‚Äã</a></h3><p>In many modern systems implementing POSIX time, a technique called  is used to handle leap seconds. Instead of adding or removing a whole second, the effects of the leap second are distributed gradually over a longer period (typically 24 hours). This has several implications: First of all during the smear period, <strong>seconds are slightly longer or shorter than a standard second</strong> (measured in atomic time). This ensures a smooth transition instead of an abrupt jump in time. On the other hand compared to stepping the clock is strictly increasing, which is a requirement for many applications.</p><h4>Example: Smearing and Step Adjustment Tables<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#example-smearing-and-step-adjustment-tables\" aria-label=\"Direct link to Example: Smearing and Step Adjustment Tables\" title=\"Direct link to Example: Smearing and Step Adjustment Tables\">‚Äã</a></h4><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>In this table:  ignores the leap second entirely, treating  as if it doesn't exist while  gradually adjusts the clock, ensuring smoother transitions but at the cost of seconds having slightly different durations.</p><p>This is how Google's NTP servers handle leap seconds. They smear the leap second over a 24-hour period, ensuring that the clock is always strictly increasing. More information can be found <a href=\"https://developers.google.com/time/smear\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p><h2>Implications for Applications<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#implications-for-applications\" aria-label=\"Direct link to Implications for Applications\" title=\"Direct link to Implications for Applications\">‚Äã</a></h2><h3>Why It Usually Doesn‚Äôt Matter<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#why-it-usually-doesnt-matter\" aria-label=\"Direct link to Why It Usually Doesn‚Äôt Matter\" title=\"Direct link to Why It Usually Doesn‚Äôt Matter\">‚Äã</a></h3><p>For tasks like scheduling events, calculating ages, or displaying time in user-facing applications, JavaScript (and POSIX) time is precise enough and 99% of the problem will be derived from timezones.</p><p>For applications requiring high precision, such as those in scientific research or financial systems, the discrepancies caused by leap seconds and smearing can lead to significant errors. Accurate timekeeping is crucial to ensure the reliability and accuracy of measurements and transactions. During a leap second, a single POSIX timestamp can refer to two different moments in UTC, creating ambiguities and potential errors in systems that demand precise time synchronization.</p>","contentLength":6446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iakow2/how_long_is_a_second_in_js_why_some_are_longer/"},{"title":"Improve Rust Compile Time by 108X","url":"https://burn.dev/blog/improve-rust-compile-time-by-108x","date":1737912528,"author":"/u/Handsome_AndGentle","guid":503,"unread":true,"content":"<p>\nBefore you get too excited, the techniques used to reduce compilation\n          time are not applicable to all Rust projects. However, I expect the\n          learnings to be useful for any Rust developer who wants to improve\n          their project's compilation time. Now that this is clarified, let's\n          dive into the results.\n</p><p>\nWe started with a compilation time of 108 seconds for the matmul\n          benchmarks, which was reduced to only 1 second after all the\n          optimizations. The most effective optimization was the element-type\n          generics swap, where we instantiated generic functions with predefined\n          \"faked\" element types to reduce the amount of LLVM code generated. The second optimization also had a major impact, further\n          reducing the compilation time by nearly 3√ó. This was achieved by using\n          our comptime system instead of associated const generics to represent the\n          matmul instruction sizes. Finally, the last optimization‚Äîalso the simplest‚Äîwas\n          to reduce the LLVM optimization level to zero, which is particularly useful\n          for debug builds, such as tests.\n</p><div><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\"></svg></div><div><p>\nCompilation times are measured using incremental compilation.\n</p></div><p>\nFirst, let me explain the situation that led me to investigate our\n          compile time. During the last iteration of CubeCL, we refactored our matrix multiplication GPU kernel to work with\n          many different configurations and element types. CubeCL is a dialect\n          that lets you program GPUs for high-performance computing applications\n          directly in Rust. The project supports multiple compiler backends,\n          namely WebGPU, CUDA, ROCm, and Vulkan with more to come.\n</p><p>\nThe refactoring of the matrix multiplication kernel was done to\n          improve tensor cores utilization across many different precisions. In\n          fact, each kernel instance works across 3 different element types: the\n          global memory element type, the staging element type, and the\n          accumulation element type. These are all different since we normally\n          want to accumulate in a higher precision element type, as this is\n          where numerical approximation is most sensitive. Also, the tensor\n          cores don't work across all input precisions; if you have f32 inputs,\n          you need to convert those to tf32 element types (staging) to call the\n          tensor cores instructions. To add to the complexity, tensor cores\n          instructions only work across fixed matrix shapes that also depend on\n          the precisions. For instance, f16 staging matrices work across all\n          shapes from (32, 8, 16), (16, 16, 16), (8, 32, 16). But tf32 only\n          works on (16, 16, 8).\n</p><p>\nIn our first refactoring, we represented the shape of the matrices\n          supported by the instructions using const associated types, since this\n          is the abstraction component that makes the most sense in this case.\n          For the element types, we naturally used generic arguments for traits\n          and functions - pretty much what any developer would do in this\n          scenario. However, with all the possible combinations, we ended up\n          with a compilation time of 1m48s using the cache.\n</p><p>\nYes, you read that right: 1m48s just to rebuild the matmul benchmark\n          if we change anything in the bench file.\n</p><p>\nFor the purpose of this optimization, we only consider incremental\n          compilation using cargo caching, since this is the most important one\n          to speed up dev iteration time. Changing one configuration to test if\n          an optimization worked took almost 2 minutes just to create the binary\n          to execute a few matmuls.\n</p><p>\nWell, we need to understand that the Rust compiler is actually very\n          fast. The slow parts are the linking and LLVM. The best way to improve\n          compilation time is to reduce the amount of LLVM IR generated.\n</p><p>\nIn our specific case, each combination of the matmul would generate a\n          whole new function - this is what zero-cost abstraction means. There\n          is no dynamic dispatch; every type change duplicates the code to\n          improve performance at the cost of a bigger binary. Before all of our\n          optimizations, the binary generated was 29M, and after we reduced it\n          to 2.5M - a huge difference.\n</p><p>\nTo reduce the amount of code generated, we had to use different Rust\n          techniques to make our abstractions for the matmul components. In our\n          case, we don't need zero-cost abstractions, since the code written in\n          Rust for the matmul components actually generates the code that is\n          used to compile at runtime a kernel that will be executed on the GPU.\n          Only the GPU code needs to be fast; the JIT Rust code takes almost no\n          time during runtime. Zero-cost abstraction would actually be optimal\n          in a setting where we would perform ahead-of-time compilation of\n          kernels.\n</p><p>\nEver wonder why LibTorch or\n          cuBLAS have executables that are\n          GIGABYTES in size? Well, it's because all kernels for all precisions with\n          all edge cases must be compiled to speed up runtime execution. This is\n          necessary in a compute-heavy workload like deep learning.\n</p><p>\nHowever, CubeCL is different - it performs JIT compilation, therefore\n          we don't need to compile all possible variations ahead of time before\n          creating the binary: we can use dynamic abstractions instead! This is\n          one of the two optimizations that we made for the matmul components.\n          Instead of relying on const associated types, we leveraged the\n          comptime system to dynamically have access to the instruction sizes\n          during the compilation of a kernel at runtime. This is actually the\n          second optimization that we made and helped us go from 14s compilation\n          time to around 5s.\n</p><p>\nHowever, the biggest optimization was quite hard to pull off and is\n          linked to the generic element types passed in each function. We still\n          wanted to use zero-cost abstraction in this case, since passing around\n          an enum listing what element type operations are on would be terrible\n          in terms of developer experience. However, the hint to improve our\n          compilation time was that when you write a function that will execute\n          on the GPU, we have an attribute on top .\n</p><p>\nWe want the code to look and feel like normal Rust, but the macro\n          actually parses the Rust code written and generates another function,\n          which we normally call the expand function, where the actual GPU IR is\n          built for the function. That code will actually run during runtime,\n          not the code that the user is writing. The element types generics are\n          only used to convert the generic element type into the enum IR element\n          type. In the expand functions, we also pass a context where all IR is\n          tracked.\n</p><p>\nSo the optimization was to pass a fake generic element type, called  instead of the actual element type like . When\n          compiling a kernel, we first register the real element type in the\n          context, using the const generic item to differentiate multiple\n          element types if a function has multiple generic element types. Since\n          we always call the expand function with the exact same generic items\n          for all element types, we only generate one instance of that function,\n          and the element types are fetched at runtime using the context.\n</p><p>\nThe most tedious part was actually implementing that optimization\n          while trying not to break our components. The biggest problem caused\n          by that optimization is that we can't support generic dependencies\n          between traits over the element type in launch functions of CubeCL.\n</p><div><pre tabindex=\"0\" data-language=\"rust\"><code></code></pre></div><p>\nIt makes sense though - we don't want to recompile all the instruction\n          types for all different precisions. Since our optimization is\n          activated at the boundaries of CPU code and GPU code, where cube\n          functions are identified as launchable, we need the generic trait to\n          not have a dependency on the element type. They are going to be\n          switched by our macro. We use generic associated types instead of\n          traits with generic element types.\n</p><p>\nThis is known as the family pattern, where a trait is describing a family of types.\n</p><div><pre tabindex=\"0\" data-language=\"rust\"><code></code></pre></div><p>\nUsing this pattern, we can inject the family type at the boundaries of\n          CPU and GPU code and instantiate the inner instruction type with the\n          expand element type.\n</p><div><pre tabindex=\"0\" data-language=\"rust\"><code></code></pre></div><p>\nMigrating most of the components to the new pattern, we reduced\n          compile time from 1m48s to about 14s.\n</p><p>\nIt was a lot of work, and I don't expect all projects to face cases\n          like this, but it was worth it! Now waiting for about 5 seconds after\n          trying something in the code to see if performance is improved doesn't\n          break the flow, but almost 2 minutes did.\n</p><p>\nWe essentially leveraged the fact that CubeCL is a JIT compiler and\n          not an AOT compiler, which is very appropriate for throughput-focused\n          high-performance applications.\n</p><h2>Playing with LLVM optimization settings</h2><p>\nSince our benchmarks are compiled with optimization level set to 3, we\n          could still improve the compilation time further to about 1s by\n          reducing the optimization level to zero. Another 5X speedup that we\n          can have by simply adjusting the LLVM optimization level.\n</p><p>\nWe decided not to keep that optimization in production, since we want\n          the benchmarks to have the same LLVM optimization level as user\n          applications. However, we activated it for testing, since we often\n          rerun tests to ensure we don't break correctness when implementing or\n          optimizing kernels.\n</p><h2>Not a Rust Compiler Issue</h2><p>\nAll of our optimizations actually created tons of code - we used proc\n          macros, associated type generics, const generics, and tons of complex\n          features from the Rust type system.\n</p><p>\nThe Rust compiler is actually very fast; the slow part is really the\n          linking and optimizing of the LLVM IR. If there's one thing to take\n          from this post, it's that you shouldn't worry about using complex\n          features of Rust, but make sure you don't generate huge binaries.\n          Reducing the binary size will improve compile time even if you use\n          complex methods to do so! \"Less code compiles faster\" is not exactly\n          right. \"Less generated code compiles faster\" is what we have to keep\n          in mind!\n</p>","contentLength":10722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iakht9/improve_rust_compile_time_by_108x/"},{"title":"'First AI software engineer' is bad at its job","url":"https://www.theregister.com/2025/01/23/ai_developer_devin_poor_reviews/","date":1737886945,"author":"/u/Wownever","guid":507,"unread":true,"content":"<p>A service described as \"the first AI software engineer\" appears to be rather bad at its job, based on a recent evaluation.</p><p>The auto-coder is called ‚ÄúDevin‚Äù and was <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.cognition.ai/blog/introducing-devin\">introduced</a> in March 2024. The bot‚Äôs creator, an outfit called Cognition AI, has made claims such as ‚ÄúDevin can build and deploy apps end to end,\" and \"can autonomously find and fix bugs in codebases.\" The tool <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.cognition.ai/blog/devin-generally-available\">reached general availability</a> in December 2024, starting at $500 per month.</p><p>\"Devin is an autonomous AI software engineer that can write, run and test code, helping software engineers work on personal tasks or their team projects,\" Cognition's <a target=\"_blank\" rel=\"nofollow\" href=\"https://docs.devin.ai/get-started/devin-intro\">documentation</a> declares. It \"can review PRs, support code migrations, respond to on-call issues, build web applications, and even perform personal assistant tasks like ordering your lunch on DoorDash so you can stay locked in on your codebase.\"</p><p>The service uses Slack as its main interface for commands, which are sent to its computing environment, a Docker container that hosts a terminal, browser, code editor, and planner. The AI agent supports API integration with external services. This allows it, for example, to send email messages on a user's behalf via SendGrid.</p><p>Devin is a \"<a target=\"_blank\" rel=\"nofollow\" href=\"https://cognition.ai/blog/evaluating-coding-agents\">compound AI system</a>,\" meaning it relies on multiple underlying AI models, a set that has included OpenAI's GPT-4o and can be expected to evolve over time.</p><p>In theory, you should be able to ask it to undertake tasks like migrating code to <a target=\"_blank\" rel=\"nofollow\" href=\"https://nbdev.fast.ai/\">nbdev</a>, a Jupyter Notebook development platform, and expect it to do so successfully. But that may be asking too much.</p><p>Early assessments of Devin have found problems. Cognition AI posted a <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=UTS2Hz96HYQ\">promo video</a> that supposedly showed the AI coder autonomously completing projects on the freelancer-for-hire platform Upwork. Software developer <a target=\"_blank\" rel=\"nofollow\" href=\"https://github.com/carlbrown\">Carl Brown</a> analyzed that vid and debunked it on his <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=tNmgmwEtoWE\">Internet of Bugs YouTube channel</a>.</p><p>Now, three data scientists affiliated with <a target=\"_blank\" rel=\"nofollow\" href=\"https://answer.ai\">Answer.AI</a>, an AI research and development lab founded by Jeremy Howard and Eric Ries, have <a target=\"_blank\" rel=\"nofollow\" href=\"https://devin.ai/\">tested Devin</a> and found it completed just three out of 20 tasks successfully.</p><p>In an <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.answer.ai/posts/2025-01-08-devin.html#what-is-devin\">analysis</a> conducted earlier this month by <a target=\"_blank\" rel=\"nofollow\" href=\"https://hamel.dev/\">Hamel Husain</a>, <a target=\"_blank\" rel=\"nofollow\" href=\"https://isaac-flath.github.io/website/blog.html\">Isaac Flath</a>, and <a target=\"_blank\" rel=\"nofollow\" href=\"https://johnowhitaker.dev/\">Johno Whitaker</a>, Devin started well, successfully pulling data from a Notion database into Google Sheets. The AI agent also managed to create a planet tracker for checking claims about the historical positions of Jupiter and Saturn.</p><p>But as the three researchers continued their testing, they encountered problems.</p><p>\"Tasks that seemed straightforward often took days rather than hours, with Devin getting stuck in technical dead-ends or producing overly complex, unusable solutions,\" the researchers explain in their report. \"Even more concerning was Devin‚Äôs tendency to press forward with tasks that weren‚Äôt actually possible.\"</p><p>As an example, they cited how Devin, when asked to deploy multiple applications to the infrastructure deployment platform <a target=\"_blank\" rel=\"nofollow\" href=\"https://railway.com/\">Railway</a>, failed to understand this wasn't supported and spent more than a day trying approaches that didn't work and hallucinating non-existent features.</p><p>Of <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.answer.ai/posts/2025-01-08-devin.html#appendix-tasks-attempted-with-devin\">20 tasks presented to Devin</a>, the AI software engineer completed just three of them satisfactorily ‚Äì the two cited above and a third challenge to research how to build a Discord bot in Python. Three other tasks produced inconclusive results, and 14 projects were outright failures.</p><p>The researchers said that Devin provided a polished user experience that was impressive when it worked.</p><p>\"But that‚Äôs the problem ‚Äì it rarely worked,\" they wrote.</p><p>\"More concerning was our inability to predict which tasks would succeed. Even tasks similar to our early wins would fail in complex, time-consuming ways. The autonomous nature that seemed promising became a liability ‚Äì Devin would spend days pursuing impossible solutions rather than recognizing fundamental blockers.\"</p><p>Cognition AI did not respond to a request for comment. ¬Æ</p>","contentLength":3870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iab2wq/first_ai_software_engineer_is_bad_at_its_job/"},{"title":"üîí What's OAuth2, anyway?","url":"https://www.romaglushko.com/blog/whats-aouth2/","date":1737886623,"author":"/u/roma-glushko","guid":506,"unread":true,"content":"<p>Have you ever logged into a website using your Google or Facebook account?\nOr connected an app to access your GitHub data? If so, you‚Äôve already used OAuth2, whether you knew it or not.</p><p>OAuth2 is the world‚Äôs most popular, extensible authorization framework.\nIt allows you to integrate a couple of systems together by delegating access to your data from one service to another.\nBut here is the thing - most people don‚Äôt really understand how OAuth2 really works.</p><p>Personally, I‚Äôve implemented several applications that were using OAuth2.\nThe process was so straightforward that I had no need to stop and think about the protocol itself along the way.\nThat‚Äôs by design. OAuth2 is built to be super simple to implement client applications, not to wrestle with complex authentication requirements.</p><p>But if we pause and dig deeper, there‚Äôs a lot to learn from the software engineering point of view.</p><p>In this article, we will uncover the ‚Äúwhys‚Äù behind the OAuth2 protocol design and\nbreak down the most common authentication grants.</p><p>It‚Äôs helpful to start with the historical context of the problem that OAuth2 was created to solve\nand consider alternatives we‚Äôd have without it.</p><p>Imagine we want to build a user-friendly deployment platform like Fly.io or Vercel.\nRight away, we hit the key problem: how can our customers import their code into our platform?</p><p>These days, almost everyone uses Git.\nWe could try building a Git hosting functionality directly into the platform,\nbut that‚Äôs a huge piece of work, while our primary business goal is resource management, autoscaling, load balancing, etc.\nOn top of that, most of our customers are probably already using one of the existing popular Git hosting services like GitHub, GitLab, or Bitbucket.\nUnfortunately, we don‚Äôt have any way to convince these platforms to integrate with us.</p><p>So, what‚Äôs our options? How could we possibly get access to our customers‚Äô Git repositories hosted elsewhere?</p><p>Our customers log into their Git hosting services using their credentials.\nWhy can‚Äôt they just share their credentials to us?</p><p>We could store their credentials securely and then, when needed, log in to the Git service on their behalf,\nuse their session cookies, and fetch the required Git repositories.</p><div>Plain User Credentials Sharing (sounds great, huh?)</div><p>At first glance, this sounds like a straightforward idea to let our platform work with customer data, even when they are not around.</p><p>But then we realize, it‚Äôs riddled with problems:</p><ul><li>. The platform gets full access to everything that our customers can do without a way to limit or control that, even if we only need to access their repositories.</li><li>. It‚Äôs hard to distinguish between sessions created by the users and those initialized by the platform. If the login process is the same for both, it‚Äôs hard to implement more advanced login security measures like MFA.</li><li>. Once shared, the credentials can be cached and leaked in unexpected ways even if you removed them from the platform UI. The only way to fully revoke access is to change your password.</li><li>. If you change your password, this would effectively break the platform‚Äôs access to your data.</li><li>. The platform must store the credentials securely, which is a significant responsibility. If the shared credentials are managed in a sloppy way, they may be breached and expose the whole customer account.</li></ul><p>It‚Äôs a problem that one and the same set of credentials with broad, top level permissions are used for two vastly different purposes.\nWhat could we do about that?</p><p>Apparently, if we want to do any better, we need to keep the main credentials private.\nInstead, we could introduce an alternative type of credentials just for using in integrations.</p><p>Let‚Äôs call these Personal Access Tokens (PATs). Think of them as a static secret string with a relatively long lifespan.\nTechnically, each PAT could have a custom set of permissions assigned to it, limiting what the platform can do with the associated data.</p><p>Whenever a customer wants to integrate Git repositories with a new service, like our deployment platform, they would generate a new personal access token with the necessary permissions  and share it with the service.</p><div>Personal Access Tokens Sharing</div><p>This approach is a great improvement over sharing the plain credentials, since it addresses its major problems.\nHowever, there are still a couple of things to keep in mind:</p><ul><li>Keeping track of expiration dates and replacing stale tokens gets tedious very quickly if you need to manage more than a handful of tokens.</li><li>To minimize the management burden, token lifetime could be extended (we are talking about months or even years). Unfortunately, in that case, a token gets compromised, malicious actors will have plenty of time to exploit it.</li></ul><p>But do customers really need to manage their tokens for every service and integration they use?\nCould we simplify this further, so customers will have to do as little as possible to enable new integrations and the whole token management process is automated by the party that needs it?</p><p>That‚Äôs exactly why we need something like the OAuth framework.</p><p><a href=\"https://datatracker.ietf.org/doc/html/rfc6749\" target=\"_blank\" rel=\"noopener noreferrer\">OAuth2</a> is a framework that defines how access or permissions are requested or delegated from one\nan authoritative entity (like the user) to third-party applications.</p><p>The core idea behind OAuth2 is to give users the power to decide what applications (beyond those that are natively supported by the resource server)\ncan access their data. It ensures that the access is controlled and convenient, allowing these applications use your data whenever they need to, even when you‚Äôre not around, to extend the base functionality of the resource server.\nLet‚Äôs break it down a bit.</p><p>Without mechanisms like OAuth2, the resource server essentially controls which applications can access with your data.\nI imagine this happens through partnerships, where two companies collaborate to integrate their services into each other‚Äôs offerings (often in a very custom, non-standardized way).</p><p>This approach is centralized because:</p><ul><li>Only allowed partner‚Äôs applications are at play</li><li>Everything else is effectively blocked.</li></ul><p>OAuth2 introduces a middle ground, allowing the third-party applications to use the resource server, as long as users are willing to\ngrant permissions to their data or functionality. In this model, the resource server decides nothing for end users (unless it‚Äôs blocking malicious applications to protect users from abuse).</p><p>This creates a powerful form of decentralization that:</p><ul><li>Let resource owner extend the resource server‚Äôs functionality in a few clicks</li><li>Help to build an ecosystem of tools and applications around the resource server</li></ul><p>OAuth2 defines three main roles to organize the delegation process:</p><ul><li>. This is a service that the client application needs to access, either on user or their own behalf. For example, in our case, it‚Äôs the Git hosting provider like GitHub.</li><li> (the user if it‚Äôs a person). The entity that holds permissions to the resource server and can grant access to the client application.</li><li>. This service issues resource access tokens for the client application in exchange for various forms of authorization or grants.</li><li> (a.k.a. the client, OAuth application). An application or service that accesses the protected resource server, typically on behalf of the resource owner.</li></ul><div>OAuth2 Roles and High-Level Interactions Between Them</div><p>OAuth2 introduces the Authorization Server, acts as a middleman between\nthe Resource Owner (who has the authority) and the Client Application (that needs some of that authority).\nThe Authorization Server is trusted by the target Resource Server (which provides some functionality based on the authority).</p><p>The Resource Owner reviews the permission request and gives a consent to grant the access to the Client Application via the Authorization Server.</p><p>Depending on <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#oauth2-flows\">the authorization flow</a>, the Client Application receives an authorization grant in some form and uses it to trade for an access token (or a pair of tokens)\nfrom the Authorization Server.</p><p>Finally, the Client Application uses the access token to access the Resource Server on behalf of the Resource Owner.</p><p>The Resource Server knows how to validate <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#access-tokens\">the access tokens</a> issued by the Authorization Server, typically through an internal request to the Authorization Server.</p><p>The journey into the OAuth2 world begins with Client Applications.\nThere are two types of Client Applications, categorized by their abilities to keep secrets:</p><ul><li> like in-browser JS applications, desktop or native mobile apps. Any secrets embedded in this type of application can be reverse-engineered and extracted, even if you try to obfuscate their distributions or encrypt them.</li><li><strong>Private (or confidential) applications</strong>, which are typically any web applications with frontend and, most importantly, backend parts. The backend is capable of securely storing secrets and establishing protected communication with the Authorization Server.</li></ul><p>OAuth2 assumes there are much more Client Applications than Authorization and Resource Servers,\nso it aims to simplify the Client Application side as much as possible.\nThis not only reduces the work to do to implement a Client Application, but also limits opportunity for implementing insecure Clients.</p><p>To plug our Client Applications into the OAuth2 workflow, they first need to be registered with the Authorization Server.</p><p>The OAuth2 doesn‚Äôt make any assumptions how the registration process should work,\nbut it‚Äôs typically a part of the OAuth2 provider website‚Äôs settings e.g. functionality to create and manage OAuth apps.</p><div>The Gitlab OAuth Client Registration</div><p>The registration form usually includes:</p><ul><li>Redirect URL(s) - A list of allowed URLs for redirects in interactive authorization flows, such as the <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#authorization-code\">authorization code</a> or <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#implicit\">implicit</a> flows.</li><li><a href=\"https://www.romaglushko.com/blog/whats-aouth2/#scopes\">Scopes</a> - A list of delegated access to the Resource Server‚Äôs functionality e.g. read Git repositories, create issues, etc.</li><li>Miscellaneous information like application name, icon, privacy and terms of service URLs, etc.</li></ul><p>There are other, less popular client registration approaches. For example, I‚Äôve seen:</p><ul><li>Registration via internal admin requests to the Authorization Server like <a href=\"https://github.com/ory/hydra/tree/master\" target=\"_blank\" rel=\"noopener noreferrer\">ORY Hydra</a>.</li><li>Declarative registration by creating Kubernetes Custom Resources in the cluster using <a href=\"https://github.com/ory/hydra-maester\" target=\"_blank\" rel=\"noopener noreferrer\">ORY Hydra Maester</a>.</li></ul><p>At the end of registration, you typically receive the client credentials:</p><ul><li> (a.k.a App ID) - a public, non-secret identifier of your Client Application.</li><li> - a secret password that the Client Application keeps privately.</li></ul><p>The client credentials are used to:</p><ul><li>authenticate the Client Application requests to the Authorization Server</li><li>bind <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#oauth2-flows\">a specific authorization flow</a> to the Client Application that has started it. This ensures it‚Äôs not possible to finish that flow with completely different Client Application</li></ul><p>The client ID is tied to authorization grants and refresh tokens, so it‚Äôs essential to keep it unchanged.\nChanging it would invalidate all authorizations (e.g. <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#refresh-tokens\">refresh tokens</a>) that you‚Äôve already obtained.</p><p>On the other hand, the client secret can, and should be, rotated periodically.\nChanging the secret would have an effect of ‚Äúrotation‚Äù of all refresh tokens received by the Client Application\neven though the tokens would not be affected.\nThis is because if the client credentials were leaked along with some refresh tokens,\nmalicious actors would not be able to obtain new access tokens using the old client secret after the client secret rotation.</p><p>This significantly simplifies the process of secret rotation as you need to rotate only one secret\ninstead of rotating thousands of refresh tokens for each end user that has ever authorized your Client Application.</p><p>In practice, you may want to also have a bunch of others that are not defined in OAuth2 directly:</p><ul><li><strong>Access token introspection endpoint</strong> (it has its own <a href=\"https://www.rfc-editor.org/rfc/rfc7662.html\" target=\"_blank\" rel=\"noopener noreferrer\">RFC</a>) that returns metadata information associated with the given access token. It can be used by resource servers to validate incoming access tokens.</li><li><strong>Authorization grant revocation endpoint</strong> that allows it to revoke the whole authorization grant.</li><li><strong>Token revocation endpoint</strong> that allows to revoke the issued access and refresh tokens.</li><li>and a bulk of other endpoints that were introduced in the all follow-up RFCs and drafts if you need that.</li></ul><p>Historically, the Authorization Server OAuth2 endpoints were not fixed nor was there a way to discover them.\nThe endpoints were extracted by the provider documentation and hardcoded in the OAuth2 libraries or Client Application\n(here is <a href=\"https://github.com/markbates/goth/blob/master/providers/github/github.go#L29-L34\" target=\"_blank\" rel=\"noopener noreferrer\">an example</a> from the goth library).</p><p>The Authorization Server is represented as a separate component conceptually, but the protocol has no requirements on how it should be implemented under the hood.\nIt could be either a separate microservice, or it can be a part of the Resource Server.</p><p>One important assumption that OAuth2 protocol makes implicitly is that one authorization server can potentially handle authorizations for multiple Resource Servers.\nThis means that among all OAuth2 components, the Authorization Servers are the rarest to implement.\nThat‚Äôs why they are responsible for handling <a href=\"https://datatracker.ietf.org/doc/html/rfc6819\" target=\"_blank\" rel=\"noopener noreferrer\">a lot of security nuances</a> around the Authorization Server implementations.\nYour OAuth2 is essentially as secure as your Authorization Server.</p><p>The Authorization Server generates access tokens as a result of the successful authorization flow.</p><p>The access tokens are a special credential that serves as an alternative method of authentication for the Resource Server.\nThey can be also seen as an abstraction around the exact authorization flow.\nThere could be multiple authorization flows supported by the Authorization Server, but they all will result in access tokens that have the same format.\nThis makes them easier to validate for the Resource Server that doesn‚Äôt need to know too much information about how the specific token was obtained.</p><div>Access Tokens unifies the authorization flows</div><p>The concept of access token is also important because we can generate multiple access tokens with different reduced subset of <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#scopes\">the originally requested scopes</a>.\nIf there was no access tokens as a separate credential and we were using <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#authorization-code\">the authorization code</a>,\nlet‚Äôs say, for that purpose, it would have all permission scopes requested by the client application at the point of passing authorization flow.</p><p>OAuth2 doesn‚Äôt define how the access tokens should look like.\nThey are opaque strings to the Client Applications and likely Resource Servers too.</p><p>Apart from that, when an access token is generated, the Authorization Server indicates what type of token was issued.</p><p>In the wild, Authorization Servers may issue bearer tokens as:</p><ul><li>a unique random string. The string should be non-guessable and not possible to generate outside the Authorization Server.</li><li>or as a self-contained JWT token that includes the signed meta information.</li></ul><p>Other types of tokens are theoretically possible, but I have never seen them in the wild.</p><p>OAuth2 requires Authorization Servers to generate access tokens only.\nIf so, the generated token is considered as a long-lived and that‚Äôs not great for two reasons:</p><ul><li>The access tokens are linked to the client application, but they are usually passed to the resource server without any additional proof of token possession. Hence, if they are leaked, the malicious actors would have enough time to exploit them.</li><li>The access token is linked to the original access scopes and there is no way to generate a new access token with a subset of scopes without going through the whole authorization flow again.</li></ul><p>To address these concerns, it‚Äôs the best practice to keep access tokens short-lived.\nAlong with that, you can generate a separate, long-lived token that generates you fresh access tokens as needed.\nThis type of token is called <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#refresh-tokens\">refresh token</a>.</p><p>Authorization scopes are a set of functionalities that the Resource Owner delegates to the Client Application,\nallowing the Client to access resources as thought it were the original owner.</p><p>The scopes are simply a space-separated list of strings, where each string specifies a particular access type.\nThe scope format is not defined in the OAuth2 protocol, but they are normally structured like this: <code>{resource}_{access level}</code>.</p><ul><li> may allow the Client to read the current user (e.g. Resource Owner) profile information</li><li> may allow the Client Application to commit to the repositories to which the Resource Owner has access to.</li></ul><p>As you can see, the scopes are fairly coarse-grained, they don‚Äôt grant access to specific resources,\nbut rather work on the resource types and access levels (e.g. read/write/admin).</p><p>Scopes are additive, meaning when multiple scopes are requested, they are combined to broaden the Client Application‚Äôs or access token‚Äôs permissions.</p><p>Authorization flows, also known as authorization grants, are how permissions are delegated to the Client Applications.\nRegardless of the flow you use, the end result is <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#access-tokens\">a set of access tokens</a> that enable the Client Application to directly access the Resource Server.</p><p>The main differences between flows are:</p><ul><li>whenever it‚Äôs interactive or non</li><li>the number of participants involved (2- or 3-leg flows)</li></ul><p>We‚Äôll start by reviewing the most canonical and secure OAuth2 flow called the authorization code flow.</p><p>This flow is interactive and works for Client Applications that can keep secrets and perform browser redirects, typical for web services with a backend.</p><p>The flow consists of two stages:</p><div>OAuth2 Authorization Code Flow</div><p>The whole authorization code flow can be divided into two main parts:</p><ul><li>The interactions that happen indirectly between the Authorization Server and the Client Application using the browser as a mediator. These actions are performed via the frontend channel and can be potentially intercepted or manipulated along the way (e.g. a malicious browser extension may try to sniff the code parameters).</li><li>The interactions occur directly between Authorization Server and the client via trusted backend channel.</li></ul><p>The authorization code flow is designed so that it‚Äôs not possible to get access delegation by using only information transmitted via the frontend channel.</p><p>In order to start the OAuth2 flow, the client application needs to request the authorization with the needed scopes from the Resource Owner.\nThis happens by redirecting the resource owner to the authorization server‚Äôs  endpoint.</p><p>The authorization URL usually contains the following URL parameters:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>The  is what defines what kind of interactive flow we are going to perform. It‚Äôs always  for the authorization code flow (or  for <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#implicit\">the implicit flow</a>).</li><li>The  is required as the authorization code is strictly assigned to the client application that has initialized the flow (to prohibit finishing the flow from another Client Application).</li><li>The  is the URL of the client application callback page where the authorization code will be passed after\nthe authorization consent. This URL must be specified in <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#client-registration\">the client registration settings</a>.</li></ul><p>The Resource Owner browser should already have a user session (e.g. session cookie) with the Authorization Server (or login otherwise),\nso the redirect can leverage that to seamlessly show the authorization consent screen.</p><div>The Example of the Authorization Consent Screen</div><p>Let‚Äôs note that the client application communicates with the Authorization Server indirectly via HTTP redirects\nand the Resource Owner browser. This way the Client Application doesn‚Äôt have to know about the Resource Owner credentials or session\nwhich is itself the key problem the OAuth2 protocol was born to solve.</p><p>Because of that, the authorization consent page should not have any client-specific CORS configuration.\nThis remains true for all OAuth2 flows.</p><p>Once the Resource Owner approves the delegation of access to the Client Application, the Authorization Server redirects\nthe Resource Owner back to the Client Application callback URL specified during the authorization request.</p><p>The client callback redirect looks like this:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>the  parameter is called the authorization code (it gives this flow its name).</li><li>the  is returned back if it was specified originally to let the Client Application verify the integrity of the flow.</li></ul><p>The authorization code is a one-time-use token that represents the specific Resource Owner‚Äôs consent to give to the specific Client Application.\nIt is tied to the client ID that has obtained it, so it‚Äôs not possible to exchange it from another Client Application.\nSo even if the code was leaked somehow, you would need to have valid client credentials to turn it into access tokens.</p><p>Finally, to finish the flow, we need to exchange the authorization code for the access tokens.\nThis is done via the OAuth2 token endpoint:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>The  defines what kind of flow or grant we want to use to trade for access tokens. It‚Äôs a universal endpoint used in the other flows too, but in the case of this flow, it‚Äôs always going to be .</li><li>The  is mandatory to provide in the authorization code exchange.</li></ul><p>In response, if everything went fine, you would get response like this:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"json\" data-theme=\"dark\"><code data-language=\"json\" data-theme=\"dark\"></code></pre><pre data-language=\"json\" data-theme=\"light\"><code data-language=\"json\" data-theme=\"light\"></code></pre></div><p>That‚Äôs all. Now you need to persist the access and refresh tokens and use them to access the Resource Server.</p><p>The analysis of real-world attacks on the authorization flows has shown that it can be further secured.\nSpecifically, malicious actors can intercept the authorization code or try to inject it into the callback URL to do token exchange via unauthorized workflows.\nThese attack vectors are the most probable in public applications like native applications.</p><p>PKCE is a simple way to prove that the authorization code was obtained via the legitimate authorization request.\nThe beauty of PKCE is that it just slightly extends the authorization &amp; token requests without major changes to the flow.</p><div>OAuth2 Authorize Code with PKCE</div><ul><li>The Client Applications generate a random string called the  and then hash it with a cryptographically secure algorithm as SHA256. The hashed value is called the .</li><li>The Client Application keeps the original  privately and shares the  and the hash code (e.g. ) as query params in the authorization request.</li><li>The Authorization Server remembers the  and the . No other changes are needed to the existing Authorization Server responses.</li><li>Then, the client sends the  during token exchange. The Authorization Server computes the hash of that value and compares it with the  passed during the authorization request.</li></ul><p>PKCE supports two hashing methods:</p><ul><li> - the SHA256 hashing algorithm</li></ul><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"plain\" data-theme=\"dark\"><code data-language=\"plain\" data-theme=\"dark\"></code></pre><pre data-language=\"plain\" data-theme=\"light\"><code data-language=\"plain\" data-theme=\"light\"></code></pre></div><ul><li> - the plain text method. It‚Äôs basically just <code>code_challenge = code_verifier</code>. The  method should be avoided as it doesn‚Äôt really introduce any challenges. It can only protect you from attacks where nefarious actors can intercept the Authorization Server responses.</li></ul><p>The PKCE extension allows the public clients to finally leverage the authorization code flow securely.\nHowever, the Authorization Server must be ready to support PKCE for public clients which boils down to not requiring these clients to provide any client secrets.</p><p>The refresh token is an optional but highly recommended additional token that <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#code-exchange\">the OAuth2 token endpoint</a> can return to you.\nUnlike <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#access-tokens\">the access token</a>, the refresh token is meant to be a long-lived token (either no expiration time or an extended period of time like half a year)\nthat is sent to the authorization server only.</p><p>Essentially, the refresh token is an ‚Äúinternal‚Äù authorization grant because it implies the authorization that the resource owner has given to the Client Application.</p><p>The refresh token is important for two reasons:</p><ul><li>it allows to keep access tokens short-lived, so minimize the attack surface if they are leaked</li><li>it allows to the generation of access tokens with the reduced access scope that is more limited than the scopes granted to the Client Application during authorization. This enables the clients to implement the least privilege principle on their side.</li></ul><div>OAuth2 Refresh Token Flow</div><p>In order to refresh your access token, you send a request to the OAuth2 token endpoint with the  set to :</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><p>The refresh token is linked to the specific client credentials, so it‚Äôs not possible to leverage it with an unauthorized client.</p><p>The refresh token request generally returns the same response as we have seen in the <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#code-exchange\">authorization code exchange</a>.\nIt contains the new active access token, its expiration time and the actual access scopes.\nIn some cases (GitHub and GitLab do this, for instance), the refresh token request may actually also refresh your previous refresh token, so if the token response\ncontains the  field and it‚Äôs different from your current refresh token, it means that this is your new refresh token to persist and use going forward.</p><p>The refresh token request generally invalidates all previous access tokens (and refresh tokens).</p><p>We have said that <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#authorization-code\">the authorization code flow</a> is designed to make it impossible to\nget Resource Owner delegation by using only information passed via the frontend channel (e.g. the authorization code and client ID).\nIn order to achieve this, that flow requires the Client Application to have a secure backend channel.\nBut what if the application is public and doesn‚Äôt have a place to put a secret, so it remains a secret?</p><p>The original OAuth2 specification introduced a simplified version of the authorization code flow\nthat makes a significant security trade-off in order to support public applications, first of all, in-browser JS applications\nlike browser extensions or single page applications (SPAs) without backends. It‚Äôs called the implicit flow.</p><p>The implicit flow is also an interactive, redirect-based flow, but there is no explicit code exchange via the backend channel.\nInstead, it happens implicitly and the Client Application just receives the access token in the callback URL.</p><p>The authorization request looks close to what we have seen in the authorization code but\nthis time we have to specify  as :</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><p>In JS applications, there are a few ways you can do this request:</p><ul><li>Do a full-page redirect to the Authorization Server</li><li>Open a separate popup window and do the redirect there and then close it when the callback URL is hit.</li></ul><p>If you specify the authorization  parameter, the best place to temporarily persist it will be <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage\" target=\"_blank\" rel=\"noopener noreferrer\">window.sessionStorage</a>.</p><p>Once the Resource Owner approves the delegation, the Authorization Server redirects them back to the client callback URL which would look like this:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><p>The  is returned right away in the callback URL along with other parameters. This is a simple GET request,\nso the sensitive access token is a part of the URL and can be potentially intercepted by other browser extensions, malicious scripts injected via XSS attacks, etc.\nAdditionally, the whole callback URL is cached in the browser history along with the access token.\nThat‚Äôs the main reason why the implicit flow is considered insecure.</p><p>All parameters are returned as URL fragments which means they are intended to be used by browser client applications only (e.g. not shared with any backend servers).</p><p>Since in-browser applications cannot keep secrets, the returned access token is super short-lived (like 1-2 hours).\nFor the same reason, OAuth2 requires no refresh tokens in the implicit flow.</p><p>Finally, the Client Application can use the retrieved access token to access the Resource Server in the same way we have seen in the authorization code flow.\nThere is one specific though. The Resource Server should be ready to accept these in-browser application requests\nby having CORS policies configured.</p><p>Looking back, there are basically two pieces of information that help to identify validity of the client in the implicit flow:</p><p>There is no client secret or any other sensitive information to put into the public client application.</p><p>Let‚Äôs continue our what-if thought process. What if there is no resource owner and the Client Application wants to act on its own behalf?</p><p>This is a common situation when you have a dozen of internal services that have to communicate with each other and you want to secure that communication somehow\nto create a zero-trust environment.</p><p>In this case, there is no Resource Owner involved, so there is no need for the whole frontend channel to be involved.\nAll we need is to make the Authorization Server accept the client credentials as a valid reason to issue the access tokens.\nTherefore, this flow is called the client credentials flow.</p><div>OAuth2 Client Credentials Flow</div><p>The client credentials flow is a non-interactive flow that enables confidential trusted Client Applications\nto access the Resource Server (or other internal Client Application).\nSo the only request we need here is to the token endpoint:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>The  is set to  to indicate that validity of the client credentials is the reason to give us an access token.</li><li>The  are optional but recommended to achieve the least privileged access.</li></ul><p>That‚Äôs it. The response is the same as in other flows.\nThere is no big reason to issue refresh tokens here, because the client credentials act as one, so it‚Äôs generally omitted.</p><p>Another thing is the access scope. Since the Client Application acts on its own behalf,\nit may not be limited to the resources available to a specific Resource Owner.\nThere has to be a way for the Resource Server to differentiate this level of access versus regular resource owner delegation.\nI have seen two ways of doing this:</p><ul><li>use a separate set of scopes to mark such an internal, wide access</li><li>add a custom claim to the JWT access token and account for it during access token validation</li></ul><h3>Resource Owner Credentials</h3><p>The most paradoxical flow out of all OAuth2 standard flows is the resource owner credentials (ROC) flow.\nIt‚Äôs paradoxical because it was discouraged from use since day one of the OAuth2 protocol, everyone says it‚Äôs a very bad idea to use it, yet still it made it into the specification.\nWhy did that happen?</p><p>Theoretically, there might be situations where you absolutely have to use your username and password all around to access some resources.\nWithout the ROC flow, you would be even less secure than if you used it.\nThis is because the flow limits the credential exposure over the network which reduces the chance of credential leakage. Also, it allows you to limit the access scope (rather than giving the client absolutely all access you have).</p><p>In this flow, the Resource Owner passes their credentials (e.g. username and password) directly to the Client Application.\nThen the application uses the credentials as an authorization grant to issue a pair of access and refresh tokens.\nThe resource credentials are then discarded and the client uses the tokens solely to access the protected resources going forward.</p><div>OAuth2 Resource Owner Credentials Flow</div><p>This is a backend channel only flow, so the Client Application exclusively communicates with the token endpoint of the\nAuthorization Server:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>The  has to be  to indicate the ROC flow</li><li>The credentials e.g. username and password are passed as a part of the token request</li><li>it‚Äôs possible to pass the  param to reduce the delegated access level. Otherwise, it would be the full access that the Resource Owner has (whatever that means for the given Resource Server).</li></ul><p>In which cases this flow could make some sense?</p><ul><li>You should have a high degree of trust to share your main credentials with the Client Application. Ideally, it should be something you control (e.g. the first-party client).</li><li>Your Client Application is highly privileged. For example, it does some actions on behalf of your tenant or organization admins. This is <a href=\"https://learn.microsoft.com/en-us/entra/identity-platform/v2-oauth-ropc\" target=\"_blank\" rel=\"noopener noreferrer\">how Microsoft Entra supports it</a>. At the same time, personal accounts could not use this flow (e.g. partially because there are other protections in order to login like MFA).</li></ul><p>Apparently, the primary target of the original OAuth2 specification was the browser application use case,\nbut after OAuth2 gained popularity, it has found its way into other contexts. For example, not every environment has an ability to open a browser and do the redirect-based flows\nlike authorization code. A few examples:</p><ul><li>When you have got a new TV and you want to watch Netflix on it, you need to authorize that device to access your account and subscription.</li><li>When you want to analyze your Snowflake data in a cloud-hosted, containerized Jupyter notebook, there might be no easy way to open a browser (it‚Äôs a headless linux under the hood).</li><li>When you try to connect to your game portal from a console that may have a browser, but only limited input capabilities (e.g. no full-fledged keyboard)</li></ul><p>Thankfully, there is an extension to the original OAuth2 specification that codifies so-called the device authorization flow.</p><p>The device authorization (or device code) flow is a special kind of interactive flow that doesn‚Äôt assume any direct interactions between the Client Application residing on the device\nand the Resource Owner‚Äôs browser.</p><p>Instead, the Client Application instructs how the resource owner can authorize it via browser indirectly by showing the verification URL to visit, QR code to scan or just a call to open the provider‚Äôs mobile application.</p><h4>Device Authorization Request</h4><p>In order to implement the device authorization flow, they introduced a new endpoint for kicking off the flow called the device authorization endpoint (because it has a completely different semantic than the standard, browser-based  endpoint):</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>The  is required to identify the Client Application.</li><li>There is no client secret because the device client is close to the public clients in terms of the ability to keep secrets e.g. any built-in secrets can be extracted.</li></ul><p>The device authorization endpoint returns something like this:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"json\" data-theme=\"dark\"><code data-language=\"json\" data-theme=\"dark\"></code></pre><pre data-language=\"json\" data-theme=\"light\"><code data-language=\"json\" data-theme=\"light\"></code></pre></div><ul><li>The  is where the end user should go to type in the . The URL should be short enough to type in manually. Alternatively, the Authorization Server may give another URL to transform into a QR code, for example. That URL generally contains the user code as a query param.</li><li>The  is what the device client application keeps secretly in memory and then uses as a grant during polling the token endpoint.</li></ul><p>The device code serves as a proof of starting the authorization flow. If there was no device code and the device client had only client ID as the client identifier,\nattackers may figure out that ID and then try to send the token requests to get the access &amp; refresh tokens before the real device that has requested it.</p><p>The resource owner has to trigger (or retrigger if the previous request has timed out) the authorization flow, but at the same time,\nwe pass no information about that user during initializing the authorization request. The authorization server can only match the resource owner with the corresponding client ID after typing in the user code on the verification page.\nTo be fair, we pass no Resource Owner identifier directly in other interactive flows, too,\nbut the authorization redirect leverages browser cookies there, so the Authorization Server can identify the end user right off the bat.</p><p>Then, the user code is shown somehow to the Resource Owner.\nGenerally, it‚Äôs just printed on the device screen, so the user can type it from there.</p><p>The device authorization is a time-bound process (the lifetime is specified as  field in the response).\nThe authorization lifetime is typically around 15 minutes.</p><p>Because there is no way for the Authorization Server to tell the device client when the authorization is granted (that role is played by the callback URL in the other interactive flows)\nand it‚Äôs a big assumption that the device can accept inbound requests, the protocol only assumes that the device is connected to the internet and can do outbound requests.</p><p>With these assumptions, the device can poll the token endpoint every so often until the authorization is granted, the authorization request is expired or denied.\nThe default polling interval is 5 seconds.</p><p>The polling happens against the token endpoint:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><ul><li>The  usually has to indicate what flow we are trying to complete. In this case, the flow code is unusual which means that the flow name is <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#more-grants\">not standard (or custom)</a>.</li><li>The  is also sent to verify the device that is trying to obtain tokens.</li></ul><p>The specification doesn‚Äôt require client authentication when accessing the token endpoint, but it‚Äôs possible and some providers\nuse that (e.g. <a href=\"https://developers.google.com/identity/protocols/oauth2/limited-input-device\" target=\"_blank\" rel=\"noopener noreferrer\">Google‚Äôs Device Authorization flow implementation</a>).\nIn that case, it‚Äôs still true that you cannot persist the client secret on the end device\nand should probably have a backend service somewhere to poll the token endpoint for the device.</p><p>It‚Äôs very likely that the device client would need to poll the token endpoint a couple of times before the end user actually authorizes it.\nIn this case, the token endpoint should return a special error indicating that the authorization is not yet granted:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"json\" data-theme=\"dark\"><code data-language=\"json\" data-theme=\"dark\"></code></pre><pre data-language=\"json\" data-theme=\"light\"><code data-language=\"json\" data-theme=\"light\"></code></pre></div><p>If the client polls it too eagerly, another special error is returned that says to expand the polling interval by 5 seconds:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"json\" data-theme=\"dark\"><code data-language=\"json\" data-theme=\"dark\"></code></pre><pre data-language=\"json\" data-theme=\"light\"><code data-language=\"json\" data-theme=\"light\"></code></pre></div><p>When the authorization is finally granted, the token endpoint should return the regular token response we have seen in <a href=\"https://www.romaglushko.com/blog/whats-aouth2/#code-exchange\">the authorization code flow</a>.</p><p>What else can we trade for access &amp; refresh tokens? The OAuth2 specification defines a way to extend the standard grant types\nwith a custom one. This is called an extension or third-party assertion grant.</p><p>The assertion grant is a backend channel only flow where the Client Application sends the Authorization Server\na special third-party assertion that proves the client‚Äôs rights to access the protected Resource Server.</p><p>As with any other backend channel only flow, this one only uses the token endpoint:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"http\" data-theme=\"dark\"><code data-language=\"http\" data-theme=\"dark\"></code></pre><pre data-language=\"http\" data-theme=\"light\"><code data-language=\"http\" data-theme=\"light\"></code></pre></div><p>The grant type in this case is a unique string in a form of URN that includes the organization name and other grant type information. For example:</p><div data-rehype-pretty-code-fragment=\"\"><pre data-language=\"plain\" data-theme=\"dark\"><code data-language=\"plain\" data-theme=\"dark\"></code></pre><pre data-language=\"plain\" data-theme=\"light\"><code data-language=\"plain\" data-theme=\"light\"></code></pre></div><p>It‚Äôs only important that the target Authorization Server recognizes it and knows how to validate it.</p><p>The assertion is usually a self-contained secure token that is cryptographically signed by the assertion provider.\nPractically, there are two types of assertions you can see in the wild:</p><p>The client authentication may be optional in this case (if so, the refresh token may not be issued as that grant requires client authentication).</p><p>After we have reviewed all main OAuth2 flows, which one should you choose for your specific application?</p><p>I have tried to come up with the following decision tree that asks the main questions to help you.</p><ul><li>Always try to use the authorization code flow with PKCE if possible, no matter if it‚Äôs a public or confidential client application. This may not be possible because your provider may not support it yet.</li><li>If PKCE is not supported, then the authorization code is only good for private clients unless the dynamic client registration is supported. For public clients, you should go with the implicit flow and dive into the number of recommendations and considerations to implement as securely as possible.</li><li>If your client application cannot open a browser with the resource owner session or is limited in terms of input capabilities, and your users don‚Äôt really trust it, then go with the device code flow.</li><li>Before falling back to the resource owner credentials flow, try to see if API keys can help you achieve the same goal.</li></ul><p>Thinking about why OAuth2 protocol has been designed the way it is, turned out to be a great exercise\nin threat modeling with immediate, straightforward and practical approaches to mitigate these threats.\nThey can be reused to solve similar security concerns in other contexts outside of OAuth protocol,\nso you can benefit from a deep understanding of the protocol even if you are not a security expert who has to know the ins and outs of OAuth2.</p><p>Apart from that, OAuth2 is such a vast area that we have been able to only answer the fundamental why questions and\nreview the most popular delegation grants in this article.</p><p>A lot of interesting OAuth2 extensions are just briefly referenced, but you would not see them that often in the wild yet,\nso that was acceptable to leave them out for now.</p><p>If you would like to see follow-up articles on OAuth2 protocol and its extensions, please let me know.</p>","contentLength":39696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iaazm7/whats_oauth2_anyway/"}],"tags":["dev"]}