{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Why OOP & FP are the Two Main Paradigms","url":"https://www.youtube.com/watch?v=l_3AGwVwP_k","date":1739624173,"author":"/u/OkMemeTranslator","guid":594,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq0rlf/why_oop_fp_are_the_two_main_paradigms/"},{"title":"Lessons from David Lynch: A Software Developer's Perspective","url":"https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/","date":1739612430,"author":"/u/aijan1","guid":597,"unread":true,"content":"<p>David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He’s perhaps best known for the groundbreaking TV series <a href=\"https://en.wikipedia.org/wiki/Twin_Peaks\">Twin Peaks</a>, which inspired countless shows, including The X-Files, The Sopranos, and Lost.</p><p>Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down – even those who truly deserved it.</p><p>Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that <a href=\"https://en.wikipedia.org/wiki/Mulholland_Drive_(film)\">Mulholland Drive</a> remained compulsively watchable while refusing to yield to interpretation.</p><p>While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I’d like to share my perspective on his life lessons from a software developer’s viewpoint.</p><blockquote><p>Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.</p></blockquote><p>We’ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one –because they’re so rare– write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it’s a film, a painting, or software.</p><blockquote><p>The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.</p></blockquote><p>Software development is part art, part engineering. We don’t build the same software over and over again – virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it’s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.</p><p>It’s a good habit to listen to what users have to say, but they often can only describe their problems – they rarely come up with good ideas to solve them. And that’s OK. It’s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.</p><blockquote><p>My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.</p></blockquote><p>Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  – that magical state of mind where we lose track of time and produce code effortlessly. That’s why many developers hate meetings – they are toxic to our productivity.</p><blockquote><p>I believe you need technical knowledge. And also, it’s really, really great to learn by doing. So, you should make a film.</p></blockquote><p>Software development is one of those rare fields where a college degree isn’t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.</p><p>The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It’s crucial to never stop learning, experimenting, and iterating on our craft.</p><blockquote><p>Happy accidents are real gifts, and they can open the door to a future that didn’t even exist.</p></blockquote><p>Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.</p><p>Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.</p><blockquote><p>I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.</p></blockquote><p>Be kind to your teammates, don’t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety –that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by <a href=\"https://rework.withgoogle.com/en/guides/understanding-team-effectiveness\">Google’s research</a> on the subject.</p><p>It’s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.</p><blockquote><p>Most of Hollywood is about making money - and I love money, but I don’t make the films thinking about money.</p></blockquote><p>Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.</p><p>What makes these projects remarkable is that they didn’t emerge from corporate boardrooms – they were built by communities of passionate developers, collaborating across the world.</p><p>Money is just a means to an end. Unfortunately, many get this confused.</p><p>David, thank you for making the world a better place!</p>","contentLength":5845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipy01t/lessons_from_david_lynch_a_software_developers/"},{"title":"Kafka Delay Queue: When Messages Need a Nap Before They Work","url":"https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need","date":1739596108,"author":"/u/Sushant098123","guid":596,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipu9n3/kafka_delay_queue_when_messages_need_a_nap_before/"},{"title":"Modern Java Deep Dive","url":"https://www.youtube.com/watch?v=z4qsidg261E","date":1739591964,"author":"/u/BlueGoliath","guid":593,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipt4pe/modern_java_deep_dive/"},{"title":"Incremental Archival from Postgres to Parquet for Analytics","url":"https://www.crunchydata.com/blog/incremental-archival-from-postgres-to-parquet-for-analytics","date":1739554943,"author":"/u/gtobbe","guid":592,"unread":true,"content":"<p>PostgreSQL is commonly used to store event data coming from various kinds of devices. The data often arrives as individual events or small batches, which requires an operational database to capture. Features like <a href=\"https://www.crunchydata.com/blog/native-partitioning-with-postgres\">time partitioning</a> help optimize the storage layout for time range filtering and efficient deletion of old data.</p><p>The PostgreSQL feature set gives you a lot of flexibility for handling a variety of IoT scenarios, but there are certain scenarios for it is less suitable, namely:</p><ul><li>Long-term archival of historical data</li><li>Fast, interactive analytics on the source data</li></ul><p>Ideally, data would get automatically archived in cheap storage, in a format optimized for large analytical queries.</p><p>We developed two open source Postgres extensions that help you do that:</p><ul><li><a href=\"https://github.com/CrunchyData/pg_parquet\">pg_parquet</a> can export (and import) query results to the <a href=\"https://parquet.apache.org/\">Parquet</a> file format in object storage using regular COPY commands</li><li><a href=\"https://github.com/crunchydata/pg_incremental\">pg_incremental</a> can run a command for a never-ending series of time intervals or files, built on top of <a href=\"https://github.com/citusdata/pg_cron\">pg_cron</a></li></ul><p>With some simple commands, you can set up a reliable, fully automated pipeline to export time ranges to the columnar Parquet format in S3.</p><p>Then, you can use a variety of analytics tools to query or import the data. My favorite is of course <a href=\"https://www.crunchydata.com/blog/crunchy-data-warehouse-postgres-with-iceberg-for-high-performance-analytics\">Crunchy Data Warehouse</a>.</p><p>On any PostgreSQL server that has the pg_parquet and pg_incremental extensions, you can set up a pipeline that periodically exports data to in S3 in two steps.</p><p>The pg_incremental extension has a create_time_interval_pipeline function that will run a given command once the time interval has passed, with 2 timestamp parameters set to the start and end of the hour. We cannot directly use query parameters in a COPY command, but we can define a simple PL/pgSQL function that generates and executes a custom COPY command using the parameters.</p><pre><code>-- existing raw data table\ncreate table events (\n  event_id bigint not null generated by default as identity,\n  event_time timestamptz not null default now(),\n  device_id bigint not null,\n  sensor_1 double precision\n);\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n-- define an export function that wraps a COPY command\ncreate or replace function export_events(start_time timestamptz, end_time timestamptz)\nreturns void language plpgsql as $function$\nbegin\n  execute format(\n    $$\n      copy (select * from events where event_time &gt;= %L and event_time &lt; %L)\n      to 's3://mybucket/events/%s.parquet' with (format 'parquet');\n    $$,\n    start_time, end_time, to_char(start_time, 'YYYY-MM-DD-HH')\n  );\nend;\n$function$;\n\n-- export events hourly from the start of the year, and keep exporting in the future\nselect incremental.create_time_interval_pipeline('event-export',\n  time_interval := '1 hour',                      /* export data by the hour                */\n  batched := false,                               /* process 1 hour at a time               */\n  start_time := '2025-01-01',                     /* backfill from the start of the year    */\n  source_table_name := 'events',                  /* wait for writes on events to finish    */\n  command := $$ select export_events($1, $2) $$   /* run export_events with start/end times */\n);\n\n</code></pre><p>By running these commands, Postgres will export all the data from the start of the year into hourly Parquet files in S3, and will keep doing so after every hour and automatically retry on failure.</p><p>To use pg_parquet Crunchy Bridge, you can add your S3 credentials for pg_parquet to your Postgres server via the dashboard under Settings -&gt; Data lake.</p><p>Once data is in Parquet, you can use a variety of tools and approaches to query the data. If you want to keep using Postgres, you can use <a href=\"https://www.crunchydata.com/products/warehouse\">Crunchy Data Warehouse</a> which has two different ways of working with Parquet data.</p><p>The simplest way to start querying Parquet files in S3 in Crunchy Data Warehouse is to use a lake analytics table. You can easily create a table for all Parquet files that match a wildcard pattern:</p><pre><code>create foreign table events_parquet ()\nserver crunchy_lake_analytics\noptions (path 's3://mybucket/events/*.parquet');\n</code></pre><p>You can then immediately query the data and the files get cached in the background, so queries will quickly get faster.</p><p>A downside of querying Parquet directly is that eventually we will have a lot of hourly files that match the pattern, and there will be some overhead from listing them for each query (listing is not cached). We also cannot easily change the schema later.</p><p>A more flexible approach is to import the Parquet files into an Iceberg table. Iceberg tables are also backed by Parquet in S3, but the files are compacted and optimized, and the table supports transactions and schema changes.</p><p>You can create an Iceberg table that has the same schema as a set of Parquet files using the definition_from option. You could also load the data using load_from, but we’ll do that separately.</p><pre><code>create table events_iceberg () using iceberg\nwith (definition_from = 's3://mybucket/events/*.parquet');\n</code></pre><p>Now we need a way to import all existing Parquet files and also import new files that show up in S3 into Iceberg. This is another job for pg_incremental. Following a similar approach as before, we create a function to generate a COPY command using a parameter.</p><pre><code>-- define an import function that wraps a COPY command to import from a URL\ncreate function import_events(path text)\nreturns void language plpgsql as $function$\nbegin\n  execute format($$copy events_iceberg from %L$$, path);\nend;\n$function$;\n\n-- create a pipeline to import new files into a table, one by one.\n-- $1 will be set to the path of a new file\nselect incremental.create_file_list_pipeline('event-import',\n   file_pattern := 's3://mybucket/events/*.parquet',\n   list_function := 'crunchy_lake.list_files',\n   command := $$ select import_events($1) $$,\n);\n\n-- optional: do compaction immediately\nvacuum events_iceberg;\n\n</code></pre><p>After running these commands, your data will be continuously archived from your source Postgres server into Iceberg in S3. You can then run fast analytical queries directly from Crunchy Data Warehouse, which uses a combination of parallel, vectorized query processing and file caching to speed up queries. You can additionally set up (materialized) views and assign read permissions to the relevant users.</p><p>No complex ETL pipelines required.</p><p>To give you a sense of the performance benefit of using Parquet, we loaded 100M rows into the source table, which got automatically mirrored in Parquet and Iceberg via our pipelines. We then ran a simple analytical query on each table:</p><pre><code>select device_id, avg(sensor_1) from events group by 1;\n</code></pre><p>The runtimes in milliseconds are shown in the following chart:</p><p>In this case the source server is a standard-16 instance (4 vcpus) on Crunchy Bridge, and the warehouse is a warehouse-standard-16 instance (4 vcpus). So, using Crunchy Data Warehouse we can analyze 100M rows in well under a second on a small machine, and get &gt;10x speedup with Iceberg.</p><p>The use of compression also means the size went from 8.9GB in PostgreSQL to 1.2GB in Iceberg using object storage.</p><p>With pg_parquet and pg_incremental, you can incrementally export data from PostgreSQL into Parquet in S3, and with Crunchy Data Warehouse you can process and analyze that data very quickly while still using PostgreSQL.</p><p>One of the nice characteristics of the approach described in this blog is that the pipelines are fully transactional. It means that every import or export step either fully succeeds or fails and then it will be retried until it does succeed. That’s how we can create production-ready pipelines with a few simple SQL commands.</p><p>Under the covers, pg_incremental keeps track of which time ranges or files have been processed. The bookkeeping happens in the same transaction as the COPY commands. So if a command fails because of an ephemeral S3 issue, the data will not end up being ingested twice or go missing. Having transactions takes away a huge amount of complexity for building reliable pipelines. There can of course be other reasons for pipeline failures that cannot be resolved through retries (e.g. changing data format), so it is still important to <a href=\"https://github.com/crunchydata/pg_incremental#monitoring-pipelines\">monitor</a> your pipelines.</p>","contentLength":8175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipg7oy/incremental_archival_from_postgres_to_parquet_for/"},{"title":"Siren Call of SQLite on the Server","url":"https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/","date":1739551579,"author":"/u/sausagefeet","guid":595,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipevoh/siren_call_of_sqlite_on_the_server/"}],"tags":["dev","reddit"]}