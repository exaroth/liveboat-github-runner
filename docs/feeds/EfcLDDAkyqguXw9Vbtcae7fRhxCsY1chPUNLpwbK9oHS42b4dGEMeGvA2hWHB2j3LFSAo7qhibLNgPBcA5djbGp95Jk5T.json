{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Dart is not just for Flutter, it's time we start using it on the server. I built wailuku an open source web framework inspired by express.js to help those who want to transtition from js to dart.","url":"https://github.com/aminedakhlii/wailuku","date":1745184383,"author":"/u/PaleContribution6199","guid":546,"unread":true,"content":"<p>why use dart on the server ? </p><p>1- unified language for full stack as Flutter now supports almost all platforms + web 2- compiled language</p><p>3- null safety and type safe</p><p>4- a strong community with a variety of packages that server almost every scenario</p><p>I think it's time dart gets more recognition on the server, so I built wailuku, a lightweight backend framework that emulates express.js syntax. I'd be super helpful if I can get some feedback, suggestions and contributions. </p>","contentLength":470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k3wymf/dart_is_not_just_for_flutter_its_time_we_start/"},{"title":"TensorFlow implementation for optimizers","url":"https://github.com/NoteDance/optimizers","date":1745161065,"author":"/u/NoteDancing","guid":544,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k3odxr/tensorflow_implementation_for_optimizers/"},{"title":"Jujutsu: different approach to versioning","url":"https://thisalex.com/posts/2025-04-20/","date":1745155667,"author":"/u/indeyets","guid":547,"unread":true,"content":"<p><em>In the <a href=\"https://thisalex.com/posts/2025-01-19/\">previous article</a> I told about my history with different version control systems and how I ended up using Jujutsu. This part will focus on why I think it is an important improvement over the git's status-quo and why I use it daily.</em></p><p>Let's start by defining the aforementioned status-quo. Typical user sees interaction with Git as moving files between 3 stages:</p><ol><li>Files are edited in the file system. Git doesn't know anything about them. Some of the commands can compare the changed parts against other stages (think  or ).</li><li>Some [parts] of the files are added to the \"staging area\" using  command. These changes are not yet part of the tracked history, but Git knows about them.</li><li>Proper content-snapshot with identifier and metadata is injected into the repository using  command.</li></ol><p>Linus called this system \"<a href=\"https://git-scm.com/docs/git\">the stupid content tracker</a>\" because at its core it doesn't try to do anything smart. It doesn't try to understand the content, it doesn't try to understand the changes, it doesn't try to understand the history. It just stores the snapshots and the metadata. In the grand scheme of things, it is up to the user to make sense of it. And to be effective with git, users  to make sense of it.</p><p><a href=\"https://jj-vcs.github.io/\">Jujutsu</a>, a VCS which I started to use recently, makes some very different choices. Let's see how it handles this. I will be talking about the concepts and showing the commands. If you want to follow along, you can install it and try it out in parallel.</p><p>On macOS with <a href=\"https://brew.sh/\">Homebrew</a> it is as simple as . You can find instructions for other platforms on the <a href=\"https://jj-vcs.github.io/jj/latest/install-and-setup/\">official site</a>.</p><p>Jujutsu allows you to work with arbitrary git repositories. You can use both  and  at the same time. Adding jujutsu support to repo is as simple as running  in it. Jujutsu will detect existing git repository and will configure itself to transparently sync with it.</p><div><div><p>Jujutsu has plans to provide several opensource backend implementations: git-based one and the native one. Google has a backend powered by Piper, their internal VCS system, but it is not public. Native backend is not ready yet, and while it will have some interesting properties, for now it is not a practical topic. The rest of this post would be talking only about git-backend.</p></div></div><h2>Everything's a part of \"change\"</h2><p>Git, as I shown above, expects user to work on files and then to \"commit\" their snapshot to the repository. Thus, the verb became a noun and we say that Git-repository stores commits.</p><p>Jujutsu uses a different terminology: repository stores changes. At any point in time, whenever you edit, add or remove a file, these modifications become a part of the \"current change\". You never need to commit them explicitly.</p><p>As soon as we connected jj to the repository it has created a new  change with . We can check this by running  (or ) command.</p><ul><li> is an id of the change. it would remain static no matter how we change the contents in the future.</li><li> is a content-hash which corresponds to the id of the git commit. As soon as we add a single new letter to the files it will change.</li><li> means that this change is allocated, but it doesn't have any content yet. Again, as soon as we modify a single letter in files, this marker will disappear.</li><li> means that we didn't bother to set the description of the change yet (it corresponds to the \"commit message\" in git).</li></ul><p>Jujutsu doesn't require you to set descriptions of changes until you want to make them public. In Jujutsu it is perfectly fine to have multiple non-described changes. I still prefer to set descriptions early on, but it is not a requirement.</p><p>The command to set the description is . This command does not create a new change. It modifies the description of current change. You can call it several times and it will be modifying description of the same change.</p><p>How do you start a new change? Well, you just use  for that.</p><h2>Branches don't have names</h2><p>Let's take a look at the log of our changes. We'll use  for that.</p><p>The thing I'd like to focus on is the  marker, which stayed at exactly the same place where it was in this git repository initially. Branches in jujutsu do not have names, which might be contra-intuitive at first. Previously, we created a single branch of changes starting from git's \"main\" branch, but we can create more like this:  and they wouldn't be lost, as Jujutsu's log would give us access to all of them without any issues. Strictly speaking, this is exactly what was done as part of  under the hood: it created one empty change for us automatically.</p><p>And, as we can create multiple independent branches of changes from any point of the tree, Jujutsu can not meaningfully make assumptions about moving the marker.</p><p>In previous paragraphs I said \"marker\", but the proper Jujutsu term for this is \"bookmark\". Literally, the \"main\" is a bookmark in tree, which can be referenced by name, and moved as we need (just as we move a bookmark, when we read the book).</p><p>So, whenever we're ready to tell: \"this is, what everyone should consider  from now on!\", we just type <code>jj bookmark move main --to=@</code>. At this moment, git's \"main\" branch would start to point at this commit (\"current\" commit has special id ) and the whole branch would become visible to the git. Other branches we created would be mostly hidden from git tools (but the commits would still be stored in git's repository).</p><p>The next step would be able to push the  to the upstream repository (such as github) using  command. Except that we still have some undescribed changes. It is not prohibited to have that in git, but it would definitely look strange. Let's play nice and that.</p><p>One way to do it is to use . A lot of jj's commands accept  argument which allows to specify the change-id, which is the target of command. In this case, change-id is , but we can use  as it was hilighted in the log. Jujutsu always highlights shortest non-ambiguous prefix of the change-id in the log.</p><p>Another approach is more generic. We can use  to resume work on the change and just call  without arguments while we do it.</p><p>Whether we modify the description or the contents of the change it's content-hash would change, but it's change-id would not. And, as descendent commits are linked via change-ids and bookmarks are attached to change-ids as well, the branch would maintain it's shape after the changes. So, now we should finally be able to push our changes.</p><div><div><p>Editing pre-existing changes of the upstream branch and pushing them to git again would result in a force-push. From Git's point of view these new commits are not connected to the old ones and history rewriting happens.</p><p>Jujutsu uses several heuristics to prevent \"dangerous\" situations such as \"moving\" the branch which was already merged into something else, but overall it is much more forgiving in this regard.</p><p>In cases when several people work on the same branch such pushes would result in the need to re-sync local branches. In the future, when we have <a href=\"https://en.wikipedia.org/wiki/Forge_(software)\">forges</a> with native jujutsu support, these operations would be trivial and boring (in a good way).</p></div></div><p>In the previous section we were modifying the change in the middle of the branch. How would Jujutsu react if we introduced the conflict? Let's check!</p><p>You see that the upper change is marked with a red \"conflict\" word now, but Jujutsu still allows us to work on whatever we want. It doesn't force us to resolve conflict now. The most interesting part is, that we do not have to resolve the conflict at the point where it was introduced.</p><p>I'll create a new change at the top of the branch:  and will edit the code to resolve the conflict there. Take a look at the log now:</p><p>We have intermediate change in \"conflicted\" state, but above it the branch is clean again. And we could leave it at that, but let's apply the fix to the broken change instead to get a clean history. We will use  command for that:</p><p> gathers the modifications from the current change and makes them a part of the specified change instead. Done. Conflict is gone. New empty change is automatically created for us. We can continue working on the branch.</p><p> without  argument applies modifications to the parent change. It can be used to emulate a staging area of git. The basic idea is that you keep the parts which you're sure about in one change and experimental modifications in the next one. And as soon as you're sure in the quality of your experimental code you squash it into the proper change. A lot of people consider this to be the recommended flow for Jujutsu.</p><p>Until this point I was talking about linear changes and diverging branches, but nothing about merges. Two reasons: a). I wanted to lay the groundwork; b). There isn't much to talk about :)</p><p>In Jujutsu, one doesn't make a merge of branches. Instead, one makes a change with several parents: , where , etc. are either change-ids or branch-names (or some other interesting things we did not talk about yet).</p><p>Resulting change would reside on it's own anonymous branch, as usually and you'll need to assign some bookmark to it to make it visible in git.</p><p>Thus, equivalent of  would look like this:</p><pre data-lang=\"sh\"><code data-lang=\"sh\"></code></pre><p>It is more verbose, but gives you more control over the process.</p><p>I hope I managed to spark your interest in jujutsu. This post is definitely not an exhaustive guide, but I wanted to give you a taste of what it is like to work with it.</p><p>If you want to learn more, I encourage you to check out:</p>","contentLength":9238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k3mjlz/jujutsu_different_approach_to_versioning/"},{"title":"A small dive into Virtual Memory","url":"https://www.youtube.com/watch?v=ultz9m0n0GE","date":1745154269,"author":"/u/1337axxo","guid":545,"unread":true,"content":"<div><p>Hey guys! I recently made this small introduction to virtual memory. I plan on making a follow up that's more practical if it interests some people :)</p></div>   submitted by   <a href=\"https://www.reddit.com/user/1337axxo\"> /u/1337axxo </a>","contentLength":181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k3m3oy/a_small_dive_into_virtual_memory/"},{"title":"F1 Race Prediction Algorithm (WIP): A sophisticated Formula 1 race simulation tool that models and predicts F1 race outcomes with realistic parameters based on driver skills, team performance, track characteristics, and dynamic weather conditions.","url":"https://github.com/mehmetkahya0/f1-race-prediction","date":1745154154,"author":"/u/mehmettkahya","guid":549,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k3m2fd/f1_race_prediction_algorithm_wip_a_sophisticated/"},{"title":"8 Kubernetes Deployment Strategies and How They Work","url":"https://www.groundcover.com/blog/kubernetes-deployment-strategies","date":1745137483,"author":"/u/tapmylap","guid":548,"unread":true,"content":"<p>One of Kubernetes's killer features is that it offers a flexible way to deploy applications. Admins can choose from a variety of deployment strategies, each of which offers a different approach to application lifecycle management. Depending on factors like application availability requirements or how carefully you want to be able to test a new deployment before entrusting mission-critical workloads to it, one Kubernetes deployment strategy may be a better fit than another.</p><p>To provide guidance on how to select the best deployment strategy for a given workload, this article compares eight popular Kubernetes deployment techniques, explaining their pros and cons. It also offers tips on optimizing your Kubernetes deployment strategy no matter which type or types of deployments you choose.</p><h2>What is a Kubernetes deployment strategy?</h2><p>A Kubernetes deployment strategy is the configuration that manages how Kubernetes runs and manages an application. Deployment strategies are typically defined in YAML files, which describe how Kubernetes should deploy the initial pods and containers associated with an app, as well as how it should manage updates over the course of the application’s lifecycle.</p><p>Having different deployment strategy options is part of what makes Kubernetes so powerful and flexible. Depending on what an application does, you may need to manage its deployment in a specific way.</p><p>For example, with some applications, it’s possible to run multiple versions of the app at the same time within the same Kubernetes cluster. In that case, you could use a deployment strategy that updates application instances one by one. But this typically wouldn’t work if all application instances need to connect to a shared database or maintain a shared global state. An appropriate deployment strategy for that scenario would require updating all application instances at the same time, in order to maintain consistency between versions.</p><h2>Top 8 Kubernetes deployment strategies</h2><p>To illustrate what Kubernetes deployment strategies look like in practice, here are eight examples of popular deployment patterns.</p><p>A recreate deployment tells Kubernetes to delete all existing instances of a pod before creating a new one. Recreate deployment strategies are useful for situations where you need all application instances to run the same version at all times.</p><p>To configure a recreate deployment, include a spec like the following in your deployment configuration:</p><div><pre><code>spec:\n  replicas: 3\n  strategy:\n\ttype: Recreate</code></pre></div><p>This creates a deployment with three pod replicas and uses the recreate deployment strategy to maintain a consistent version across each replica.</p><p>A rolling deployment (which is the default deployment strategy that Kubernetes uses if you don’t specify an alternative) manages pod updates by applying them incrementally to each pod instance. In other words, it works by <a href=\"https://www.groundcover.com/blog/kubernetes-pod-restart\">restarting Kubernetes pods</a> one by one.</p><p>Rolling updates are a useful deployment strategy when it’s important to avoid downtime. Since this approach updates pod instances incrementally, it ensures that while one pod instance is being updated, other instances remain available to handle requests.</p><p>The following spec configures a rolling deployment strategy:</p><div><pre><code>spec:\n  replicas: 3\n  strategy:\n\ttype: RollingUpdate\n\trollingUpdate:\n  \tmaxUnavailable: 1    \t# Maximum number of Pods that can be unavailable during the update\n  \tmaxSurge: 1          \t# Maximum number of Pods that can be created over the desired number</code></pre></div><p>In a blue/green deployment strategy, you maintain two distinct Kubernetes deployments – a blue deployment and a green one – and switch traffic between them. The advantage of this approach is that it allows you to test one version of your deployment and confirm that it works properly before directing traffic to it.</p><p>To implement a blue/green deployment, first create two Kubernetes deployments. Use the deployment metadata field to apply a unique label to each one.</p><p>Then, define a Kubernetes service that specifies which of the two Kubernetes deployments should receive requests. For example, the following service sends traffic to the blue deployment by matching the label “blue”:</p><div><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-service\nspec:\n  selector:\n\tapp: my-app\n\tcolor: blue  # Switch this to \"green\" during the cutover\n  ports:\n  - protocol: TCP\n\tport: 80\n\ttargetPort: 80\n  type: ClusterIP</code></pre></div><p>As noted in the comment within the service definition, you can modify the selector to “green” in order to switch traffic to your other deployment.</p><p>Blue/green deployments minimize the risk of downtime because they allow you to vet a new deployment fully before using it to handle production traffic. A downside, however, is that blue/green deployments require you to run two complete instances of your application at the same time. This is not an efficient use of resources, since only one of the instances is handling traffic.</p><p>A canary deployment strategy switches traffic between distinct deployments gradually. It’s similar to a blue/green strategy in that it requires two different deployments. But whereas a blue/green deployment cuts traffic over from one deployment to the other all at once, the canary method directs some requests to one deployment while sending others to the other deployment.</p><p>The advantage of this approach is that it allows you to detect problems with one of the deployments before they impact all users. It’s called a “canary” deployment because it’s analogous to using canaries in coal mines to detect the buildup of toxic gases before they reach a level that would harm humans, since canaries are especially sensitive to gases like carbon monoxide.</p><p>To set up a canary deployment, first create two deployments for your application. The number of pod replicas for each deployment should reflect which percentage of traffic you want the deployment to handle. For instance, if you want one deployment to receive 60 percent of your traffic and the other to receive 40 percent, create 6 replicas in the first deployment and 4 in the second. Both deployments should match the same application label.</p><p>Then, create a service that directs traffic to the matching application based on the deployment metadata. For example:</p><div><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-service\nspec:\n  selector:\n\tapp: my-app\n  ports:\n  - protocol: TCP\n\tport: 80\n\ttargetPort: 80\n  type: ClusterIP</code></pre></div><p>To modify the balance between traffic over time in order to switch traffic gradually from one deployment to the other, scale the replicas within each deployment accordingly using the <a href=\"https://www.groundcover.com/blog/kubectl-scale\">kubectl scale deployment</a> command.</p><h3>5. A/B testing deployment</h3><p>In an A/B testing deployment, you run two distinct deployments and route traffic between them based on request type or user characteristics.</p><p>For example, imagine you want to distinguish between requests from “testing” users and requests from “production” users. You could do this by differentiating between user types in request headers and routing requests on this basis. Requests with the header  would go to one deployment, while those with  would route to another.</p><p>To implement an A/B testing deployment strategy, first create two deployments. Then, install a service mesh or ingress controller, such as <a href=\"https://www.groundcover.com/blog/istio-service-mesh\">Istio</a>, and configure it with a routing rule that selects a deployment based on header strings.</p><p>For instance, the following Istio virtual service (which targets the app  based on the deployment metadata) sends requests with  in the header to one deployment, while routing all others to the other deployment:</p><div><pre><code>apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: my-app\nspec:\n  hosts:\n  - my-app\n  http:\n  - match:\n\t- headers:\n    \t    end-user:\n      \t       exact: testing     \t# A/B testing condition\n\troute:\n\t- destination:\n    \t  host: my-app\n    \t  subset: v2\n  - route:\n\t- destination:\n    \t  host: my-app\n    \t  subset: v1</code></pre></div><p>A shadow deployment strategy involves running two deployments. One deployment handles all requests and routes responses back to users. Meanwhile, the second deployment runs “in the shadows” and also processes some or all requests, but the responses don’t go back to users. Instead, they’re analyzed for testing purposes and then dropped.</p><p>The value of a shadow deployment is that it lets you test a deployment by feeding it genuine user requests without relying on it to process those requests for production users. If the deployment is buggy, it won’t impact users, but you’ll still be able to detect the issue.</p><p>To create a shadow deployment in Kubernetes, first set up two deployments for your application. Next, create a virtual service that mirrors traffic to the shadow instance, while also sending traffic to the primary instance. For example:</p><div><pre><code>apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: my-app\nspec:\n  hosts:\n  - my-app\n  http:\n  - route:\n\t- destination:\n    \t    host: my-app\n    \t    subset: v1\n\tmirror:\n  \t    host: my-app\n  \t    subset: v2\n\tmirrorPercentage:\n  \t   value: 100.0   # 100% of traffic is mirrored to v2</code></pre></div><h3>7. Best-effort controlled rollout</h3><p>A best-effort controlled rollout is similar to a canary deployment in that it gradually switches traffic from one deployment to another. But it does so in a more controlled manner, often by using predefined conditions (like crossing a certain CPU usage threshold for one deployment) to determine when to change the amount of traffic going to each deployment.</p><p>To implement a best-effort controlled rollout, you would typically use a third-party deployment controller like <a href=\"https://argoproj.github.io/rollouts/\" target=\"_blank\">Argo</a> and configure it to manage your rollout based on predefined criteria. For example:</p><div><pre><code>spec:\n  replicas: 5\n  strategy:\n\tcanary:\n  \t  steps:\n    \t    - setWeight: 20\n    \t    - pause: { duration: 2m }\n    \t    - setWeight: 40\n    \t    - pause: { duration: 5m }\n    \t    - setWeight: 100</code></pre></div><p>This configures a type of canary deployment. But unlike a generic canary deployment strategy where you simply add replicas to each deployment over time, the rollout in this case is carefully controlled based on specific criteria.</p><p>A ramped slow rollout replaces pod instances incrementally, with delays between the replacements. It’s similar to a rolling deployment, but a key difference is that with a ramped slow deployment, there is a pause between each update. This provides time to run tests and confirm that the previous update was successful before moving on to the next one.</p><p>To implement a ramped slow rollout, use a controller like Argo and configure pauses between each rollout event. For instance:</p><div><pre><code>spec:\n  replicas: 5\n  strategy:\n\tcanary:\n  \t  steps:\n  \t    - setWeight: 20\n  \t    - pause:\n      \t        duration: 2m\n  \t- setWeight: 40\n  \t- pause:\n      \t       duration: 5m\n  \t- setWeight: 100</code></pre></div><h2>Choosing the right deployment strategy by use case</h2><p>The main consideration for deciding which deployment strategy to adopt is the type of use case you need to support. Here’s a look at common use cases and the best deployment strategies for each one.</p><p>For stateless applications, a simple rolling deployment usually makes the most sense. If there is no application, it’s not typically important to keep pod instances in sync, so you can update them one by one without causing problems.</p><p>For most stateful applications, a recreate deployment strategy works best. Recreate deployments ensure that application versions remain consistent across all instances, helping to keep state in sync.</p><p>Note as well that typically, you’d use a StatefulSet instead of a deployment to run a stateful application. (For details, check out our article on <a href=\"https://www.groundcover.com/blog/kubernetes-statefulset-vs-deployment\">Kubernetes StatefulSet vs deployment</a>.) But you can apply most types of deployment strategies to StatefulSets as well as to deployments.</p><h3>High-traffic applications</h3><p>For applications that receive a lot of traffic on a continuous basis, a canary deployment or one of its variants (like a best-effort controlled rollout or a ramped slow rollout) is usually the best fit. These methods make it possible to route traffic between multiple deployments, which in turn helps to balance load and ensure that no one deployment becomes overwhelmed.</p><h3>Mission-critical and zero-downtime apps</h3><p>For use cases where you can’t tolerate any downtime, consider a blue/green deployment. This approach allows you to validate a new deployment fully before sending traffic to it.</p><p>A shadow deployment could also be a good choice for this use case. It would allow you to real-user perform testing on a new deployment before directing requests to it.</p><p>A/B testing deployment strategies may also work well for certain mission-critical apps, especially if only certain users or requests are critical. In that case, you can send the high-value requests to one deployment that you’ve carefully tested, while routing less critical ones to another deployment.</p><h3>Batch processing and background jobs</h3><p>For use cases that involve processing data in batches or running background jobs, a simple recreate or rollout deployment strategy typically works well. More complex and advanced deployment strategies aren’t usually necessary for these use cases because you usually don’t need to worry as much about the potential for downtime or running multiple versions of an application at the same time.</p><h2>Factors to consider when selecting a deployment strategy</h2><p>Beyond aligning deployment strategies with use cases, it’s also important to consider the following factors:</p><ul role=\"list\"><li><strong>Deployment downtime tolerance</strong>: The less downtime you can accept for a workload, the more important it is to use a low-risk deployment strategy, like rolling or blue/green Kubernetes deployments.</li><li><strong>Traffic flow and load management</strong>: If you need fine-grained control over traffic flow and load balancing, consider an A/B testing or controlled rollout deployment, which allows you to route traffic based on predefined rules.</li><li><strong>Failover, rollback, and reversal mechanisms</strong>: If it’s important to be able to revert to a previous version of a deployment, use either a blue/green or canary deployment method. These approaches make it possible to switch back to one deployment in the event that a newer deployment turns out to be buggy.</li><li><strong>Security and compliance considerations</strong>: Some Kubernetes deployments are subject to specific security and compliance requirements. For instance, if you need to ensure that security patches roll out to all application instances (to avoid leaving some vulnerable instances), you’d want a recreate deployment method. Or, if you need to route requests for users based in a certain area to a specific deployment to meet compliance rules that apply to those users, you could use an A/B testing deployment strategy.</li><li><strong>Scalability and auto-healing capabilities</strong>: Canary deployments are useful because they provide control over deployment scalability. They can also offer some auto-healing capabilities because Kubernetes will automatically attempt to maintain the number of replicas specified for each deployment – so if some replicas fail, Kubernetes can self-heal by restoring them.</li></ul><h2>Best practices for a seamless Kubernetes deployment</h2><p>No matter which deployment strategy you choose, the following best practices can help minimize risk and simplify administration:</p><ul role=\"list\"><li>: Some deployment strategies (like recreate and rolling Kubernetes deployments) are much simpler than others (like best-effort controlled rollouts). In general, simpler is better. Don’t implement a complex deployment strategy, or one that requires the use of additional tools (like Istio or Argo) unless you need the special capabilities it provides.</li><li>: Prior to entrusting production traffic to a deployment, it’s a best practice to test it first. You can do this using synthetic requests, or you can direct real user requests to a deployment via a method like shadow deployments.</li><li><strong>Monitor and observe Kubernetes deployments</strong>: To detect issues with a deployment, it’s critical to monitor and observe all application instances. Methods like blue/green and canary deployments are only useful if you have the monitoring and observability data necessary to detect problems with one deployment and route traffic appropriately.</li><li><strong>Consider resource overhead</strong>: Deployment strategies that require you to maintain multiple Kubernetes deployments at the same time create more resource overhead (because more deployments require more resources to run). This can lead to poorer performance because Kubernetes cluster resources are tied up with redundant deployments. For this reason, it’s important to evaluate how many spare resources your Kubernetes cluster has and choose a deployment method accordingly.</li></ul><h2>Optimizing and monitoring Kubernetes deployments with groundcover</h2><p>When it comes to monitoring Kubernetes deployments and troubleshooting problems, groundcover as you covered. Using hyper-efficient eBPF-based observability, groundcover clues you in - in real time - to deployment performance issues like dropped requests or high latency rates. We also continuously track CPU, memory, and other performance metrics, so you’ll know right away if any of your Kubernetes deployments are at risk of becoming overwhelmed.</p><p>We can’t tell you exactly which deployment strategy is best for a given workload. But we can give you the observability data you need to make an informed decision about Kubernetes deployment strategies.</p><h2>A balanced approach to Kubernetes deployment</h2><p>Ultimately, Kubernetes deployment strategies boil down to balancing performance and control on the one hand with risk and complexity on the other. If you just want to deploy an application simply and quickly, Kubernetes lets you do that – although simple deployment methods are sometimes more risky.</p><p>You can also opt for more complex and fine-tuned deployment strategies that – like a complex chess move – require more expertise to carry out, but that can pay off in the long run by delivering a better balance between risk and performance.</p>","contentLength":17892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1k3hz8g/8_kubernetes_deployment_strategies_and_how_they/"}],"tags":["dev","reddit"]}