{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Years ago I saw a programming presentation using pirate language. I don't know the presenter, I don't know what it was about, but it was funny and impressive and would like to see it again, but can't find it in the abyss of the youtube.","url":"https://en.wikipedia.org/wiki/International_Talk_Like_a_Pirate_Day","date":1737968740,"author":"/u/keszegrobert","guid":575,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ib410o/years_ago_i_saw_a_programming_presentation_using/"},{"title":"Node module whose effect can be achieved by typing 2 (!) characters","url":"https://github.com/davidmarkclements/flatstr/blob/master/index.js","date":1737965146,"author":"/u/Totally_Dank_Link","guid":577,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ib36bo/node_module_whose_effect_can_be_achieved_by/"},{"title":"Lifelong Learning: 88+ Resources I Don't Regret as a Senior Software Engineer","url":"https://thetshaped.dev/p/lifelong-learning-88-plus-resources-i-do-not-regret-as-senior-software-engineer","date":1737954912,"author":"/u/pepincho","guid":576,"unread":true,"content":"<p>The Internet is flooded with content, materials, and resources.</p><p>Knowing the most useful newsletters, books, courses, creators, and tools is hard.</p><p>In this article, I want to share some incredible resources that I‚Äôve found valuable in my experience and day-to-day job.</p><p>Each resource on this list taught me something new and helped me learn and grow as an engineer.</p><p>As a Senior Software Engineer, I need to keep up with the industry trends, updates, libraries, tools, vulnerabilities, etc, so that I can apply this knowledge at my job.</p><p>Here are most of the newsletters I read every week.</p><p>I use these four newsletters to stay up-to-date with the Web and JavaScript world.</p><p><strong>I‚Äôd suggest avoiding reading all these newsletter at once because you‚Äôll feel overwhelmed.</strong></p><p>Depending on your current needs, priorities, and career aspirations, you might want to choose a few of them and come to the rest when needed.</p><p>Books are a great way to learn and grow as an individual.</p><p>We can learn from a lot of people even though they‚Äôre not in front of us.</p><p>We can learn from their mistakes and see what lessons they have learned.</p><p>However, I‚Äôve found that if I read a book and don‚Äôt apply my knowledge from it as soon as possible, the value from reading the book drops drastically.</p><p><strong>Think in advance how reading a particular book will help you in your day-to-day tasks, job, and personal life. If you can‚Äôt apply the knowledge immediately, postpone reading it.</strong></p><p>Each book has taught me something or sparked a new idea and way of thinking.</p><p>It‚Äôs not necessary to apply everything from each book but rather look for the things that most suits you at the moment and adapt them to your lifestyle.</p><p>A great mistake I made in the past was to try to apply everything on 100%.</p><p><strong>The true wisdom comes when you find the 10-20% of the book to apply at the moment.</strong></p><p>At each phase of our lives we need different things, so think twice before applying anything directly. Be conscious.</p><ul><li><p>Don‚Äôt try to read and follow everything at once.</p></li><li><p>Think in advance how reading a particular book will help you in your day-to-day tasks, job, and personal life.</p></li><li><p>If you can‚Äôt apply the knowledge immediately, postpone reading the book or newsletter.</p></li><li><p>As you might see, there‚Äôre no courses. My preferred way is to read and apply what I read immediately in my day-to-day job or side-projects.</p></li></ul><p>That's all for today. I hope this was helpful.</p><p>What are the 1-2 resources you‚Äôve found life changing? Share them in the comments üëÄ üëá</p><p>Become a better React Software Engineer. Join 17,400+ engineers who are improving their skills every week.</p><p>I share daily practical tips to level up your skills and become a better engineer.</p><p><em>Thank you for being a great supporter, reader, and for your help in growing to 17.6K+ subscribers this week üôè</em></p><p><em>You can also hit the like ‚ù§Ô∏è button at the bottom to help support me or share this with a friend. It helps me a lot! üôè</em></p>","contentLength":2890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ib0lvg/lifelong_learning_88_resources_i_dont_regret_as_a/"},{"title":"DeepSeek R1 API First Look: How This Open-Source Model Outperforms OpenAI","url":"https://www.kaishira.com/2025/01/26/deepseek-r1-api-first-look-how-this-open-source-model-outperforms-openai/","date":1737922386,"author":"/u/haberveriyo","guid":579,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iaoyh2/deepseek_r1_api_first_look_how_this_opensource/"},{"title":"How long is a second in JS? | Why Some Are Longer Than Others","url":"https://docs.timetime.in/blog/how-long-is-a-second-in-js","date":1737912975,"author":"/u/iagolast","guid":580,"unread":true,"content":"<p>When you ask the question \"How long is a second in JavaScript?\", it seems like a straightforward query. However, the answer reveals layers of complexity intertwined with history, science, and the foundations of modern computing. Let‚Äôs dive deep to understand how humanity has measured time and how it connects to JavaScript‚Äôs timekeeping.</p><h3>The Early Days of Timekeeping<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#the-early-days-of-timekeeping\" aria-label=\"Direct link to The Early Days of Timekeeping\" title=\"Direct link to The Early Days of Timekeeping\">‚Äã</a></h3><p>Time measurement began with observing natural periodic phenomena. Early civilizations looked to the moon's phases and the apparent movement of the sun to divide time into manageable units. These observations gave rise to the concepts of days, months, and years.</p><p>As human needs evolved, a more granular division of time became necessary. The ancient Egyptians and Babylonians divided the day into 24 hours, likely influenced by their base-12 numbering system. Later, hours were divided into 60 minutes and minutes into 60 seconds, creating the framework we still use today.</p><p>While these early methods sufficed for centuries, they lacked precision. By the mid-20th century, advancements in technology demanded a more accurate measurement of time. The invention of the atomic clock in the 1950s marked a revolutionary step. These clocks measure time based on the vibrations of cesium atoms, offering unparalleled precision.</p><p>As a result, the definition of a second was updated to:</p><blockquote><p>\"The duration of 9,192,631,770 periods of the radiation corresponding to the transition between the two hyperfine levels of the ground state of the cesium-133 atom.\"</p></blockquote><p>This definition formed the foundation for what we now call international atomic time (TAI)</p><p><a href=\"https://en.wikipedia.org/wiki/Universal_Time\" target=\"_blank\" rel=\"noopener noreferrer\">Universal Time</a> is based on the Earth's rotation. It‚Äôs intuitive and tied to our everyday experience of day and night. However, the Earth's rotation is not perfectly consistent due to various factors such as gravitational interactions and geophysical processes. As a result, we could say that Universal Time is imprecise and variable.</p><p><a href=\"https://en.wikipedia.org/wiki/International_Atomic_Time\" target=\"_blank\" rel=\"noopener noreferrer\">Atomic Time</a>, derived from atomic clocks, is based on physical phenomena and is both precise and consistent. It serves as the backbone for modern timekeeping systems.</p><p>In the 1970s, <a href=\"https://en.wikipedia.org/wiki/Coordinated_Universal_Time\" target=\"_blank\" rel=\"noopener noreferrer\">Coordinated Universal Time (UTC</a>) was introduced to reconcile the discrepancies between Universal Time and Atomic Time. UTC aligns with Universal Time but incorporates adjustments, such as the addition or removal of leap seconds, to compensate for the Earth's irregular rotation.</p><p>Leap seconds are occasional one-second adjustments added to UTC to keep it in sync with Universal Time. These adjustments ensure that UTC remains within 0.9 seconds of Universal Time. However, they introduce complications for systems that rely on precise time calculations.</p><p>Here‚Äôs the improved article in English, incorporating the original tables:</p><h3>ECMAScript and POSIX Time<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#ecmascript-and-posix-time\" aria-label=\"Direct link to ECMAScript and POSIX Time\" title=\"Direct link to ECMAScript and POSIX Time\">‚Äã</a></h3><p>This approach treats every day as having exactly  (24 hours), ignoring both leap seconds and astronomical variations in the Earth's rotation. While this simplifies calculations and suffices for most applications, it introduces limitations in scenarios where precise timekeeping is critical.</p><p>Although POSIX and UTC appear similar, . Here are the key differences:</p><ol><li><ul><li>UTC includes leap seconds to adjust official time to the Earth's rotation.</li><li>POSIX does not count leap seconds, meaning that <strong>POSIX days always have 86,400 seconds</strong>, even when UTC days have 86,401 or 86,399 seconds due to a leap second.</li></ul></li><li><p><strong>Simplified Interoperability</strong>:</p><ul><li>POSIX assumes all seconds are of equal duration. This makes calculating time differences straightforward but causes discrepancies when leap seconds occur.</li><li>For example, during a leap second (e.g., ), POSIX simply ignores the additional second, making timestamps near the leap second ambiguous or inaccurate.</li></ul></li></ol><h3>Step Adjustment: The Traditional Solution<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#step-adjustment-the-traditional-solution\" aria-label=\"Direct link to Step Adjustment: The Traditional Solution\" title=\"Direct link to Step Adjustment: The Traditional Solution\">‚Äã</a></h3><p>Historically, systems that needed to account for leap seconds would use a technique called . This method can be described as just ignoring the leap second, causing the clock to jump forward or backward by one second at the exact moment of the leap second.</p><p>Lets see a table to understand this:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>As you can see from the both TAI and UTC are strictly increasing, but POSIX (step) has a jump in time at the leap second. This means that 2 different UTC seconds are mapped to the same POSIX second. This can cause issues in some applications. The biggest problem is that some apps can go \"back in time\" when a leap second is added.</p><p>Lets say that a payment is requested at , and approved during the next second at . Our logs will show something like:</p><div><div><pre tabindex=\"0\"><code></code></pre></div></div><p>This is a problem because the payment was approved before it was requested. This is why most system use a different approach to handle leap seconds.</p><h3>Smearing: The Practical Solution<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#smearing-the-practical-solution\" aria-label=\"Direct link to Smearing: The Practical Solution\" title=\"Direct link to Smearing: The Practical Solution\">‚Äã</a></h3><p>In many modern systems implementing POSIX time, a technique called  is used to handle leap seconds. Instead of adding or removing a whole second, the effects of the leap second are distributed gradually over a longer period (typically 24 hours). This has several implications: First of all during the smear period, <strong>seconds are slightly longer or shorter than a standard second</strong> (measured in atomic time). This ensures a smooth transition instead of an abrupt jump in time. On the other hand compared to stepping the clock is strictly increasing, which is a requirement for many applications.</p><h4>Example: Smearing and Step Adjustment Tables<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#example-smearing-and-step-adjustment-tables\" aria-label=\"Direct link to Example: Smearing and Step Adjustment Tables\" title=\"Direct link to Example: Smearing and Step Adjustment Tables\">‚Äã</a></h4><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>In this table:  ignores the leap second entirely, treating  as if it doesn't exist while  gradually adjusts the clock, ensuring smoother transitions but at the cost of seconds having slightly different durations.</p><p>This is how Google's NTP servers handle leap seconds. They smear the leap second over a 24-hour period, ensuring that the clock is always strictly increasing. More information can be found <a href=\"https://developers.google.com/time/smear\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.</p><h2>Implications for Applications<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#implications-for-applications\" aria-label=\"Direct link to Implications for Applications\" title=\"Direct link to Implications for Applications\">‚Äã</a></h2><h3>Why It Usually Doesn‚Äôt Matter<a href=\"https://docs.timetime.in/blog/how-long-is-a-second-in-js#why-it-usually-doesnt-matter\" aria-label=\"Direct link to Why It Usually Doesn‚Äôt Matter\" title=\"Direct link to Why It Usually Doesn‚Äôt Matter\">‚Äã</a></h3><p>For tasks like scheduling events, calculating ages, or displaying time in user-facing applications, JavaScript (and POSIX) time is precise enough and 99% of the problem will be derived from timezones.</p><p>For applications requiring high precision, such as those in scientific research or financial systems, the discrepancies caused by leap seconds and smearing can lead to significant errors. Accurate timekeeping is crucial to ensure the reliability and accuracy of measurements and transactions. During a leap second, a single POSIX timestamp can refer to two different moments in UTC, creating ambiguities and potential errors in systems that demand precise time synchronization.</p>","contentLength":6446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iakow2/how_long_is_a_second_in_js_why_some_are_longer/"},{"title":"Improve Rust Compile Time by 108X","url":"https://burn.dev/blog/improve-rust-compile-time-by-108x","date":1737912528,"author":"/u/Handsome_AndGentle","guid":578,"unread":true,"content":"<p>\nBefore you get too excited, the techniques used to reduce compilation\n          time are not applicable to all Rust projects. However, I expect the\n          learnings to be useful for any Rust developer who wants to improve\n          their project's compilation time. Now that this is clarified, let's\n          dive into the results.\n</p><p>\nWe started with a compilation time of 108 seconds for the matmul\n          benchmarks, which was reduced to only 1 second after all the\n          optimizations. The most effective optimization was the element-type\n          generics swap, where we instantiated generic functions with predefined\n          \"faked\" element types to reduce the amount of LLVM code generated. The second optimization also had a major impact, further\n          reducing the compilation time by nearly 3√ó. This was achieved by using\n          our comptime system instead of associated const generics to represent the\n          matmul instruction sizes. Finally, the last optimization‚Äîalso the simplest‚Äîwas\n          to reduce the LLVM optimization level to zero, which is particularly useful\n          for debug builds, such as tests.\n</p><div><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"720pt\" height=\"432pt\" viewBox=\"0 0 720 432\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\"></svg></div><div><p>\nCompilation times are measured using incremental compilation.\n</p></div><p>\nFirst, let me explain the situation that led me to investigate our\n          compile time. During the last iteration of CubeCL, we refactored our matrix multiplication GPU kernel to work with\n          many different configurations and element types. CubeCL is a dialect\n          that lets you program GPUs for high-performance computing applications\n          directly in Rust. The project supports multiple compiler backends,\n          namely WebGPU, CUDA, ROCm, and Vulkan with more to come.\n</p><p>\nThe refactoring of the matrix multiplication kernel was done to\n          improve tensor cores utilization across many different precisions. In\n          fact, each kernel instance works across 3 different element types: the\n          global memory element type, the staging element type, and the\n          accumulation element type. These are all different since we normally\n          want to accumulate in a higher precision element type, as this is\n          where numerical approximation is most sensitive. Also, the tensor\n          cores don't work across all input precisions; if you have f32 inputs,\n          you need to convert those to tf32 element types (staging) to call the\n          tensor cores instructions. To add to the complexity, tensor cores\n          instructions only work across fixed matrix shapes that also depend on\n          the precisions. For instance, f16 staging matrices work across all\n          shapes from (32, 8, 16), (16, 16, 16), (8, 32, 16). But tf32 only\n          works on (16, 16, 8).\n</p><p>\nIn our first refactoring, we represented the shape of the matrices\n          supported by the instructions using const associated types, since this\n          is the abstraction component that makes the most sense in this case.\n          For the element types, we naturally used generic arguments for traits\n          and functions - pretty much what any developer would do in this\n          scenario. However, with all the possible combinations, we ended up\n          with a compilation time of 1m48s using the cache.\n</p><p>\nYes, you read that right: 1m48s just to rebuild the matmul benchmark\n          if we change anything in the bench file.\n</p><p>\nFor the purpose of this optimization, we only consider incremental\n          compilation using cargo caching, since this is the most important one\n          to speed up dev iteration time. Changing one configuration to test if\n          an optimization worked took almost 2 minutes just to create the binary\n          to execute a few matmuls.\n</p><p>\nWell, we need to understand that the Rust compiler is actually very\n          fast. The slow parts are the linking and LLVM. The best way to improve\n          compilation time is to reduce the amount of LLVM IR generated.\n</p><p>\nIn our specific case, each combination of the matmul would generate a\n          whole new function - this is what zero-cost abstraction means. There\n          is no dynamic dispatch; every type change duplicates the code to\n          improve performance at the cost of a bigger binary. Before all of our\n          optimizations, the binary generated was 29M, and after we reduced it\n          to 2.5M - a huge difference.\n</p><p>\nTo reduce the amount of code generated, we had to use different Rust\n          techniques to make our abstractions for the matmul components. In our\n          case, we don't need zero-cost abstractions, since the code written in\n          Rust for the matmul components actually generates the code that is\n          used to compile at runtime a kernel that will be executed on the GPU.\n          Only the GPU code needs to be fast; the JIT Rust code takes almost no\n          time during runtime. Zero-cost abstraction would actually be optimal\n          in a setting where we would perform ahead-of-time compilation of\n          kernels.\n</p><p>\nEver wonder why LibTorch or\n          cuBLAS have executables that are\n          GIGABYTES in size? Well, it's because all kernels for all precisions with\n          all edge cases must be compiled to speed up runtime execution. This is\n          necessary in a compute-heavy workload like deep learning.\n</p><p>\nHowever, CubeCL is different - it performs JIT compilation, therefore\n          we don't need to compile all possible variations ahead of time before\n          creating the binary: we can use dynamic abstractions instead! This is\n          one of the two optimizations that we made for the matmul components.\n          Instead of relying on const associated types, we leveraged the\n          comptime system to dynamically have access to the instruction sizes\n          during the compilation of a kernel at runtime. This is actually the\n          second optimization that we made and helped us go from 14s compilation\n          time to around 5s.\n</p><p>\nHowever, the biggest optimization was quite hard to pull off and is\n          linked to the generic element types passed in each function. We still\n          wanted to use zero-cost abstraction in this case, since passing around\n          an enum listing what element type operations are on would be terrible\n          in terms of developer experience. However, the hint to improve our\n          compilation time was that when you write a function that will execute\n          on the GPU, we have an attribute on top .\n</p><p>\nWe want the code to look and feel like normal Rust, but the macro\n          actually parses the Rust code written and generates another function,\n          which we normally call the expand function, where the actual GPU IR is\n          built for the function. That code will actually run during runtime,\n          not the code that the user is writing. The element types generics are\n          only used to convert the generic element type into the enum IR element\n          type. In the expand functions, we also pass a context where all IR is\n          tracked.\n</p><p>\nSo the optimization was to pass a fake generic element type, called  instead of the actual element type like . When\n          compiling a kernel, we first register the real element type in the\n          context, using the const generic item to differentiate multiple\n          element types if a function has multiple generic element types. Since\n          we always call the expand function with the exact same generic items\n          for all element types, we only generate one instance of that function,\n          and the element types are fetched at runtime using the context.\n</p><p>\nThe most tedious part was actually implementing that optimization\n          while trying not to break our components. The biggest problem caused\n          by that optimization is that we can't support generic dependencies\n          between traits over the element type in launch functions of CubeCL.\n</p><div><pre tabindex=\"0\" data-language=\"rust\"><code></code></pre></div><p>\nIt makes sense though - we don't want to recompile all the instruction\n          types for all different precisions. Since our optimization is\n          activated at the boundaries of CPU code and GPU code, where cube\n          functions are identified as launchable, we need the generic trait to\n          not have a dependency on the element type. They are going to be\n          switched by our macro. We use generic associated types instead of\n          traits with generic element types.\n</p><p>\nThis is known as the family pattern, where a trait is describing a family of types.\n</p><div><pre tabindex=\"0\" data-language=\"rust\"><code></code></pre></div><p>\nUsing this pattern, we can inject the family type at the boundaries of\n          CPU and GPU code and instantiate the inner instruction type with the\n          expand element type.\n</p><div><pre tabindex=\"0\" data-language=\"rust\"><code></code></pre></div><p>\nMigrating most of the components to the new pattern, we reduced\n          compile time from 1m48s to about 14s.\n</p><p>\nIt was a lot of work, and I don't expect all projects to face cases\n          like this, but it was worth it! Now waiting for about 5 seconds after\n          trying something in the code to see if performance is improved doesn't\n          break the flow, but almost 2 minutes did.\n</p><p>\nWe essentially leveraged the fact that CubeCL is a JIT compiler and\n          not an AOT compiler, which is very appropriate for throughput-focused\n          high-performance applications.\n</p><h2>Playing with LLVM optimization settings</h2><p>\nSince our benchmarks are compiled with optimization level set to 3, we\n          could still improve the compilation time further to about 1s by\n          reducing the optimization level to zero. Another 5X speedup that we\n          can have by simply adjusting the LLVM optimization level.\n</p><p>\nWe decided not to keep that optimization in production, since we want\n          the benchmarks to have the same LLVM optimization level as user\n          applications. However, we activated it for testing, since we often\n          rerun tests to ensure we don't break correctness when implementing or\n          optimizing kernels.\n</p><h2>Not a Rust Compiler Issue</h2><p>\nAll of our optimizations actually created tons of code - we used proc\n          macros, associated type generics, const generics, and tons of complex\n          features from the Rust type system.\n</p><p>\nThe Rust compiler is actually very fast; the slow part is really the\n          linking and optimizing of the LLVM IR. If there's one thing to take\n          from this post, it's that you shouldn't worry about using complex\n          features of Rust, but make sure you don't generate huge binaries.\n          Reducing the binary size will improve compile time even if you use\n          complex methods to do so! \"Less code compiles faster\" is not exactly\n          right. \"Less generated code compiles faster\" is what we have to keep\n          in mind!\n</p>","contentLength":10722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iakht9/improve_rust_compile_time_by_108x/"}],"tags":["dev"]}