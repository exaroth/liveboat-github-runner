{"id":"6C2W1azD1rBsV4qJANNfruBrvGgdqLWrPGWtk6Mp1WgfCNcfvBZHFQw","title":"The System Design Newsletter","displayTitle":"Dev - System Design Newsletter","url":"https://newsletter.systemdesign.one/feed","feedLink":"https://newsletter.systemdesign.one/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":8,"items":[{"title":"System Design Interview: Design Twitter/X Timeline - A Frontend Deep Dive","url":"https://newsletter.systemdesign.one/p/system-design-interview-twitter","date":1764084989,"author":"Neo Kim","guid":38,"unread":true,"content":"<p>Traditional system design interviews tend to focus on backend architecture:</p><p>API servers, databases, caching layers, load balancers, and service-oriented designs.</p><p>‚ÄúFront-end‚Äù system design, by contrast, deals with the problems of scale and complexity that manifest in the browser or client environment.</p><p>Instead of optimizing for query latency or data replication, the focus shifts to rendering efficiency, client structure, network utilization, and managing data consistency between the client and server.</p><p>System design interviews matter because they reveal how well an engineer can think beyond individual components and consider the entire user experience as a system. Front-end system design interviews are a relatively new but increasingly important part of the hiring process for front-end engineers. They test not only one‚Äôs technical ability to build user interfaces but also the architectural thinking required to scale those interfaces to millions of users.</p><p>For companies with complex, dynamic interfaces such as Twitter/X, Netflix, Airbnb, Google Docs, and Figma, the frontend client is no longer a thin layer that merely renders data. It is a distributed system in its own right, managing state, orchestrating data fetching, caching results, handling concurrency, and maintaining real-time synchronization with the backend.</p><p>Designing this layer effectively can have a dramatic impact on user engagement and perceived performance, ultimately driving business goals.</p><p>Front-end system design is therefore not merely about choosing frameworks or UI libraries. It is about building systems that can evolve, scale, and deliver consistent performance as features grow and teams expand. Understanding this distinction is essential before diving into the architecture of complex interfaces like the Twitter/X timeline, where milliseconds of latency and subtle interaction patterns can define the entire product experience.</p><p>Your AI tools are only as good as the context they have.  pieces together knowledge from your team‚Äôs GitHub, Slack, Confluence, and Jira, so your AI tools generate production-ready code.</p><p>Yangshun is the creator of , a platform for frontend engineers to prepare for front-end interviews.</p><p>Check out his website and social media:</p><p>He was previously a Staff Engineer at Meta. Also, he created Docusaurus 2, Blind 75, and the <a href=\"https://www.techinterviewhandbook.org?ref=neokim\">Tech Interview Handbook</a>.</p><p>P.S. , their biggest sale of the year. Don‚Äôt miss out on this opportunity!</p><h2><strong>Types of questions in front-end system design interviews</strong></h2><p>Front-end system design interviews fall into two categories: </p><p>‚Äúapplication design and component design.‚Äù</p><p>While both draw on the same foundation of architectural thinking, they operate at different levels of abstraction and test different problem-solving instincts.</p><p>The  focuses on how to structure and architect an entire product or large feature. Candidates might be asked to design something like the Twitter timeline, a media streaming website, or a collaborative document editor.</p><p>These questions assess a candidate‚Äôs ability to design systems that can handle complex data flows, synchronize with back-end services, manage client-side state, and maintain consistent performance as the application scales.</p><p>Strong answers demonstrate an understanding of how to divide an application into layers, manage global versus local state, handle rendering efficiently, and make trade-offs between client-side and server-side responsibilities.</p><p>Each kind of app brings unique challenges that candidates should spend most of their time discussing:</p><ul><li><p><strong>Social media (e.g., Facebook, Twitter)</strong>: Timeline API, pagination, rendering posts in various formats, performance</p></li><li><p><strong>E-commerce &amp; travel booking (e.g., Amazon, Airbnb)</strong>: SEO, performance, forms, localization</p></li><li><p><strong>Media streaming (e.g., Netflix, YouTube)</strong>: Media streaming protocols, media player implementations</p></li><li><p><strong>Collaborative apps (e.g., Google Docs, Google Sheets)</strong>: Real-time collaboration models, conflict resolution approaches</p></li></ul><p>The  narrows the focus to building a single, self-contained piece of UI that appears simple on the surface but hides significant complexity beneath. Common examples include designing an autocomplete input, image carousels, or a modal dialog.</p><p>These components must handle accessibility, keyboard navigation, focus management, animations, and edge cases such as async data fetching and nested interactions.</p><p>Interviewers use these problems to evaluate a candidate‚Äôs depth of understanding in UI behavior, browser APIs, rendering models, and user experience details.</p><p>Although these two categories differ in scope, they complement each other‚Ä¶</p><ul><li><p>Application-level design tests a candidate‚Äôs ability to think in systems and to understand how data and state flow through a complex interface.</p></li><li><p>Component-level design tests a candidate‚Äôs ability to execute on the smallest building blocks of those systems with precision and craft.</p></li></ul><p>Together, they form a complete picture of what it means to design and engineer modern front-end applications at scale.</p><h2><strong>Framework for approaching front-end system design interviews</strong></h2><p>Many engineers struggle with system design interviews because they approach problems reactively, jumping straight into technical details before establishing a clear structure, or simply just hopping around topics with no clear flow, and risk ending up not sufficiently covering the important areas of the question.</p><p>I know it all too well because that was how I failed my first few system design interviews.</p><p>Therefore, I came up with an easy-to-remember mnemonic,&nbsp;&nbsp;to help me remember how to approach system design questions.</p><p>The  offers a systematic way to think through design questions methodically. It ensures you start with clarity, communicate your reasoning, and cover both breadth and depth.</p><ul></ul><p>Begin by clarifying what problem you are solving and what success looks like.</p><p>Identify both functional and non-functional requirements, what the system must do, and how well it must perform. This is also where you scope the problem to fit the interview.</p><p>For example, when asked to design the Twitter timeline, you might clarify whether the focus is on rendering the feed, handling infinite scroll, or supporting real-time updates.</p><p>Once the goals are clear, outline the high-level structure of the system and describe its responsibilities.</p><p>In front-end design interviews, it‚Äôs common to have the layers for data fetching, state management, and the view (what users see and interact with). You should explain how data flows through these layers, where caching happens, and how user actions propagate changes.</p><p>The goal is to show that you can decompose a complex UI into well-defined subsystems that interact cohesively.</p><p>Describe the main data entities and state in the system.</p><p>With Twitter, that would be the feed, tweets, composer message, etc., and how they relate to one another. A solid understanding of data modeling shows you can design efficient structures for rendering what the user needs to see and interact with.</p><p>Define how different parts of the system communicate.</p><p>This includes both internal interfaces, such as component boundaries and state management APIs, and external interfaces, such as REST or GraphQL endpoints. Each API should consist of the intention, parameters, and response shape.</p><p>After establishing a functional design, discuss the aspects of the system that are unique or complex and how you would improve it for scale and performance.</p><p>This can include performance techniques like DOM virtualization, lazy loading, media optimizations, and more.</p><p>Use the  as a guide to ensure a comprehensive answer‚Ä¶ not strict linear steps.</p><p>Lead the discussion with requirements, then be flexible to dive wherever the conversation or technical complexity requires.</p><ul><li><p>Architecture, Data model, and the Interface (API) are usually discussed as a whole and not strictly in order.</p></li><li><p>Some candidates prefer to design the data model first before the Architecture and API, which might make more sense depending on the question.</p></li><li><p>Lastly, end with optimizations and deep dives, but revisit any of the earlier areas if you‚Äôre discussing new requirements or extending to new features.</p></li></ul><h2><strong>Design Twitter/X Timeline‚Äôs frontend</strong></h2><p>Let‚Äôs apply the RADIO framework to a common front-end system design question:</p><p>‚ÄúDesign Twitter/X Timeline.‚Äù</p><p>By the end of this newsletter, we will have a solid front-end design for implementing a Twitter timeline, including specific optimizations for each part.</p><p>From here on, we‚Äôll refer to the product as ‚ÄúTwitter‚Äù rather than ‚ÄúX‚Äù, since ‚ÄúTwitter‚Äù is more readable and recognizable.</p><p>Let‚Äôs define some core functional requirements of the Twitter timeline:</p><ul><li><p>Users can browse a timeline UI containing a list of tweets</p></li><li><p>Timeline contains text and image-based tweets</p></li><li><p>Users can perform actions on tweets, such as liking and retweeting</p></li><li><p>Users can click on a tweet to view the replies</p></li><li><p>When users scroll to the bottom of the timeline, more tweets load</p></li><li><p>Users can create new tweets</p></li></ul><p>In reality, Twitter‚Äôs timeline supports more formats, such as videos, polls, and other actions, like replying to tweets.</p><p>But let‚Äôs first focus on the core functionality: consuming timeline tweets and creating new tweets.</p><p>Besides functional requirements, we can also discuss how to improve non-functional aspects such as performance, user experience, and accessibility through optimizations and deep dives.</p><p>A well-structured front-end architecture separates concerns across distinct layers, each responsible for a specific set of tasks.</p><p>Before we discuss the layers involved, we have two decisions to make:</p><p>(1) where to render the webpage (client vs. server vs. hybrid),</p><p>(2) how navigations occur (the traditional way of full-page reloads vs. client-side).</p><p>A large-scale application like Twitter can be built using several rendering approaches, each offering different trade-offs among performance, complexity, and infrastructure costs.</p><ul><li><p>server-side rendering (SSR),</p></li><li><p>client-side rendering (CSR),</p></li></ul><p><strong>Server-side rendering (SSR)</strong></p><p>SSR generates the full HTML for a page on the server before sending it to the client.</p><p>This leads to faster first-paint times and better SEO because users and crawlers receive meaningful content immediately. Once the HTML is loaded, the client ‚Äúhydrates‚Äù the page to attach event listeners and to enable interactivity.</p><p>Modern websites rely heavily on SSR to ensure quick perceived performance.</p><p><strong>Client-side rendering (CSR)</strong></p><p>In this model, the server first delivers a minimal HTML shell and a JavaScript bundle.</p><p>The browser then executes the JavaScript to fetch data and render the UI. CSR offers great interactivity once loaded, since subsequent updates can happen entirely on the client without page reloads.</p><p>However, it has slower first load performance and weaker SEO, since users initially see a blank page/spinner until the data is fetched.</p><p>Hybrid architectures combine SSR and CSR to get the best of both worlds.</p><p>The server renders the initial page load for speed and SEO, and the client handles dynamic updates. This is often implemented using frameworks like Next.js and Remix. Twitter could render the first batch of tweets server-side and then fetch and render new tweets client-side as the user scrolls.</p><p>Since timelines are highly personalized, rendering timelines on the server primarily benefits performance instead of SEO. In reality, Twitter likely uses CSR because SSR requires more infrastructure resources, and fetching a timeline on Twitter is already pretty quick (takes less than 1s); the benefits SSR brings don‚Äôt outweigh the complexity required.</p><p>In practice, frameworks like <a href=\"https://www.nextjs.org\">Next.js</a>, <a href=\"https://reactrouter.com/\">React Router</a>, and <a href=\"https://tanstack.com/\">Tanstack Start</a> help you implement these various rendering patterns in your app, even allowing for custom rendering per page.</p><p>Website navigation follows either a single-page application or a multi-page application approach‚Ä¶ and the choice has direct implications for user experience and performance.</p><p>A <strong>single-page application (SPA)</strong> is a web app that loads a single HTML page and dynamically updates its content as the user interacts with the app, without requiring a full page reload. This works by using JavaScript to modify the page URL, fetch data from the server, and update the DOM. This results in fast, seamless navigation and an app-like experience.</p><p>In <strong>multi-page applications (MPA)</strong>, each route corresponds to a separate HTML page. Navigating between pages triggers a full-page reload, which can simplify server-side rendering and improve SEO. Content-heavy sites typically use MPAs, but they can feel slower and less fluid for highly interactive features.</p><p>For a social media app like Twitter, SPAs are preferred, and SPAs rely on CSR.</p><p>More importantly, pages in SPAs can benefit from data in a shared client store. Most users access a tweet from the timeline. In an SPA, the key details of the tweet (text, media) are already loaded on the page and stored in the store; navigation to the tweet details page is instant and requires no server-side interaction. Additional data, such as replies, is fetched after navigation occurs.</p><p>On the other hand, in an MPA, navigation blows away the current page state. Hence, users will experience longer delays when navigating between pages, as they have to wait for the server to finish generating the HTML.</p><p>With rendering and navigation decided, we can define the layers of the Twitter front end, which can be broken down into four main layers: , , , and the .</p><p>The view layer is what users interact with directly.</p><p>It includes the timeline page, tweet detail page, and a tweet composer component. It‚Äôs responsible for rendering data from the store and triggering user actions, such as liking a tweet or composing a new one. Its main goal is to provide a fast, interactive, and accessible user experience while remaining declarative and predictable in how it reacts to data changes.</p><p>In practice, these are your JavaScript frameworks/libraries, such as <a href=\"https://react.dev/\">React</a>, <a href=\"https://vuejs.org/\">Vue</a>, <a href=\"https://svelte.dev/\">Svelte</a>, etc.</p><p>The store acts as the source of truth for client-side data and state.</p><p>It holds all application state in memory: the timeline, tweets, users, composer message, etc. And ensures consistency across different parts of the interface.</p><p>When a user performs an action, such as liking a tweet, the store immediately updates the local state (often optimistically) before synchronizing with the server. It also caches data for quick retrieval, normalizes entities to prevent duplication, and triggers re-renders in the view layer whenever data changes. The store decouples the UI from the backend, keeping the app responsive even during network delays. </p><p>In an SPA, this layer is initialized once and then persisted + updated throughout the session.</p><p>In practice, these are your state management libraries, such as <a href=\"https://redux.js.org/\">Redux</a>, <a href=\"https://zustand.docs.pmnd.rs/\">Zustand</a>, <a href=\"https://jotai.org/\">Jotai</a>, etc.</p><p>The data access layer abstracts away the communication with the backend APIs.</p><p>It handles network requests, response parsing, and caching policies, and defines how data moves between the server and the store. This layer may also include retry logic, pagination handling, and transformations that convert raw API responses into normalized store-friendly structures. By centralizing all network operations here, the front end gains flexibility to change APIs, add batching, or introduce service workers without affecting the rest of the system.</p><p>In an SPA, this layer is also initialized once and persisted throughout the session, just like the store layer.</p><p>The server exposes HTTP endpoints for fetching timelines, posting tweets, and performing engagement actions such as likes or retweets.</p><p>While the server defines the data model and business rules, it relies on the front end to manage presentation logic, caching, and responsiveness. This layered architecture creates a clean separation of responsibilities. The view focuses on presentation; the store manages state; the data access layer handles communication; and the server provides data.</p><p>Since it‚Äôs an SPA, the store and data access layers are initialized on the first load, persisted throughout the session across navigations, and continuously updated as requests and user interactions occur.</p><p>In the architecture layers diagram, several data entities are passed around.</p><p>All of them either originate from the server or are being sent to the server. Hence, we can focus on the server APIs.</p><p>Most of these APIs require authentication as they‚Äôre personalized or specific to the user. It‚Äôs important to note that the user‚Äôs ID shouldn‚Äôt be included in the request and shouldn't be relied on as the source of truth for identity, as it can be spoofed and create a security vulnerability. The user‚Äôs identity should be derived from session cookies sent with the request.</p><p>We‚Äôll assume the user is logged in and omit discussion of authentication and authorization mechanisms, since the focus is on the Twitter product.</p><pre><code>GET /timeline?count=10&amp;cursor=abc</code></pre><p>Returns a list of tweets. Pagination parameters are included so that clients know how to fetch the next page of tweets.</p><p>We‚Äôll look at pagination in more detail in the optimizations section.</p><p>Returns a single tweet. Depending on the desired experience, replies can be fetched using another API (e.g.,) or included in the tweet payload.</p><p>This API is available to the public, but if a session cookie is included in the request, the results can include personalized fields such as whether the user has liked the tweet.</p><p>Create a new tweet. It accepts the following parameters:</p><p>When creating tweets with attached media, the media is first uploaded to blob storage, and an ID is returned to include in the POST request payload. Therefore, we also need an API for uploading media.</p><p>This API creates a tweet for a user; hence, it requires authentication.</p><p>Accepts binary data, uploads into blob storage, and returns a media object ID that can be included when creating a tweet. An alternative is for the API to provide a pre-signed URL that the client can use to upload directly to blob storage.</p><pre><code>POST /tweets/{id}/like\n\nPOST /tweets/{id}/retweet</code></pre><p>Common actions that can be taken on tweets. There are more, but these are the two main ones.</p><p>Next, let‚Äôs look at the core entities that should be modelled in a way that is both efficient for rendering and resilient to frequent updates.</p><p>Similar to backend data models, which focus on efficient storage and relationships, we can model front-end data to prioritize ease of access, normalization, and caching.</p><p>At the center of the timeline is the Tweet entity.</p><p>Each tweet contains a rich set of information:</p><ul><li><p>and possibly media attachments.</p></li></ul><p>On the front end, data is often stored in a normalized form rather than as nested objects.</p><p>This means separating the author data and media into distinct entities, each referenced by an ID. This approach makes updates more efficient. For example, when a user‚Äôs avatar or display name changes, every tweet referencing that user can automatically reflect the update without having redundant data.</p><p>The User entity represents the authors and participants in the timeline.</p><p>It includes metadata such as:</p><ul><li><p>and relationship flags (for example, whether the current user follows them).</p></li></ul><p>Because user data is shared across many tweets and UI surfaces, it‚Äôs often cached at the global application level to avoid repeated network requests and duplicated state.</p><p>The Timeline entity itself represents the ordered feed of tweets.</p><p>It can be thought of as a list of tweet IDs, annotated with pagination information for fetching older or newer tweets.</p><p>Timelines are dynamic; as the user scrolls down, older tweets are fetched and appended to the list, and newer tweets can also be added to the top when the timeline is stale.</p><h3><strong>Normalized store structure</strong></h3><p>These entities live in the client store following a normalized structure.</p><p>Data normalization within client-side stores is the process of structuring data so that each unique entity is stored exactly once, with relationships between entities represented by references rather than nested copies. This concept mirrors relational database design, but applied in the context of a browser application.</p><p>On Twitter, a tweet might include a nested user object for its author, another for a retweet, and yet another for a reply. If these user objects were duplicated across many tweets, updating a single piece of user information (e.g., a display name or avatar) would require updating every instance across the store.</p><p>Normalization prevents this by separating users, tweets, and other entities into their own collections, using their IDs as keys. Entities then store references to other entities via IDs rather than embedding full user data.</p><p>This structure makes updates and caching far more efficient. If a user‚Äôs profile changes, only the corresponding user entry needs to be updated, and every component referencing that user will automatically reflect the change.</p><p>Normalized data also simplifies pagination, deduplication, and cache merging when new results arrive from the server. Most importantly, it keeps the client-side store lean and organized as the app grows in complexity, ensuring data consistency across views and interactions.</p><blockquote><p>Data fetched from the server <strong>doesn‚Äôt need to be in the finalized, normalized form</strong>.</p></blockquote><p>Normalization can be done on the server or the client, depending on where the processing should occur. If a product serves markets where user devices aren‚Äôt powerful‚Ä¶ it might be better, performance-wise, to have the server handle normalization.</p><p>As of 2025, Twitter normalizes timeline data on the server and sends a compact payload to clients.</p><h2><strong>Optimizations and deep dive</strong></h2><p>With the core pieces of the Twitter front-end system laid out, let‚Äôs look at how the app can be optimized:</p><p>We‚Äôll look at general optimizations across the board and zoom in on specific optimizations for each part of the app.</p><p> are the first touchpoint in this process.</p><p>When users open the app or navigate between views, the UI should always provide a clear signal that content is being retrieved, with spinners.</p><p>Twitter uses a single loading spinner, but a better approach would be to use skeleton placeholders‚Äîgray blocks resembling tweet shapes, rather than traditional spinners.</p><p>Skeleton screens preserve layout stability and make loading feel shorter because the visual structure is already in place. As data arrives, the placeholders seamlessly transition into actual content.</p><p> requires a similarly thoughtful approach.</p><p>When something goes wrong, the interface must communicate the issue clearly without breaking immersion. For transient failures, such as a dropped network connection, the timeline can display an inline error bar or toast notification, something non-blocking that encourages the user to retry.</p><p>More severe cases, such as an empty feed because of an API failure, call for a fallback UI that explains what happened and provides recovery options.</p><p>Twitter shows a ‚ÄúRetry‚Äù button alongside a short message instead of a blank page, maintaining context while guiding the user back to normalcy.</p><h3><strong>Performance optimizations</strong></h3><p>Performance is one of the most critical dimensions of front-end system design, especially for an application as dynamic and content-heavy as the Twitter timeline.</p><p>Let‚Äôs look at some performance techniques:</p><h4>Reducing initial page load size through code splitting</h4><p>Code splitting is a key performance optimization technique that allows web applications to load only the JavaScript necessary for the current view, rather than delivering the entire bundle upfront.</p><p>Twitter‚Äôs features, such as timelines, profiles, and notifications, each have distinct dependencies; sending all the code at once would significantly delay the initial load.</p><p>By splitting the code into smaller, route- or feature-specific chunks, the app reduces its initial JavaScript bundle size, improving time-to-first-render and time-to-interactive.</p><p>In Twitter, code splitting is used in the following scenarios:</p><ul><li><p>: Lazily loading only the JavaScript code for the tweet formats shown in the timeline. There are many types of tweets: text, image, video, ads, etc., and each has a different appearance and interactions. Only the code needed for the tweet formats in the timeline is downloaded.</p></li><li><p>: Lazily loading JavaScript code for components that are only shown upon user interaction, e.g., Emojipicker when clicking on the emoji button in the composer, auto-completion of hashtags, and profile card that only appears when hovering over a tag/mention.</p></li><li><p>: Other pages, such as the tweet detail page, can be loaded lazily when a user clicks a tweet, rather than bundled with the timeline code.</p></li></ul><p>This improves performance for the average user, who might never visit certain routes or perform certain actions. Combined with pre-fetching (loading likely future chunks in the background), code splitting provides a balance between speed and flexibility.</p><p>Modern build tools like Webpack, Rollup, and Vite support dynamic imports, which make code splitting straightforward.</p><h4>Optimizing rendering performance through list virtualization</h4><p>The timeline consists of a long list of tweet components, each with rich media and nested interactions.</p><p>Rendering every tweet at once would overwhelm the DOM, so Twitter relies on  that render only the visible items in the viewport, using invisible s so that the scrollbars are still present without actual content.</p><p>As users scroll, off-screen elements are recycled or unmounted to minimize memory usage. This approach keeps frame rates high and prevents layout thrashing.</p><h4>Improving perceived performance through optimistic updates</h4><p>Optimistic updates work by immediately reflecting a user‚Äôs action in the interface before the server confirms the change, creating the illusion of zero latency.</p><p>For actions such as liking, retweeting, or following, this technique ensures that the interface reacts instantly, maintaining the sense of momentum that defines a smooth social media experience.</p><p>When a user likes a tweet, for example, the heart icon fills in and the like count increments right away. Behind the scenes, the client simultaneously sends a request to the server to record the action. If the server responds successfully, nothing more needs to happen, since the UI has already been updated. But if the request fails, the client rolls back to the previous state. This approach hides network latency from the user, making interactions feel immediate even on slow or unreliable connections.</p><p>From an engineering perspective, implementing optimistic updates requires careful state management.</p><p>The client must maintain a consistent local representation of the data and reconcile it with the server‚Äôs eventual response. Using a client store, this is made easier as data is shared across many views (e.g., the same tweet appearing on multiple pages/placements), the store tracks a tweet‚Äôs state in a normalized fashion and has to update a Tweet‚Äôs  value by  and  to ; there are no duplicated instances of tweets to search through and update.</p><h4>Efficient media rendering</h4><p>Every tweet can contain images, GIFs, or videos, and these assets are often the largest contributors to load time and memory usage.</p><p>Hence, it‚Äôs important to optimize how media is loaded, displayed, and recycled to maintain both perceived performance and smooth scrolling.</p><p>The first principle is .</p><p>Media should never be fetched or decoded until it‚Äôs near the user‚Äôs viewport. This minimizes initial bandwidth consumption and prevents layout shifts during scrolling. Modern browsers support the  attribute for images, but more advanced control can be achieved by using the  API, which allows the app to load assets slightly before they appear on screen.</p><p>For videos and GIFs, loading low-resolution thumbnails or poster frames first gives the illusion of instant availability while deferring full playback until user interaction.</p><p>Next comes <strong>media optimization and sizing</strong>.</p><p>Twitter‚Äôs front-end requests media in multiple resolutions and serves the appropriate variant based on device type, screen density, and layout size. This is typically achieved using the  and  elements or equivalent client-side logic. On high-density displays, high-resolution assets are used selectively to maintain visual sharpness without overfetching.</p><p>For videos, adaptive bitrate streaming ensures smooth playback even under varying network conditions, seamlessly switching between quality levels.</p><p>Finally,  plays a significant role.</p><p>Progressive image decoding and blurred low-resolution placeholders (often called LQIP or ‚Äúblurhash‚Äù) give users an instant visual cue that content is loading, reducing the sense of delay. Combined with skeleton loaders and smooth transitions, the feed feels continuously alive, even when data and media are still being fetched.</p><h3><strong>Timeline pagination approaches</strong></h3><p>Two common approaches to pagination in modern web applications are:</p><ul></ul><p>Both are mechanisms for fetching data in chunks, but they differ in how they identify the next set of results and how well they handle dynamic, real-time data.</p><p>It relies on numerical offsets to determine which results to fetch next.</p><p>For example, a request might specify  to get the next ten tweets after the first twenty.</p><p>Offset-based pagination is straightforward and easy to implement, but it becomes inefficient and unreliable as the dataset grows or changes frequently. When new tweets are inserted into the feed, offsets can shift, leading to duplicate or missing items. It also performs poorly for large offsets, as databases must skip an increasing number of records to reach the desired position.</p><p>Cursor-based pagination, on the other hand, uses a unique identifier (often a tweet ID or timestamp) as a cursor to mark the boundary between pages.</p><p>Instead of asking for the ‚Äúnext 10 results after offset 20,‚Äù the client requests ‚Äúthe next 10 results after tweet ID X.‚Äù</p><p>Cursor-based pagination is more stable and efficient because it doesn‚Äôt depend on the dataset‚Äôs size or ordering at query time. It works well in environments where data is frequently updated, such as Twitter‚Äôs timeline, where new tweets appear constantly and older tweets can be deleted or re-ranked.</p><p>For Twitter, <strong>cursor-based pagination is the rational choice</strong>.</p><p>It aligns naturally with the timeline's chronological nature, avoids inconsistencies caused by real-time updates, and scales efficiently across millions of records. It also enables bidirectional navigation, allowing users to load newer tweets and scroll down (to fetch older ones) without re-fetching or skipping content.</p><p>By combining cursor-based pagination with techniques such as infinite scrolling and background prefetching, Twitter can maintain a seamless, continuous feed experience that feels instantaneous, even as large volumes of data are loaded behind the scenes.</p><h3><strong>Tweet formatting/rich text</strong></h3><p>Tweets on the timeline are more than just plain text; there are often mentions, links, hashtags, etc., within the block of text.</p><p>Hence, the tweet content must be stored in a format that also captures this information, and such a format is known as .</p><p>Let‚Äôs use an example tweet.</p><blockquote><p>‚ÄúCheck out @greatfrontend‚Äôs website at https://www.greatfrontend.com #webdev‚Äù</p></blockquote><p>‚Ä¶ and see how we can store it as a formatted string using various approaches:</p><p>This approach involves saving the text as an HTML string.</p><p>The tweet renderer can parse this string with something like the  API. It then walks the parsed tree and maps each custom tag to a corresponding UI component.</p><p>Markup is intuitive and familiar. Most engineers can read and understand it quickly. It mirrors how the browser already interprets hierarchical text, and it‚Äôs easy to copy or render in environments that are already HTML aware.</p><p>However, because this is a string-based format, it is prone to malformed markup or escaping issues. It can also make transformations, editing, or range updates more difficult because operations require string manipulation rather than structured edits.</p><p>Sanitization and security also require careful handling if the content is not fully trusted.</p><p>This approach involves saving the text as a tree-like object, where each object can have children, which are an array of objects.</p><p>The renderer recursively walks the tree and chooses how to present each node. This is conceptually similar to rendering parsed HTML, but the format is tailored to the app‚Äôs specific rich text model.</p><p>This structure is easy to traverse, transform, and validate. Editing becomes predictable because each piece of text or formatting exists as its own object. It also avoids string parsing and maps cleanly to a React component hierarchy.</p><p>However, the representation is verbose. Serializing or storing trees often takes more space than storing plain text with annotations. It can also be more complex to generate and maintain, especially if the text supports many nested or overlapping styles.</p><p>This approach provides a plaintext version of the tweet, along with additional metadata indicating which portions of the text represent special elements such as URLs, hashtags, mentions, etc.</p><p>The plain text remains intact, while the entity list acts as an overlay that the renderer uses to determine which spans receive special treatment.</p><p>This approach is compact and efficient. It stores the text exactly once. The model is simple to delete, index, or paginate. Range-based systems also work well for basic inline styles and are a good fit for storage or API payloads.</p><p>However, managing ranges becomes tricky when the text changes, as inserting or deleting characters requires updating the affected indexes. Range collisions and nested structures are harder to express. Systems that allow overlapping formatting usually require more sophisticated bookkeeping.</p><p>There is no best format, and every format can work. That said, entity ranges are usually the preferred option. The payload is compact; the text is preserved verbatim, and the back end can easily generate or consume the model. The plaintext version is also useful for displaying within SEO text/description fields.</p><p>Accessibility is an essential part of front-end system design, especially for an application as widely used as Twitter.</p><p>The goal is to ensure that every user, regardless of ability, can effectively consume, navigate, and interact with the timeline.</p><h4>Semantic tags and ARIA attributes</h4><p>At the foundation, semantic HTML provides the structure that assistive technologies rely on.</p><p>Each tweet, button, and input element must use appropriate HTML tags and ARIA roles so that screen readers can interpret them accurately. For example, tweets can be represented as list items within a feed, and actions such as like, retweet, and reply should be actual buttons, not generic  with click handlers. This ensures that keyboard navigation and assistive software can interact with the app predictably.</p><p>DOM is also sprinkled with  that are invisible to non-screen reader users but read out by screen readers:</p><ul><li><p>Timeline list container is labelled with <code>aria-label=‚ÄùHome timeline‚Äù</code></p></li><li><p>Tweet actions appear as icon buttons, but they have  attributes specified on them to show the current values and the action they afford.</p></li></ul><p>Twitter provides several keyboard shortcuts to help users navigate the feed and perform actions on tweets. Hitting the  keys reveals the list of keyboard shortcuts.</p><p>Focus management is about how users navigate the interface using a keyboard or assistive technology.</p><p>A well-designed focus system lets users reach every interactive component logically, predict focus transitions, and always know their location in the interface.</p><p>Native elements like , and  come with built-in focus behavior and accessibility metadata, so they should always be preferred over generic  or  elements with click handlers. For non-semantic interactive components, developers can use the tabindex attribute to make elements focusable or remove them from the tab order when hidden. Setting  allows an element to receive focus naturally, while  lets it be focused programmatically without cluttering the tab sequence.</p><p>Each tweet‚Äôs DOM element has a  added to it so that every tweet can be focused. In conjunction with keyboard shortcuts, the app knows which tweet to perform actions on when the relevant keys are pressed.</p><p>Twitter also adds skip links near the start of the document order and points to the timeline. It‚Äôs typically visually hidden by default and becomes visible when focused. This ensures that screen reader and keyboard users can reach it immediately without having to tab through other navigation items that appear earlier in the DOM. In contrast, sighted users don‚Äôt see an out-of-place button.</p><pre><code>&lt;button aria-label=‚ÄùSkip to home timeline‚Äù role=‚Äùbutton‚Äù&gt;&lt;/button&gt;</code></pre><p>Lastly, consistent visual indicators such as outlines, shadows, or subtle color changes help users identify which element currently has focus. These cues must be clearly visible against all background themes and respect user preferences for reduced motion.</p><p>Designing the Twitter timeline is a complex front-end system design challenge that brings together concepts from rendering architecture, data management, performance optimization, and user experience.</p><p>Each layer of the system must balance responsiveness, performance, and accessibility to deliver a fast, intuitive interface that feels effortless despite the complexity.</p><p>üëã I‚Äôd like to thank  for writing this newsletter!</p><p>Plus, don‚Äôt forget to check out his site and social media:</p><p>, their biggest sale of the year. Don‚Äôt miss out on this opportunity!</p><p>This week, I‚Äôll write about <em>‚ÄúHow a stock exchange achieves ultra-low latency at scale.‚Äù</em></p><p>There‚Äôs just one catch: <strong>this will be exclusive to my golden members.</strong></p><p>When you upgrade, you‚Äôll get:</p><ul><li><p><strong>High-level architecture of real-world systems.</strong></p></li><li><p>Deep dive into how popular real-world systems actually work.</p></li><li><p><strong>How real-world systems handle scale, reliability, and performance.</strong></p></li></ul><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 190,001+ readers strong, very close to 191k. Let‚Äôs try to get 191k readers by 30 November. Consider sharing this post with your friends and get rewards.</p>","contentLength":37920,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/5b0adb2b-8df2-44ca-926a-1178d3eff0f1_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"A Systematic Breakdown of 15 Cloud Architecture Pitfalls","url":"https://newsletter.systemdesign.one/p/cloud-system-design","date":1763906638,"author":"Neo Kim","guid":37,"unread":true,"content":"<p>Cloud computing has changed the way we build and run systems:</p><p>It gives us managed services that improve reliability. But this flexibility also means small mistakes can spread quickly. One incorrect setting, a missing tag policy, or a bad scaling rule can affect many environments at once. These issues can quickly become expensive or risky.</p><p>Many teams run into ‚Äúsimilar‚Äù problems‚Ä¶ not because they‚Äôre careless, but because cloud platforms behave differently from traditional on-premise systems.</p><p>This newsletter will walk you through 15 common pitfalls in cloud environments.</p><ul><li><p>The pitfall - quick example or pattern</p></li><li><p>Why it happens - what design mistake causes it</p></li><li><p>Architectural impact - which parts of the system it affect</p></li><li><p>How to prevent it - ways to avoid it</p></li></ul><p>Design systems that adapt to change instead of breaking under it‚Ä¶ and build systems that stay reliable even when things go wrong.</p><p>My favorite <a href=\"https://datacamp.pxf.io/e192Xj\">Black Friday deal</a> is here! DataCamp is offering  for 600+ hands-on courses, certifications, and career tracks. </p><p>Explore their top tracks to boost your career in 2026!</p><p>Or learn some of the most in-demand skills:</p><p>DataCamp makes learning practical with:</p><p>‚úÖ Industry-recognized certifications</p><p>I want to introduce <a href=\"https://www.linkedin.com/in/magdalena-wojnarowska-pietrzak/\">Magdalena</a> as a guest author.</p><p>She‚Äôs an infrastructure and cloud architect passionate about automation and security.</p><p>Check out her blog and social media:</p><p>You can learn in depth about the most common cloud pitfalls and how to avoid them with ‚Äú<strong>Mind the Gap: Most Common Cloud Mistakes</strong>‚Äù.</p><p>You‚Äôll also get a 50% discount for the ebook on Gumroad when you use the code‚Äì<a href=\"https://mwojnarowskapietrzak.gumroad.com/l/mindthegap/SYSTEMDESIGN\">SYSTEMDESIGN</a>. (Valid until 6 December.)</p><p>Orphaned resources are cloud assets that are no longer in use but continue to run.</p><ul><li><p>unattached storage disks,</p></li></ul><p>They pile up over time, cost ‚Äúmoney‚Äù, and clutter dashboards‚Ä¶ thus making it harder to track what‚Äôs important.</p><p>A typical example is running a proof-of-concept on a large, expensive virtual machine and forgetting to shut it down or delete it. The VMs keep running, and the bill keeps growing until someone notices it.</p><p>Cloud makes it easy to create new resources. But deleting them is usually a manual task‚Äîand people forget. As every resource has a cost, even tiny leftovers add up. If left untracked, nobody knows what can be safely removed.</p><p>Orphaned resources make it hard to understand where money is going. They can also create security risks. For example:</p><ul><li><p>A forgotten test VM might still hold credentials.</p></li><li><p>An old IP address might still be allowed in firewall rules.</p></li><li><p>A leftover snapshot might contain sensitive data.</p></li></ul><ul><li><p>Use tags: owner, purpose, and expiry date.</p></li><li><p>Automate the cleanup of unused or expired resources.</p></li><li><p>Send regular cost reports grouped by tags and resource age.</p></li><li><p>Add cleanup to every project‚Äôs ‚Äúdone‚Äù checklist.</p></li><li><p>Schedule automatic scans to find and remove old resources.</p></li></ul><p>Forgotten resources quietly waste money. While configuration mistakes break systems quickly.</p><p>Misconfiguration is one of the most common causes of cloud incidents.</p><p>A single wrong setting or missing rule can affect reliability, security, or cost. And its impact often spreads long before anyone notices.</p><ul><li><p>Encryption not turned on for storage</p></li><li><p>A storage bucket accidentally left public</p></li><li><p>Copy-pasting templates without checking differences</p></li></ul><p>When these patterns are automated, a single mistake amplifies across multiple accounts or regions.</p><p>Cloud services offer hundreds of settings: permissions, scaling rules, encryption options, network controls, and so on.</p><p>Teams often rely on defaults or copy settings between environments without reviewing them. When Infrastructure as Code () templates contain errors, those errors propagate every time the template is used.</p><p>And fast release cycles make it worse; there‚Äôs rarely enough time for reviews.</p><ul><li><p>Expose sensitive services to the public</p></li></ul><p>Because cloud automation pushes changes at scale, a tiny error can impact an entire production environment within minutes.</p><ul><li><p>Keep configurations in version control; review and test every change.</p></li><li><p>Use policy-as-code tools (Terraform Validate, AWS Config, Azure Policy).</p></li><li><p>Detect configuration drift and automatically enforce compliance.</p></li><li><p>Keep clear configuration baselines for production and non-production systems.</p></li><li><p>Monitor continuously for anomalies or security alerts.</p></li></ul><p>Strong configurations are helpful, but they only work if teams communicate effectively. The next challenge isn‚Äôt in the code‚Ä¶ it‚Äôs in coordination.</p><h2><strong>3. Poor Communication Between Teams</strong></h2><p>Cloud systems rely on effective coordination among development, operations, security, networking, and finance teams.</p><p>If they don‚Äôt share information clearly, assumptions drift, ownership becomes unclear, and issues surface late in production.</p><ul><li><p>A networking team updates routing rules for compliance, but doesn‚Äôt inform data engineers who depend on that network.</p></li><li><p>Pipelines fail the next day - not because of a bug, but because one team didn‚Äôt know what another had changed.</p></li></ul><p>Cloud projects involve many specialties:</p><ul><li><p>Developers focus on features.</p></li><li><p>Operations focus on reliability.</p></li><li><p>Security focuses on compliance.</p></li></ul><p>Each team uses different tools and has different priorities.</p><p>If teams don‚Äôt share information clearly, critical details get lost. Teams may not know which workloads are important or how specific changes impact cost or performance.</p><p>Poor communication causes:</p><ul><li><p>Inconsistent architecture</p></li><li><p>Pipelines that fail to integrate</p></li><li><p>Security exceptions approved without full context</p></li><li><p>Conflicting Identity and Access Management () roles</p></li></ul><p>During incidents, no one is sure who owns what, which slows down the response and recovery.</p><ul><li><p>Define clear ownership and contact points for each system or account.</p></li><li><p>Keep shared documentation and Architecture Decision Records ().</p></li><li><p>Hold cross-team reviews for key infrastructure or cost changes.</p></li><li><p>Encourage a ‚Äúyou build it, you run it‚Äù mindset.</p></li><li><p>Use consistent tagging, naming, and shared communication channels (shared dashboards and so on).</p></li></ul><p>Communication gaps often cause teams to take shortcuts, such as relying on a single tool to solve every problem. But that approach can backfire!</p><h2><strong>4. Believing One Tool Solves Everything</strong></h2><p>Many teams assume that a single cloud platform, monitoring tool, or automation system can handle every need.</p><p>It feels efficient at first:</p><blockquote><p>One interface, one workflow, one place to learn.</p></blockquote><p>But as systems grow, this approach limits flexibility‚Ä¶ and creates more problems than it solves.</p><ul><li><p>A team standardized on a single deployment tool that worked well for virtual machines (but didn‚Äôt support containers).</p></li><li><p>And developers began writing custom scripts to fill the gaps, and environments drifted apart.</p></li><li><p>So tracking changes became difficult.</p></li></ul><p>Cloud tools evolve fast, and many are marketed as ‚Äúall-in-one‚Äù solutions.</p><p>Teams under pressure to simplify or reduce costs often choose a single tool for everything: observability, CI/CD, security, deployments, and so on.</p><p>At first, it seems easier to manage and train people on a single platform. But it becomes difficult over time:</p><ul><li><p>The tool doesn‚Äôt scale across many accounts.</p></li><li><p>It doesn‚Äôt work well in hybrid or multi-cloud environments.</p></li><li><p>It lacks the features needed for new services.</p></li></ul><p>What started as ‚Äúsimplicity‚Äù ends up as inflexibility.</p><p>Relying on one tool can cause:</p><ul><li><p>Poor multi-cloud or hybrid support</p></li><li><p>Large parts of the system break if the tool fails</p></li><li><p>Missing metrics, especially for newer workloads</p></li><li><p>Slow innovation, as teams wait for features that may never arrive</p></li></ul><ul><li><p>Choose tools based on fit, not just standardization.</p></li><li><p>Review tool usage regularly as your architecture evolves.</p></li><li><p>Design for interoperability using APIs and modular pipelines.</p></li><li><p>Limit the number of tools you use. Also, group tools by purpose (monitoring, IaC, deployment). Plus, assign clear ownership.</p></li></ul><p>Relying just on one tool is risky. However, a deeper issue arises when there‚Äôs a lack of understanding about how the cloud actually works.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://newsletter.systemdesign.one/p/cloud-system-design?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>Share this post &amp; get rewards for the referrals.</p></div></div><h2><strong>5. Weak Understanding of Cloud Mechanics</strong></h2><p>Many cloud problems come from a lack of understanding of how the cloud actually works: from billing and scaling to data movement and networking.</p><p>Without this knowledge, even well-designed systems can become expensive or unreliable.</p><ul><li><p>A team built a data analytics job that copied large datasets between regions daily.</p></li><li><p>They assumed transfers were free, just like in their on-premise data center.</p></li><li><p>Their first month‚Äôs bill showed network charges higher than the compute costs.</p></li></ul><p>Cloud platforms hide many infrastructure details.</p><p>Although this makes things easier, it doesn‚Äôt remove responsibility. Teams used to traditional environments expect fixed costs and predictable performance.</p><p>But in the cloud, everything is usage-based:</p><ul><li><p>How much data you process</p></li></ul><p>Services scale automatically, but not always as teams expect. There‚Äôs often a gap between how a service is described and how it behaves with real workloads.</p><p>A weak understanding of cloud mechanics can cause:</p><ul><li><p>Systems that scale too slowly or too fast</p></li><li><p>Inefficient storage usage (e.g., active data on slow, cheap tiers)</p></li><li><p>Silent cost growth in data transfer and API calls</p></li><li><p>Misunderstanding reliability features like regional redundancy or eventual consistency</p></li></ul><p>These issues can cause high costs, data loss, or availability risks.</p><ul><li><p>Use cost calculators and load tests early.</p></li><li><p>Build and test small prototypes before scaling up.</p></li><li><p>Understand SLAs, scaling mechanisms, and regional data flows.</p></li><li><p>Add platform training to onboarding and architecture reviews.</p></li><li><p>Test how systems behave during failures or heavy load.</p></li></ul><p>Misunderstanding the platform often leads to copying old data-center patterns in the cloud. This could limit the benefits of using the cloud.</p><h2><strong>6. Rebuilding On-Prem in the Cloud</strong></h2><p>A common and costly mistake is treating the cloud like a traditional data center.</p><p>Many teams migrate their virtual machines, networks, and firewalls without changing the design. This feels safe and familiar. But it overlooks key cloud benefits, such as elasticity, automation, and managed services.</p><ul><li><p>A company copied its data-center VMs and networks directly into the cloud.</p></li><li><p>Costs increased, updates remained manual, and scaling was limited.</p></li><li><p>Only after moving to managed databases and autoscaling groups did the system become efficient.</p></li></ul><p>Most migrations happen under time pressure.</p><p>Teams take the fastest route‚Ä¶ and lift their on-premise environment into the cloud to avoid redesign work (‚Äúlift-and-shift‚Äù strategy).</p><ul></ul><p>‚Ä¶ continue, it brings technical debt into a platform that charges for every resource used by the unit (time/call/execution).</p><p>Rebuilding on-premise in the cloud can cause:</p><ul><li><p>Poor use of managed services</p></li><li><p>High costs from fixed-size infrastructure</p></li><li><p>Limited scalability because of manual maintenance</p></li><li><p>Weak resilience compared to cloud-native designs</p></li><li><p>Overreliance on network-based security instead of identity-based access</p></li></ul><p>These issues make systems harder to scale, more expensive to run, and slower to recover.</p><ul><li><p>Modernize step by step: start small and iterate slowly.</p></li><li><p>Adopt cloud-native services where they reduce effort.</p></li><li><p>Use identity and roles as the primary security boundary.</p></li><li><p>Design for flexibility and modularity, so the system evolves.</p></li><li><p>Run pilot projects to test scaling, recovery, and automation patterns.</p></li></ul><p>After moving to the cloud, structure and visibility become critical. Without proper governance, even small setups can turn chaotic.</p><h2><strong>7. Missing Governance: No Tags, No Naming, No Monitoring</strong></h2><p>Without clear governance, even well-built cloud environments quickly become confusing.</p><p>When resources have unclear names, lack tags, or are not monitored, costs increase and ownership becomes unclear.</p><p>Teams can‚Äôt answer simple questions like:</p><blockquote></blockquote><ul><li><p>A company discovered that half of its monthly bill was because of untracked compute instances and unused storage.</p></li><li><p>None had tags or meaningful names.</p></li><li><p>It took weeks to trace the ownership. </p></li><li><p>And only after adding tagging rules and cost dashboards could they safely remove the unnecessary resources.</p></li></ul><p>Early in the cloud adoption process, teams focus on speed rather than structure.</p><p>They create resources fast, without standard tags or names. And as the environment grows across projects and regions, these missing details become a problem:</p><ul><li><p>Monitoring and cost alerts are added too late</p></li><li><p>Nobody knows which VM supports production</p></li><li><p>Or which storage bucket belongs to a retired project</p></li></ul><p>As a result, the environment becomes difficult to understand.</p><p>Missing governance leads to:</p><ul><li><p>Poor visibility and unclear ownership</p></li><li><p>Security teams unable to trace exposed resources</p></li><li><p>Finance unable to link costs to owners or projects</p></li><li><p>Untracked resources piling up and wasting money</p></li><li><p>Automation tools failing because of missing tags and names</p></li></ul><ul><li><p>Use clear and consistent naming conventions.</p></li><li><p>Treat governance as core architecture, not as cleanup work.</p></li><li><p>Centralize monitoring, cost reporting, and alerting across all accounts.</p></li><li><p>Use governance tools (AWS Config, Azure Policy, GCP Organization Policies).</p></li><li><p>Define mandatory tagging standards: owner, environment, purpose, cost center, and expiry.</p></li></ul><p>Good governance improves visibility. But it doesn‚Äôt guarantee security, especially when teams rely too much on network boundaries and overlook identity and access controls.</p><h2>8. Treating Network as the Main Security Layer</h2><p>In traditional data centers, the network perimeter is usually the primary line of defense:</p><blockquote><p>You could isolate systems, add firewalls, control traffic, and so on.</p></blockquote><p>But in the cloud, that model no longer works. Identity and permissions now define the real security boundary‚Ä¶ yet many teams still rely mainly on network rules and overlook identity.</p><ul><li><p>A company isolated workloads in private subnets.</p></li><li><p>But they gave broad permissions to their automation tools.</p></li><li><p>When a CI/CD credential was compromised, attackers could access data directly through APIs.</p></li><li><p>The network stayed closed, but identity access was enough to cause damage.</p></li></ul><p>Teams coming from on-premise environments often bring the same perimeter-based mindset into the cloud.</p><ul></ul><p>This approach may work in small setups, but cloud environments are constantly evolving. Many services, such as serverless functions and managed databases, live outside fixed networks altogether.</p><p>Relying too heavily on network controls causes:</p><ul><li><p>Strict rules blocking valid communication</p></li><li><p>Misconfigured firewalls exposing internal systems</p></li><li><p>No protection against identity-based attacks (the most common breach type)</p></li></ul><ul><li><p>Treat identity (IAM roles, service accounts, least-privilege policies) as the first layer of defense.</p></li><li><p>Use network rules as an extra layer of protection, not as the core security layer.</p></li><li><p>Apply zero-trust principles: verify every request based on identity and context, not location.</p></li><li><p>Review access paths regularly and use automated policy validation tools.</p></li><li><p>Prefer private endpoints and managed connectivity for sensitive services over custom VPNs.</p></li></ul><p>Solid security foundations are essential, but resilience also requires flexibility. Static designs fail‚Ä¶ as soon as conditions change.</p><h2><strong>9. Static Designs That Don‚Äôt Handle Change</strong></h2><p>Cloud systems are built to adapt:</p><blockquote><p>Services evolve, usage changes, and regions shift.</p></blockquote><p>But many teams design architectures as if nothing will ever change. Static, rigid designs may look stable at first‚Ä¶ yet they fail the moment workloads, connections, or platform features change.</p><ul><li><p>An application launched with fixed VM clusters and manual scaling.</p></li><li><p>During seasonal peaks, performance dropped sharply, so engineers had to add capacity manually.</p></li><li><p>After switching to autoscaling (with limits) and serverless processing, the system automatically handled the load and ‚Äúreduced costs‚Äù during quiet periods.</p></li></ul><p>Teams coming from on-premise environments often use a ‚Äúset it and forget it‚Äù mindset. They keep the old patterns:</p><ul></ul><p>Sometimes internal rules (long approval queues or fear of automation mistakes) also slow down change. The result is a system that works for current traffic but can‚Äôt grow with demand.</p><ul><li><p>Crashes during traffic spikes</p></li><li><p>Slow recovery during incidents</p></li><li><p>Wasted money during low traffic</p></li><li><p>Infrastructure that becomes outdated as services evolve</p></li></ul><p>They also discourage testing and experimentation.</p><ul><li><p>Design for elasticity: autoscaling, serverless, and event-driven models.</p></li><li><p>Use infrastructure as code for flexible and repeatable deployments.</p></li><li><p>Review the architecture regularly as usage and platform features evolve.</p></li><li><p>Introduce chaos testing to validate how systems respond to failure.</p></li></ul><p>A system that can‚Äôt adapt is one problem. But even flexible systems waste money if development environments mirror production too closely.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://newsletter.systemdesign.one/p/cloud-system-design?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>Share this post &amp; get rewards for the referrals.</p></div></div><h2><strong>10. Treating Development Like Production (Same Tiers, Policies, Permissions)</strong></h2><blockquote><p>Development and production should NOT look identical.</p></blockquote><p>When both environments use the same instance sizes, retention rules, and permissions, the result isn‚Äôt better control; it‚Äôs unnecessary cost and reduced flexibility.</p><p>Development should be a safe environment for testing ideas‚Ä¶ not a complete replica of the production environment.</p><ul><li><p>Every test stack used the same compute tiers, observability tools, and storage.</p></li><li><p>A company cloned its entire production setup into development to ‚Äúkeep things consistent‚Äù.</p></li><li><p>Within months, development costs reached almost half of the production costs.</p></li></ul><p>Teams often copy production settings into dev or test environments to avoid surprises. But in the cloud,</p><ul><li><p>Strict production policies</p></li></ul><p>‚Ä¶ and identical configurations slow development, increase costs, and limit experimentation.</p><p>Treating all environments the same leads to:</p><ul><li><p>Broad permissions exposing secrets</p></li><li><p>High costs from expensive dev resources</p></li><li><p>Strict rules blocking quick tests or experiments</p></li><li><p>A slow, rigid setup that adds little value between releases</p></li></ul><ul><li><p>Use smaller, cheaper instance types for non-production environments.</p></li><li><p>Set different permission levels for development and production.</p></li><li><p>Define separate budgets, data retention rules, and access policies.</p></li><li><p>Use anonymized or synthetic data instead of production data.</p></li><li><p>Tune monitoring and alerts for each environment‚Äôs purpose.</p></li><li><p>Automate provisioning with clear parameters per environment (dev, test, prod).</p></li></ul><p>As environments grow, tracking costs across accounts becomes difficult. Without visibility, spending becomes hard to explain and control.</p><p>Cloud costs become clear only when someone actually tracks them‚Ä¶</p><p>Many teams run for months without knowing where their money is going. And this can quickly erode trust from finance and leadership.</p><ul><li><p>A project stored large datasets across many buckets.</p></li><li><p>Without cost reports, nobody noticed that storage and data transfer costs were growing faster than compute.</p></li><li><p>After adding tagging rules and cost dashboards, the team identified unused datasets and significantly reduced the monthly bill.</p></li></ul><p>Cloud billing provides detailed information but involves complexity.</p><p>Costs spread across services, regions, and accounts. And without proper structure, it‚Äôs hard to understand who is spending what.</p><ul></ul><p>If you prioritize speed without tracking costs, it creates ‚Äúmystery spend‚Äù, where everyone assumes someone else is monitoring usage.</p><p>Poor cost visibility leads to:</p><ul><li><p>Slow or stalled optimization efforts</p></li><li><p>Over-provisioned or idle resources staying online</p></li><li><p>Design and scaling decisions disconnected from financial reality</p></li><li><p>Storage and autoscaling choices that cost more than expected</p></li><li><p>Teams afraid to delete resources because the impact is unknown</p></li></ul><ul><li><p>Assign clear cost ownership to teams.</p></li><li><p>Tag resources by cost center, owner, and environment.</p></li><li><p>Use built-in cost tools (AWS Cost Explorer, Azure Cost Management, GCP Billing Reports).</p></li><li><p>Set budgets, forecasts, and anomaly alerts per project or account.</p></li><li><p>Review cost data regularly in architecture and operations meetings.</p></li><li><p>Add dashboards that show spend trends alongside performance metrics.</p></li></ul><p>Poor cost visibility often hides waste, and one common reason is collecting too much data.</p><h2><strong>12. Collecting Too Much Data</strong></h2><p>Logging and monitoring everything may seem like a safe approach.</p><p>However, it often generates noise, increases costs, and makes real issues harder to identify. The problem isn‚Äôt collecting data; it‚Äôs collecting the ‚Äúright data.</p><ul><li><p>One team enabled full request and response logging to debug an issue.</p></li><li><p>But they forgot to turn it off.</p></li><li><p>Within a month, logs grew to several terabytes, tripling monitoring costs and slowing searches.</p></li><li><p>After adding retention limits and log sampling, they reduced data volume without losing visibility.</p></li></ul><p>Observability platforms make it easy to collect every log, metric, and trace. Without limits or retention rules, systems end up flooding themselves with low-value data. This leads to noise, high costs, and unmanageable dashboards.</p><p>Uncontrolled data collection causes:</p><ul><li><p>High storage and monitoring costs</p></li><li><p>Crowded dashboards and buried alerts</p></li><li><p>Security and compliance risks from excessive log retention</p></li><li><p>Telemetry processing consuming bandwidth or compute</p></li><li><p>Application performance degradation in extreme cases</p></li></ul><ul><li><p>Set different log levels and retention periods per environment.</p></li><li><p>Use sampling and filtering for high-volume metrics or traces.</p></li><li><p>Centralize logs, but enforce quotas and expiration policies.</p></li><li><p>Use structured formats (JSON, key-value) to avoid repeating data.</p></li><li><p>Review logging policies regularly and remove unnecessary sources.</p></li><li><p>Compress or archive historical logs into cheaper storage tiers.</p></li></ul><p>Too much data often leads teams to automate aggressively in an effort to regain control. But without safeguards, it introduces a new set of risks.</p><h2><strong>13. Automation Without Safeguards</strong></h2><p>Automation brings speed and consistency to cloud operations.</p><p>But without safety checks, it can also cause significant damage. The same script that deploys infrastructure efficiently can destroy it if misconfigured.</p><ul><li><p>One team scheduled a nightly cleanup script to remove idle dev resources.</p></li><li><p>A small typo in the ‚Äúdev‚Äù tag filter caused it to delete production resources instead.</p></li><li><p>Adding dry-run checks and tighter permissions would have prevented the mistake.</p></li></ul><p>Teams automate provisioning, scaling, cleanup, and patching to save time‚Ä¶ but under pressure, they often skip validation steps or safety controls.</p><ul><li><p>No dry-run or confirmation mode</p></li><li><p>Scripts running with admin-level permissions</p></li><li><p>Broad filters like ‚Äúdelete all unused resources‚Äù</p></li></ul><p>Once automated, these processes run fast, and errors can spread across accounts or regions before anyone notices.</p><p>Unsafe automation can cause:</p><ul><li><p>Misconfigured infrastructure</p></li><li><p>Recovery delays (automation outruns backups or rollbacks)</p></li><li><p>Reduced trust in automation, pushing teams back to slow manual work</p></li></ul><ul><li><p>Add dry-run or confirmation modes before destructive actions.</p></li><li><p>Limit permissions and filter by tag, environment, or resource ID.</p></li><li><p>Test automation in sandbox accounts or with dummy data.</p></li><li><p>Add approval steps to critical workflows.</p></li><li><p>Log all automation actions and monitor for anomalies.</p></li><li><p>Use version control and define clear rollback procedures.</p></li></ul><p>Automation speeds up workflows‚Ä¶ but it can also mask growing costs associated with data transfer.</p><h2><strong>14. Ignoring Data Transfer and Hidden Costs</strong></h2><p>Cloud data transfer may seem simple, but it often masks significant costs.</p><p>Many teams underestimate how quickly network egress adds up. A small architectural choice can suddenly appear as a significant line item on the monthly bill.</p><ul><li><p>A company replicated video archives between regions every day for redundancy.</p></li><li><p>Soon, the egress charges were higher than the cost of storing the entire dataset.</p></li><li><p>Switching to regional redundancy and using a CDN dramatically reduced transfer costs.</p></li></ul><p>Cloud platforms make moving data easy‚Ä¶ but NOT free.</p><p>It‚Äôs often assumed that internal or inter-service traffic has no cost, just like in traditional networks. But every gigabyte (replication, cross-region backups, and analytics queries) leaving a region or zone adds to the bill.</p><p>Ignoring data transfer leads to:</p><ul><li><p>Inefficient data placement</p></li><li><p>Scalability issues when traffic flows across regions</p></li></ul><ul><li><p>Keep compute and data in the same region.</p></li><li><p>Understand pricing for inter-region, inter-zone, and external transfers.</p></li><li><p>Use caching, CDNs, and data locality patterns to minimize data movement.</p></li><li><p>Compress or deduplicate data before transferring it.</p></li><li><p>Add data-flow and transfer-cost notes to architecture diagrams.</p></li><li><p>Monitor egress with alerts in cost dashboards.</p></li></ul><p>Managing costs and transfers is one side of resilience‚Ä¶ the other is recovery. Weak backup strategies often stay unnoticed until disaster strikes!</p><div data-attrs=\"{&quot;url&quot;:&quot;https://newsletter.systemdesign.one/p/cloud-system-design?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>Share this post &amp; get rewards for the referrals.</p></div></div><h2><strong>15. Weak Backup and Recovery Strategies</strong></h2><p>Backups can create a false sense of safety, especially when they‚Äôre never tested.</p><blockquote><p>It‚Äôs often assumed that, as cloud storage is durable, recovery is guaranteed. </p></blockquote><p>But recovery depends on correct setup, permissions, and validation‚Ä¶ not just snapshots or replication.</p><ul><li><p>A company discovered during an audit that its encrypted backups couldn‚Äôt be restored.</p></li><li><p>Encryption keys had been rotated months earlier.</p></li><li><p>The data was intact but completely inaccessible.</p></li><li><p>A backup you can‚Äôt restore is not helpful at all.</p></li></ul><p>Cloud platforms offer durability and automation, which leads to the assumption that backups ‚Äújust work‚Äù. </p><p>Some common issues that block restores:</p><ul><li><p>Missing or rotated encryption keys</p></li><li><p>Incorrect IAM roles or expired credentials</p></li><li><p>Automation overwriting or deleting snapshots without checks</p></li></ul><p>In reality, recovery depends on configuration, identity, and timing. Without regular restore tests, you never know if recovery works as expected.</p><p>Weak recovery strategies cause:</p><ul><li><p>Incomplete or corrupted backups</p></li><li><p>Backups stored in the wrong region</p></li><li><p>Retention-policy failures that break compliance</p></li></ul><ul><li><p>Run regular restore drills and record the results.</p></li><li><p>Test restores with rotated keys and in separate accounts.</p></li><li><p>Store immutable, cross-region copies of critical datasets.</p></li><li><p>Monitor backup success, size trends, and restore times.</p></li><li><p>Define and track RTO (Recovery Time Objective) and RPO (Recovery Point Objective).</p></li><li><p>Use separate permissions for backup management to prevent accidental deletion.</p></li></ul><p>All these pitfalls share a common lesson: </p><blockquote><p>The cloud is constantly evolving, and only flexible designs can keep up.</p></blockquote><p>Each pitfall explained in this newsletter comes from a misunderstanding‚Ä¶</p><p><em>‚ÄúCloud is a static platform.‚Äù</em></p><p>Instead, it‚Äôs a living system: constantly changing, adapting.</p><p>Good cloud design accepts this reality and prepares for it. It uses automation safely, contains mistakes before they spread, and builds systems that fail gracefully.</p><p>Architects can minimize damage by focusing on:</p><ul><li><p>Clarity over complexity: make ownership and purpose obvious to everyone.</p></li><li><p>Automation with guardrails: automate checks and safety rules, not just deployments.</p></li><li><p>Learning through testing: validate assumptions early and often so incidents don‚Äôt turn into surprises.</p></li></ul><p>Cloud reliability isn‚Äôt about preventing every error. It‚Äôs about preparing for them and designing systems that can survive change.</p><p>üëã I‚Äôd like to thank  for writing this newsletter!</p><p>Plus, don‚Äôt forget to check out her blog and socials:</p><p>Also get a 50% discount for her ebook, ‚Äú<strong>Mind the Gap: Most Common Cloud Mistakes</strong>‚Äù, on Gumroad when you use the code‚Äì<a href=\"https://mwojnarowskapietrzak.gumroad.com/l/mindthegap/SYSTEMDESIGN\">SYSTEMDESIGN</a>. (Valid until 6 December.)</p><p>Next week, I‚Äôll write about <em>‚ÄúHow a stock exchange achieves ultra-low latency at scale.‚Äù</em></p><p>There‚Äôs just one catch: <strong>this will be exclusive to my golden members.</strong></p><p>When you upgrade, you‚Äôll get:</p><ul><li><p><strong>High-level architecture of real-world systems.</strong></p></li><li><p>Deep dive into how popular real-world systems actually work.</p></li><li><p><strong>How real-world systems handle scale, reliability, and performance.</strong></p></li></ul><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 190,001+ readers strong, very close to 191k. Let‚Äôs try to get 191k readers by 29 November. Consider sharing this post with your friends and get rewards.</p>","contentLength":27375,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/f2882928-cf17-47dc-8041-52c11f352214_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Stock Exchange Works","url":"https://newsletter.systemdesign.one/p/stock-exchange-system-design","date":1763463950,"author":"Neo Kim","guid":36,"unread":true,"content":"<ul><li><p><em>I created block diagrams for this newsletter using <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a>.</em></p></li></ul><p>Today is a big day because I‚Äôm excited to announce that the doors are officially open for our brand-new newsletter series...</p><p>INTRODUCING: </p><p>This newsletter series will elevate your software engineering career.</p><blockquote><p><em>‚ÄúI want to ace system design interviews, but don‚Äôt know where to start.‚Äù</em></p><p><em>‚ÄúI want to master system design so I can become good at work.‚Äù</em></p><p><em>‚ÄúIt‚Äôs time. I should learn how big companies engineer their systems.‚Äù</em></p></blockquote><p>Here‚Äôs what you‚Äôll get inside Design, Build, Scale:</p><ul><li><p><strong>High-level architecture of real-world systems.</strong></p></li><li><p>Deep dive into how popular real-world systems actually work.</p></li><li><p><strong>How real-world systems handle scale, reliability, and performance.</strong></p></li></ul><p>And here‚Äôs the best part:</p><p>You‚Äôll get 10x the results you currently get with 1/10th of your time, energy, and effort.</p><p>Your AI tools are only as good as the context they have.  pieces together knowledge from your team‚Äôs GitHub, Slack, Confluence, and Jira, so your AI tools generate production-ready code.</p><h2>What is a Stock Exchange? (The Simple Answer)</h2><p>Imagine the stock market as a farmer‚Äôs market.</p><p>Each stock is like a stall area in the market where people gather to buy or sell a product. More trade brings more PROFIT for the stock exchange through fees. So their job is to facilitate as many ‚Äútransactions‚Äù as possible. Each seller can set up a stall and specify the price they‚Äôre willing to sell for.</p><p>If nobody buys, they might lower their price.</p><p>Buyers want the cheapest price, so they usually go to the stall offering the lowest price first.</p><p>In an ‚Äúideal‚Äù world, buyers would stand in a queue, ordered by the price they‚Äôre willing to pay for fairness. This means buyers offering higher prices stand closer to the front. Plus, a buyer can adjust their position in the queue by changing the price they‚Äôre willing to pay. A trade occurs when a buyer‚Äôs price meets or exceeds a seller‚Äôs price.</p><p>That‚Äôs when the exchange matches them!</p><ul><li><p>BUY orders get sorted in decreasing order (highest bid first).</p></li><li><p>SELL orders get sorted in increasing order (buy as cheaply as possible).</p></li><li><p>The point where they overlap is the market price.</p></li></ul><p>In reality, it‚Äôs much more complicated‚Ä¶ but this is the basic idea.</p><h2>Requirements</h2><p>Don‚Äôt worry, it‚Äôs simple!</p><ul><li><p>An exchange that trades ONLY stocks.</p></li><li><p>Users can place a BUY or SELL order.</p></li><li><p>Also users can cancel their order at any time.</p></li><li><p>Exchange must match buyers and sellers in real time.</p></li><li><p>And restrict the number of shares a user can trade per day.</p></li><li><p>It should also publish market data in real-time.</p></li></ul><p>Exchanges make trading fair and transparent.</p><ul><li><p>New orders = ~50% messages,</p></li><li><p>Executions = ~2% messages.</p></li></ul><h2>Trading Terms </h2><p>An app or site that lets users BUY or SELL, and view prices from an exchange.</p><p>A list of all current BUY and SELL orders for a stock, showing who wants to buy or sell, how much, and at what price.</p><p>A BUY or SELL order where you don‚Äôt set a price. It executes immediately at the current market price, as long as there‚Äôs enough liquidity. Plus, it receives priority in the order book.</p><p>A BUY or SELL order where you choose the ‚Äúexact‚Äù price.</p><ul><li><p>Buy at $100 ‚Üí fills at $100 or lower.</p></li><li><p>Sell at $100 ‚Üí fills at $100 or higher.</p></li></ul><p>The difference between the highest BUY price and the lowest SELL price.</p><blockquote><p>Profit = Sell price - Buy price</p></blockquote><h2>Stock Exchange Architecture</h2><p>An exchange interacts with many ‚Äúexternal‚Äù services:</p><ul><li><p>User data management &amp; tracking.</p></li><li><p>Service for sending emails, text messages, and mobile app notifications.</p></li></ul><p>But let‚Äôs focus on the core design itself‚Ä¶</p><p>Its architecture is asynchronous and event-sourced.</p><p>Here are the three key components of an exchange:</p><ul><li><p>Broker - allows people to buy and sell stocks on the exchange.</p></li><li><p>Gateway - entry point for brokers to send BUY or SELL orders to the exchange.</p></li><li><p>Matching Engine - component that matches buy and sell orders to create trades.</p></li></ul><p>A broker interacts with an exchange to:</p><ul><li><p>Send order requests - place orders, receive status updates, and access trade information.</p></li><li><p>Receive market data - historical data for analysis, stream live trade data, and so on.</p></li></ul><p>A user connects to the broker using REST APIs and WebSockets for real-time data transfer. While brokers use the Financial Information Exchange () protocol to communicate with the gateway.</p><p>FIX is a bidirectional communication protocol for secure data exchange through a ‚Äúpublic network‚Äù. It assigns unique sequence numbers to order messages. Besides, it uses checksums and message length to verify data integrity.</p><p>It receives orders from brokers and converts them into the exchange‚Äôs internal format. Then it sends the trade execution results back to the users. It‚Äôs also possible to put a gateway near exchange servers (<a href=\"https://questdb.com/glossary/exchange-co-location-strategies/\">co-location</a>) for low latency.</p><p>A gateway has three key parts:</p><ul><li><p>Risk Manager - ensures the user has enough funds and blocks any unusual trading activity. Also, it determines exchange fees.</p></li><li><p>Wallet - stores the user‚Äôs funds and assets for trading.</p></li><li><p>Order Manager - assigns sequence numbers for fairness and updates order states. Plus, it sends cleared orders to the matching engine.</p></li></ul><p>Gateway validates an order with the risk manager and wallet service. </p><p>Then it passes the request to the order manager. Think of the  as a lightweight list containing ALL orders: open, cancelled, and rejected ones. It updates the order state and handles cancel requests.</p><p>Order manager assigns a globally increasing sequence number to each order (trade or cancel) and execution fill (for both BUY and SELL).</p><p>A sequence number guarantees:</p><ul><li><p>Ordering for fairness, timeliness, and accuracy.</p></li><li><p>Fast recovery and deterministic replay.</p></li></ul><p>Plus, sequence numbers make it easy to find missing events in both inbound and outbound sequences.</p><p>After clearance, the order manager routes the order to the matching engine.</p><p>Each message type (new order, cancel, trade) has its own topic queue, with its own sequence numbers. This separation helps to:</p><ul><li><p>Maintain order within each stream.</p></li><li><p>Process each message type independently and reduce contention.</p></li></ul><p>And make recovery easier as the exchange could ‚Äúreplay‚Äù events per topic in exact order.</p><p>It verifies sequence numbers, matches BUY and SELL orders, and creates market data based on trades.</p><p>A matching engine has two key parts:</p><ul><li><p>Order Book - an in-memory list of BUY and SELL orders.</p></li><li><p>Matching Logic - matches BUY and SELL orders and sends those trade results as market data.</p></li></ul><p>It keeps an in-memory list of open orders for each stock (symbol). </p><p>There are actually two lists for each stock:</p><ul><li><p>Another one for SELL orders. </p></li></ul><p>Each list gets sorted by price and timestamp in a first-in, first-out (FIFO) manner.</p><p>And each price level gets a separate queue of orders (doubly linked list):</p><ul><li><p>New orders get added at the tail - O(1) time complexity.</p></li><li><p>Filled or canceled orders get removed from the queue using a pointer - O(1) time complexity.</p></li></ul><p>It keeps track of the highest bid and lowest ask for quick matching. For example, BUY@96 and SELL@98.</p><p>Searching through all price levels and linked lists could be slow - O(n). So it uses an index to map order IDs to the order object in memory: order ID ‚Üí pointer (reference). It allows fast cancellations by looking up the order by ID in the index in O(1) and then setting the amount to 0.</p><p>A closed order gets removed from the order book and gets added to the transaction history.</p><p>It checks if a message follows the correct sequence and matches BUY and SELL orders when:</p><p>The same order sequence (input) must always produce the same execution sequence (output). So the matching function must be fast and accurate, and deterministic. </p><p><em><strong>Reminder: this is a teaser of the subscriber-only newsletter series, exclusive to my golden members.</strong></em></p><p>When you upgrade, you‚Äôll get:</p><ul><li><p><strong>High-level architecture of real-world systems.</strong></p></li><li><p>Deep dive into how popular real-world systems actually work.</p></li><li><p><strong>How real-world systems handle scale, reliability, and performance.</strong></p></li></ul>","contentLength":7798,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/d69b02ac-e9a9-4f9c-abb6-c473ce1678d5_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"I Studied How Top 0.1% Engineering Teams Do Code Reviews","url":"https://newsletter.systemdesign.one/p/code-review-best-practices","date":1763199339,"author":"Neo Kim","guid":35,"unread":true,"content":"<ol><li><p>Keep pull requests SMALL, so they‚Äôre easy to understand and review. Plus, small pull requests create fewer problems later.</p></li><li><p>Watch for duplicate &amp; dead code. Remove unused code and abstract logic to avoid duplication.</p></li></ol><ol start=\"3\"><li><p>Good tests prevent regressions and show how code should work. So verify whether the new code has ‚Äúsufficient‚Äù test coverage.</p></li></ol><ol start=\"4\"><li><p>Choose the correct reviewer for each change: code owners or domain experts can quickly catch domain-specific issues. If you assign many reviewers,,, ensure each understands their responsibilities to prevent delays.</p></li><li><p>Write clear pull request descriptions that explain the ‚Äúwhat‚Äù and ‚Äúwhy‚Äù of changes. Also, link relevant tickets and attach screenshots that help reviewers understand the context.</p></li></ol><ol start=\"6\"><li><p>Use a code review CHECKLIST. It could cover design, readability, security, testing, and so on. This ensures consistency in reviews and reduces the chances of missing common issues.</p></li><li><p>Automate easy parts. Use tests, linters, and static analysis to catch errors and style issues. This way, reviewers can focus on logic &amp; architecture.</p></li></ol><p>I‚Äôm happy to partner with  on this newsletter. Code reviews usually delay feature deliveries and overload reviewers. And I genuinely believe CodeRabbit solves this problem.</p><ol start=\"8\"><li><p>Use review metrics to find ‚Äúbottlenecks‚Äù. Measure: review time and bug rates, and pull request size. Then adjust the process based on data to improve speed without sacrificing quality.</p></li><li><p>Review quickly‚Ä¶ but don‚Äôt rush! The goal is to improve code health, not just quick approvals.</p></li><li><p>Keep reviews SHORT. It‚Äôs hard to stay focused after reading 100+ lines of code. If the change is big, break it up into smaller parts or focus on one section at a time to give effective feedback.</p></li></ol><ol start=\"11\"><li><p>Get early feedback on big features to save time later. This helps to catch issues early and makes reviews more manageable.</p></li><li><p>Ask for a review ONLY after tests &amp; builds pass. This prevents wasting the reviewer‚Äôs time on broken code. Besides, it signals the code is stable enough to review.</p></li><li><p>Use review tools effectively to save time - threaded comments, suggested edits, and templates, and so on. The correct setup makes reviews smoother.</p></li><li><p>Watch out for potential bugs &amp; logic mistakes that tests might miss. Think about ‚Äúrace conditions or extreme inputs‚Äù. Human reviewers can often spot bugs that automated tests miss, especially in complex logic.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://newsletter.systemdesign.one/p/code-review-best-practices?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>I‚Äôll send you some rewards for the referrals.</p></div></div></li><li><p>Encourage ALL team members to take part in code reviews. And don‚Äôt let the same people handle all reviews. Rotation spreads knowledge and avoids burnout.</p></li><li><p>You can‚Äôt review code effectively if you don‚Äôt understand what it does. So read the code carefully and run it locally if necessary. </p></li></ol><ol start=\"17\"><li><p>Keep the feedback within the ‚Äúscope‚Äù. If you notice any issues outside the scope of the change, log them separately. This keeps reviews constructive and prevents endless delays.</p></li><li><p>Review in layers: design then details. This approach helps you catch both major and minor issues efficiently.</p></li><li><p>Compare the implementation with the requirements. Ensure it handles acceptance criteria and edge cases and error conditions correctly.</p></li><li><p>Enforce coding standards for CONSISTENCY. Suggest refactoring if the logic is hard to follow.</p></li><li><p>Use AI tools to summarize changes or find issues. It saves time! But use those as a helper... and not a replacement for human reviews.</p></li></ol><p>Guess what? When you open a pull request,  can generate a summary of code changes for the reviewer. It helps them quickly understand complex changes and assess the impact on the codebase. Speed up the code review process.</p><ol start=\"22\"><li><p>Set clear ‚Äúguidelines‚Äù for how reviews get approved. For example, have at least two reviewers for critical code changes.</p></li><li><p>Consider how code performs at scale in ‚Äúperformance-critical‚Äù areas. Look out for things that might cause slowdowns in critical paths - unnecessary loops and so on. Remember: fixing issues is easier during review than in production.</p></li><li><p>Use reviews as an opportunity to share KNOWLEDGE and grow together. Share tips and best practices, especially with junior engineers.</p></li><li><p>Ensure the code handles errors ‚Äúgracefully‚Äù. Functions must deal with null inputs or external call failures without crashing. Good error handling makes the system robust &amp; easy to debug.</p></li><li><p>Adjust practices to fit your team‚Äôs needs. What works at one company might not work for another. Keep experimenting until you find your ideal flow.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://newsletter.systemdesign.one/p/code-review-best-practices?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>I‚Äôll send you some rewards for the referrals.</p></div></div></li><li><p>Always review with the bigger picture in mind. Think about how the change interacts with the codebase. And consider cross-cutting concerns: performance, concurrency, and backward compatibility.</p></li><li><p>It‚Äôs better to clarify‚Ä¶ than to assume. So ask clarifying questions when something is unclear about the change. A simple question can prevent misunderstandings or reveal missing requirements.</p></li><li><p>If possible, run the code locally, especially for complex &amp; critical code changes. Seeing it in action can reveal issues that reading won‚Äôt.</p></li></ol><ol start=\"30\"><li><p>Focus on code correctness &amp; clarity,,, not personal style. If an issue is purely stylistic and not covered by a guideline, consider letting it pass or marking it as a nitpick. Remember, reviews are about improving the codebase.</p></li><li><p>Suggest a solution when pointing out a problem. If a function is complex, propose breaking it into smaller functions or using a design pattern. Reviews are most valuable when they teach‚Ä¶ not just criticize.</p></li><li><p>Consider whether the documentation requires any updates because of the change. An API change may need changes to the API docs or the README file. Ensure everything remains accurate and complete.</p></li></ol><ol start=\"33\"><li><p>Treat code review as a ‚Äúteam effort‚Äù, not a fight. Focus on making the product better rather than proving someone wrong. A friendly tone makes feedback easier to accept.</p></li><li><p>Mention explicitly which comments are essential &amp; which are optional. Label important fixes separately from small ‚Äúnice-to-have‚Äù ideas. This helps the author to prioritize and stay focused.</p></li></ol><p>Bet you didn‚Äôt know‚Ä¶brings instant code reviews directly to your terminal, seamlessly integrating with Claude Code, Cursor CLI, and other AI coding agents. While they generate code, CodeRabbit ensures it‚Äôs production-ready - catching bugs, security issues, and AI hallucinations before they hit your codebase.</p><ol start=\"35\"><li><p>Involve a neutral third party in disagreements over CRITICAL issues - ask a tech lead or architect. Also, create a follow-up task if the problem is outside the current scope.</p></li><li><p>Explain the ‚Äúwhy‚Äù behind your feedback. Understanding the reason behind feedback helps others learn. This way, they‚Äôre less likely to repeat the issue.</p></li><li><p>Secure code protects users and the business. So always think about SECURITY. Be cautious of weak data validation, exposed data, or improper error handling.</p></li><li><p>Be open to discussion when opinions differ. Ask for the author‚Äôs reasoning and listen before insisting. Talking through disagreements often leads to better solutions.</p></li></ol><ol start=\"39\"><li><p>Point out what‚Äôs done well too‚Ä¶ it motivates people to keep doing it. Keep a balance between criticism and appreciation for high morale.</p></li><li><p>Don‚Äôt use code reviews for PERFORMANCE EVALUATIONS! Reviews exist to improve code, not to measure people. When engineers feel safe, they write better code &amp; review honestly.</p></li><li><p>Respond to feedback with curiosity,,, not defensiveness. Treat comments as learning opportunities.</p></li><li><p>Having another set of eyes helps catch mistakes. So make sure someone else reviews ‚Äúevery‚Äù change. Even small changes benefit from peer review.</p></li></ol><p>I could go on and on and on.</p><p>But if those 42 ways aren‚Äôt enough to 10x your code reviews, then probably anything else I say will go in one ear and right out the other.</p><p>As far as AI code reviews to catch bugs, security flaws, and performance issues you write code?</p><p>It brings real-time, AI code reviews straight into VS Code, Cursor, and Windsurf.</p><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 190,001+ readers strong, very close to 191k. Let‚Äôs try to get 191k readers by 21 November. Consider sharing this post with your friends and get rewards.</p>","contentLength":8110,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/55f5464a-1e63-472c-a6fa-8d58539c2561_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"21 Frontend System Design Concepts for Software Engineers","url":"https://newsletter.systemdesign.one/p/frontend-system-design","date":1762855122,"author":"Neo Kim","guid":34,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p>If you‚Äôre coming from the backend, you probably think the frontend is just ‚ÄúHTML, CSS, maybe some JavaScript.‚Äù But honestly? Modern frontend engineering has grown into something much closer to backend system design.</p><p>Just like your APIs need to be fast, scalable, and reliable, frontend apps also have to handle millions of users, load content quickly, and stay observable and secure.</p><p>This newsletter is a quick introduction to frontend system design.</p><p>We‚Äôll take concepts you already know from the backend, like caching, deployment pipelines, observability, and security, and see how they apply in the browser.</p><p>By the end, you‚Äôll see that the frontend isn‚Äôt just about buttons and forms. It‚Äôs about building systems that run right in the user‚Äôs browser.</p><p>Your AI tools are only as good as the context they have.  connects your code, docs, and conversations so Cursor, Claude, and Copilot finally understand your system like your best engineer.</p><p>She‚Äôs a web developer, technical writer, and content creator with a love for frontend architecture and building things that scale.</p><p>Check out her work and socials:</p><p>You‚Äôll often find her writing about web development, sharing UI tips, and building tools that make developers‚Äô lives easier.</p><h2><strong>Rendering &amp; Delivery Models</strong></h2><p>One of the first things to understand is how webpages reach your users.</p><p>The way you build and load them affects how fast, reliable, and smooth your site feels. You can pre-build pages, render them on the server, build them in the browser, or mix these approaches.</p><p>Building web pages works much like a server handles API responses. The trade-offs change depending on when and where the HTML gets generated.</p><p>Let‚Äôs start with pre-built pages and move to fully dynamic ones. We‚Äôll see how each affects speed, scalability, and content freshness.</p><h3><strong>1 Static Site Generation (SSG)</strong></h3><p>Before SSG, websites worked in two fundamental ways. The server either built the page for every request, or the browser built it on the client side. That means:</p><ul><li><p>Every request needed work to generate the page.</p></li><li><p>Pages could get slow if many people visit at once.</p></li><li><p>Caching was tricky, so scaling was hard.</p></li></ul><p>SSG solves this by pre-building the HTML when you deploy your site. The system can fetch data during the build process, even for pages with dynamic content, which means all content is baked into static HTML files before any user visits them.</p><p>During the build process, the framework executes data-fetching code, queries your database, and generates complete HTML files for each route. The framework then uploads them to your CDN or hosting provider.</p><p>When users request a page, they receive a fully formed HTML document immediately, without waiting for server-side processing or client-side data fetching.</p><p>This makes SSG super fast for users because there‚Äôs no rendering delay. The trade-off is that if your content changes, you‚Äôll need to rebuild and redeploy to update the static files, which is why SSG works best for content that doesn‚Äôt change frequently.</p><p>It‚Äôs like preparing API responses in advance; the hard work is done before anyone asks.</p><ul><li><p>Easy to handle millions of users.</p></li><li><p>SEO is better because pages get fully rendered from the start.</p></li></ul><p>Documentation sites, marketing landing pages, or personal blogs where content updates happen through deployments, not user actions.</p><h3><strong>2 Incremental Static Regeneration (ISR)</strong></h3><p>Static Site Generation (SSG) is fast, but what if your content changes frequently? Rebuilding the whole site every time would be a pain.</p><p>That‚Äôs where Incremental Static Regeneration (ISR) comes in.</p><p>Pages are still pre-built, but they can update automatically without a full redeploy.</p><p>You just set a revalidation time; after that period, the next visitor triggers a background rebuild of that specific page on the server, not a full deployment. The old version loads instantly, so users don‚Äôt wait. After regeneration, the new version replaces the cached one. This occurs per page, not site-wide, allowing you to set different revalidation intervals for individual pages.</p><p>It‚Äôs like cached API responses with an expiry timer; users might glimpse an older version until it‚Äôs refreshed, but the update happens quietly behind the scenes.</p><ul><li><p>Just as fast as SSG, but the content stays fresh.</p></li><li><p>Perfect for dynamic websites like blogs or e-commerce sites.</p></li><li><p>Works with CDNs, so updates happen without downtime.</p></li></ul><p>E-commerce product pages where most content (e.g., descriptions or images) is static, but some parts, like prices or stock info, update occasionally.</p><p>ISR keeps the page fresh without full redeploys, while real-time data, such as live prices, can come from APIs.</p><h3><strong>3 Server-Side Rendering (SSR)</strong></h3><p>Server-Side Rendering (SSR) works the other way around: the server builds the page for each request. It fetches the data, generates the HTML, and sends it to the user.</p><p>Unlike Static Site Generation (SSG), which is great for mostly static pages, SSR is useful when content needs to stay fresh or personalised. For example, dashboards, user profiles, or live feeds. Because the system generates pages in real time, they always display the latest data instead of relying on pre-built files.</p><p>Think of it like a regular API endpoint; everything gets computed on demand.</p><ul><li><p>Keeps content fresh and easy to personalise.</p></li><li><p>Perfect for pages that need real-time data or personalised content.</p></li></ul><p> Under heavy traffic, SSR can slow down because it builds each page on demand, but caching can help balance load and speed.</p><p>Social media feeds, admin dashboards, or user-specific pages where content varies by session.</p><h3><strong>4 Client-Side Rendering (CSR)</strong></h3><p>CSR means the browser does most of the work instead of the server. The server sends only a basic HTML page and some JavaScript. The browser then loads the data and builds the page on the fly.</p><p>This approach is useful when you need rich interactivity, real-time updates, or pages that change often based on user actions  - things that static or server-rendered pages can‚Äôt handle easily.</p><p>Think of it like sending raw JSON and letting the client put it together.</p><p> The first page load can be slower because the browser needs to download and run JavaScript before showing the content. Since pages get built in the browser, search engines might not see them immediately, so you might need extra setup like pre-rendering or server-side rendering for better SEO.</p><ul><li><p>Reduces pressure on the server.</p></li><li><p>Makes the app more interactive and responsive.</p></li><li><p>Works best for apps people use for a long time, like dashboards or editors.</p></li></ul><p>Complex apps like Figma, Notion, or Google Docs, where the app is highly interactive and users stay on the page for extended sessions.</p><p>Sometimes, one approach just isn‚Äôt enough.</p><p>Different parts of your app might have different needs. For example, some pages stay mostly the same, while others need fresh or personalised data. That‚Äôs where hybrid rendering comes in.</p><p>It mixes different strategies:</p><ul><li><p>Server-side rendering (SSR) for pages that need live or personalised content,</p></li><li><p>Static site generation (SSG) for pages that rarely change,</p></li><li><p>And client-side rendering (CSR) for sections with lots of interactivity.</p></li></ul><p>Think of it like combining pre-computed API responses with on-demand endpoints - all in the same system.</p><ul><li><p>You get the best of everything: speed, fresh content, and interactivity.</p></li><li><p>Allows you to choose the right approach for each page or component.</p></li><li><p>Reduces overloading the server while keeping content dynamic where needed.</p></li></ul><p>Large-scale apps like e-commerce platforms often combine different rendering strategies:</p><ul><li><p>The homepage and category pages use static generation for speed.</p></li><li><p>Product pages use incremental static regeneration to keep content fresh.</p></li><li><p>User account pages use server-side rendering for personalised data.</p></li><li><p>The shopping cart uses client-side rendering for real-time updates without page reloads.</p></li></ul><h3><strong>6 Content Delivery Networks (CDNs) &amp; Edge Delivery</strong></h3><p>No matter which rendering method you choose, serving content efficiently is super important. CDNs keep copies of your static files on servers worldwide. This lets users download them from a nearby location instead of your main server.</p><p>This is especially useful for global audiences. For example, when someone in India visits a site hosted in the US, the CDN delivers the content from a local server, making it load much faster.</p><p>Edge rendering takes this idea a step further. Instead of just serving static files, it can actually run code or build pages at the edge, closer to the user, which reduces latency even more.</p><p>Think of it like having caches and compute nodes near your users, so requests go to a nearby server instead of your main database.</p><ul><li><p>Faster load times everywhere.</p></li><li><p>Easy to scale to millions of users.</p></li><li><p>Works perfectly with SSG, ISR, SSR, or hybrid setups.</p></li></ul><p>Any globally distributed application. Media sites like The New York Times use CDNs to serve articles instantly worldwide.</p><h2><strong>Performance &amp; Optimisation</strong></h2><p>Now that you understand how your pages get rendered, the next obvious question is, ‚Äú<em>How quickly do they actually load?</em>‚Äù</p><p>Even the most beautiful app can be frustrating if it takes too long to open or lags while being used. In frontend system design, speed really matters.</p><h3><strong>7 Web Performance Metrics</strong></h3><p>To really understand your app‚Äôs speed, there are a few key metrics you should watch closely:</p><ul><li><p><strong>TTFB (Time to First Byte): </strong>The time it takes for your browser to get the first piece of data back from the server or CDN after making a request.</p></li><li><p><strong>FCP (First Contentful Paint):</strong> The moment when something first appears on the screen, like text, an image, or a button, so the user knows the page is loading.</p></li><li><p><strong>LCP (Largest Contentful Paint):</strong> The time it takes for the main part of the page, like a large image or headline, to fully appear on the screen.</p></li><li><p><strong>CLS (Cumulative Layout Shift):</strong> It measures how much the page layout jumps around while loading, like when text or buttons suddenly shift because images or ads are still loading.</p></li></ul><p>These are basically the frontend versions of response time, throughput, and latency in backend systems. It‚Äôs important to keep a close eye on them; users can notice even minor delays of a few hundred milliseconds.</p><ul><li><p>You can spot slow pages before users even notice.</p></li><li><p>Improves engagement and reduces bounce rates.</p></li><li><p>Helps guide your optimisations for a smoother experience.</p></li></ul><p>E-commerce sites must optimise for LCP (product images) and CLS (avoid layout shifts during checkout). News sites focus on FCP to show headlines quickly.</p><p>Of course, fast pages aren‚Äôt just about metrics; they‚Äôre also about smart resource management.</p><p>Not everything on a page needs to load immediately. Lazy loading means loading heavy assets, like images, videos, or big components, only when they‚Äôre actually needed.</p><p>This works by using techniques like the<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API\"> Intersection Observer API</a> or conditional imports, which tell the browser to fetch those resources only when they come into view or are triggered by user interaction.</p><p>It‚Äôs like fetching extra data from an API only when the user asks for it.</p><ul><li><p>Cuts down the initial load time.</p></li><li><p>Makes the pages feel faster and smoother.</p></li><li><p>Saves bandwidth for users who don‚Äôt need everything immediately.</p></li></ul><p>Image-heavy sites like Pinterest or Instagram use lazy loading extensively; images below the fold don‚Äôt load until you scroll.</p><h3><strong>9 Service Workers &amp; Caching</strong></h3><p>Once you‚Äôve optimised loading, you can make your app faster and more reliable using service workers and caching.</p><p>Service workers are background scripts that run in a separate thread from your main web page. They can intercept network requests and cache important files or data, helping your app load faster and even work offline.</p><p>Think of them as a smart middle layer between the browser and the network; if something is already cached, it‚Äôs served instantly instead of being fetched again.</p><ul><li><p>Reduces the load on servers.</p></li><li><p>Keeps apps usable even with poor or no internet connection.</p></li></ul><p>Progressive Web Apps like Twitter Lite or Starbucks PWA, which cache core UI and recent content, so users can browse even on unstable mobile networks.</p><p>Once your UI loads quickly, the next step is to think about the data behind it.</p><p>In real apps, this data (also called state) can come from different places: </p><ul><li><p>Some live inside a single component(a reusable piece of the UI, like a button),</p></li><li><p>Some are shared across the app,</p></li><li><p>And others come from APIs.</p></li></ul><p>How you manage this state can make or break your app‚Äôs speed, reliability, and scalability.</p><h3><strong>10 State Management (Local, Global, Server Cache)</strong></h3><ul><li><p>data that lives inside a single component, used for things like toggles, forms, or small interactions. It‚Äôs simple to manage and doesn‚Äôt add much complexity.</p></li><li><p> data that‚Äôs shared across multiple components or pages, like user info or theme settings. Tools like <a href=\"https://redux.js.org/\">Redux</a>, <a href=\"https://zustand-demo.pmnd.rs/\">Zustand</a>, or <a href=\"https://react.dev/learn/passing-data-deeply-with-context\">React Context</a> help manage it.</p></li><li><p> stores frequently used API data on the client so the app doesn‚Äôt have to fetch it again and again, making it faster and reducing server load.</p></li></ul><p>Think of it like database caching: by deciding where data should live, you can make your app more responsive, reliable, and easier to scale.</p><ul><li><p>Keeps your app responsive.</p></li><li><p>Reduces unnecessary API calls.</p></li><li><p>Makes scaling smoother as your app grows.</p></li></ul><p>Local state for a modal‚Äôs open/closed status. Global state for theme preference (dark mode) that affects every component. Server-side cache for user profile data displayed by multiple components.</p><h3><strong>11 API Caching with Expiration</strong></h3><p>Caching doesn‚Äôt stop at the component level. You can store API responses in memory, <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API\">IndexedDB</a>(a browser database for larger data)or <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage\">localStorage</a>(for smaller key-value data) and set expiration rules to make sure data stays fresh.</p><p>It‚Äôs like having a Redis cache server, but right in the browser instead of on your server.</p><ul><li><p>Keeps data up-to-date for users.</p></li><li><p>Reduces repeated server requests.</p></li><li><p>Makes your app feel faster.</p></li></ul><p>A news app might cache articles for a few minutes so users can read offline, while comments refresh more often to stay up to date. Similarly, a SaaS dashboard could cache chart data while the user is on the page, then refresh it when they come back later.</p><h3><strong>12 GraphQL vs REST (Reducing Over/Under-Fetching)</strong></h3><p>How you fetch data also affects performance.</p><ul><li><p> Can sometimes send too much data or not enough, making your app fetch extra information or require additional requests.</p></li><li><p> A query language for APIs that lets the client ask for exactly the data it needs, avoiding extra or missing information. This avoids over-fetching or under-fetching data and helps reduce unnecessary requests.</p></li></ul><p>It‚Äôs like how you optimise database queries on the backend to make them faster and use less bandwidth, but this happens on the frontend.</p><p>GraphQL sits between the client and the server as one endpoint. The client asks for exactly the data it needs, and the server‚Äôs GraphQL layer collects that data from databases or other APIs, then sends back a clean, organised response.</p><p>This way, you make one flexible request instead of several REST calls, making it faster and more data-efficient.</p><ul><li><p>Saves bandwidth, especially on mobile networks.</p></li><li><p>Reduces unnecessary requests.</p></li><li><p>Simplifies client-side data handling.</p></li></ul><p>GraphQL works best for complex apps that need data from many places at once, like GitHub. One GraphQL query can get a pull request, comments, and author info in a single request instead of several REST calls. While REST is simpler and great for apps with stable data, like blogs or public APIs that rely on caching.</p><h3><strong>13 Pagination Strategies (Cursor vs Offset)</strong></h3><p>Loading large lists or tables all at once can be heavy. Pagination helps break the data into manageable chunks.</p><ul><li><p> Uses page numbers or record counts (like  or ) to fetch data. It‚Äôs simple and works well for lists that don‚Äôt change often. But the list order shifts if new items are added or old ones are removed. This can make the same offset return different items, leading to duplicates or missing entries.</p></li></ul><ul><li><p> Uses a pointer to mark where the last item ended, so the next request starts right after it. It‚Äôs more reliable for live or frequently updated data (social feeds or chat messages) because it keeps track of the exact position in the dataset. That means even if new items are added or removed while you‚Äôre scrolling, you won‚Äôt see duplicates or miss entries.</p></li></ul><ul><li><p>Handles large datasets efficiently.</p></li><li><p>Prevents slowdowns and performance bottlenecks.</p></li><li><p>Keeps dynamic lists reliable and consistent.</p></li></ul><ul><li><p> best for data tables with stable data and clear page numbers, such as admin panels or product catalogs.</p></li><li><p>: ideal for infinite scroll feeds like social media timelines, notification lists, or any real-time list where items are frequently added or removed.</p></li></ul><h3><strong>14 Real-Time Data &amp; Networking (WebSockets, SSE, Polling)</strong></h3><p>Finally, some apps need live updates, like chat apps, dashboards, or notifications. How you handle real-time data matters.</p><ul><li><p> Let the client and server send messages to each other in real time, both ways, without constantly asking for updates.</p></li><li><p><strong>Server-Sent Events (SSE):</strong> The server can push updates to the client in real time, but communication only goes one way, from server to client.</p></li><li><p> The client regularly asks the server for updates. It‚Äôs simple to set up, but it can put more load on the server.</p></li></ul><p>It‚Äôs like building event-driven systems on the backend, but here it happens in the browser.</p><ul><li><p>Supports live dashboards, chat, and notifications.</p></li><li><p>Improves interactivity and user engagement.</p></li><li><p>Allows you to choose the right strategy for your app‚Äôs needs.</p></li></ul><ul><li><p> chat apps (Slack), multiplayer games, collaborative editing (Google Docs).</p></li><li><p> live notifications, stock tickers, server logs streaming to a dashboard.</p></li><li><p> simple use cases like checking for new emails or status updates.</p></li></ul><h2><strong>Architecture &amp; Scalability</strong></h2><p>As your app grows, managing complexity becomes just as important as writing features. Frontend architecture isn‚Äôt just about code; it‚Äôs about building systems that are maintainable, scalable, and predictable.</p><p>When multiple teams work on the same app, things can get messy fast.</p><p>Micro frontends let each team build and deploy their part of the app separately. For example, one team handles the dashboard while another builds the settings page. Technically, the app is divided into smaller frontend projects that are combined at runtime to work as one seamless app.</p><p>A module federation feature (for example, in tools like <a href=\"https://webpack.js.org/concepts/module-federation/\">Webpack</a>) lets these separate projects share code (like components or utilities) directly in the browser, without rebuilding or duplicating code across projects.</p><ul><li><p>Teams can develop features faster and in parallel.</p></li><li><p>Reduces duplicated code across bundles.</p></li><li><p>Supports independent deployment cycles, so updates don‚Äôt block each other.</p></li></ul><h3><strong>16 Component-Based Architecture &amp; Design Systems</strong></h3><p>Components are the building blocks of your app. A design system ensures these components stay consistent and reusable across teams and projects.</p><p>It‚Äôs like having reusable backend modules or libraries, but for your UI.</p><ul><li><p>Makes the UI predictable and easier to maintain.</p></li><li><p>Encourages code reuse across pages and projects.</p></li><li><p>Helps teams scale efficiently without creating chaos.</p></li></ul><ul><li><p>Used by companies with many products or teams to keep design consistent, like <a href=\"https://shopify.dev/docs/api/app-home/polaris-web-components\">Shopify‚Äôs Polaris</a> or <a href=\"https://carbondesignsystem.com\">IBM‚Äôs Carbon</a>, which are open-source design systems containing ready-to-use UI components, styles, and guidelines.</p></li><li><p>Even small startups benefit: a shared set of 10‚Äì20 components (like buttons and modals) helps teams build faster and keep the UI consistent.</p></li></ul><h3><strong>17 Build &amp; Deployment Pipelines (CI/CD for Frontend)</strong></h3><p>Frontend apps also benefit from CI/CD (Continuous Integration and Continuous Deployment) pipelines, just like backend services. These pipelines automatically handle steps like building the app, running tests, and deploying updates.</p><p>In simple terms, every time you push code, CI/CD tools check that nothing breaks and then safely release the latest version, making deployments faster, more reliable, and less manual.</p><ul><li><p>Minimises human errors during deployment.</p></li><li><p>Enables fast, reliable releases.</p></li><li><p>Makes scaling and frequent updates much smoother.</p></li></ul><p>Works for any app with regular updates, from small teams auto-deploying to Vercel to big companies like Netflix releasing thousands of times a day. It keeps updates fast, safe, and reliable.</p><h2><strong>User Experience &amp; Reliability</strong></h2><p>Your users don‚Äôt care about your architectures or caching strategies; they just want the app to be fast, reliable, and easy to use.</p><h3><strong>18 Accessibility (a11y) &amp; Mobile-First Design</strong></h3><p>Accessibility and mobile-first design aren‚Äôt just design principles; they‚Äôre system-level considerations. Accessibility ensures your app‚Äôs UI and code structure work for everyone, including people using assistive technologies.</p><p>Mobile-first design forces you to build efficient layouts, load lighter assets, and prioritize key features, all of which influence performance, scalability, and overall frontend architecture.</p><ul><li><p>Makes your app easier and more pleasant to use.</p></li><li><p>Ensures a consistent experience across devices.</p></li></ul><p>Government sites (accessibility is legally required in many countries), e-commerce, and content platforms. Mobile-first is essential for apps in developing markets where mobile is the main or only device.</p><h3><strong>19 Progressive Web Apps (PWAs) &amp; Offline-First</strong></h3><p>Progressive Web Apps (PWAs) are web apps that behave like native apps. They can work offline, send notifications, and even be installed on a device.</p><p>They use a few key technologies:</p><ul><li><p>Service workers run in the background to cache important files like HTML, CSS, and API responses.</p></li><li><p>A web app manifest defines how the app looks and behaves when installed.</p></li><li><p>And HTTPS keeps everything secure.</p></li></ul><p>Together, these make the app fast, reliable, and installable.</p><ul><li><p>Users can access your app anywhere.</p></li><li><p>Improves reliability and user trust.</p></li></ul><p>Apps where offline access is valuable: Twitter Lite, Starbucks PWA, field service apps, and news apps.</p><h3><strong>20 Security Basics (XSS, CSRF, CSP, Authentication)</strong></h3><p>Speed means nothing without security. Frontend isn‚Äôt just about the UI; it‚Äôs also the first line of defence for your app.</p><ul><li><p><strong>XSS (Cross-Site Scripting): </strong>Stop attackers from injecting malicious scripts into your app.</p></li><li><p><strong>CSRF (Cross-Site Request Forgery): </strong>Protect your forms and actions that change data from being triggered by attackers without the user‚Äôs consent.</p></li><li><p><strong>CSP (Content Security Policy):</strong> A rule set that helps prevent malicious scripts from running in your app.</p></li><li><p> Make sure user tokens and sessions are stored and handled securely in the browser.</p></li></ul><ul><li><p>Protects your users and their data.</p></li><li><p>Prevents common attacks before they reach the backend.</p></li><li><p>Builds trust and helps with compliance.</p></li></ul><p>Any app handling sensitive data. Financial apps need strict CSP and token handling. Social platforms must prevent XSS to avoid account takeovers. E-commerce sites need CSRF protection on checkout to prevent unauthorised purchases.</p><h3><strong>21 Observability &amp; Error Monitoring (Client-Side)</strong></h3><p>Even if everything works well, things can still break in production. That‚Äôs why observability is important.</p><p>Frontend errors are just like 500 errors in your backend; they happen. Monitoring tools like <a href=\"https://sentry.io/welcome/\">Sentry</a> or <a href=\"https://logrocket.com/\">LogRocket</a> help you track:</p><ul><li><p> errors that happen in your JavaScript code while the app is running.</p></li><li><p> parts of your app that slow it down or make it lag.</p></li><li><p><strong>User interactions leading to errors: </strong>actions by users that trigger bugs or crashes in your app.</p></li></ul><p>These tools add a small script to your app. When something breaks, it collects information like the error message, what the user was doing, and browser details. Then it sends that data to the tool‚Äôs server, where you can see and fix the issue from your dashboard.</p><ul><li><p>Detects and resolves issues faster.</p></li><li><p>Keeps your app stable and performant.</p></li><li><p>Improves the overall user experience and trust.</p></li></ul><p>Used in production apps with real users. SaaS teams track errors right after deployment, e-commerce sites watch checkout issues, and session replay tools help support teams see what confused users without extra bug reports.</p><p>Frontend system design is basically backend system design, just happening in the user‚Äôs browser.</p><p>Every choice you make, like rendering method, caching strategy, state management, architecture, and security, affects speed, scalability, and reliability.</p><p>So next time you‚Äôre building a frontend, ask yourself:</p><ul><li><p><strong>Where should computation happen?</strong> On the server, in the client‚Äôs browser, or at the edge?</p></li><li><p><strong>When does the data need to be up-to-date?</strong> Prebuilt, cached, or real-time?</p></li><li><p><strong>How can we keep the app fast and reliable?</strong> Lazy loading, smart caching, or micro frontends?</p></li><li><p> Can the architecture handle 10x traffic? 100x?</p></li><li><p> Will new developers understand the architecture? Can teams work independently?</p></li></ul><p>Think of your frontend as a distributed system. Treat it that way, and your users will get an app that‚Äôs fast, smooth, and seamless, exactly what they expect.</p><p>üëã I‚Äôd like to thank  for writing this newsletter!</p><p>Plus, don‚Äôt forget to check out her work and socials:</p><p>You‚Äôll often find her writing about web development, sharing UI tips, and building tools that make developers‚Äô lives easier.</p><p>I‚Äôm excited to let you know we‚Äôre launching Design, Build, Scale!</p><p>(3-part newsletter series that breaks down popular interview questions.)</p><p>Exclusive to PAID members...</p><p>Here are a few of the things you‚Äôll get:</p><ul><li><p><strong>High-level architecture of real-world case studies.</strong></p></li><li><p>Deep dive into how popular real-world systems actually work.</p></li><li><p><strong>How real-world systems handle scale, reliability, and performance.</strong></p></li><li><p>10x the results you currently get with 1/10th of your time, energy, and effort.</p></li></ul><p>Start Date: November 2025</p><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 180,001+ readers strong, very close to 181k. Let‚Äôs try to get 181k readers by 21 November. Consider sharing this post with your friends and get rewards.</p><ul><li><p>Block diagrams created with <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a></p></li></ul>","contentLength":25594,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/f4bf7078-9dfa-4bf9-a47b-13a7dcd6d756_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"21 Git Commands for Software Engineers","url":"https://newsletter.systemdesign.one/p/commands-in-git","date":1762423558,"author":"Neo Kim","guid":33,"unread":true,"content":"<p>Download my system design playbook for FREE on newsletter signup:</p><p>Some of these are basic, and some are pretty advanced. ALL of them are super useful to anyone who writes code.</p><p>Curious to know how many were new to you:</p><ol></ol><ul><li><p>A common mistake (I‚Äôve made!)</p></li></ul><p>Here are some to understand before you get started:</p><ul><li><p>Working directory: files you‚Äôre currently editing.</p></li><li><p>Staging area: a collection of changes prepared for the next commit. </p></li><li><p>Local repository: full project history stored on your local machine. </p></li><li><p>Remote repository: shared version of the project hosted on a server, such as GitHub or GitLab.</p></li></ul><blockquote><p>Create a new Git repository in the current directory.</p></blockquote><p>Running it inside an existing repository. This can be confusing and break Git commands as it‚Äôll create nested repositories. So check if you‚Äôre already inside a repository using .</p><p>Git won‚Äôt track changes until you initialize a repository. So it‚Äôs your first step towards version controlling the files.</p><p>Now that you‚Äôve set up a repository, let‚Äôs check its current status before going further.</p><blockquote><p>Display the current state of the working directory and staging area. This shows which changes are staged, unstaged, or untracked.</p></blockquote><p>Not running it enough. This could cause some files to be overlooked or confusion about what‚Äôs added and committed. So run it often to stay aware of the repository‚Äôs state.</p><p>It tells you what needs to be done - for example, what to add or commit. Plus, you can avoid surprises by viewing every change and new file.</p><blockquote><p>List, create, or delete branches in your repository.</p></blockquote><p>Assuming it switches branches when you create a new one. For example,</p><p>This creates a branch, but doesn‚Äôt automatically switch to it. You stay on your current branch unless you run:</p><p>You can work on various features or bug fixes without affecting the main code using branches. Besides, branches keep your work ‚Äúorganized and separated‚Äù, so you can experiment safely.</p><p>I‚Äôm happy to partner with  on this newsletter. Code reviews usually delay feature deliveries and overload reviewers. And I genuinely believe CodeRabbit solves this problem.</p><blockquote><p>Switch to a different branch, or restore files from a specific commit.</p></blockquote><p>Switching between branches without committing or stashing your changes first. This can cause merge conflicts or prevent the switch altogether.</p><p>It lets you switch between branches, try out features, or view past code. All without losing your place. Plus, it allows you to restore specific files from an earlier commit.</p><p>(In newer versions, try  for clarity!)</p><blockquote><p>Show the difference between two versions of your files, commits, or branches.</p></blockquote><p>Assuming it shows all changes, but shows only UNSTAGED ones. If you want to view staged changes as well, run:</p><p>It helps you catch mistakes and understand changes before committing or merging them.</p><blockquote><p>Shows the difference between the working directory and the last commit ().</p></blockquote><p>Assuming it shows only unstaged changes compared to the last commit. But it shows both STAGED and UNSTAGED changes.</p><ul><li><p>: shows only unstaged changes.</p></li><li><p>: shows only staged changes.</p></li><li><p>: shows everything that changed since the last commit.</p></li></ul><p>It ensures you don‚Äôt accidentally skip reviewing any changes before committing!</p><p>Now that you‚Äôve reviewed the changes... the next step is to save them to the local repository.</p><blockquote><p>Add changes from the working directory to the staging area.</p></blockquote><p>Running ‚Äú‚Äù without checking what‚Äôs included. This might stage unwanted files or incomplete changes. Instead, you can stage specific files or lines using ‚Äú‚Äù.</p><ul><li><p>‚Üí Adds all changes in the current directory and its sub-folders. But if you‚Äôre in a sub-folder, it won‚Äôt include files in parent directories.</p></li><li><p> ‚Üí Adds only non-hidden files in the current directory. For example, it skips those starting with a dot, like .env or .gitignore.</p></li><li><p> ‚Üí Adds all changes from the repository root, including hidden files. It works from any directory in the project and is the safest way to add everything.</p></li></ul><p>It lets you decide what to include in the next commit. Put simply, your commit might include nothing or miss important changes if you use it wrong.</p><p>Did you know?  brings real-time, AI code reviews straight into VS Code, Cursor, and Windsurf.</p><p>Use it to catch bugs, security flaws, and performance issues you write code.</p><blockquote><p>Record staged changes in the local repository as a new snapshot.</p></blockquote><ul><li><p>Without reviewing staged files,</p></li><li><p>Without a clear commit message. </p></li></ul><p>This makes the commit history hard to understand.</p><p>A commit is the fundamental unit of work in Git; it records what changed and why. Good commit messages make debugging, reviewing, and collaboration easier.</p><p>Yet if you get interrupted before committing and need to switch tasks, you can temporarily save your current work. Let‚Äôs find out how‚Ä¶</p><blockquote><p>Temporarily save uncommitted changes so you can work on something else.</p></blockquote><p>Assuming it saves all changes, including new (untracked) files. But in reality, it stash only TRACKED files. So if you‚Äôve new files, those remain in your working directory and won‚Äôt be part of the stash.</p><p>You also risk losing untracked files if you then delete or clean your workspace. If you want to stash untracked files, run:</p><p>It lets you pause work on a feature and switch branches without losing progress. This can be handy for quickly cleaning your workspace for a code review or hotfix. You can then return later and resume where you left off.</p><p>Once you‚Äôre back to working on your project, sync the changes your team made while you were busy.</p><blockquote><p>Download changes from a remote repository without merging them into your code.</p></blockquote><p>Assuming it automatically updates your branch, which it doesn‚Äôt. You still need to merge or rebase afterward.</p><p>It‚Äôs the safest way to review incoming changes before deciding to MERGE them locally.</p><blockquote><p>Combine changes from one branch onto another.</p></blockquote><p>Git will prompt you to resolve conflicts manually before completing the merge. But if you leave merge conflicts unresolved, it can create messy code and errors.</p><p>This is essential for collaboration on a project. It allows team members to work in parallel and integrate their changes.</p><p>Remember, a clean merge history makes tracking and debugging easier later on.</p><blockquote><p>Equal to running git fetch followed by git merge.</p></blockquote><p>Running it without reviewing changes can create conflicts. Plus, if you pull with uncommitted changes, it can create hard-to-resolve merge conflicts. Or even overwrite your local changes.</p><p>It keeps your branch up to date with your team‚Äôs latest work. </p><blockquote><p>Re-apply commits from one branch onto another.</p></blockquote><p>Rebasing rewrites commit history. So if you rebase a public branch that others are working on, its history no longer matches theirs. This can create confusing conflicts and broken pull requests. Plus, it could result in data loss if commits get dropped accidentally during the process.</p><p>If you want to clean up commit history and rebase with care, run this:</p><p>It avoids merge commits. So you can integrate updates from the main branch onto the feature branch without cluttering the history. As a result, you have:</p><ul><li><p>Clean and linear commit history,</p></li><li><p>Easy to understand branch evolution and pull requests.</p></li></ul><p>Now that your local repository is up to date with the team‚Äôs work, let‚Äôs find out how you can share your changes with others.</p><blockquote><p>Upload commits from the local repository to a remote one.</p></blockquote><p>Pushing without first pulling the latest changes from the remote repository. This can cause conflicts or reject the push operation altogether. So always pull and then RESOLVE any conflicts before pushing. This keeps the history clean and consistent across the repository.</p><p>It‚Äôs how your commits become part of the shared project and keep the team aligned.</p><p>Guess what? When you open a pull request,  can generate a summary of code changes for the reviewer. It helps them quickly understand complex changes and assess the impact on the codebase. Speed up the code review process.</p><blockquote><p>Marks a specific commit with a label, often to represent a release version.</p></blockquote><p>Forgetting to push tags to the remote. This means your team members won‚Äôt see those version labels in the repository. And this can create confusion about releases or version tracking. So after tagging, run this:</p><pre><code>#push all tags\ngit push --tags\n\nOR\n\n#push a specific tag\ngit push origin &lt;tag&gt;</code></pre><p>Don‚Äôt forget to confirm your tags are on the remote if you use them for deployment or release tracking.</p><p>It helps you label releases and milestones in the project history. Plus, tags make the deployment and distribution of releases easier.</p><p>Mistakes happen. Git provides powerful commands to fix the problems. Let‚Äôs see those next...</p><blockquote><p>Undo changes by moving the current branch to a specific commit. Also optionally update the staging area and working directory.</p></blockquote><p>Running it carelessly can make you lose hours of work without an easy recovery. For example, you‚Äôll permanently lose your uncommitted changes by running this:</p><p>Instead, do these for a safe undo‚Ä¶</p><ul><li><p>If you want to undo a commit but keep the changes staged, run this:</p></li></ul><ul><li><p>If you want to undo a commit and move changes back to your working directory, run this:</p></li></ul><p>It lets you fix mistakes without creating new commits. This means you‚Äôve full control over what your branch includes.</p><blockquote><p>Create a new commit to reverse the changes from an earlier commit.</p></blockquote><p>Assuming it removes a commit from history. Instead, it creates a new commit to undo previous changes.</p><p>Here are 2 heuristics you should know:</p><ul><li><p>If you want to undo changes on a SHARED branch (safely), run this:</p></li></ul><ul><li><p>If you want to remove a commit from a LOCAL branch that hasn‚Äôt been pushed yet, run this:</p></li></ul><p>Reverting is the safe way to undo a commit on a shared branch. It fixes mistakes from past commits without rewriting history. This way, everyone‚Äôs copies stay in sync, and you‚Äôve a correction record.</p><blockquote><p>Apply a specific commit from another branch onto your current branch.</p></blockquote><p>Cherry-picking the same commit more than once or onto the wrong branch. This can duplicate changes and create conflicts.</p><p>Here‚Äôs what you should do instead:</p><ul><li><p>Check the commit hash and current branch before cherry-picking.</p></li><li><p>Use  or  to check if the commit already exists.</p></li></ul><p>It lets you apply bug fixes or backport changes without merging the entire branch. This gives you precise control over what changes move where.</p><blockquote><p>Finds the commit that introduced a bug by binary searching through the commit history.</p></blockquote><p>After running , Git puts you in a detached  state. If you forget to reset, you may continue working without being on a real branch. This can lead to confusion or lost commits.</p><p>It saves you time by speeding up the debugging process. Instead of checking every commit one by one, it finds the commit that caused the bug in a few steps.</p><p>If you want to collaborate effectively, you‚Äôve got to better understand your project history. And know who made which changes. Onward.</p><p>Bet you didn‚Äôt know‚Ä¶brings instant code reviews directly to your terminal, seamlessly integrating with Claude Code, Cursor CLI, and other AI coding agents. While they generate code, CodeRabbit ensures it‚Äôs production-ready - catching bugs, security issues, and AI hallucinations before they hit your codebase.</p><blockquote><p>Show the commit history of the current branch.</p></blockquote><p>Not knowing how to exit the log viewer. If you want to quit and return to the command line, press .</p><p>The log is a record of all changes; it includes information about who made which changes and when. This helps you understand the context behind a change or debug a failure.</p><blockquote><p>Show who last changed each line of a file and when.</p></blockquote><p>Misusing it to point fingers can damage team trust and overlook the actual reasons behind a change. Instead, you should:</p><ul><li><p>Use it to understand code history and ask better questions.</p></li><li><p>Combine it with  or  to understand the full context of a change.</p></li></ul><p>Remember, it‚Äôs meant to help understand code history, not to place guilt on someone. </p><p>It lets you figure out why part of the code is the way it is, or find when a bug first appeared. Plus, it shows you who changed each line and which commit introduced the change. This provides context and can point you to the right person to ask about it.</p><p>These 21 commands are the foundation of Git. Once you understand them, everything else is just a variation or combination.</p><p>If I missed something, just let me know in the comments üëá</p><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 180,001+ readers strong, very close to 181k. Let‚Äôs try to get 181k readers by 10 November. Consider sharing this post with your friends and get rewards.</p>","contentLength":12377,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/7653424e-5c6e-45dd-81af-291961e04e25_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"Everything You Need to Know to Understand How LLMs Like ChatGPT Actually Work","url":"https://newsletter.systemdesign.one/p/llm-concepts","date":1762165820,"author":"Neo Kim","guid":32,"unread":true,"content":"<p>Download my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines 33 must-know LLM concepts.</em></p><p>You‚Äôve probably used an AI tool like ChatGPT, Gemini, or Claude, asked a simple question, and gotten an answer that was slow, vague, or just wrong.</p><p>As a fix, many offer lists of techniques or ‚Äúprompts‚Äù to maximize models. However, what you really need is a solid understanding of how they work.</p><p>People use terms like tokens, embeddings, and hallucinations to explain things. But if you don‚Äôt understand those words, it‚Äôs tough to know what to fix.</p><p>This article is a plain-English field guide to that vocabulary and the foundations of this field. You don‚Äôt need the math, the specific prompts, or the coding skills; you just need to know what these terms mean in practice so you can get better results, spot failure modes, and make sensible choices about models and settings.</p><p>Along the way, we‚Äôll use simple examples to make the concepts easier to digest. Don‚Äôt worry about memorizing every detail; this guide exists so your next interaction with LLMs feels more natural, less like magic, and so you can recognize their mistakes more clearly.</p><p>Let‚Äôs begin with the first and most basic ingredient.</p><p>He‚Äôs the founder of <a href=\"https://academy.towardsai.net/\">Towards AI</a>, where his focus is on making AI more accessible by helping people learn practical AI skills for the industry alongside 500k+ fellow learners.</p><p>He‚Äôll also be turning this article into a series of videos, so if you‚Äôd like a step-by-step refresher through video, it‚Äôs worth following him along on:</p><p>After you develop the core foundations through this newsletter, you can go further with his . It‚Äôs a great starting point to learn how to bring AI into your daily work and boost your productivity.</p><p>(You‚Äôll also get a 15% discount when you use the code .)</p><p>Running Kafka in the cloud shouldn‚Äôt feel like setting money on fire. But for most teams, it does! Storage costs skyrocket, clusters crawl, and scaling takes hours instead of seconds.</p><p>That‚Äôs why top companies like Grab, JD.com, and others are switching to  - an open source, diskless Kafka¬Æ on S3.</p><p>Here‚Äôs what happens when you make the switch:</p><ul><li><ul><li><p>Scale in 10 seconds (not 43 hours)</p></li><li><p>Reassign partitions in seconds (no downtime, no stress)</p></li></ul></li><li><ul><li><p>Zero inter-zone data transfer fees</p></li><li><p>No more over-provisioning</p></li></ul></li><li><ul><li><p>Data lands fresh in seconds</p></li><li><p>Seamless integration with Iceberg</p></li></ul></li><li><ul><li><p>Works with your existing Kafka features &amp; tools</p></li><li><p>Keeps pace with the Kafka community</p></li></ul></li></ul><p>If Kafka had been designed for the cloud from day one, <a href=\"https://github.com/AutoMQ/automq?utm_source=neo_newsletter_202511\">AutoMQ</a> would be the result.</p><p>While today‚Äôs generative models are built upon a decade of progress, 2022 was the year when they triggered an ‚ÄúAha!‚Äù moment for most. Generative AI is a subfield of machine learning. It involves training artificial intelligence models on large volumes of real-world data to generate new content (text, images, code, etc.) that resembles human creation.</p><p>This may have been a mouthful. Let‚Äôs clarify these terms first before we jump into LLMs. Here‚Äôs a plain-English map of those first words you‚Äôll see.</p><ul><li><p> is the big umbrella: getting computers to do things that look intelligent.</p></li><li><p> lives inside AI: systems learn from data instead of hard-coded rules.</p></li><li><p> is a way for computers to learn patterns by practicing on a huge number of examples.</p></li><li><p> (natural language processing) is the part of AI that works with human language. As simple as that.</p></li><li><p> is the branch that  new content (text, images, audio, code). Whatever it is, it just means it  things rather than  things, as done with more classical AI systems.</p></li><li><p> (Large Language Models) are deep learning models within the generative AI family that specialize in .</p></li></ul><p>That‚Äôs all you need for now: AI ‚Üí ML ‚Üí DL ‚Üí (NLP) ‚Üí . With the labels straight, we can understand what an LLM actually does.</p><p>An LLM is a powerful autocomplete system. It‚Äôs a machine built to answer one simple question over and over again: ‚ÄúGiven this sequence of text, what is the most probable next token?‚Äù That ‚Äúpiece of text‚Äù is called a ‚Äîit can be a word, a part of a word (like runn and ing), or punctuation.</p><p>For example, if a user asks ChatGPT, ‚ÄúWhat is fine-tuning?‚Äù, it doesn‚Äôt ‚Äúknow‚Äù the answer. It just predicts the next token, one at a time:</p><ol><li><p>The most probable first token is ‚Äú‚Äù.</p></li><li><p>Given that, the next most probable token is ‚Äú‚Äù.</p></li><li><p>Next is ‚Äú‚Äù, and so on...</p></li></ol><p>Until it generates a full sentence: ‚Äú<strong>Fine-tuning is the process of training a pre-trained model further on a smaller, specific dataset.</strong>‚Äù</p><p>It‚Äôs called a  for three simple reasons:</p><ul><li><p>: It has billions of internal variables (called ) and was trained on a massive amount of text.</p></li><li><p>: It‚Äôs specialized for understanding and generating human language.</p></li><li><p>: It‚Äôs a mathematical representation of the patterns it learned.</p></li></ul><p>So, at its heart, an LLM is just a very advanced guessing machine: predicting the next token again and again until a full answer appears. So, how does it get good at making those guesses in the first place?</p><p>To get there, the model undergoes a long study session of its own‚Äîa process called . For example, if a student is asked to read every book in a giant library (a huge slice of the internet, in the case of LLMs). They‚Äôre not trying to memorize pages word-for-word. Instead, they learn patterns‚Äîhow words, sentences, and ideas fit together‚Äîwell enough to predict the next piece of text in any sentence. This is how a base model like GPT-5 is built during pre-training.</p><h2>The Hidden Machinery: What Happens Underground</h2><p>You don‚Äôt need to know every nut and bolt of how an LLM works to use one. Understanding key building blocks like tokens, embeddings, and parameters makes the system seem less magical. It also helps you spot where models shine, where they stumble, and how to get better results when you work with them.</p><p>An LLM is, at its core, a mathematical system. That creates a basic problem: it only understands numbers, not words. So how can an LLM possibly read a user‚Äôs question like </p><p>The first step is to translate the text into something the model can work with. That starts by breaking the sentence into its smallest meaningful chunks, called . A  can be a whole word (hello), a common part of a word (runn + ing), or even just punctuation (?).</p><p>A special program called a  handles this. For any question, it first splits the text into a list of tokens:</p><p>[‚ÄúWhat‚Äù], [‚Äúis‚Äù], [‚Äúfine‚Äù], [‚Äú-‚Äù], [‚Äútuning‚Äù], [‚Äú?‚Äù]</p><p>Then, the tokenizer swaps each unique token for a specific ID number. The question finally becomes a sequence of numbers the model can understand, like [1023, 318, 5621, 12, 90177, 30]. Now, the model has a list of numbers it can finally work with.</p><p>But tokens alone don‚Äôt carry meaning; they‚Äôre just IDs. To make sense of them, the model needs another layer.</p><p>With tokenization, we turned the user‚Äôs question into a list of ID numbers ([1023, 318, 5621...]), but right now, these numbers are just random labels. The ID for ‚Äúcat‚Äù doesn‚Äôt relate to the ID for ‚Äúkitten.‚Äù So, the model can‚Äôt grasp their meanings or how they connect.</p><p>This is where  come in. An  is a special list of numbers‚Äîa vector‚Äîthat represents the meaning of a token. Instead of a random ID, each token gets a rich set of coordinates that places it on a giant ‚Äúmap of meaning.‚Äù</p><p>On this map, words with similar meanings, like ‚Äúdog‚Äù and ‚Äúpuppy,‚Äù are placed very close together. This allows the model to understand relationships numerically. For example, the distance and direction from ‚Äúking‚Äù to ‚Äúqueen‚Äù on the map is the same as from ‚Äúman‚Äù to ‚Äúwoman.‚Äù</p><p>This concept is also how chatbots and search systems can find information phrased in different ways. If you ask a search engine about ‚Äúcars,‚Äù embeddings help it realize that data mentioning ‚Äúautomobiles‚Äù is still relevant.</p><p>Embeddings don‚Äôt float around randomly; they all exist inside a much larger structure.</p><p>Now that the model has a question in the form of an , that embedding doesn‚Äôt sit alone. This ‚Äúunderstanding‚Äù of meaning happens inside what we call the ‚Äîthe vast ‚Äúmap of meaning‚Äù where all embeddings live.</p><p>It isn‚Äôt a physical space, but a mathematical one created by the model. During training, the model organizes concept embeddings in this space. This setup ensures their positions and distances show real-world relationships.</p><p>For example, if you ask a chatbot about ‚Äúfine-tuning,‚Äù your question‚Äôs embedding will be close to those of others about training methods. The model‚Äôs job is then simple: look around the neighborhood and pull in the closest matches.</p><p>Behind this ability lies something deeper: the model‚Äôs internal settings‚Äîthe part that decides how all of this gets organized.</p><p>The base model for systems like ChatGPT has billions of internal variables. These are known as . These aren‚Äôt rows in a database or a list of facts. They‚Äôre the adjustable settings that give the model its ability to represent grammar, concepts, and patterns.</p><p>A good way to picture them is as a massive wall of knobs and dials. At the start, every knob is set randomly‚Äînothing useful yet. During training, the model plays its prediction game trillions of times. Each wrong guess makes a small change to the dials. This moves them closer to the settings needed for the right answer.</p><p>After many tiny updates, the final setup of knobs encodes everything the model has learned. This includes language patterns, idea connections, and bits of general knowledge.</p><p>Left untouched, billions of random knobs are meaningless on their own. They gain knowledge only through the long process of training.</p><h2>How LLMs Learn: How the Machinery is Trained</h2><p>The process that turns random parameters into a knowledgeable system is called .</p><p>In this foundational phase, the model is exposed to a massive dataset of text and code from the internet. Its one objective is simple: predict the next  in a sequence. Every time it guesses, it checks its answer against the real token. Then, the training algorithm tweaks its billions of parameters just a bit. After trillions of repetitions, these microscopic updates encode the statistical patterns of language. That‚Äôs how a base model like GPT-5 learns grammar, common facts, and basic reasoning skills, before it is adapted for real-world use.</p><p>This training can be understood in two steps: the task itself and the result of repeating it on an enormous scale.</p><p><strong>1. The Task: Predict the Next Token</strong></p><p>The model is shown a snippet of text, for example: <em>‚ÄúFine-tuning is the process of‚Ä¶‚Äù</em> Its job is to guess the missing piece. At first, guesses are random. But each mistake prompts a small correction to its parameters, making the right answer, like  a little more likely next time.</p><p><strong>2. The Result: A Pattern-Matching Engine</strong></p><p>After trillions of these corrections, the model becomes remarkably good at spotting patterns. It has seen phrases like <em>‚Äúfine-tuning is the process of training‚Äù</em> so often that the connection sticks. But it isn‚Äôt ‚Äúunderstanding‚Äù or ‚Äúthinking.‚Äù It‚Äôs just reproducing patterns it has learned.</p><p>Pre-training gives us a model packed with patterns from the internet. But at this stage, it‚Äôs still just a text predictor. To see why that‚Äôs a problem, we need to look at the difference between a  and an .</p><h3><strong>6. Base Model vs. Instruct Model</strong></h3><p>When a model finishes , it‚Äôs called a . It holds vast knowledge, but it‚Äôs not yet designed to be a helpful assistant. For example, if you used a raw base model and asked, , it might simply continue the sentence in a predictive way or give a generic, unhelpful definition. It‚Äôs powerful as a text predictor, but it isn‚Äôt specifically trained to follow instructions or engage in conversation.</p><p>To make it useful for applications like chatbots, search assistants, or copilots, we need an .</p><p>An  is a base model that has received extra training. This training, known as , is done with a specific dataset of instruction-and-answer pairs. This process doesn‚Äôt teach the model new facts; instead, it teaches it how to behave. It learns user intent. It gives clear explanations and structures responses well. Models like ChatGPT and Claude are instruct models. They are designed from the ground up to be helpful and responsive, making them essential for task-oriented applications.</p><p>One way to turn a base model into an instruct model is by training it further on carefully chosen examples. This extra step is known as .</p><p> is the process of taking a model that has completed  and further training it on a smaller, high-quality dataset to specialize it for a specific task. Instead of the whole internet, the dataset might contain just a few thousand carefully selected examples relevant to the target use case.</p><p>One of the best-known examples is . The base model generates all types of text. By fine-tuning on billions of lines of open-source code, Copilot learns to write, complete, and suggest code that matches developers‚Äô styles. The fine-tuned version doesn‚Äôt ‚Äúknow more‚Äù about programming. It‚Äôs just better aligned with real-world code patterns. This makes it much more reliable in practice.</p><p>This targeted training makes small changes to the model‚Äôs . It helps the model copy the style and accuracy of the specific dataset.</p><p>Fine-tuning gives smaller models the right balance of cost and quality. But good answers aren‚Äôt just about accuracy; they also need to be safe, clear, and appropriate. That‚Äôs where model behavior comes in.</p><h2>Shaping Behavior: Turning Raw Knowledge Into a Helpful Assistant</h2><p>With fine-tuning, a model may now be able to follow instructions. But what makes an answer a  answer? A raw model trained on the internet may provide a technically correct but confusing answer for beginners. It might also repeat harmful stereotypes it learned during training.</p><p>This is the core challenge of : ensuring that an LLM‚Äôs behavior aligns with human values and intentions, specifically to make it <em>helpful, honest, and harmless</em>. For example, ChatGPT is aligned to turn down requests for unsafe content. It simplifies complex ideas when asked. It also steers clear of biased or offensive language. The goal is to make the model behave in ways that are useful and socially acceptable, not just statistically accurate.</p><p>Alignment doesn‚Äôt add new facts to the model. Instead, it shapes how the model responds. This is achieved through specific techniques, including the one we‚Äôll discuss next.</p><h3><strong>9. Reinforcement Learning from Human Feedback (RLHF)</strong></h3><p>So, how do we achieve ? We can‚Äôt just tell a model, ‚ÄúBe more helpful.‚Äù We need a way to teach it what human quality and helpfulness actually look like. This is where a powerful training technique called RLHF comes in.</p><p>Instead of just training on text, RLHF tunes the model based on human preferences. Here‚Äôs how it works in practice:</p><ol><li><p> The model is asked a question (e.g., ) and generates several possible answers. Human reviewers then rank these responses from best to worst.</p></li><li><p> This ranking data is used to train a separate  whose only job is to predict how a human would likely rate any given answer.</p></li><li><p><strong>The Model Learns from the Judge:</strong> The language model generates answers again, but this time the reward model scores them. The LLM‚Äôs internal  are nudged to favor higher-scoring responses, gradually teaching it to produce answers that align better with human preferences.</p></li></ol><p>This process helps models like ChatGPT and Claude learn what humans value in responses. These qualities include clarity, helpfulness, politeness, and safety. It does this without manually coding those behaviors. However, a model only generates a response when given an input.</p><h2>How You Talk to Them: Interaction Layer</h2><h3><strong>10. Prompt (System vs. User)</strong></h3><p>The input, the complete set of instructions and context sent to the model, is called a .</p><p>A well-designed prompt often has two distinct parts:</p><ul><li><p> This outlines the main instructions that set the model‚Äôs role and limits. It acts as a permanent guide for the model‚Äôs behavior in every interaction. For example, ChatGPT might run with a hidden system prompt like:</p></li></ul><blockquote><p>‚ÄúYou are a helpful assistant. Answer as clearly and concisely as possible, and avoid unsafe or biased content.‚Äù</p></blockquote><ul><li><p> This is the specific, immediate question or command from the user. For instance:</p></li></ul><p>The model processes both of these together. The system prompt tells it how to behave, while the user prompt tells it what to do. This separation is crucial for ensuring the model‚Äôs responses are consistently helpful and on-topic.</p><p>A prompt works well for a single exchange, but conversations are rarely just one turn. To stay coherent across multiple turns, the model relies on its .</p><p>For a chatbot or assistant to be useful, it must handle follow-up questions. If a user asks, <em>‚ÄúCan you explain that differently?‚Äù</em>, the model needs to remember what ‚Äúthat‚Äù refers to. This memory is managed by the .</p><p>The  is the maximum number of  the model can ‚Äúsee‚Äù and process at one time. This includes the system prompt, the full conversation history, and the response it is currently generating. The model cannot see anything outside this window.</p><p>This memory limit is critical. If a chat with ChatGPT, Claude, or Gemini lasts too long, the app has to shorten the history. It often removes the oldest messages. This keeps the model from losing recent context.</p><p>Within that window, how you structure the prompt is what steers any single answer.</p><h3><strong>12. Zero-shot vs. Few-shot Learning</strong></h3><p>These terms describe two fundamental techniques for structuring a  to control the model‚Äôs output. The choice between them depends on how much guidance the model needs to perform a specific task correctly.</p><ul><li><p> This is when we provide an instruction with . This approach relies entirely on the model‚Äôs pre-existing ability to understand and execute the command. For a general-purpose assistant (e.g., ChatGPT), when a user asks a direct question like, ‚ÄúWhat is fine-tuning?‚Äù, it is a zero-shot request. We are trusting the aligned model to know how to form a good answer without any examples.</p></li><li><p> This is when we include both an instruction and  (called ‚Äúshots‚Äù) of the desired output directly in the prompt. This technique is essential when we need to guide the model‚Äôs output format or style. For a chatbot, if we wanted to ensure all summaries are formatted as three concise bullet points, we would provide examples of this format in the prompt before giving it the new text to summarize.</p></li></ul><p>Few-shot prompting is a powerful way to get more reliable and consistently structured outputs from the model.</p><h3><strong>13. Reasoning &amp; Chain-of-Thought (CoT)</strong></h3><p>Sometimes, a user may ask a chatbot a tricky question that needs several steps to answer. For example, they might say, ‚ÄúCompare RAG to fine-tuning and explain which is better to stop hallucinations.‚Äù If the model tries to answer directly, it can easily make a logical error.</p><p>This is a failure of . To improve this, we use a prompt engineering technique called . Instead of just asking for the final answer, we add a simple instruction to the prompt: ‚ÄúLet‚Äôs think step by step.‚Äù</p><p>This makes the model create a logical chain of steps. First, it outlines the definition of RAG. Then, it fine-tunes and compares them. Finally, it gives the conclusion. By ‚Äúshowing its work,‚Äù the model dramatically increases the accuracy of its reasoning for complex problems.</p><p>Newer, reasoning-focused models take it a step further: they often have this ‚Äústep-by-step‚Äù thinking built in. They don‚Äôt need a specific prompt. Instead, they create their own internal thought process. This helps them answer complex problems more accurately. For example, state-of-the-art models like <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro\">Google Gemini 2.5 Pro</a>, <a href=\"https://openai.com/index/introducing-gpt-5/\">OpenAI GPT-5</a>, and <a href=\"https://www.anthropic.com/news/claude-opus-4-1\">Anthropic Claude Opus 4.1</a> exhibit these advanced reasoning capabilities.</p><p>A lot happens between pressing enter and seeing a response. The way the model processes the input, how long it takes, and the character of its answer all depend on a few key factors.</p><h2>Running in Real Time: What Happens When You Hit Enter</h2><p>Once a chatbot like ChatGPT receives a complete prompt, it begins the process of generating an answer. This process of a trained model creating an output is called .</p><p>When you see the answer appearing word by word, you are watching inference in real time. It is not writing a full sentence at once. Instead, it predicts the single most probable next , adds that token to the sequence, and then repeats this cycle over and over again. This token-by-token generation continues until it produces a special ‚Äúend-of-sequence‚Äù token or reaches its maximum output length.</p><p>The time a user has to wait between asking a question and receiving a complete answer is called . This is a critical factor in user experience, as high latency can make an AI system feel slow and unresponsive.</p><p>Because  happens token-by-token, latency is measured in two key parts:</p><ul><li><p><strong>Time-to-first-token (TTFT):</strong> The time it takes for the first piece of the answer to appear. A low TTFT is crucial because it signals to the user that the assistant is working.</p></li><li><p> The speed at which subsequent tokens are generated. This determines the ‚Äútyping speed‚Äù of the model.</p></li></ul><p>For a chatbot to feel effective, both of these latency metrics need to be low.</p><p>Speed is only half the runtime story; once tokens are flowing, you still choose how predictable or varied they should be.</p><h3><strong>16. Temperature &amp; Deterministic vs. Stochastic Outputs</strong></h3><p>Temperature adjusts the level of randomness in the model‚Äôs token selection. When ChatGPT or Claude answers the same question multiple times, should it give the exact same response every time? We control this using a key  setting called .</p><ul><li><p>A  output ensures the same input always produces the exact same output. We achieve this with a very low temperature (e.g., 0.0), which forces the model to always pick the single most probable next token. This is ideal for factual definitions where consistency is key.</p></li><li><p>A  output means the same input can produce different responses. A higher temperature allows the model to choose from a wider range of probable tokens, making its answers more varied. This can be useful if a user asks the chatbot to <em>‚Äúexplain it another way.‚Äù</em></p></li></ul><p>Once you start using LLMs, you‚Äôll notice they don‚Äôt always know the right answer. That failure mode, confident but false output, is called . This can be partly solved through architectures and extensions built on top of LLMs.</p><h2>Architectures and Extensions: Building Beyond the Basics</h2><p> is the principle of forcing an LLM‚Äôs output to be based only on a verifiable, external source of truth that we provide. This is one of the most straightforward ways to mitigate hallucination (to some extent). Instead of relying on its vast but unreliable internal memory, we connect the model to trusted data sources. If the information isn‚Äôt available, a grounded system should be able to say it doesn‚Äôt know‚Äîrather than guess.</p><h3><strong>18. Retrieval-Augmented Generation (RAG)</strong></h3><p>So, how do we technically implement  in real time? The architecture used for this is <strong>Retrieval-Augmented Generation (RAG)</strong>. It enhances accuracy by connecting the model to a knowledge base or external data source the moment it‚Äôs needed.</p><p>A widely known example is . When you ask a question, it doesn‚Äôt just pull from memory. It searches the web, finds relevant sources, and adds this information to its answer. The RAG process works in three steps:</p><ol><li><p> The system first searches trusted documents or web sources to find the most relevant text snippets.</p></li><li><p> These snippets are then automatically added to the prompt, giving the LLM a kind of ‚Äúcheat sheet‚Äù with the correct information.</p></li><li><p> The LLM is instructed to generate an answer based  on the retrieved evidence.</p></li></ol><p>This way, every response is grounded in a verifiable source, which both improves accuracy and helps users trust the output. Models can be expanded to do more tasks. They can have different levels of autonomy to address other limitations.</p><p>There are two fundamental ways to build an  system, each with different levels of control and flexibility.</p><ul><li><p>A  is a system where the developer defines a <strong>fixed, predictable sequence of steps</strong>. The LLM is a component within this process. A RAG system like Perplexity always follows the same ‚ÄúRetrieve ‚Üí Augment ‚Üí Generate‚Äù process. Workflows are highly reliable and controllable.</p></li><li><p>An  is a system where the LLM acts as the  that directs its own process. Instead of following a fixed path, the agent is given a goal and a set of tools (like a web search or a calculator). It then dynamically plans which tools to use, and in what order, to achieve the goal. Agents are far more flexible but less predictable.</p></li></ul><p>Most chatbots today are reactive. They wait for a question and give one answer.  raises a bigger question: can the system plan and carry out multi-step tasks on its own?</p><p>An  system allows an LLM to plan and perform actions to reach a complex goal. This changes the model from a simple tool into the ‚Äúbrain‚Äù of a system that can act independently. For example, instead of just answering , an agentic assistant could take a request like <em>‚ÄúCreate a study guide on fine-tuning.‚Äù</em> It could then search for documents, pull out key concepts, and put them into a structured summary‚Äîwithout needing more input from a human.</p><p>Orchestration (workflow vs. agent) is only part of the design. The other part is the model you pick. This includes size, modality, and deployment. These factors affect cost, latency, privacy, and capability.</p><h2><strong>Different Flavors of Models:</strong> LLM Families and Tradeoffs</h2><h3><strong>21. Proprietary vs. Open-Source Models</strong></h3><p>At some point, anyone building with LLMs faces a practical decision: <em>which kind of model should I use?</em> If you‚Äôre just experimenting, this choice may not be very important. You‚Äôll probably begin with a proprietary API like ChatGPT since it‚Äôs easy to use and available. But as soon as you want to deploy something at scale, reduce costs, or customize a system, the type of model you choose becomes critical.</p><p>There are three main categories, each with significant tradeoffs in terms of cost, control, and complexity:</p><ul><li><p> These models (like OpenAI‚Äôs GPT-5) are owned and operated by a company. You access them as a paid service, and cannot see or modify the model‚Äôs internal workings. Many developers start here because proprietary models offer powerful capabilities and are easy to integrate using APIs.</p></li><li><p> These models (like Meta‚Äôs Llama 3.1, Mistral 7B, or Google‚Äôs Gemma 2) are released with their weights available to the public. However, they aren‚Äôt fully ‚Äúopen-source.‚Äù The training data and methods are usually kept private, and licenses can have restrictions. Open-weight models give you transparency and flexibility to run them yourself while still benefiting from cutting-edge performance.</p></li><li><p> Truly open models share not just the weights. They also provide the training code, data, and methods. All this is under permissive licenses. They maximize control and reproducibility, but they often fall short of the best proprietary or open-weight systems in performance.</p></li></ul><h3><strong>22. API (Application Programming Interface)</strong></h3><p>Whichever model you choose, proprietary, open-weight, or open-source, you‚Äôll need a way for your application to actually talk to it. In most cases, especially when starting out, that connection happens through an .</p><p>An <strong>API (Application Programming Interface)</strong> is a way for our app to talk to the model provider. It sends a  and gets back the generated text. Think of it like ordering food with a delivery app. The app doesn‚Äôt cook your meal. Instead, it sends your order to the restaurant. Then, it delivers the dish to you. In the same way, your code doesn‚Äôt run the massive LLM‚Äîit sends a request to the provider‚Äôs servers through the API, and the model returns the response.</p><p>For example, when you use ChatGPT in your browser, you‚Äôre not running GPT-5 on your laptop. Your message is sent through an API to OpenAI‚Äôs servers, where the model generates a reply and sends it back to your screen.</p><p>Even if you use open-weight models on your own, you‚Äôll still likely share them through an API. This lets your application interact with them the same way.</p><h3><strong>23. SLM (Small Language Model)</strong></h3><p>While powerful, large models can be costly to run. A third option is emerging that offers a solution: the <strong>SLM (Small Language Model)</strong>.</p><p> are efficient models with fewer parameters, usually under 15 billion. They are made to excel at specific tasks. Their small size makes them fast, cheap to run, and capable of operating on local devices like a laptop or smartphone.</p><p>For example,  and  are SLMs that can run on consumer-grade hardware. This allows apps to offer features like private chat, offline assistants, or on-device copilots. A personal AI on your phone means you keep your data private. You won‚Äôt need to send anything to the cloud. This leads to lower costs and access even when you‚Äôre offline.</p><h3><strong>24. Modality &amp; Multimodality</strong></h3><p>Currently, many models only handle a single type of input: text. That type of input is called a . If you upload a diagram and ask, <em>‚ÄúWhat does this chart mean?‚Äù</em>, a text-only model can‚Äôt help. The model needs to be . It should read your question and look at the image at the same time.</p><p>Some modern systems already do this. GPT-4o and Gemini 1.5 Pro can take text, images, and audio together, which makes their answers more contextual and useful.</p><p>A quick note on image generation: many tools use an LLM with a . This model starts with noise and gradually ‚Äúde-noises‚Äù it to create a picture, guided by the text. Popular examples include Stable Diffusion, Midjourney, and DALL¬∑E. Other models are multimodal. They combine text and image generation in one system. This lets them understand and create visuals without needing an external tool.</p><p>The first approach is modular and flexible; the second feels more seamless. Both involve tradeoffs across quality, control, speed, and cost.</p><p>Reasoning models are a newer class of LLMs designed for multi-step problem solving. They don‚Äôt rush to reply. Instead, they take a moment to jot down some notes. This helps them stay focused on tasks. It helps to compare options, follow rules, do simple math, or answer ‚Äúexplain-then-decide‚Äù questions. You can think of them as models with a built-in version of ‚Äúlet‚Äôs think step by step.‚Äù Choose a reasoning model when the tough part is thinking. This includes synthesizing ideas, weighing trade-offs, or linking steps together. Expect a trade-off. These models usually take longer and can cost more. However, compact instruct models are still better for quick definitions, short rewrites, or simple lookups.</p><p>Architectural categories alone don‚Äôt tell us whether a model is suitable. To move from tradeoffs on paper to practical use, we need ways of measuring model capability in a standardized way.</p><h2> How We Know If They‚Äôre Any Good</h2><p>When choosing between models like GPT-4o, Llama 3.1, or Claude 3, how do we objectively compare their raw capabilities? The answer is .</p><p> are standardized tests used to measure and compare the abilities of different LLMs. These cover a wide range of tasks: general knowledge (e.g., ), coding (e.g., ), or logical reasoning (e.g., ). Testing models on the same benchmark gives us scores. These scores help us rank the models and find their strengths and weaknesses. This is important before using them in real-world applications.</p><p>It‚Äôs important to note that benchmarks are  The best-performing model for coding may not excel in reasoning or summarization. Also, new benchmarks are always emerging. This means rankings shift as models improve and focus on different tasks.</p><p>Two popular, real-world benchmark leaderboards are:</p><p>A high  score shows that a model is capable, but it doesn‚Äôt guarantee it will perform well in your application. Even the best model can give bad answers. This happens if prompts are weak, the retrieval system shows irrelevant documents, or the outputs are unclear.</p><p>That‚Äôs why we also measure ‚Äîspecific indicators of quality tailored to the use case. For example, in a RAG-based chatbot or assistant, two common metrics are:</p><ul><li><p> Does the answer stick  to the retrieved documents? (Helps measure hallucination control.)</p></li><li><p> Does the answer directly address the user‚Äôs question? (Measures the quality of retrieval and grounding.)</p></li></ul><p>Metrics let us move from ‚Äúis the model good in general?‚Äù to ‚Äúis the system good for  users?‚Äù</p><p>We have our , like faithfulness and relevance. But how do we evaluate them across thousands of student conversations? Manually checking every single answer is impossible. This is a problem of evaluation at scale.</p><p>The solution is a technique called . This method uses a powerful, state-of-the-art LLM (the ‚Äújudge‚Äù) to automate the evaluation of another model‚Äôs output. We provide the judge model with the original prompt, the candidate model‚Äôs response, and an evaluation rubric based on our metrics. The judge then returns a score and explanation.</p><p>This makes it possible to run large-scale evaluations quickly and consistently. For example, many research labs now use GPT-5 or Claude Opus as ‚Äújudges‚Äù. They evaluate smaller models on tasks like faithfulness, reasoning, and style.</p><p>Benchmarks and metrics help us see how capable a model is, but they don‚Äôt show the full picture. To really understand how these systems behave, we also need to look closely at the ways they go wrong.</p><h2>Where They Fail (and How to Patch Them)</h2><p>One major issue with large language models is hallucination. This is when they confidently create false information. LLMs aim to predict the next likely word, not to check facts. As a result, they can generate text that sounds convincing but is completely made up. For example, a model might invent citations, creating realistic-looking references to non-existent research papers. Lawyers have reported models fabricating court cases. Users have also seen them generate biographies with incorrect career details. The danger lies not just in wrong answers, but in how persuasively they are presented. This makes errors harder to spot. In fields like medicine, finance, or law, even one hallucination can cause major harm.</p><h3><strong>30. Poor Mathematical &amp; Logical Reasoning</strong></h3><p>Although LLMs appear fluent, they are not built to follow strict logic or carry out calculations. They can mimic mathematical expressions, but without the reliability of a calculator or solver. This weakness appears when the model multiplies large numbers or solves multi-step problems. It may provide the correct first step but then go off track, leading to an inconsistent conclusion. For example, earlier GPT versions often struggled with basic math. They confidently claimed that 7 √ó 8 = 54. They also had trouble with logical puzzles that needed careful thinking. These errors highlight the model‚Äôs nature as a pattern-matcher, not a thinker. In practice, this makes them risky for tasks like financial modeling, scientific analysis, or debugging code, unless paired with external tools that enforce step-by-step precision.</p><p>Every LLM inherits bias from its training data. Internet text reflects a wide range of human perspectives, including stereotypes and prejudices. Here, bias means a tendency in how the model responds. Some biases can be helpful, while others are harmful. A biased model may favor one group over another‚Äîfor instance, linking men with technical jobs and women with caregiving roles. Studies show that models can produce biased outcomes in job recommendations, sentiment analysis, and image generation. This issue is both social and practical: biased outputs can erode trust, reinforce inequalities, or harm a brand‚Äôs reputation. On a positive note, intentional ‚Äúbiasing‚Äù can promote useful traits, such as helping models maintain a patient or supportive tone in customer service.</p><p>Another structural limitation is the knowledge cutoff. Since models are trained on data available only up to a certain date, their knowledge is frozen in time. GPT-3.5, for example, could not answer questions about events after 2021, including the launch of ChatGPT itself. A user asking about a new AI paper from last week or the latest programming language version might receive an outdated or made-up answer. This lag makes LLMs unreliable in fast-moving areas. This includes current events, new research, and company-specific knowledge. Without retrieval mechanisms or fine-tuning on more recent data, they cannot bridge this temporal gap, and users must be cautious not to treat them as up-to-date sources.</p><h3><strong>33. Guardrails / Safety Filters</strong></h3><p>Even when a model is accurate, it can still fail by producing content that is unsafe, inappropriate, or off-topic. Guardrails and safety filters are systems designed to prevent this. They work by screening both the user‚Äôs input and the model‚Äôs output against defined rules, ensuring the assistant stays within safe and relevant boundaries. A good example is when someone asks a chatbot how to build a weapon. A well-protected system will refuse. In contrast, an unprotected one might try to help. Companies like OpenAI and Anthropic enforce such filters to block responses related to violence, self-harm, or private data. Without these measures, AI applications can face reputational damage, violate regulations, or harm user experiences. Guardrails are what turn a raw language model into something that can be trusted in professional and everyday use.</p><p>Each weakness‚Äîhallucination, reasoning errors, bias, outdated knowledge, and lack of guardrails‚Äîhas its own technical fix. None of them is sufficient alone, and each comes with tradeoffs.</p><ul><li><p> is best reduced through , often using <em>Retrieval-Augmented Generation (RAG)</em>. Instead of relying on its unreliable memory, RAG adds trusted documents to the prompt. This keeps the model‚Äôs answer tied to verifiable sources, but it needs a strong knowledge base to work well.</p></li><li><p> can improve when we pair models with tools like calculators, code interpreters, or structured workflows. Here, the model doesn‚Äôt try to do all the work itself but acts as a ‚Äúrouter‚Äù that delegates subtasks to the right resource. This boosts reliability in math, logic, and multi-step tasks. However, it comes with increased latency and more complex systems.</p></li><li><p> is managed through  like Reinforcement Learning from Human Feedback (RLHF), carefully designed , and . Together, these methods nudge the model toward outputs that are helpful and fair. Importantly, bias can also be shaped deliberately: for example, configuring a support assistant to always adopt a patient and encouraging tone.</p></li><li><p> can be overcome in multiple ways. RAG lets you add private or recent documents to old training data. Fine-tuning on newer datasets helps models fit specific areas. Live web search offers real-time access. Each option offers a different mix of freshness, accuracy, privacy, and cost. Therefore, the right choice depends on the situation.</p></li><li><p> serve as the final safety layer. They filter incoming queries and outgoing responses. This enforces scope and prevents harmful, irrelevant, or sensitive outputs. Effective guardrails combine static rules with dynamic monitoring, allowing flexibility without compromising safety.</p></li></ul><p>In practice, the hard part isn‚Äôt knowing that hallucination or bias exist, or that tools like retrieval and fine-tuning are available. The real challenge lies in deciding which combination of techniques makes sense for a specific context. A financial assistant, a medical chatbot, and an educational tutor each need different mixes of retrieval, reasoning, alignment, and guardrails.</p><p>Every decision‚Äîaccuracy vs. cost, freshness vs. safety‚Äîinvolves a tradeoff. Building reliable AI isn‚Äôt about removing limits but about designing systems that handle them well.</p><p>Large language models are advanced pattern-matchers, not sources of truth. Their strengths include fluency, reasoning, and broad knowledge. But they also have weaknesses like hallucinations, bias, and outdated information. What matters is how we design around them: choosing the right mix of prompting, retrieval, fine-tuning, and guardrails.</p><p>If you remember one thing, let it be this: knowing the basic concepts gives you the ability to use LLMs more effectively and see their limits clearly. That‚Äôs the difference between treating them as magic or completely unreliable and building systems you can trust.</p><p>Join his  to learn how to use AI effectively in your daily work. It‚Äôll teach you how to apply key ideas for real productivity gains and make smarter decisions about when to prompt, retrieve, or verify AI outputs. </p><p>(Use the code  to get 15% off!)</p><p>Plus, follow his highly relevant video series to learn more about AI:</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 180,001+ readers strong, very close to 181k. Let‚Äôs try to get 181k readers by 5 November. Consider sharing this post with your friends and get rewards.</p>","contentLength":40871,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/9229094e-e2d1-46e8-a103-a5c5e1bb86b4_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How RPC Actually Works","url":"https://newsletter.systemdesign.one/p/how-rpc-works","date":1761735789,"author":"Neo Kim","guid":31,"unread":true,"content":"<p>Download my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines how RPC works.</em></p><p>How do hundreds of internal services at a company like Netflix or Google talk to each other? And that too, at millions of times per minute?</p><p>A standard REST API, while great for public use, often creates too much overhead for this internal, high-speed chatter. Because it typically uses text-based formats like JSON. While easy for humans to read, it‚Äôs inefficient for services that need to have millions of conversations per day.</p><p>Instead, these services need to talk in a language that is incredibly compact and fast for computers to process. Plus, they need a pre-agreed set of rules, like a shared blueprint, that guarantees both sides know exactly how to talk to each other without errors.</p><p>And most importantly, it should offer a good developer experience without sacrificing performance.</p><p>Modern applications use many distributed services. Yet this creates a core challenge:</p><p>How to achieve extreme machine-to-machine efficiency? A function call across a network should feel as simple as a local one.</p><p>The answer is Remote Procedure Call). It‚Äôs a protocol designed for high-performance internal communication.</p><p>I want to introduce <a href=\"https://x.com/asmah2107\">Ashutosh</a> as a guest author.</p><p>He‚Äôs a software engineer at YouTube, ex-Google Search, and Microsoft Azure.</p><p>Connect with him if you want to get deep dives into system design, AI, and software engineering.</p><p> is a conversational AI and career matchmaker that works on behalf of each person. You spend 15-20 minutes on the phone with him, talking about your experience, your ambitions, and your non-negotiables.</p><p>Dex then scans thousands of roles and companies to identify the most interesting and compatible opportunities.</p><p>Once we‚Äôve found a match,  connects you to hiring managers and even helps you prep for interviews.</p><p>Thousands of exceptional engineers have already signed up, and we‚Äôre partnered with many of the UK‚Äôs leading start-ups, scale-ups, hedge funds, and tech companies.</p><p>Don‚Äôt waste another day at a job you hate. Speak with Dex today.</p><h2>What Is a Remote Procedure Call</h2><p>RPC is a protocol that allows a function call to cross from one program‚Äôs address space into another. This frees the developer from manually managing the raw network connection points () or translating data into a sendable format ().</p><p>When your code calls a function like , you aren‚Äôt concerned with the complex algorithm running inside the math library. You simply trust its public API and know the function‚Äôs signature.</p><p>RPC extends this exact concept across the network.</p><p>The client application gets a stub object - a local proxy that mirrors the remote service‚Äôs API. But its methods contain only the logic to forward the call over the network. When you call a function on this stub, it handles all the complex network communication behind the scenes to execute the code on the server.</p><p>So, a call like <code>result = paymentService.charge(userId, amount)</code>doesn‚Äôt call the payment logic directly. Instead, it calls a local proxy method, which then makes a remote operation. This makes the remote call feel like a simple local library call.</p><p>Before we go further, let‚Äôs understand this first:</p><h2>How Remote Procedure Call Works</h2><h4>1. Client Stub (The Local Representative)</h4><p>Your code doesn‚Äôt call the remote service directly. Instead, it talks to the  - a local representative whose only job is to act as a stand-in for the remote service.</p><p>It has the same methods you want to use, for example, ‚Äú. But its real purpose is to kick off the remote communication process.</p><h4>2. Marshaling (Packing the Message)</h4><p>The stub takes the parameters you provided, like ,  and marshals them. </p><p>Marshaling is a fancy word for serializing. It means converting the data from a programming language format to a standardized format, so you can send it over the network.</p><p>Think of it as neatly packing your instructions into a universal shipping container that any server in the world can open. This format is often JSON or a more compact, faster format like <a href=\"https://protobuf.dev/overview/\">Protocol Buffers</a>.</p><p>The client then sends this packed message across the network to the server, using efficient protocols like <a href=\"https://en.wikipedia.org/wiki/HTTP/2\">HTTP/2</a>. This is the ‚Äúremote‚Äù part of the call.</p><p>On the server, a ‚Äúskeleton‚Äù is listening. It‚Äôs the counterpart of the client stub.</p><p>Its job is to receive the incoming message and unmarshal (or deserialize) it. Imagine unpacking the shipping container back into a data format the server can understand.</p><p>The skeleton then calls the actual service method with this unpacked data.</p><p>After the server finishes its work, the entire process happens in reverse.</p><p>The skeleton marshals the return value, for example, . Then sends it back across the network, and the client stub unmarshals it.</p><p>After that, the line of code that made the original call receives the result.</p><p>Next, I‚Äôll walk you through different RPC failure scenarios and approaches to handle them.</p><h2>Handling Failures in Remote Procedure Call</h2><p>In a distributed system, the network might be unreliable.</p><p>Connections drop. Services slow down. Failures occur.</p><p>A robust RPC implementation isn‚Äôt just about successful calls, but also about gracefully handling failures. Here are some strategies:</p><h4>1. Timeouts (Don‚Äôt Wait Forever)</h4><p>A timeout is a simple deadline. </p><p>Whenever the client makes a request, it starts a timer. If it doesn‚Äôt receive a response before the timer runs out, it gives up on the call.</p><p>: Without timeouts, your application could get stuck waiting indefinitely for a response from a slow or dead service. This would tie up resources, such as threads or memory, and could eventually freeze your entire application. Timeouts are the first line of defense to protect the client‚Äôs health.</p><h4>2. Retries (If at First Call Doesn‚Äôt Succeed)</h4><p>Many network errors are transient: a temporary glitch that quickly resolves itself.</p><p>Sometimes, a server might take too long to respond, or a network packet might get lost in transit. In these cases, it makes sense to retry the failed request automatically.</p><p><em>Important Caveat (Idempotency)</em>: Yet retries can be dangerous if the same request causes side effects each time it runs. An operation is idempotent if performing it many times has the same effect as doing it once.</p><ul><li><p>Safe to Retry (Idempotent): Getting a user‚Äôs account balance. Doing it again won‚Äôt change the system‚Äôs state.</p></li><li><p>Dangerous to Retry (Not Idempotent): Charging a user‚Äôs credit card. Retrying it might cause duplicate payments.</p></li></ul><p>So developers must design their systems with idempotency safeguards, such as unique transaction IDs, to retry safely.</p><h4>3. Circuit Breakers (Protecting the System)</h4><p>When a service fails or slows down, sending it more requests will only make things worse.</p><p>A circuit breaker is a smart pattern that prevents this from happening.</p><p>If the client notices many failed calls to a service, the circuit breaker ‚Äútrips‚Äù and stops sending requests to that service for a short time. During this period, any new calls fail instantly instead of being sent over the network.</p><p> Circuit breakers prevent cascading failures. They give the failing service time to recover. And keep the client from wasting time and resources on calls that are likely to fail.</p><h4>4. Deadline Propagation (Don‚Äôt Start Work You Can‚Äôt Finish)</h4><p>A single user request often triggers a chain of calls between several services. For example, Service A calls B, and B calls C.</p><p>If the original request has a total timeout of 500ms, and Service A already uses 300ms. Then there‚Äôs no point in Service C starting a task that takes another 400ms because the client would have already stopped waiting.</p><p> fixes this by passing the remaining time limit along with every call. So each service knows the remaining time and can decide whether it can finish its part before the deadline.</p><p>This approach prevents wasted work and keeps the system fast and responsive. Services can ‚Äúfail fast‚Äù instead of doing long operations for a request that‚Äôs already too late to matter.</p><p>Like every technology, RPC isn‚Äôt perfect; it has some strengths, but also tradeoffs to watch out for. Let‚Äôs keep going!</p><h2>RPC: The Good, the Bad, and the Ugly</h2><ul><li><p>Simple programming model: You can call remote functions just like local ones, which makes your code cleaner and simpler.</p></li><li><p>High performance: RPC uses compact, binary formats that computers can process quickly.</p></li><li><p>Strong typing: Because both sides share a strict contract, it often detects mistakes early before they cause problems.</p></li><li><p>Language interoperability: Client and server can use different programming languages and still communicate smoothly.</p></li></ul><ul><li><p>Tight coupling: If the server‚Äôs API changes, the client usually needs to update too, which can slow down development.</p></li><li><p>Less discoverable than REST: You can‚Äôt easily test or browse RPC APIs without the specific contract files.</p></li><li><p>Requires specialized tooling: You need special tools to generate the code for the client and server.</p></li><li><p>Abstraction hides network realities: Because remote calls look like local ones, developers might forget they‚Äôre dealing with the network and forget to handle timeouts or errors properly.</p></li></ul><p>Now let‚Äôs look at how RPC actually works in real-world systems and the challenges of running it at scale.</p><p>Using RPC in real systems comes with several practical challenges. Here are four common ones developers need to handle:</p><h4><strong>1. Service Discovery (How Do Services Find Each Other?)</strong></h4><p>In modern systems, servers are always changing; they‚Äôre added, removed, or restarted, often getting new IP addresses.</p><p>But you can‚Äôt just hardcode a server‚Äôs location in your code. So how does a client know where to send its request?</p><p>That‚Äôs where  comes in.</p><p>Think of it like a live, self-updating contact list. You don‚Äôt need to remember your friend‚Äôs exact home address. Instead, you simply look up their name in your contact list to find the latest address.</p><p>A central registry (like <a href=\"https://developer.hashicorp.com/consul\">Consul</a> or <a href=\"https://zookeeper.apache.org/\">Zookeeper</a>) keeps track of all healthy, running services and their locations. So when a client wants to talk to the Payment Service, it asks the registry, ‚ÄúWhere can I find it?‚Äù.</p><p>The registry then returns a list of healthy servers, and the client picks one to send the RPC call.</p><h4><strong>2. API Evolution (How Do You Update Services Without Breaking Everything?)</strong></h4><p>When a service adds a new feature, it might need to change a function‚Äôs parameters. Things can break if clients do not update right away; this is because of tight coupling.</p><p>The solution is .</p><p>Think of a survey form you‚Äôve sent out. You can safely add a new optional question; old forms still work. But if you delete a question or make a new one required, all old forms become invalid.</p><p>RPCframeworks like <a href=\"https://grpc.io/\">gRPC</a> follow strict rules for updating APIs. For example, you can safely add optional fields, but you shouldn‚Äôt remove or change existing ones. This allows new versions of a service to run smoothly while older clients keep working.</p><h4><strong>3. Streaming (More Than Just Request and Response)</strong></h4><p>RPC doesn‚Äôt limit itself to the ‚Äúone request, one response‚Äù model.</p><p>Modern frameworks like gRPC support , where data flows continuously between client and server.</p><p>A regular RPC call is like sending a text message and getting one reply. A streaming RPC is like a live phone call or video stream; both sides can exchange information in real time.</p><ul><li><p>Server Streaming: Client sends one request and gets back a stream of responses (e.g., subscribing to live updates).</p></li><li><p>Client Streaming: Client sends a stream of messages and gets back one response (e.g., uploading a large file).</p></li><li><p>Bidirectional Streaming: Both client and server can send messages at any time (e.g., a real-time chat).</p></li></ul><h4><strong>4. Error Handling &amp; Status Codes (Communicating What Went Wrong)</strong></h4><p>When something fails, the client needs more detail than just an ‚Äúit failed‚Äù message. So it can take the necessary steps.</p><p>That‚Äôs where  come in.</p><p>Just like HTTP status codes (404 Not Found, 403 Forbidden, 500 Internal Server Error), RPC frameworks have their own standardized codes:</p><ul></ul><p>By returning a specific error code, the server tells the client what to do next.</p><ul><li><p>Retry the call if the service was temporarily unavailable.</p></li><li><p>Report a user input error if the argument was invalid.</p></li><li><p>Or stop trying if it's impossible to fix the problem automatically.</p></li></ul><p>Now that we‚Äôve seen RPC in real-world systems, let‚Äôs look at how it compares to other communication patterns.</p><h2>Choosing the Right Communication Pattern</h2><ul><li><p>RPC (<a href=\"https://grpc.io/\">gRPC</a>, <a href=\"https://thrift.apache.org/\">Thrift</a>): Best for high-performance internal microservices.</p></li><li><p>REST: Ideal for public APIs and resource-oriented operations.</p></li><li><p><a href=\"https://graphql.org/\">GraphQL</a>: Great for client-specific data fetching requirements.</p></li><li><p>Message Queues: Perfect for asynchronous, decoupled workflows.</p></li></ul><p>RPC hides most of the network details, but it doesn‚Äôt make them disappear; you still need to design for failures.</p><p>While gRPC with Protocol Buffers has become the new standard for fast, reliable communication between internal services.</p><p>Remember, a successful RPC setup depends on supporting systems. So service discovery, tracing, and monitoring are necessary to keep everything running smoothly.</p><p>üëã I‚Äôd like to thank  for writing this newsletter!</p><p>Plus, don‚Äôt forget to connect with him on:</p><p>He offers good deep dives into system design, AI, and software engineering.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>üì∞</p><p>Thank you for supporting this newsletter.</p><p>You are now 180,001+ readers strong, very close to 181k. Let‚Äôs try to get 181k readers by 31 October. Consider sharing this post with your friends and get rewards.</p><ul><li><p>Block diagrams created with <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a></p></li></ul>","contentLength":13493,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/6952f430-02f4-4dbb-8054-20b9dc33b9d0_1280x720.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}