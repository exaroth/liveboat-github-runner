{"id":"6C2W1azD1rBsV4qJANNfruBrvGgdqLWrPGWtk6Mp1WgfCNcfvBZHFQw","title":"The System Design Newsletter","displayTitle":"Dev - System Design Newsletter","url":"https://newsletter.systemdesign.one/feed","feedLink":"https://newsletter.systemdesign.one/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":7,"items":[{"title":"How Does Google Docs Work 🔥","url":"https://newsletter.systemdesign.one/p/how-does-google-docs-work","date":1755863445,"author":"Neo Kim","guid":32,"unread":true,"content":"<p>Unlock access to every deep dive article by becoming a paid subscriber:</p><p>I spent hours studying how Google Docs works so you don't have to. And I wrote this newsletter to make the key concepts simple and easy for you.</p><p><em>Note: This post is based on my research and may differ from real-world implementation.</em></p><p>Once upon a time, there lived a data analyst named Maria.</p><p>She emailed draft copies many times to different people to prepare monthly reports.</p><p>So she wasted a ton of time and was frustrated.</p><p>Until one day, when she decides to use Google Docs for it.</p><p>Google Docs allows collaborative editing over the internet. It means many users can work on the same document in real-time.</p><p>Yet it’s difficult to implement Google Docs correctly for 3 reasons:</p><ul><li><p>Concurrent changes to the same document should converge to the same version.</p></li><li><p>Concurrent changes to the same document must avoid conflicts.</p></li><li><p>Any changes should be visible in real-time to each user.</p></li></ul><p>Also a user should be able to make changes while they’re offline.</p><p>A simple approach to handle concurrency is using pessimistic concurrency control.</p><p>is amechanism for handling concurrency using a lock. It offers strong consistency, but doesn’t support collaborative editing in real-time. Because it needs a central coordinator to handle data changes, only 1 user can edit at a time. Put simply, only a single document copy is available for write operations at once, while other document copies are read-only.</p><p>Besides it doesn’t support offline changes.</p><p>Also a network round-trip across the Earth takes 200 milliseconds. </p><p>This might cause a poor user experience. So they do  The idea is to keep a document copy for each user locally and then run operations locally for high responsiveness. Thus creating the illusion of lower latency than reality.</p><p>And the system propagates the changes to all users for consistency.</p><p>A simple approach for latency hiding is using the mechanism.</p><p>Yet it resolves a conflict without waiting for coordination by applying the most recent update. So there’s a risk of data loss when there are concurrent changes in high-latency networks.</p><p>It might be a good choice when concurrency is low. But it isn’t suitable for this use case.</p><p>An alternative approach to latency hiding is through <strong>differential synchronization</strong>.</p><p>It keeps a document copy for each user and tracks the changes locally. The system doesn’t send the entire document when something changes, but only the difference ().</p><p>Yet there’s a performance overhead in sending a diff for every change. Also differential synchronization only tracks diffs, and not the reason behind a change. So conflict resolution might be difficult.</p><p>While resolving conflicts manually affects the user experience.</p><p>OT is an algorithm to show document changes without wait times on high-latency networks. It allows different document copies to accept write operations at once. Also it handles conflict resolution automatically without locks or user interventions. </p><p>Besides OT tolerates divergence among document copies and converges them later.</p><p>Think of <strong>operational transformation</strong> as an event-passing mechanism; it ensures each user has the same document state even with unsynchronized changes.</p><p>With OT, the system saves each change as an event. Put simply, a change doesn’t affect the underlying character of a document; instead, it adds an event to the revision log. The system then displays the document by replaying the revision log from its start.</p><p>Operational transformation saves a document as a set of operations, but it's complex to implement properly.</p><h2>How Does Google Docs Work</h2><p>Google Docs uses a client-server architecture for simplicity.</p>","contentLength":3631,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/4b73f9d4-a8d6-4101-9dff-df53a7332de1_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"HTTP Headers to Build 10X APIs 🔥","url":"https://newsletter.systemdesign.one/p/http-headers","date":1755602100,"author":"Neo Kim","guid":31,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines the must-know HTTP headers.</em></p><p><em>I created the block diagrams in this newsletter with</em></p><p>Once upon a time, there lived a junior software engineer.</p><p>He worked for a tech company named Hooli.</p><p>Although extremely bright, he never got promoted.</p><p>So he was sad and frustrated.</p><p>Until one day, when he had the idea to apply for a job at a unicorn startup.</p><p>And worked hard on solving LeetCode.</p><p>But the interviewer asked him just about HTTP headers.</p><p>And he failed to answer, so the interview was over in 7 minutes.</p><p>So he studied <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers\">web docs</a> later to fill the knowledge gap.</p><p>Authorization can make or break your application’s security and scalability. From managing dynamic permissions to implementing fine-grained access controls, the challenges grow as your requirements and users scale.</p><p>This  will guide you through the 6 key requirements all authorization layers should include to avoid technical debt:</p><ul><li><p>Architectural and design considerations for building a scalable and secure authorization layer.</p></li><li><p>20+ technologies, approaches, and standards to consider for your permission management systems.</p></li><li><p>Practical insights, based on 500+ interviews with engineers, architects, and IAM professionals.</p></li></ul><p>Learn how to create an authorization solution that evolves with your business needs, while avoiding technical debt.</p><p>An HTTP request-response has 2 parts:</p><ul><li><p>Header: tiny pieces of metadata</p></li></ul><p>A request means the client asks for something. While a response means the server sends back something.</p><p>An HTTP header consists of a name and a value. Although headers are invisible to the users, they’re important in transferring information. Yet extra headers could consume bandwidth and affect performance. So it’s necessary to use them correctly.</p><p>And the server sends a status code with each response. It tells the client whether its request succeeded, failed, or needs extra action.</p><p>He failed the interview, so you don’t have to.</p><p>Here’s a summary of what he learned:</p><p>Both requests and responses include these headers.</p><p>It controls the caching behavior of browsers and intermediary caches. For example, caching in proxy servers and CDN. Put simply, it helps to determine if the browser should serve data from its cache when the user revisits a site.</p><p>Here are some of its directives:</p><pre><code><code>Cache-Control: max-age=3600, public</code></code></pre><ul><li><p>It means cache the response in each infrastructure layer for 3600 seconds.</p></li></ul><ul><li><p>It means don’t cache the response anywhere.</p></li></ul><ul><li><p>It means cache the response. But the browser must re-validate with the server before using it.</p></li></ul><p>An incorrect configuration of this header might display stale data to the user. Also there’s a risk of storing private data. Besides forgetting to cache static resources, such as images or scripts, affects the performance badly.</p><p>It indicates when the server sent the response. It’s used to calculate cache freshness and also debug clock issues.</p><pre><code>Date: Sun, 22 Aug 2025 14:00:00 GMT</code></pre><ul><li><p>It means the server sent the response on 22 August 2025 at 14:00 GMT.</p></li></ul><p>An incorrect configuration of this header will make the cache freshness calculation wrong. Also it'd make debugging harder.</p><p>It’s added by proxy servers automatically. And helps to track and debug network routing paths. Put simply, it shows the intermediary servers such as proxies, load balancers, and CDN.</p><pre><code>Via: 1.1 proxy1.example.com, 1.1 proxy2.example.com</code></pre><ul><li><p>It means the message passed through  and  using HTTP/1.1.</p></li></ul><p>So there’s no impact on users if this header is missing. But its absence can make debugging difficult when there are many proxies.</p><p>The client sends these headers to the server to give information about the client or the request.</p><p>It contains the hostname and port number of the server receiving the request. </p><p>Yet it defaults to port 443 for HTTPS if the client specifies nothing. It’s helpful when the server handles different sites via <a href=\"https://en.wikipedia.org/wiki/Virtual_hosting\">virtual hosting</a> on the same IP address.</p><ul><li><p>It means the request is for the blog subdomain (virtual host) on the server.</p></li></ul><p>The server might return the default site or a 400 error status code if this header is missing. Put simply, an incorrect header will prevent the request from reaching the correct site.</p><p>It informs the server about the client’s browser and operating system. It’s useful for analytics, compatibility workarounds, and content optimization (mobile vs desktop).</p><pre><code>User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/114.0.0.0 Safari/537.36</code></pre><ul><li><p>It means the request came from Google Chrome 114 on a 64-bit Windows 10 machine.</p></li></ul><p>Some older browsers might receive incompatible content if this header is missing. Also a wrong header might serve incorrect data to the mobile client. Thus affecting performance badly.</p><p>It informs the server of the content format expected in the response. For example, JSON or HTML.</p><ul><li><p>It means the client prefers a JSON response.</p></li></ul><p>The server responds with its default format if this header is missing. And the client might fail to process the response. Besides the server responds with a 406 Not Acceptable error code if it doesn’t support the requested format.</p><p>It informs the server of the human language expected in the response. Put simply, it tells the browser’s language preference. For example, English or German.</p><pre><code>Accept-Language: en-US, de;q=0.9</code></pre><ul><li><p>It means the client prefers content in US English. And prefers German only if it’s unavailable.</p></li></ul><p>The server falls back to its default language if this header is missing or wrongly set. Thus affecting user experience.</p><p>It informs the server of the compression algorithm expected in the response. For example, gzip or deflate. It helps to reduce the response size by compressing it, thus saving bandwidth.</p><pre><code>Accept-Encoding: gzip, deflate, br</code></pre><ul><li><p>It means the client can handle responses compressed with gzip, deflate, or brotli (br).</p></li></ul><p>The server sends an uncompressed response if this header is missing. Thus affecting performance badly. Also an incorrect header might create a response in the wrong compression format. And the client might fail to decode it.</p><p>It lets the client send its cookies to the server. Think of a  as a piece of data stored on the browser. It helps to remember things like logins, preferences, or sessions.</p><pre><code><code>Cookie: session_id=abc567; theme=dark</code></code></pre><ul><li><p>It means the client is sending its stored data, such as session ID and theme preference, to the server.</p></li></ul><p>The server might start a new session if this header is missing. Thus affecting the user experience badly.</p><p>It contains the site URL that sent the client to the current page. Put simply, it shows where the request came from. And it’s commonly used in analytics.</p><pre><code>Referer: https://example.com/page1</code></pre><ul><li><p>It means the request came from page1 on example.com</p></li></ul><p>The site traffic analytics might become inaccurate if this header is missing.</p><p>It lets the client send authentication credentials, such as tokens or API keys. Imagine <strong>authentication credentials</strong> as a password or token to prove who the client is. So the server can allow it to access specific resources.</p><pre><code>Authorization: Bearer eyJhbKciOiJm</code></pre><ul><li><p>It means the client is using a Bearer token as proof of identity to access the server. Think of the  as a temporary digital key for access.</p></li></ul><p>The server responds with a 401 Unauthorized error code if this header is missing or invalid. While server responds with a 403 Forbidden error code if the client has insufficient permission.</p><p>It lets the client download specific byte ranges of a file. It’s useful for streaming media or resuming broken downloads.</p><ul><li><p>It means the client is telling the server to send only the first 500 bytes of a file.</p></li></ul><p>The server sends the entire file on each request if this header is missing. Thus wasting bandwidth.</p><p>It tells the server to send the resource only if an update occurred on it after a specific period. Put simply, it lets the client make conditional GET requests.</p><pre><code>If-Modified-Since: Tue, 11 Jun 2024 10:00:00 GMT</code></pre><ul><li><p>It means the client is asking the server to send the resource only if it has changed since 11 June 2024, 10 hours GMT.</p></li></ul><p>The server sends a fresh copy every time if this header is missing. Thus wasting bandwidth. While the server responds with 304 Not Modified without a body if the resource hasn’t changed.</p><p>It tells the server to send the resource only if the client’s ETag doesn’t match the server’s current ETag. Think of  as a unique resource version identifier for a resource. Put simply, it lets the client make conditional GET requests, but much more precisely.</p><ul><li><p>It means the client is asking the server to send the resource only if its current ETag differs from </p></li></ul><p>The server sends a fresh copy every time if this header is missing. Thus wasting bandwidth and affecting performance.</p><p>The client doesn't add this header, but the proxy does. </p><p>It lets the server know the client's IP address even if the request passed through proxies or load balancers. It’s useful for analytics, logging, and rate limiting.</p><pre><code>X-Forwarded-For: 203.0.113.21</code></pre><ul><li><p>It means the request originally came from the client with IP address 203.0.113.21</p></li></ul><p>While the server only sees the proxy or load balancer’s IP address if the header is missing. Thus making rate limiting difficult and analytics less accurate.</p><p>The proxy or load balancer adds this header.</p><p>It tells the server the original protocol the client used before the request passed through proxies or load balancers. For example, HTTP or HTTPS.</p><p>It helps the server to generate correct absolute URLs that match the client’s protocol.</p><ul><li><p>It means the original client used HTTPS.</p></li></ul><p>The server might return insecure HTTP links to the client if this header is missing.</p><p>The proxy or load balancer adds this header. </p><p>It tells the server the original port the client used before the request passed through proxies or load balancers. For example, port 80 for HTTP or 443 for HTTPS.</p><p>It helps the server generate correct absolute URLs for redirects.</p><ul><li><p>It means the original client used port 443 (default for HTTPS).</p></li></ul><p>The server might return incorrect absolute links to the client if this header is missing.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p>The server sends these headers to give instructions to the client.</p><p>It’s used by the server to redirect the client to a specific URL. For example, the server sends this header after a form submission to tell the browser where the new resource is.</p><pre><code>Location: https://example.com/welcome</code></pre><ul><li><p>It means the server is telling the client to redirect to example.com/welcome</p></li></ul><p>The client might get redirected to a broken URL if this header is wrong. While the client will stay on the same URL if this header is missing.</p><p>It lets the server store cookies on the client. The client then includes it in future requests. Imagine a  as a small piece of data stored on the client. It allows session management, personalization, and tracking.</p><pre><code>Set-Cookie: session_id=abc123; Path=/; HttpOnly</code></pre><ul><li><p>It means the server is telling the browser to store a session cookie for the entire site and block JavaScript from accessing it.</p></li></ul><p>Each request might get treated as a new session if this header is missing. Also there might be security risks or broken logins if this header is wrong.</p><h4>Access-Control-Allow-Origin</h4><p>It’s the main header for cross-origin requests (). </p><p>It tells the browser which sites are allowed access to a resource on a specific site. While the browser blocks the response if it's missing.</p><p>Imagine an e-commerce store that uses an external payment service. The store tries to fetch the payment confirmation from the payment service.</p><p>Yet browsers have the  It means JavaScript can only read responses from the same site unless the other site allows it. So without CORS, the browser blocks the response. (Even if the request went through successfully.)</p><p>With CORS enabled, the payment service responds with this header:</p><pre><code>Access-Control-Allow-Origin: ecommerce-store.com</code></pre><ul><li><p>It means allow store’s JavaScript to read the payment confirmation.</p></li></ul><p>And the e-commerce store displays the payment confirmation data. This approach prevents random sites from accessing sensitive data.</p><p>Think of this header with a real-world analogy:</p><ul><li><p>Access-Control-Allow-Origin header = guest list for a party</p></li><li><p>Payment service = party host</p></li><li><p>E-commerce store = guest who wants to visit the party</p></li></ul><p>If the payment service doesn’t have the e-commerce store on the guest list, the browser won’t let it in to see the data.</p><p>This header is added by a cache, such as the CDN or proxy.</p><p>It tells how many seconds the response has been in the cache since it was fetched from the origin server. Thus helping the browser decide if the cache is still fresh or needs revalidation.</p><ul><li><p>It means the cached copy is 120 seconds old.</p></li></ul><p>Each time the cache serves a response, it calculates the age and includes it in the header. The cache freshness is then determined by comparing this header value with the cache-control header.</p><p>While the client might show stale data to the user if this header is missing.</p><p>The server sends different responses for the same URL based on request headers. For example, user-agent, accept-encoding, or accept-language. So there are different cache versions for the same URL.</p><p>This header tells the cache which request headers affect the server’s response. Thus ensuring cache to serve the right version of a resource to different clients.</p><ul><li><p>It means the server might send different responses for desktop and mobile users.</p></li></ul><p>The cache might serve the wrong version of a site if this header is missing. Thus breaking pages or showing wrong language.</p><p>It tells the client whether the server supports partial requests. It’s useful for downloading parts of a large media file and for streaming media.</p><ul><li><p>It means the server supports partial requests in bytes.</p></li></ul><p>The client assumes the server doesn’t support partial requests if this header is missing. Thus downloading the file in full.</p><p>It indicates which part of a resource is being sent in response to a partial request. This lets the client assemble the resource correctly from different parts.</p><pre><code>Content-Range: bytes 0-499/1234</code></pre><ul><li><p>It means this response contains bytes 0 through 499 of a resource that is 1234 bytes in total.</p></li></ul><p>The client won't know the part it received if this header is missing. Thus it might fail to assemble the resource correctly.</p><p>It tells the browser which sources are allowed for scripts, styles, and images. Thus preventing cross-site scripting () attacks. Think of XSS as a situation where a hacker injects malicious JavaScript into a site to steal data.</p><p>A hacker could inject malicious JavaScript into the site through a form or comment box. Without this header, the browser runs the script and lets the hacker steal user data.</p><p>But with this header, the browser loads resources only from trusted sources. Thus blocking malicious scripts.</p><pre><code>Content-Security-Policy: default-src 'self'; script-src 'self'</code></pre><ul><li><p>It means the site only allows content and scripts to load from its own domain.</p></li></ul><p>The browser checks every resource against these rules and blocks anything not on the allowed list.</p><h4>Strict-Transport-Security</h4><p>It tells the browser to always use HTTPS for this domain for a specific period.</p><pre><code>Strict-Transport-Security: max-age=31536000; includeSubDomains</code></pre><ul><li><p>It means the browser uses HTTPS only for this site and its subdomains for the next 1 year.</p></li></ul><p>First-time visitors to a site might get downgraded to insecure HTTP if this header is missing. Thus making them vulnerable to man-in-the-middle attacks. </p><p>So include this header with the maximum age for security.</p><p>These headers describe the content body of a request or response.</p><p>It indicates the format of the message body, so the receiver knows how to process it.</p><pre><code>Content-Type: application/json</code></pre><ul><li><p>It means that the body of the message is in JSON format.</p></li></ul><p>The client might fail to interpret the content if this header is wrong or missing.</p><p>It tells the browser how to handle the response body. For example, whether to display it inline or download it as a file.</p><pre><code>Content-Disposition: attachment; filename=\"report.pdf\"</code></pre><ul><li><p>It means the browser prompts the user to download the file.</p></li></ul><p>The browser might just show the file inline if this header is missing. Thus affecting user experience.</p><p>It tells the receiver how big the body is. For example, when uploading or downloading a file.</p><ul><li><p>It means the message body is 348 bytes long.</p></li></ul><p>This helps the client show accurate progress bars for uploads or downloads.</p><p>The receiver might fail to understand where the body ends if this header is missing. Thus misinterpreting the message.</p><p>It tells the client which compression algorithm was applied to the response body. So the client can decode it correctly.</p><ul><li><p>It means the body got compressed with gzip.</p></li></ul><p>The client might fail to decode the content and show an error if this header is wrong.</p><p>It tells the client the human language of the response body. It’s useful in multilingual sites to interpret the content correctly.</p><ul><li><p>It means the response is for US English users.</p></li></ul><p>The client might try to guess the language if this header is missing. Thus causing accessibility or localization issues.</p><p>It tells the client the date and time when the resource was last changed on the server. This helps to determine if the cached copy is still fresh.</p><pre><code>Last-Modified: Tue, 11 Jun 2024 10:00:00 GMT</code></pre><ul><li><p>It means the resource was last changed on 11 June 2024 at 10:00 GMT.</p></li></ul><p>The client wouldn’t be able to make conditional requests if this header is missing. Thus wasting bandwidth.</p><p>Entity Tag (ETag) is a response header. Think of it as a unique version identifier, like a hash or fingerprint, for a resource.</p>","contentLength":17358,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/bd0d0359-c2a8-4267-9a32-393ab47de13f_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Do Webhooks Work ⭐","url":"https://newsletter.systemdesign.one/p/how-do-webhooks-work","date":1755171760,"author":"Neo Kim","guid":30,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines how webhooks work. You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, there was an e-commerce store.</p><p>They used an external payment service to handle orders.</p><p>It means the payment service processes a payment first. </p><p>And only after that, the store sends a confirmation email to the customer.</p><p>Yet they had only a tiny number of customers.</p><p>So they checked for new payments every hour through HTTP polling.</p><p>Imagine  as repeatedly asking a server for updates.</p><p>But one day, their store became extremely popular because of a limited flash sale.</p><p>And they received a massive number of orders in a short period.</p><p>Although explosive growth is a good problem to have, they sent email notifications to customers extremely late because of delays in polling.</p><p>So they set up websockets.</p><p>Think of the  as a way for the client and server to communicate in real-time, in both directions.</p><p>But it needs extra work because of connection management, server scaling, and monitoring. Thus increasing the operational efforts and resource usage.</p><p>They wanted a simple way to solve this problem.</p><p>Imagine  as a way of sending messages to an external service when specific events happen.</p><p> is the only AI coding agent built for real engineering teams.</p><p>It understands your codebase—across 10M+ lines, 10k+ files, and every repo in your stack—so it can actually help: writing functions, fixing CI issues, triaging incidents, and reviewing PRs.All from your IDE or terminal. No vibes. Just progress.</p><p>A webhook isn’t a protocol, but a communication pattern.</p><ul><li><p>Sender: The system where the event happens (external payment service)</p></li><li><p>Event: The action that occurred (payment completion)</p></li><li><p>Receiver: The system that needs to know about the event (e-commerce store)</p></li></ul><p>The external payment service sends an event to the e-commerce store when the customer makes a successful payment.</p><p>And the e-commerce store then sends a confirmation email to the customer.</p><p>They set up a separate API endpoint on the e-commerce store to handle webhook events. And registered its URL with the payment service.</p><p>Here’s the webhook workflow:</p><ol><li><p>The user tries to buy something from the store</p></li><li><p>The user then gets sent to the payment service</p></li><li><p>The payment service creates a webhook event upon successful payment</p></li><li><p>The payment service sends an HTTP POST request to the store’s webhook URL</p></li><li><p>The store validates the payment and emails the customer to confirm the purchase</p></li></ol><p>Also the store notifies the customer in case the payment is invalid.</p><p>Webhooks let systems communicate in real time via HTTP.</p><p>Yet there’s a risk of fake data or spam reaching the webhook endpoint as it’s a public API. So it’s necessary to set up extra <a href=\"https://cheatsheetseries.owasp.org/cheatsheets/Web_Service_Security_Cheat_Sheet.html\">security</a> using signature verification for safety.</p><p>Besides an event delivery could fail because of network issues. Because of this, it’s necessary to retry with <a href=\"https://en.wikipedia.org/wiki/Exponential_backoff\">exponential backoff</a> until it succeeds.</p><p>But a retry might cause duplicate action, such as emailing the customer twice. So it’s necessary to set up the webhook API endpoint with <a href=\"https://newsletter.systemdesign.one/p/idempotent-api\">idempotency</a>.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.</p>","contentLength":3334,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/d821a8d6-0a36-4d04-b0ee-1121b04600b2_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Does HTTPS Work 🔥","url":"https://newsletter.systemdesign.one/p/how-does-https-work","date":1754567169,"author":"Neo Kim","guid":29,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines how HTTPS works. You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, the internet was a quiet place with only a tiny number of servers.</p><p>And the users communicated with servers by sending and receiving files.</p><p>They shared files through File Transfer Protocol (), email attachments, and floppy disks.</p><p>Yet it became difficult to transfer information in a simple and standardized way.</p><p>So they created the Hypertext Transfer Protocol (HTTP).</p><p>Think of  as a set of rules for transferring information on the Internet.</p><p>Although it temporarily solved the data transfer problem, there were new issues.</p><p>HTTP sends data in plaintext without encryption.</p><p>It means someone on the public network can easily access or change the information.</p><p>HTTP doesn’t support a mechanism to check if the server is the right one.</p><p>It means someone else could pretend to be the server and steal the user's data.</p><p>So it became difficult to transfer sensitive information, such as passwords or credit card numbers.</p><p> is the only AI coding agent built for real engineering teams.</p><p>It understands your codebase—across 10M+ lines, 10k+ files, and every repo in your stack—so it can actually help: writing functions, fixing CI issues, triaging incidents, and reviewing PRs.All from your IDE or terminal. No vibes. Just progress.</p><p>They wanted a secure version of HTTP.</p><p>So they created Hypertext Transfer Protocol Security (HTTPS).</p><p>Imagine  as HTTP running over an extra protocol to keep information secure.</p><p>The extra protocol is called Transport Layer Security (TLS). Think of  as a mechanism to encrypt data sent between the client and server.</p><p>TLS creates a secure connection using a process called the . It's used to establish an HTTPS connection when a user visits a site.</p><p>The browser sends a message to the server.</p><p>It includes the list of supported cryptographic algorithms, TLS versions. And also a randomly generated string called .</p><p>The server then responds with its TLS certificate and supported cryptographic algorithm. Besides it includes a randomly generated string called .</p><p>Yet it’s important to confirm the server identity for authenticity. So the browser verifies the received TLS certificate with the certificate authority. Imagine the  as a trusted organization that verifies server identity.</p><p>But both client and server must have the same key to encrypt session data efficiently.</p><p>So the browser sends a temporary key () to the server. While it’s encrypted using the server’s public key, which was taken from the TLS certificate.</p><p>Yet only the private key can decrypt the data that was encrypted using a public key. So the server decrypts the received pre-master secret using its . Thus making this data transfer secure.</p><p>Think of the  as an email address; anybody can send messages to it. While the  is like the inbox password, only the user with the password can read emails.</p><p>Both browser and server use the pre-master secret, server random, and client random to compute the  session key.</p><p>Think of the  as a symmetric cryptographic key that can encrypt and decrypt data. It’s valid either for a set period or for as long as communication is ongoing.</p><p>All future communication then gets encrypted using the session key. It means nobody can see the messages on the public network.</p><p>Yet it’s necessary to check if the TLS handshake was successful.</p><p>So the browser sends a finished message, which is encrypted using the session key. The server then responds with a finished message, encrypted using the session key.</p><p>This marks the completion of a TLS handshake.</p><p>Put simply, the TLS certificate handles authentication, while the TLS protocol handles encryption.</p><p>Thus providing authenticity, confidentiality, and integrity in data transfer.</p><p>HTTP is rarely used on public networks now.</p><p>Yet it's still in use on internal networks and legacy systems where data sensitivity is low. Because it offers convenience with simplicity.</p><p>While HTTPS has become the fundamental communication protocol of the internet.</p><p>And search engines like Google give preference to HTTPS-enabled sites in their search results. Besides HTTPS is necessary for compliance in transferring sensitive information: financial data.</p><p>But there’s an overhead with the setup and maintenance of a TLS certificate. Also HTTPS can be slightly slower than HTTP because of encryption overhead. Yet HTTP/2 and HTTP/3 offset this overhead with better performance.</p><p>So always use HTTPS on public sites because of its security and trust advantages.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p> delivers bite-sized deep dives on emerging devtools, career growth, and smart bets for builders.</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.</p><p>You can find a summary of this article <a href=\"https://www.linkedin.com/posts/nk-systemdesign-one_http-vs-https-explained-in-2-mins-or-less-activity-7359187677348274176-W5TS\">here</a>. Consider a repost if you find it helpful.</p>","contentLength":4982,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/d6c94ae9-610c-45c0-845b-781f15c15243_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How to Improve Availability Using Deployment Patterns ★","url":"https://newsletter.systemdesign.one/p/deployment-patterns","date":1754393791,"author":"Neo Kim","guid":28,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines the important deployment patterns. You will find references at the bottom of this page if you want to go deeper.</em></p><p><em>Note: This post is based on my research and may differ from real-world implementation.</em></p><p>Once upon a time, there was a tiny startup. </p><p>They offered live-streaming services.</p><p>Yet they had only a few customers.</p><p>So they deployed code overnight with a scheduled downtime.</p><p> means releasing new code to users.</p><p>But one day, a celebrity joined their platform for a livestream.</p><p>Because of this, their platform became popular among users around the world.</p><p>Although explosive growth is a good problem to have, deploying new code without downtime became difficult.</p><p>Also they were on a tight budget.</p><p>They used the  deployment pattern to keep things simple.</p><p>Put another way, they take the system offline for a short period. And replace the old version with the newer version on each server.</p><p>So the service remains unavailable during deployment. Besides any failure with the new version affected all users at once.</p><p>Until one day, when their lead engineer decided to evaluate different deployment patterns to solve this problem.</p><p>One of the biggest IT giants, TCS, laid off 12,000 people this week. And this is just the beginning of the bloodbath.</p><p>In the coming days, you’ll see not thousands, but millions more layoffs &amp; displacement of jobs. So what should you do right now to avoid getting affected?</p><p>Invest your time in learning about AI: the tools, the use cases, the workflows–as much as you can.</p><p><strong>Date: 9 August (Saturday) and 10 August (Sunday), 10 AM - 7 PM, EST time zone.</strong></p><p><em>Rated 4.9/5 by global learners–this will truly make you an AI Generalist that can build, solve &amp; work on anything with AI.</em>In just 16 hours &amp; 5 sessions, you will:</p><p>✅ <strong>Learn how AI really works</strong> by learning 10+ AI tools, LLM models, and their practical use cases.</p><p>✅<strong> Learn to build and ship products </strong>faster, in days instead of months</p><p>✅  that handle your repetitive work and free up 20+ hours weekly</p><p>✅ <strong>Create professional images and videos</strong> for your business, social media, and marketing campaigns.</p><p>✅ <strong>Turn these AI skills into $10k income</strong> by consulting or starting your own AI services business.</p><p><strong>All by global experts from companies like Amazon, Microsoft, SamurAI, and more. And it’s all for free. 🤯 🚀</strong></p><p>The rolling release offers zero-downtime deployment.</p><ul><li><p>Some servers get taken offline and updated</p></li><li><p>Those updated servers then go online</p></li><li><p>This process repeats until all servers get the new version</p></li></ul><p>Put simply, a new version is rolled out server by server until it’s fully deployed.</p><p>While the system remains available because the remaining servers handle requests during deployment. </p><p>For example, Twitter and Etsy use this pattern for routine feature deployment with minimal user impact.</p><p>Only a subset of servers runs the code initially. Because of that, the failure impact is low during deployment. Yet it’s infrastructure-driven and doesn’t focus on specific user segments. Also the rollout is relatively slow as updating each server takes time.</p><p>So use this pattern only if you need zero downtime and to avoid extra infrastructure costs.</p><p>Ready for the next technique?</p><p>The blue-green pattern offers zero-downtime deployment and instant rollback.</p><ul><li><p>Set up 2 identical environments called Blue &amp; Green</p></li><li><p>The new version gets tested on Green</p></li><li><p>While Blue runs the old version and handles requests to avoid downtime</p></li><li><p>The entire traffic then gets routed to Green using a load balancer or DNS</p></li></ul><p>Also Blue acts as a fallback until the new version on Green becomes stable. Because of that, an instant rollback is possible. Besides testing the new version in a production-like environment (Green) increases release confidence.</p><p>For example, Netflix uses this pattern for critical services to avoid service interruption.</p><p>Yet running 2 environments in parallel increases infrastructure costs and complexity. Also there’s an extra operational overhead.</p><p>So use this pattern only for critical services and high-risk deployments.</p><p>The name comes from the use of the canary bird in coal mines to detect toxic gases quickly.</p><ul><li><p>A tiny percentage of users get the new version</p></li><li><p>While the remaining traffic goes to the old version</p></li><li><p>The traffic to the new version then gets slowly increased (10% → 50% → 100%)</p></li></ul><p>Put another way, real-world users validate the new version before rolling it out entirely. Thus increasing the release confidence.</p><p>Also a failure in the new version affects only some users, and a quick rollback is possible. </p><p>For example, Amazon uses this pattern to fix problems before a global rollout.</p><p>Yet it increases the complexity with advanced monitoring and automated rollout control. Besides setting up routing rules needs extra effort.</p><p>So use this pattern for large-scale services and high-risk deployments. Or when a beta launch to a subset of users is necessary.</p><p>This pattern allows for code deployment without directly releasing it to users.</p><p>A  means conditional logic in the code.</p><ul><li><p>The developer puts the new feature behind a feature toggle</p></li><li><p>And the feature remains hidden until someone turns on the toggle</p></li><li><p>One can turn a feature toggle on or off for specific users via a configuration dashboard</p></li></ul><p>Thus separating code deployment from feature release.</p><p>For example, Google and Facebook use feature toggles for <a href=\"https://en.wikipedia.org/wiki/A/B_testing\">A/B testing</a> and safe releases.</p><p>This pattern makes it easy to enable or disable features for a specific set of users. Also it enables continuous integration by deploying code safely behind feature toggles.</p><p>Yet the code complexity increases because of conditional paths. Also testing becomes hard because one should test both on and off states. Besides feature toggles can become a technical debt if unremoved on time.</p><p>So use this pattern when you need frequent deployments without affecting users. Or when you need to do an A/B test or a beta launch.</p><p>Each deployment pattern solves a specific problem.</p><p>But there’s no silver bullet.</p><p>So choose the right pattern based on your needs: safety, speed, and cost.</p><p>While most companies use different deployment patterns within the same project. For example, a payment service might use blue-green for zero downtime. While frontend UI might use canary or feature flags for safe releases.</p><p>Ever since that day, our tiny startup installed a combination of deployment patterns and lived happily ever after.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p>Everything you need to know to land your next Data Science job. Get expert tips straight to your inbox at .</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.</p><p>You can find a summary of this article <a href=\"https://www.linkedin.com/posts/nk-systemdesign-one_if-i-had-to-deploy-code-here-are-5-patterns-activity-7358460667185491968-tKqr\">here</a>. Consider a repost if you find it helpful.</p>","contentLength":6778,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/3bc05d4c-2244-4261-9c2e-62440ace9957_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Amazon S3 Achieves Strong Consistency Without Sacrificing 99.99% Availability 🌟","url":"https://newsletter.systemdesign.one/p/s3-strong-consistency","date":1753788738,"author":"Neo Kim","guid":27,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines how AWS S3 achieves strong consistency. You will find references at the bottom of this page if you want to go deeper.</em></p><p><em>Note: This post is based on my research and may differ from actual implementation.</em></p><p>Once upon a time, there was a data processing startup.</p><p>They convert raw data into a structured format.</p><p>Yet they had only a few customers.</p><p>So an on-premise server was enough.</p><p>But one morning, they got a new customer with an extremely popular site.</p><p>This means massive data storage needs.</p><p>Yet their storage server had only limited capacity.</p><p>So they moved to Amazon Simple Storage Service (), an object storage.</p><p>It stores unstructured data without hierarchy.</p><p>S3 provides a REST API via the web server.</p><p>And stores metadata and file content separately for scale. It stores metadata of data objects in a key-value database. </p><p>Also it caches the metadata for low latency and high availability.</p><p>Although caching metadata offers performance, some requests might return an older version of metadata.</p><p>Because there could be network partitions in a distributed architecture. This means writes go to one cache partition, while reads go to another cache partition. </p><p>Thus causing eventual consistency.</p><p>But they need strong consistency in data processing for ordering and correctness.</p><p>While setting up extra app logic for strong consistency increases infrastructure complexity.</p><p><a href=\"https://coderabbit.link/bM7fHiE\">CodeRabbit</a> brings real-time, AI-powered code reviews straight into VS Code, Cursor, and Windsurf. It lets you:</p><ul><li><p>Get contextual feedback on every commit, not just at the PR stage</p></li><li><p>Catch bugs, security flaws, and performance issues asyou code</p></li><li><p>Apply AI-driven suggestions instantly to implement code changes</p></li><li><p>Do code reviews in your IDE for free and in your PR for a paid subscription</p></li></ul><p>It’s difficult to achieve strong consistency at scale without performance or availability tradeoffs.</p><p>So smart engineers at Amazon used simple ideas to solve this hard problem.</p><p>It means the write coordinator updates the cache first. Then updates the metadata store synchronously.</p><p>Thus reducing the risk of a stale cache.</p><p>Also they set up a separate service to track the cache freshness and called it . It stores only the latest version of a data object and keeps it lightweight (in-memory) for low latency. While the write coordinator notifies the witness whenever there’s a metadata update.</p><p>Besides they introduced a transaction log in the metadata store. It tracks the order of operations and allows them to check if the cache is fresh.</p><p>Here’s the read request workflow:</p><ol><li><p>The server queries the cache.</p></li><li><p>It then asks the witness to understand if the cache has the latest data.</p></li><li><p>The server queries the metadata store only if the cache is stale.</p></li></ol><p>Thus achieving strong consistency.</p><p>Put simply, they find out if the metadata cache is fresh using the witness. Think of the  as a central observer of metadata changes and a checkpoint for reads.</p><p>Also they assume the cache to be stale if the server cannot reach the witness. If so, they fetch the data directly from the metadata store.</p><p>Yet the witness shouldn’t affect S3’s overall performance or 99.99% availability.</p><p>So they scale the witness servers horizontally. And set up automation to replace failed servers quickly.</p><p>Besides they redistribute the traffic when a witness server fails.</p><p>S3 supports 100 trillion data objects at 10 million requests per second.</p><p>It offers strong read-after-write consistency using the witness and a consistent metadata cache.</p><p>Thus making the app logic simpler.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p>Meet the updated : an authorization solution that scales with your product. Manage unlimited tenants, policies, and roles. Enforce contextual &amp; continuous authorization across apps, APIs, AI agents, MCPs, and workloads. <a href=\"https://www.cerbos.dev/product-cerbos-hub?utm_campaign=system_design_CH&amp;utm_source=newsletter&amp;utm_medium=email&amp;utm_content=&amp;utm_term=\">Try it for free</a>.</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.</p><p>You can find a summary of this article <a href=\"https://www.linkedin.com/posts/nk-systemdesign-one_how-amazon-s3-achieves-strong-consistency-activity-7355922908365348864-s1X1\">here</a>. Consider a repost if you find it helpful.</p>","contentLength":4084,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/95ebfdc1-79b4-41da-9bad-b0c0432ceeb4_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How to Scale Code Reviews 🔥","url":"https://newsletter.systemdesign.one/p/how-to-do-code-review","date":1753438740,"author":"Neo Kim","guid":26,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines tips for reviewing code. You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, there was a 2-person startup.</p><p>Yet they had a tiny website and only a few customers.</p><p>So they merged the code directly into the main branch.</p><p>But one morning, their site became extremely popular.</p><p>And the number of customers and feature requests started to skyrocket.</p><p>So they hired more developers and implemented extra features.</p><p>Yet each developer wrote code with their personal preferences.</p><p>Thus worsening the code quality and standards. Also increasing the number of bugs.</p><p>So they started doing code reviews before merging new code into the codebase.</p><p>A  means someone other than the author reviews the code.</p><p>Although it temporarily solved their code quality problem, there were newer issues.</p><p>They followed a strict review process and spent time on minor issues.</p><p>Put simply, they asked for low-priority changes on a tight deadline ticket.</p><p>It means delayed pull requests and slow developer velocity.</p><p>Besides having teams across different time zones worsened the situation.</p><p>They didn’t have set guidelines for code reviews.</p><p>So developers forced their personal preferences during reviews.</p><p>It means unnecessary disagreements and demotivated developers.</p><p>Also feedback with a toxic and mean tone made things worse.</p><p>There’s a risk of misunderstanding feedback in remote or asynchronous reviews.</p><p>Also a reviewer must understand new code and how it works with the codebase.</p><p>So the reviewer might get overloaded with code reviews and parallel development tasks.</p><p>Besides the reviewers might miss bugs even with thorough reviews, especially in large pull requests.</p><p>I’m happy to partner with  on this newsletter. I’ve seen code reviews delaying feature deliveries by days and overloading reviewers. I genuinely believe CodeRabbit solves this problem.</p><p>Both the author and the reviewer should respect each other’s time and efforts.</p><p>Here are some <strong>guidelines for the code author</strong>:</p><ul><li><p>Follow the project's style guide.</p></li><li><p>Keep the changes small to make the reviews easy.</p></li><li><p>Review the code yourself before asking others to save time.</p></li><li><p>Use existing code patterns, and not personal preferences, if a style is unspecified.</p></li><li><p>Tag only the fewest number of reviewers to save everyone’s time.</p></li><li><p>Write a clear message about the changes for the reviewer to understand easily.</p></li><li><p>Use facts and data to resolve design debates, rather than opinions.</p></li></ul><p>Code reviews are each software engineer's responsibility.</p><p>Here are some <strong>guidelines for the reviewer</strong>:</p><ul><li><p>Respond to a review request within 24 hours.</p></li><li><p>Reserve at least one calendar slot each day for code reviews.</p></li><li><p>Keep the reviews polite and constructive; don’t criticize the author.</p></li><li><p>Document common review points and use a review checklist for consistent reviews.</p></li><li><p>Discuss the issue directly with the author when there’s a disagreement. Then document the solution for future reference.</p></li><li><p>Share the documentation in review comments if necessary to encourage knowledge sharing.</p></li><li><p>Approve the pull request when it’s good enough and allow minor issues to be fixed later.</p></li></ul><p>Remember, code reviews are about making progress and not perfection. So just make sure each change maintains or improves the codebase's health.</p><p>Version control platforms, such as GitHub and GitLab, include pull request features for code review. They allow inline comments, approvals, and automated checks. Also there is peer review software, such as <a href=\"https://www.gerritcodereview.com/\">Gerrit</a>, for advanced workflows.</p><p>Here’s the code review workflow from a developer perspective:</p><ol><li><ul><li><p>Write code on a separate Git branch to isolate changes.</p></li></ul><ul><li><p>Get instant feedback and fix suggestions with code editor extensions, such as <a href=\"https://coderabbit.link/bM7fHiE\">CodeRabbit</a>. Thus catching problems even before creating a pull request and saving infrastructure resources.</p></li><li><p>Commit code with a clear commit message and push it to the remote repository.</p></li><li><p>Open a pull request from the current branch to the target branch.</p></li></ul></li><li><ul><li><p>Run automated checks on the code, such as unit tests, static code analysis, security scans, and linting, to ensure code correctness. It finds style issues, code smells, or security vulnerabilities, so reviewers can focus on high-level feedback.</p></li></ul><ul><li><p>Find issues and get auto-fix suggestions using <a href=\"https://coderabbit.link/neo-kim\">CodeRabbit</a>. This approach improves developer velocity.</p></li></ul></li><li><ul><li><p>Generate a summary of code changes using <a href=\"https://coderabbit.link/neo-kim\">CodeRabbit</a>. It helps reviewers understand complex changes quickly. And assess the impact on the codebase.</p></li><li><p>The reviewer checks the changed files and leaves constructive feedback via comments.</p></li></ul></li><li><ul><li><p>Fix the code based on the reviewer’s comments.</p></li><li><p>Upload changes to the same pull request and make sure automated checks pass again.</p></li><li><p>Reply to the reviewer's feedback and resolve comments.</p></li></ul></li><li><ul><li><p>At least one reviewer approves the pull request.</p></li><li><p>Merge the pull request into the target branch and trigger continuous deployment ().</p></li><li><p>CD pipeline builds and deploys the change to the staging environment.</p></li><li><p>Once the staging tests pass, the change gets released to production.</p></li></ul></li></ol><p> means merging code changes regularly into a central repository. While  is about automatically releasing changes that pass checks into production.</p><p> is an AI code review tool to reduce developer workload.</p><p>Here’s how it helps with code review flow:</p><ul><li><p>Do routine checks and reduce the time spent on reviews from days to minutes. Thus improving developer velocity.</p></li><li><p>Do consistent reviews without getting tired or biased. Thus reducing the risk of missing important issues from human mistakes.</p></li><li><p>Provide mentorship to junior engineers through detailed feedback.</p></li></ul><p>Put simply, <a href=\"https://coderabbit.link/neo-kim\">CodeRabbit</a> complements human reviewers. It serves 1+ million repositories and has reviewed over 10 million pull requests. And it remains the most installed app on GitHub and GitLab. </p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.</p>","contentLength":5968,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/98f5d659-048f-4bb5-ac48-373406bc8c67_1280x720.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}