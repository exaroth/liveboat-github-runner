{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":3,"items":[{"title":"Before You Migrate: Five Surprising Ingress-NGINX Behaviors You Need to Know","url":"https://kubernetes.io/blog/2026/02/27/ingress-nginx-before-you-migrate/","date":1772206200,"author":"","guid":747,"unread":true,"content":"<p>As <a href=\"https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/\">announced</a> November 2025, Kubernetes will retire Ingress-NGINX in March 2026.\nDespite its widespread usage, Ingress-NGINX is full of surprising defaults and side effects that are probably present in your cluster today.\nThis blog highlights these behaviors so that you can migrate away safely and make a conscious decision about which behaviors to keep.\nThis post also compares Ingress-NGINX with Gateway API and shows you how to preserve Ingress-NGINX behavior in Gateway API.\nThe recurring risk pattern in every section is the same: a seemingly correct translation can still cause outages if it does not consider Ingress-NGINX's quirks.</p><p>I'm going to assume that you, the reader, have some familiarity with Ingress-NGINX and the Ingress API.\nMost examples use <a href=\"https://github.com/postmanlabs/httpbin\"></a> as the backend.</p><p>Also, note that Ingress-NGINX and NGINX Ingress are two separate Ingress controllers.\n<a href=\"https://github.com/kubernetes/ingress-nginx\">Ingress-NGINX</a> is an Ingress controller maintained and governed by the Kubernetes community that is retiring March 2026.\n<a href=\"https://docs.nginx.com/nginx-ingress-controller/\">NGINX Ingress</a> is an Ingress controller by F5.\nBoth use NGINX as the dataplane, but are otherwise unrelated.\nFrom now on, this blog post only discusses Ingress-NGINX.</p><h2>1. Regex matches are prefix-based and case insensitive</h2><p>Suppose that you wanted to route all requests with a path consisting of only three uppercase letters to the  service.\nYou might create the following Ingress with the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation and the regex pattern of .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>However, because regex matches are prefix and case insensitive, Ingress-NGINX routes any request with a path that starts with any three letters to httpbin:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The output is similar to:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p> The  endpoint of httpbin returns a random UUID.\nA UUID in the response body means that the request was successfully routed to httpbin.</p><p>With Gateway API, you can use an <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#httppathmatch\">HTTP path match</a> with a  of  for regular expression path matching.\n matches are implementation specific, so check with your Gateway API implementation to verify the semantics of  matching.\nPopular Envoy-based Gateway API implementations such as <a href=\"https://istio.io/\">Istio</a>, <a href=\"https://gateway.envoyproxy.io/\">Envoy Gateway</a>, and <a href=\"https://kgateway.dev/\">Kgateway</a> do a full case-sensitive match.</p><p>Thus, if you are unaware that Ingress-NGINX patterns are prefix and case-insensitive, and, unbeknownst to you,\nclients or applications send traffic to  (or ), you might create the following HTTP route.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>However, if your Gateway API implementation does full case-sensitive matches,\nthe above HTTP route would not match a request with a path of .\nThe above HTTP route would thus cause an outage because requests\nthat Ingress-NGINX routed to httpbin would fail with a 404 Not Found at the gateway.</p><p>To preserve the case-insensitive regex matching, you can use the following HTTP route.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Alternatively, the aforementioned proxies support the  flag to indicate case insensitive matches.\nUsing the flag, the pattern could be .</p><h2>2. The <code>nginx.ingress.kubernetes.io/use-regex</code> applies to all paths of a host across all (Ingress-NGINX) Ingresses</h2><p>Now, suppose that you have an Ingress with the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation, but you want to route\nrequests with a path of exactly  to .\nUnfortunately, you made a typo and set the path to  instead of .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Most would expect a request to  to respond with a 404 Not Found, since  does not match the  path of .\nHowever, because the  Ingress has the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation and the  host,\n<strong>all paths with the  host are treated as regular expressions across all (Ingress-NGINX) Ingresses.</strong>\nSince regex patterns are case-insensitive prefix matches,  matches the  pattern and Ingress-NGINX routes such requests to .\nRunning the command</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p> The  endpoint of httpbin returns the request headers.\nThe fact that the response contains the request headers in the body means that the request was successfully routed to httpbin.</p><p>Gateway API does not silently convert or interpret  and  matches as regex patterns.\nSo if you converted the above Ingresses into the following HTTP route and\npreserved the typo and match types, requests to  will respond with a 404 Not Found instead of a 200 OK.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>To keep the case-insensitive prefix matching, you can change</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Or even better, you could fix the typo and change the match to</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>3. Rewrite target implies regex</h2><p>In this case, suppose you want to rewrite the path of requests with a path of  to  before routing them to , and\nas in Section 2, you want to route requests with the path of exactly  to .\nHowever, you accidentally make a typo and set the path to  instead of  and  instead of .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The <code>nginx.ingress.kubernetes.io/rewrite-target: \"/uuid\"</code> annotation\ncauses requests that match paths in the  Ingress to have their paths rewritten to  before being routed to the backend.</p><p>Even though no Ingress has the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation,\nthe presence of the <code>nginx.ingress.kubernetes.io/rewrite-target</code> annotation in the  Ingress causes <strong>all paths with the <code>rewrite-target.example.com</code> host to be treated as regex patterns.</strong>\nIn other words, the <code>nginx.ingress.kubernetes.io/rewrite-target</code> silently adds the <code>nginx.ingress.kubernetes.io/use-regex: \"true\"</code> annotation, along with all the side effects discussed above.</p><p>For example, a request to  has its path rewritten to  because  matches the case-insensitive prefix pattern of  in the  Ingress.\nAfter running the command</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>the output is similar to:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Like in the <code>nginx.ingress.kubernetes.io/use-regex</code> example, Ingress-NGINX treats s of other ingresses with the <code>rewrite-target.example.com</code> host as case-insensitive prefix patterns.\nRunning the command</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>gives an output that looks like</p><p>You can configure path rewrites in Gateway API with the <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#httpurlrewritefilter\">HTTP URL rewrite filter</a> which does not silently convert your  and  matches into regex patterns.\nHowever, if you are unaware of the side effects of the <code>nginx.ingress.kubernetes.io/rewrite-target</code> annotation\nand do not realize that  and  are both typos, you might create the following\nHTTP route.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>As with Section 2, because  is now an  match type in your HTTP route, requests to  will respond with a 404 Not Found instead of a 200 OK.\nSimilarly, requests to  will also respond with a 404 Not Found instead of a 200 OK.\nThus, this HTTP route will break applications and clients that rely on the  and  routes.</p><p>To fix this, you can change the matches in the HTTP route to be regex matches, and change the path patterns to be case-insensitive prefix matches, as follows.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Or, you can keep the  match type and fix the typos.</p><h2>4. Requests missing a trailing slash are redirected to the same path with a trailing slash</h2><p>Consider the following Ingress:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>You might expect Ingress-NGINX to respond to  with a 404 Not Found since the  does not exactly match the  path of .\nHowever, Ingress-NGINX redirects the request to  with a 301 Moved Permanently because the only difference between  and  is a trailing slash.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"http\"></code></pre></div><p>The same applies if you change the  to .\nHowever, the redirect does not happen if the path is a regex pattern.</p><p>Conformant Gateway API implementations do not silently configure any kind of redirects.\nIf clients or downstream services depend on this redirect, a migration to Gateway API that\ndoes not explicitly configure request redirects will cause an outage because\nrequests to  will now respond with a 404 Not Found instead of a 301 Moved Permanently.\nYou can explicitly configure redirects using the <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#httprequestredirectfilter\">HTTP request redirect filter</a> as follows:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>5. Ingress-NGINX normalizes URLs</h2><p> is the process of converting a URL into a canonical form before matching it against Ingress rules and routing it.\nThe specifics of URL normalization are defined in <a href=\"https://datatracker.ietf.org/doc/html/rfc3986#section-6.2\">RFC 3986 Section 6.2</a>, but some examples are</p><ul><li>removing path segments that are just a : </li><li>having a  path segment remove the previous segment: </li><li>deduplicating consecutive slashes in a path: </li></ul><p>Ingress-NGINX normalizes URLs before matching them against Ingress rules.\nFor example, consider the following Ingress:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Ingress-NGINX normalizes the path of the following requests to .\nNow that the request matches the  path of , Ingress-NGINX responds with either a 200 OK response or a 301 Moved Permanently to .</p><p>For the following commands</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>the outputs are similar to</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"http\"></code></pre></div><p>Your backends might rely on the Ingress/Gateway API implementation to normalize URLs.\nThat said, most Gateway API implementations will have some path normalization enabled by default.\nFor example, Istio, Envoy Gateway, and Kgateway all normalize  and  segments out of the box.\nFor more details, check the documentation for each Gateway API implementation that you use.</p><p>As we all race to respond to the Ingress-NGINX retirement, I hope this blog post instills some confidence that you can migrate safely and effectively despite all the intricacies of Ingress-NGINX.</p><p>SIG Network has also been working on supporting the most common Ingress-NGINX annotations (and some of these unexpected behaviors) in <a href=\"https://github.com/kubernetes-sigs/ingress2gateway\">Ingress2Gateway</a> to help you translate Ingress-NGINX configuration into Gateway API, and offer alternatives to unsupported behavior.</p><p>SIG Network released Gateway API 1.5 earlier today (27th February 2026), which graduates features such as\n<a href=\"https://gateway-api.sigs.k8s.io/api-types/listenerset/\">ListenerSet</a> (that allow app developers to better manage TLS certificates),\nand the HTTPRoute CORS filter that allows CORS configuration.</p>","contentLength":9134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spotlight on SIG Architecture: API Governance","url":"https://kubernetes.io/blog/2026/02/12/sig-architecture-api-spotlight/","date":1770854400,"author":"","guid":746,"unread":true,"content":"<p><em>This is the fifth interview of a SIG Architecture Spotlight series that covers the different\nsubprojects, and we will be covering <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/README.md#architecture-and-api-governance-1\">SIG Architecture: API\nGovernance</a>.</em></p><p>In this SIG Architecture spotlight we talked with <a href=\"https://github.com/liggitt\">Jordan Liggitt</a>, lead\nof the API Governance sub-project.</p><p><strong>FM: Hello Jordan, thank you for your availability. Tell us a bit about yourself, your role and how\nyou got involved in Kubernetes.</strong></p><p>: My name is Jordan Liggitt. I'm a Christian, husband, father of four, software engineer at\n<a href=\"https://about.google/\">Google</a> by day, and <a href=\"https://www.youtube.com/watch?v=UDdr-VIWQwo\">amateur musician</a> by stealth. I was born in Texas (and still\nlike to claim it as my point of origin), but I've lived in North Carolina for most of my life.</p><p>I've been working on Kubernetes since 2014. At that time, I was working on authentication and\nauthorization at Red Hat, and my very first pull request to Kubernetes attempted to <a href=\"https://github.com/kubernetes/kubernetes/pull/2328\">add an OAuth\nserver</a> to the Kubernetes API server. It never\nexited work-in-progress status. I ended up going with a different approach that layered on top of\nthe core Kubernetes API server in a different project (spoiler alert: this is foreshadowing), and I\nclosed it without merging six months later.</p><p>Undeterred by that start, I stayed involved, helped build Kubernetes authentication and\nauthorization capabilities, and got involved in the definition and evolution of the core Kubernetes\nAPIs from early beta APIs, like  to . I got tagged as an API reviewer in 2016 based on\nthose contributions, and was added as an API approver in 2017.</p><p>Today, I help lead the API Governance and code organization subprojects for SIG Architecture, and I\nam a tech lead for SIG Auth.</p><p><strong>FM: And when did you get specifically involved in the API Governance project?</strong></p><h2>Goals and scope of API Governance</h2><p><strong>FM: How would you describe the main goals and areas of intervention of the subproject?</strong></p><p>The surface area includes all the various APIs Kubernetes has, and there are APIs that people do not\nalways realize are APIs: command-line flags, configuration files, how binaries are run, how they\ntalk to back-end components like the container runtime, and how they persist data. People often\nthink of \"the API\" as only the <a href=\"https://kubernetes.io/docs/reference/using-api/\">REST API</a>... that\nis the biggest and most obvious one, and the one with the largest audience, but all of these other\nsurfaces are also APIs. Their audiences are narrower, so there is more flexibility there, but they\nstill require consideration.</p><p>The goals are to be stable while still enabling innovation. Stability is easy if you never change\nanything, but that contradicts the goal of evolution and growth. So we balance \"be stable\" with\n\"allow change\".</p><p><strong>FM: Speaking of changes, in terms of ensuring consistency and quality (which is clearly one of the\nreasons this project exists), what are the specific quality gates in the lifecycle of a Kubernetes\nchange? Does API Governance get involved during the release cycle, prior to it through guidelines,\nor somewhere in between? At what points do you ensure the intended role is fulfilled?</strong></p><p>: We have <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md\">guidelines and\nconventions</a>,\nboth for APIs in general and for how to change an API. These are living documents that we update as\nwe encounter new scenarios. They are long and dense, so we also support them with involvement at\neither the design stage or the implementation stage.</p><p>Sometimes, due to bandwidth constraints, teams move ahead with design work without feedback from <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/api-review-process.md\">API Review</a>. That’s fine, but it means that when implementation begins, the API review will happen then,\nand there may be substantial feedback. So we get involved when a new API is created or an existing\nAPI is changed, either at design or implementation.</p><p><strong>FM: Is this during the Kubernetes Enhancement Proposal (KEP) process? Since KEPs are mandatory for\nenhancements, I assume part of the work intersects with API Governance?</strong></p><p>: It can. <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/README.md\">KEPs</a> vary\nin how detailed they are. Some include literal API definitions. When they do, we can perform an API\nreview at the design stage. Then implementation becomes a matter of checking fidelity to the design.</p><p>Getting involved early is ideal. But some KEPs are conceptual and leave details to the\nimplementation. That’s not wrong; it just means the implementation will be more exploratory. Then\nAPI Review gets involved later, possibly recommending structural changes.</p><p>There’s a trade-off regardless: detailed design upfront versus iterative discovery during\nimplementation. People and teams work differently, and we’re flexible and happy to consult early or\nat implementation time.</p><p><strong>FM: This reminds me of what Fred Brooks wrote in \"The Mythical Man-Month\" about conceptual\nintegrity being central to product quality... No matter how you structure the process, there must be\na point where someone looks at what is coming and ensures conceptual integrity. Kubernetes uses APIs\neverywhere -- externally and internally -- so API Governance is critical to maintaining that\nintegrity. How is this captured?</strong></p><p>: Yes, the conventions document captures patterns we’ve learned over time: what to do in\nvarious situations. We also have automated linters and checks to ensure correctness around patterns\nlike spec/status semantics. These automated tools help catch issues even when humans miss them.</p><p>As new scenarios arise -- and they do constantly -- we think through how to approach them and fold\nthe results back into our documentation and tools. Sometimes it takes a few attempts before we\nsettle on an approach that works well.</p><p><strong>FM: Exactly. Each new interaction improves the guidelines.</strong></p><p>: Right. And sometimes the first approach turns out to be wrong. It may take two or three\niterations before we land on something robust.</p><h2>The impact of Custom Resource Definitions</h2><p><strong>FM: Is there any particular change, episode, or domain that stands out as especially noteworthy,\ncomplex, or interesting in your experience?</strong></p><p>: The watershed moment was <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">Custom Resources</a>.\nPrior to that, every API was handcrafted by us and fully reviewed. There were inconsistencies, but\nwe understood and controlled every type and field.</p><p>When Custom Resources arrived, anyone could define anything. The first version did not even require\na schema. That made it extremely powerful -- it enabled change immediately -- but it left us playing\ncatch-up on stability and consistency.</p><p>When Custom Resources graduated to General Availability (GA), schemas became required, but escape\nhatches still existed for backward compatibility. Since then, we’ve been working on giving CRD\nauthors validation capabilities comparable to built-ins. Built-in validation rules for CRDs have\nonly just reached GA in the last few releases.</p><p>So CRDs opened the \"anything is possible\" era. Built-in validation rules are the second major\nmilestone: bringing consistency back.</p><p>The three major themes have been defining schemas, validating data, and handling pre-existing\ninvalid data. With ratcheting validation (allowing data to improve without breaking existing\nobjects), we can now guide CRD authors toward conventions without breaking the world.</p><h2>API Governance in context</h2><p><strong>FM: How does API Governance relate to SIG Architecture and API Machinery?</strong></p><p>: <a href=\"https://github.com/kubernetes/apimachinery\">API Machinery</a> provides the actual code and\ntools that people build APIs on. They don’t review APIs for storage, networking, scheduling, etc.</p><p>SIG Architecture sets the overall system direction and works with API Machinery to ensure the system\nsupports that direction. API Governance works with other SIGs building on that foundation to define\nconventions and patterns, ensuring consistent use of what API Machinery provides.</p><p><strong>FM: Thank you. That clarifies the flow. Going back to <a href=\"https://kubernetes.io/releases/release/\">release cycles</a>: do release phases -- enhancements freeze, code\nfreeze -- change your workload? Or is API Governance mostly continuous?</strong></p><p>: We get involved in two places: design and implementation. Design involvement increases\nbefore enhancements freeze; implementation involvement increases before code freeze. However, many\nefforts span multiple releases, so there is always some design and implementation happening, even\nfor work targeting future releases. Between those intense periods, we often have time to work on\nlong-term design work.</p><p>An anti-pattern we see is teams thinking about a large feature for months and then presenting it\nthree weeks before enhancements freeze, saying, \"Here is the design, please review.\" For big changes\nwith API impact, it’s much better to involve API Governance early.</p><p>And there are good times in the cycle for this -- between freezes -- when people have bandwidth.\nThat’s when long-term review work fits best.</p><p><strong>FM: Clearly. Now, regarding team dynamics and new contributors: how can someone get involved in\nAPI Governance? What should they focus on?</strong></p><p>: It’s usually best to follow a specific change rather than trying to learn everything at\nonce. Pick a small API change, perhaps one someone else is making or one you want to make, and\nobserve the full process: design, implementation, review.</p><p>High-bandwidth review -- live discussion over video -- is often very effective. If you’re making or\nfollowing a change, ask whether there’s a time to go over the design or PR together. Observing those\ndiscussions is extremely instructive.</p><p>Start with a small change. Then move to a bigger one. Then maybe a new API. That builds\nunderstanding of conventions as they are applied in practice.</p><p><strong>FM: Excellent. Any final comments, or anything we missed?</strong></p><p>: Yes... the reason we care so much about compatibility and stability is for our users. It’s\neasy for contributors to see those requirements as painful obstacles preventing cleanup or requiring\ntedious work... but users integrated with our system, and we made a promise to them: we want them to\ntrust that we won’t break that contract. So even when it requires more work, moves slower, or\ninvolves duplication, we choose stability.</p><p>We are not trying to be obstructive; we are trying to make life good for users.</p><p>A lot of our questions focus on the future: you want to do something now... how will you evolve it\nlater without breaking it? We assume we will know more in the future, and we want the design to\nleave room for that.</p><p>We also assume we will make mistakes. The question then is: how do we leave ourselves avenues to\nimprove while keeping compatibility promises?</p><p><strong>FM: Exactly. Jordan, thank you, I think we’ve covered everything. This has been an insightful view\ninto the API Governance project and its role in the wider Kubernetes project.</strong></p>","contentLength":10358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Node Readiness Controller","url":"https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/","date":1770084000,"author":"","guid":745,"unread":true,"content":"<img src=\"https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/node-readiness-controller-logo.svg\" alt=\"Logo for node readiness controller\"><p>In the standard Kubernetes model, a node’s suitability for workloads hinges on a single binary \"Ready\" condition. However, in modern Kubernetes environments, nodes require complex infrastructure dependencies—such as network agents, storage drivers, GPU firmware, or custom health checks—to be fully operational before they can reliably host pods.</p><p>Today, on behalf of the Kubernetes project, I am announcing the <a href=\"https://node-readiness-controller.sigs.k8s.io/\">Node Readiness Controller</a>.\nThis project introduces a declarative system for managing node taints, extending the readiness guardrails during node bootstrapping beyond standard conditions.\nBy dynamically managing taints based on custom health signals, the controller ensures that workloads are only placed on nodes that met all infrastructure-specific requirements.</p><h2>Why the Node Readiness Controller?</h2><p>Core Kubernetes Node \"Ready\" status is often insufficient for clusters with sophisticated bootstrapping requirements. Operators frequently struggle to ensure that specific DaemonSets or local services are healthy before a node enters the scheduling pool.</p><p>The Node Readiness Controller fills this gap by allowing operators to define custom scheduling gates tailored to specific node groups. This enables you to enforce\ndistinct readiness requirements across heterogeneous clusters, ensuring for example, that GPU equipped nodes only accept pods once specialized drivers are verified,\nwhile general purpose nodes follow a standard path.</p><p>It provides three primary advantages:</p><ul><li><strong>Custom Readiness Definitions</strong>: Define what  means for your specific platform.</li><li><strong>Automated Taint Management</strong>: The controller automatically applies or removes node taints based on condition status, preventing pods from landing on unready infrastructure.</li><li><strong>Declarative Node Bootstrapping</strong>: Manage multi-step node initialization reliably, with a clear observability into the bootstrapping process.</li></ul><h2>Core concepts and features</h2><p>The controller centers around the NodeReadinessRule (NRR) API, which allows you to define declarative  for your nodes.</p><h3>Flexible enforcement modes</h3><p>The controller supports two distinct operational modes:</p><dl><dd>Actively maintains the readiness guarantee throughout the node’s entire lifecycle. If a critical dependency (like a device driver) fails later, the node is immediately tainted to prevent new scheduling.</dd><dd>Specifically for one-time initialization steps, such as pre-pulling heavy images or hardware provisioning. Once conditions are met, the controller marks the bootstrap as complete and stops monitoring that specific rule for the node.</dd></dl><p>The controller reacts to Node Conditions rather than performing health checks itself. This decoupled design allows it to integrate seamlessly with other tools existing in the ecosystem as well as custom solutions:</p><ul><li><strong>Readiness Condition Reporter</strong>: A lightweight agent provided by the project that can be deployed to periodically check local HTTP endpoints and patch node conditions accordingly.</li></ul><h3>Operational safety with dry run</h3><p>Deploying new readiness rules across a fleet carries inherent risk. To mitigate this,  mode allows operators to first simulate impact on the cluster.\nIn this mode, the controller logs intended actions and updates the rule's status to show affected nodes without applying actual taints, enabling safe validation before enforcement.</p><h2>Example: CNI bootstrapping</h2><p>The following NodeReadinessRule ensures a node remains unschedulable until its CNI agent is functional. The controller monitors a custom <code>cniplugin.example.net/NetworkReady</code> condition and only removes the <code>readiness.k8s.io/acme.com/network-unavailable</code> taint once the status is True.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The Node Readiness Controller is just getting started, with our <a href=\"https://github.com/kubernetes-sigs/node-readiness-controller/releases/tag/v0.1.1\">initial releases</a> out, and we are seeking community feedback to refine the roadmap. Following our productive Unconference discussions at KubeCon NA 2025, we are excited to continue the conversation in person.</p><p>In the meantime, you can contribute or track our progress here:</p>","contentLength":3917,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}