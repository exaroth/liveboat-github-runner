{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":5,"items":[{"title":"Kubernetes v1.35 Sneak Peek","url":"https://kubernetes.io/blog/2025/11/26/kubernetes-v1-35-sneak-peek/","date":1764115200,"author":"","guid":704,"unread":true,"content":"<p>As the release of Kubernetes v1.35 approaches, the Kubernetes project continues to evolve. Features may be deprecated, removed, or replaced to improve the project's overall health. This blog post outlines planned changes for the v1.35 release that the release team believes you should be aware of to ensure the continued smooth operation of your Kubernetes cluster(s), and to keep you up to date with the latest developments. The information below is based on the current status of the v1.35 release and is subject to change before the final release date.</p><h2>Deprecations and removals for Kubernetes v1.35</h2><p>On Linux nodes, container runtimes typically rely on cgroups (short for \"control groups\").\nSupport for using cgroup v2 has been stable in Kubernetes since v1.25, providing an alternative to the original v1 cgroup support. While cgroup v1 provided the initial resource control mechanism, it suffered from well-known\ninconsistencies and limitations. Adding support for cgroup v2 allowed use of a unified control group hierarchy, improved resource isolation, and served as the foundation for modern features, making legacy cgroup v1 support ready for removal.\nThe removal of cgroup v1 support will only impact cluster administrators running nodes on older Linux distributions that do not support cgroup v2; on those nodes, the  will fail to start. Administrators must migrate their nodes to systems with cgroup v2 enabled. More details on compatibility requirements will be available in a blog post soon after the v1.35 release.</p><h3>Deprecation of ipvs mode in kube-proxy</h3><p>Many releases ago, the Kubernetes project implemented an <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-ipvs\">ipvs</a> mode in . It was adopted as a way to provide high-performance service load balancing, with better performance than the existing  mode. However, maintaining feature parity between ipvs and other kube-proxy modes became difficult, due to technical complexity and diverging requirements. This created significant technical debt and made the ipvs backend impractical to support alongside newer networking capabilities.</p><p>The Kubernetes project intends to deprecate kube-proxy  mode in the v1.35 release, to streamline the  codebase. For Linux nodes, the recommended  mode is already <a href=\"https://kubernetes.io/docs/reference/networking/virtual-ips/#proxy-mode-nftables\">nftables</a>.</p><h3>Kubernetes is deprecating containerd v1.y support</h3><p>While Kubernetes v1.35 still supports containerd 1.7 and other LTS releases of containerd, as a consequence of automated cgroup driver detection, the Kubernetes SIG Node community has formally agreed upon a final support timeline for containerd v1.X. Kubernetes v1.35 is the last release to offer this support (aligned with containerd 1.7 EOL).</p><p>This is a final warning that if you are using containerd 1.X, you must switch to 2.0 or later before upgrading Kubernetes to the next version. You are able to monitor the <code>kubelet_cri_losing_support</code> metric to determine if any nodes in your cluster are using a containerd version that will soon be unsupported.</p><h2>Featured enhancements of Kubernetes v1.35</h2><p>The following enhancements are some of those likely to be included in the v1.35 release. This is not a commitment, and the release content is subject to change.</p><p>When scheduling Pods, Kubernetes uses node labels, taints, and tolerations to match workload requirements with node capabilities. However, managing feature compatibility becomes challenging during cluster upgrades due to version skew between the control plane and nodes. This can lead to Pods being scheduled on nodes that lack required features, resulting in runtime failures.</p><p>The  framework will introduce a standard mechanism for nodes to declare their supported Kubernetes features. With the new alpha feature enabled, a Node reports the features it can support, publishing this information to the control plane through a new  field. Then, the , admission controllers and third-party components can use these declarations. For example, you can enforce scheduling and API validation constraints, ensuring that Pods run only on compatible nodes.</p><p>This approach reduces manual node labeling, improves scheduling accuracy, and prevents incompatible pod placements proactively. It also integrates with the Cluster Autoscaler for informed scale-up decisions. Feature declarations are temporary and tied to Kubernetes feature gates, enabling safe rollout and cleanup.</p><p>Targeting alpha in v1.35,  aims to solve version skew scheduling issues by making node capabilities explicit, enhancing reliability and cluster stability in heterogeneous version environments.</p><p>To learn more about this before the official documentation is published, you can read <a href=\"https://kep.k8s.io/5328\">KEP-5328</a>.</p><h3>In-place update of Pod resources</h3><p>Kubernetes is graduating in-place updates for Pod resources to General Availability (GA). This feature allows users to adjust  and  resources without restarting Pods or Containers. Previously, such modifications required recreating Pods, which could disrupt workloads, particularly for stateful or batch applications.\nPrevious Kubernetes releases already allowed you to change infrastructure resources settings (requests and limits) for existing Pods. This allows for smoother <a href=\"https://kubernetes.io/docs/concepts/workloads/autoscaling/vertical-pod-autoscale/\">vertical scaling</a>, improves efficiency, and can also simplify solution development.</p><p>The Container Runtime Interface (CRI) has also been improved, extending the  API for Windows and future runtimes while allowing  to report real-time resource configurations. Together, these changes make scaling in Kubernetes faster, more flexible, and disruption-free.\nThe feature was introduced as alpha in v1.27, graduated to beta in v1.33, and is targeting graduation to stable in v1.35.</p><p>When running microservices, Pods often require a strong cryptographic identity to authenticate with each other using mutual TLS (mTLS). While Kubernetes provides Service Account tokens, these are designed for authenticating to the API server, not for general-purpose workload identity.</p><p>Before this enhancement, operators had to rely on complex, external projects like SPIFFE/SPIRE or cert-manager to provision and rotate certificates for their workloads. But what if you could issue a unique, short-lived certificate to your Pods natively and automatically? KEP-4317 is designed to enable such native workload identity. It opens up various possibilities for securing pod-to-pod communication by allowing the  to request and mount certificates for a Pod via a projected volume.</p><p>This provides a built-in mechanism for workload identity, complete with automated certificate rotation, significantly simplifying the setup of service meshes and other zero-trust network policies. This feature was introduced as alpha in v1.34 and is targeting beta in v1.35.</p><h3>Numeric values for taints</h3><p>Kubernetes is enhancing <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\">taints and tolerations</a> by adding numeric comparison operators, such as  (Greater Than) and  (Less Than).</p><p>Previously, tolerations supported only exact () or existence () matches, which were not suitable for numeric properties such as reliability SLAs.</p><p>With this change, a Pod can use a toleration to \"opt-in\" to nodes that meet a specific numeric threshold. For example, a Pod can require a Node with an SLA taint value greater than 950 (, ).</p><p>This approach is more powerful than Node Affinity because it supports the NoExecute effect, allowing Pods to be automatically evicted if a node's numeric value drops below the tolerated threshold.</p><p>When running Pods, you can use  to drop privileges, but containers inside the pod often still run as root (UID 0). This simplicity poses a significant challenge, as that container UID 0 maps directly to the host's root user.</p><p>Before this enhancement, a container breakout vulnerability could grant an attacker full root access to the node. But what if you could dynamically remap the container's root user to a safe, unprivileged user on the host? KEP-127 specifically allows such native support for Linux User Namespaces. It opens up various possibilities for pod security by isolating container and host user/group IDs. This allows a process to have root privileges (UID 0) within its namespace, while running as a non-privileged, high-numbered UID on the host.</p><p>Released as alpha in v1.25 and beta in v1.30, this feature continues to progress through beta maturity, paving the way for truly \"rootless\" containers that drastically reduce the attack surface for a whole class of security vulnerabilities.</p><h3>Support for mounting OCI images as volumes</h3><p>When provisioning a Pod, you often need to bundle data, binaries, or configuration files for your containers.\nBefore this enhancement, people often included that kind of data directly into the main container image, or required a custom init container to download and unpack files into an . You can still take either of those approaches, of course.</p><p>But what if you could populate a volume directly from a data-only artifact in an OCI registry, just like pulling a container image? Kubernetes v1.31 added support for the  volume type, allowing Pods to pull and unpack OCI container image artifacts into a volume declaratively.</p><p>This allows for seamless distribution of data, binaries, or ML models using standard registry tooling, completely decoupling data from the container image and eliminating the need for complex init containers or startup scripts.\nThis volume type has been in beta since v1.33 and will likely be enabled by default in v1.35.</p><p>New features and deprecations are also announced in the Kubernetes release notes. We will formally announce what's new in <a href=\"https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.35.md\">Kubernetes v1.35</a> as part of the CHANGELOG for that release.</p><p>The Kubernetes v1.35 release is planned for . Stay tuned for updates!</p><p>You can also see the announcements of changes in the release notes for:</p><p>The simplest way to get involved with Kubernetes is by joining one of the many <a href=\"https://github.com/kubernetes/community/blob/master/sig-list.md\">Special Interest Groups</a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly <a href=\"https://github.com/kubernetes/community/tree/master/communication\">community meeting</a>, and through the channels below. Thank you for your continued feedback and support.</p>","contentLength":9927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Configuration Good Practices","url":"https://kubernetes.io/blog/2025/11/25/configuration-good-practices/","date":1764028800,"author":"","guid":703,"unread":true,"content":"<p>Configuration is one of those things in Kubernetes that seems small until it's not. Configuration is at the heart of every Kubernetes workload.\nA missing quote, a wrong API version or a misplaced YAML indent can ruin your entire deploy.</p><p>This blog brings together tried-and-tested configuration best practices. The small habits that make your Kubernetes setup clean, consistent and easier to manage.\nWhether you are just starting out or already deploying apps daily, these are the little things that keep your cluster stable and your future self sane.</p><p><em>This blog is inspired by the original <em>Configuration Best Practices</em> page, which has evolved through contributions from many members of the Kubernetes community.</em></p><h2>General configuration practices</h2><h3>Use the latest stable API version</h3><p>Kubernetes evolves fast. Older APIs eventually get deprecated and stop working. So, whenever you are defining resources, make sure you are using the latest stable API version.\nYou can always check with</p><p>This simple step saves you from future compatibility issues.</p><h3>Store configuration in version control</h3><p>Never apply manifest files directly from your desktop. Always keep them in a version control system like Git, it's your safety net.\nIf something breaks, you can instantly roll back to a previous commit, compare changes or recreate your cluster setup without panic.</p><h3>Write configs in YAML not JSON</h3><p>Write your configuration files using YAML rather than JSON. Both work technically, but YAML is just easier for humans. It's cleaner to read and less noisy and widely used in the community.</p><p>YAML has some sneaky gotchas with boolean values:\nUse only  or .\nDon't write , ,  or .\nThey might work in one version of YAML but break in another. To be safe, quote anything that looks like a Boolean (for example ).</p><h3>Keep configuration simple and minimal</h3><p>Avoid setting default values that are already handled by Kubernetes. Minimal manifests are easier to debug, cleaner to review and less likely to break things later.</p><p>If your Deployment, Service and ConfigMap all belong to one app, put them in a single manifest file.\nIt's easier to track changes and apply them as a unit.\nSee the <a href=\"https://github.com/kubernetes/examples/blob/master/web/guestbook/all-in-one/guestbook-all-in-one.yaml\">Guestbook all-in-one.yaml</a> file for an example of this syntax.</p><p>You can even apply entire directories with:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>One command and boom everything in that folder gets deployed.</p><p>Manifest files are not just for machines, they are for humans too. Use annotations to describe why something exists or what it does. A quick one-liner can save hours when debugging later and also allows better collaboration.</p><p>The most helpful annotation to set is <code>kubernetes.io/description</code>. It's like using comment, except that it gets copied into the API so that everyone else can see it even after you deploy.</p><h2>Managing Workloads: Pods, Deployments, and Jobs</h2><p>A common early mistake in Kubernetes is creating Pods directly. Pods work, but they don't reschedule themselves if something goes wrong.</p><p> (Pods not managed by a controller, such as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\">Deployment</a> or a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a>) are fine for testing, but in real setups, they are risky.</p><p>Why?\nBecause if the node hosting that Pod dies, the Pod dies with it and Kubernetes won't bring it back automatically.</p><h3>Use Deployments for apps that should always be running</h3><p>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment\">RollingUpdate</a>), is almost always preferable to creating Pods directly.\nYou can roll out a new version, and if something breaks, roll back instantly.</p><h3>Use Jobs for tasks that should finish</h3><p>A <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Job</a> is perfect when you need something to run once and then stop like database migration or batch processing task.\nIt will retry if the pods fails and report success when it's done.</p><h2>Service Configuration and Networking</h2><p>Services are how your workloads talk to each other inside (and sometimes outside) your cluster. Without them, your pods exist but can't reach anyone. Let's make sure that doesn't happen.</p><h3>Create Services before workloads that use them</h3><p>When Kubernetes starts a Pod, it automatically injects environment variables for existing Services.\nSo, if a Pod depends on a Service, create a <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Service</a> its corresponding backend workloads (Deployments or StatefulSets), and before any workloads that need to access it.</p><p>For example, if a Service named foo exists, all containers will get the following variables in their initial environment:</p><pre tabindex=\"0\"><code>FOO_SERVICE_HOST=&lt;the host the Service runs on&gt;\nFOO_SERVICE_PORT=&lt;the port the Service runs on&gt;\n</code></pre><p>DNS based discovery doesn't have this problem, but it's a good habit to follow anyway.</p><h3>Use DNS for Service discovery</h3><p>If your cluster has the DNS <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/addons/\">add-on</a> (most do), every Service automatically gets a DNS entry. That means you can access it by name instead of IP:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>It's one of those features that makes Kubernetes networking feel magical.</p><h3>Avoid  and  unless absolutely necessary</h3><p>You'll sometimes see these options in manifests:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>But here's the thing:\nThey tie your Pods to specific nodes, making them harder to schedule and scale. Because each &lt;, , &gt; combination must be unique. If you don't specify the  and  explicitly, Kubernetes will use  as the default  and  as the default .\nUnless you're debugging or building something like a network plugin, avoid them.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><h3>Use headless Services for internal discovery</h3><p>Sometimes, you don't want Kubernetes to load balance traffic. You want to talk directly to each Pod. That's where <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#headless-services\">headless Services</a> come in.</p><p>You create one by setting .\nInstead of a single IP, DNS gives you a list of all Pods IPs, perfect for apps that manage connections themselves.</p><h2>Working with labels effectively</h2><p><a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\">Labels</a> are key/value pairs that are attached to objects such as Pods.\nLabels help you organize, query and group your resources.\nThey don't do anything by themselves, but they make everything else from Services to Deployments work together smoothly.</p><p>Good labels help you understand what's what, even after months later.\nDefine and use <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\">labels</a> that identify semantic attributes of your application or Deployment.\nFor example;</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><ul><li> : what the app is</li><li> : which layer it belongs to (frontend/backend)</li><li> : which stage it's in (test/prod)</li></ul><p>You can then use these labels to make powerful selectors.\nFor example:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This will list all frontend Pods across your cluster, no matter which Deployment they came from.\nBasically you are not manually listing Pod names; you are just describing what you want.\nSee the <a href=\"https://github.com/kubernetes/examples/tree/master/web/guestbook/\">guestbook</a> app for examples of this approach.</p><p>Kubernetes actually recommends a set of <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\">common labels</a>. It's a standardized way to name things across your different workloads or projects.\nFollowing this convention makes your manifests cleaner, and it means that tools such as <a href=\"https://headlamp.dev/\">Headlamp</a>, <a href=\"https://github.com/kubernetes/dashboard#introduction\">dashboard</a>, or third-party monitoring systems can all\nautomatically understand what's running.</p><h3>Manipulate labels for debugging</h3><p>Since controllers (like ReplicaSets or Deployments) use labels to manage Pods, you can remove a label to “detach” a Pod temporarily.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>The  part removes the label key .\nOnce that happens, the controller won’t manage that Pod anymore.\nIt’s like isolating it for inspection, a “quarantine mode” for debugging. To interactively remove or add labels, use <a href=\"https://kubernetes.io/docs/reference/kubectl/generated/kubectl_label/\"></a>.</p><p>You can then check logs, exec into it and once done, delete it manually.\nThat’s a super underrated trick every Kubernetes engineer should know.</p><p>These small tips make life much easier when you are working with multiple manifest files or clusters.</p><p>Instead of applying one file at a time, apply the whole folder:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>This command looks for ,  and  files in that folder and applies them all together.\nIt's faster, cleaner and helps keep things grouped by app.</p><h3>Use label selectors to get or delete resources</h3><p>You don't always need to type out resource names one by one.\nInstead, use <a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors\">selectors</a> to act on entire groups at once:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>It's especially useful in CI/CD pipelines, where you want to clean up test resources dynamically.</p><h3>Quickly create Deployments and Services</h3><p>For quick experiments, you don't always need to write a manifest. You can spin up a Deployment right from the CLI:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Then expose it as a Service:</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Cleaner configuration leads to calmer cluster administrators.\nIf you stick to a few simple habits: keep configuration simple and minimal, version-control everything,\nuse consistent labels, and avoid relying on naked Pods, you'll save yourself hours of debugging down the road.</p><p>The best part?\nClean configurations stay readable. Even after months, you or anyone on your team can glance at them and know exactly what’s happening.</p>","contentLength":8447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress NGINX Retirement: What You Need to Know","url":"https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/","date":1762885800,"author":"","guid":702,"unread":true,"content":"<p>To prioritize the safety and security of the ecosystem, Kubernetes SIG Network and the Security Response Committee are announcing the upcoming retirement of <a href=\"https://github.com/kubernetes/ingress-nginx/\">Ingress NGINX</a>. Best-effort maintenance will continue until March 2026. Afterward, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. <strong>Existing deployments of Ingress NGINX will continue to function and installation artifacts will remain available.</strong></p><p>We recommend migrating to one of the many alternatives. Consider <a href=\"https://gateway-api.sigs.k8s.io/guides/\">migrating to Gateway API</a>, the modern replacement for Ingress. If you must continue using Ingress, many alternative Ingress controllers are <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">listed in the Kubernetes documentation</a>. Continue reading for further information about the history and current state of Ingress NGINX, as well as next steps.</p><p><a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress/\">Ingress</a> is the original user-friendly way to direct network traffic to workloads running on Kubernetes. (<a href=\"https://kubernetes.io/docs/concepts/services-networking/gateway/\">Gateway API</a> is a newer way to achieve many of the same goals.) In order for an Ingress to work in your cluster, there must be an <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">Ingress controller</a> running. There are many Ingress controller choices available, which serve the needs of different users and use cases. Some are cloud-provider specific, while others have more general applicability.</p><p><a href=\"https://www.github.com/kubernetes/ingress-nginx\">Ingress NGINX</a> was an Ingress controller, developed early in the history of the Kubernetes project as an example implementation of the API. It became very popular due to its tremendous flexibility, breadth of features, and independence from any particular cloud or infrastructure provider. Since those days, many other Ingress controllers have been created within the Kubernetes project by community groups, and by cloud native vendors. Ingress NGINX has continued to be one of the most popular, deployed as part of many hosted Kubernetes platforms and within innumerable independent users’ clusters.</p><p>The breadth and flexibility of Ingress NGINX has caused maintenance challenges. Changing expectations about cloud native software have also added complications. What were once considered helpful options have sometimes come to be considered serious security flaws, such as the ability to add arbitrary NGINX configuration directives via the \"snippets\" annotations. Yesterday’s flexibility has become today’s insurmountable technical debt.</p><p>Despite the project’s popularity among users, Ingress NGINX has always struggled with insufficient or barely-sufficient maintainership. For years, the project has had only one or two people doing development work, on their own time, after work hours and on weekends. Last year, the Ingress NGINX maintainers <a href=\"https://kccncna2024.sched.com/event/1hoxW/securing-the-future-of-ingress-nginx-james-strong-isovalent-marco-ebert-giant-swarm\">announced</a> their plans to wind down Ingress NGINX and develop a replacement controller together with the Gateway API community. Unfortunately, even that announcement failed to generate additional interest in helping maintain Ingress NGINX or develop InGate to replace it. (InGate development never progressed far enough to create a mature replacement; it will also be retired.)</p><h2>Current State and Next Steps</h2><p>Currently, Ingress NGINX is receiving best-effort maintenance. SIG Network and the Security Response Committee have exhausted our efforts to find additional support to make Ingress NGINX sustainable. To prioritize user safety, we must retire the project.</p><p>In March 2026, Ingress NGINX maintenance will be halted, and the project will be <a href=\"https://github.com/kubernetes-retired/\">retired</a>. After that time, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered. The GitHub repositories will be made read-only and left available for reference.</p><p>Existing deployments of Ingress NGINX will not be broken. Existing project artifacts such as Helm charts and container images will remain available.</p><p>In most cases, you can check whether you use Ingress NGINX by running <code>kubectl get pods \\--all-namespaces \\--selector app.kubernetes.io/name=ingress-nginx</code> with cluster administrator permissions.</p><p>We would like to thank the Ingress NGINX maintainers for their work in creating and maintaining this project–their dedication remains impressive. This Ingress controller has powered billions of requests in datacenters and homelabs all around the world. In a lot of ways, Kubernetes wouldn’t be where it is without Ingress NGINX, and we are grateful for so many years of incredible effort.</p><p><strong>SIG Network and the Security Response Committee recommend that all Ingress NGINX users begin migration to Gateway API or another Ingress controller immediately.</strong> Many options are listed in the Kubernetes documentation: <a href=\"https://gateway-api.sigs.k8s.io/guides/\">Gateway API</a>, <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">Ingress</a>. Additional options may be available from vendors you work with.</p>","contentLength":4650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the 2025 Steering Committee Election Results","url":"https://kubernetes.io/blog/2025/11/09/steering-committee-results-2025/","date":1762719000,"author":"","guid":701,"unread":true,"content":"<p>The <a href=\"https://github.com/kubernetes/community/tree/master/elections/steering/2025\">2025 Steering Committee Election</a> is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2025. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.</p><p>The Steering Committee oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their <a href=\"https://github.com/kubernetes/steering/blob/master/charter.md\">charter</a>.</p><p>Thank you to everyone who voted in the election; your participation helps support the community’s continued health and success.</p><p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):</p><p>They join continuing members:</p><p>Maciej Szulik and Paco Xu are returning Steering Committee Members.</p><p>Thank you and congratulations on a successful election to this round’s election officers:</p><p>Thanks to the Emeritus Steering Committee Members. Your service is appreciated by the community:</p><p>And thank you to all the candidates who came forward to run for election.</p><p>You can see what the Steering Committee meetings are all about by watching past meetings on the <a href=\"https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM\">YouTube Playlist</a>.</p><p><em>This post was adapted from one written by the <a href=\"https://github.com/kubernetes/community/tree/master/communication/contributor-comms\">Contributor Comms Subproject</a>. If you want to write stories about the Kubernetes community, learn more about us.</em></p><p><em>This article was revised in November 2025 to update the information about when the steering committee meets.</em></p>","contentLength":1462,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gateway API 1.4: New Features","url":"https://kubernetes.io/blog/2025/11/06/gateway-api-v1-4/","date":1762448400,"author":"","guid":700,"unread":true,"content":"<p>Ready to rock your Kubernetes networking? The Kubernetes SIG Network community presented the General Availability (GA) release of Gateway API (v1.4.0)! Released on October 6, 2025, version 1.4.0 reinforces the path for modern, expressive, and extensible service networking in Kubernetes.</p><p>Gateway API v1.4.0 brings three new features to the \n(Gateway API's GA release channel):</p><ul><li><strong>BackendTLSPolicy for TLS between gateways and backends</strong></li><li><strong> in GatewayClass status</strong></li></ul><p>and introduces three new experimental features:</p><ul><li><strong>Mesh resource for service mesh configuration</strong></li><li> to ease configuration burden**</li><li><strong> filter for HTTPRoute</strong></li></ul><h2>Graduations to Standard Channel</h2><p><a href=\"https://gateway-api.sigs.k8s.io/api-types/backendtlspolicy\">BackendTLSPolicy</a> is a new Gateway API type for specifying the TLS configuration\nof the connection from the Gateway to backend pod(s).\n. Prior to the introduction of BackendTLSPolicy, there was no API specification\nthat allowed encrypted traffic on the hop from Gateway to backend.</p><p>The  configuration requires a hostname. This \nserves two purposes. It is used as the SNI header when connecting to the backend and\nfor authentication, the certificate presented by the backend must match this hostname,\n is explicitly specified.</p><p>If  (SANs) are specified, the  is only used for SNI, and authentication is performed against the SANs instead. If you still need to authenticate against the hostname value in this case, you MUST add it to the  list.</p><p>BackendTLSPolicy  configuration also requires either  or .\n refer to one or more (up to 8) PEM-encoded TLS certificate bundles. If there are no specific certificates to use,\nthen depending on your implementation, you may use ,\nset to \"System\" to tell the Gateway to use an implementation-specific set of trusted CA Certificates.</p><p>In this example, the BackendTLSPolicy is configured to use certificates defined in the auth-cert ConfigMap\nto connect with a TLS-encrypted upstream connection where pods backing the auth service are expected to serve a\nvalid certificate for . It uses  with a Hostname type, but you may also use a URI type.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>In this example, the BackendTLSPolicy is configured to use system certificates to connect with a TLS-encrypted backend connection where Pods backing the dev Service are expected to serve a valid certificate for .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>GatewayClass status has a new field, .\nThis addition allows implementations to declare the set of features they support. This provides a clear way for users and tools to understand the capabilities of a given GatewayClass.</p><p>This feature's name for conformance tests (and GatewayClass status reporting) is .\nImplementations must populate the  field in the  of the GatewayClass  the GatewayClass\nis accepted, or in the same operation.</p><p>Here’s an example of a  published under GatewayClass' :</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Graduation of SupportedFeatures to Standard, helped improve the conformance testing process for Gateway API.\nThe conformance test suite will now automatically run tests based on the features populated in the GatewayClass' status.\nThis creates a strong, verifiable link between an implementation's declared capabilities and the test results,\nmaking it easier for implementers to run the correct conformance tests and for users to trust the conformance reports.</p><p>This means when the SupportedFeatures field is populated in the GatewayClass status there will be no need for additional\nconformance tests flags like , or  or .\nIt's important to note that Mesh features are an exception to this and can be tested for conformance by using\n, or by manually providing any combination of features related flags until the dedicated resource\ngraduates from the experimental channel.</p><p>This enhancement enables route rules to be explicitly identified and referenced across the Gateway API ecosystem.\nSome of the key use cases include:</p><ul><li> Allowing status conditions to reference specific rules directly by name.</li><li> Making it easier to identify individual rules in logs, traces, and metrics.</li><li> Enabling policies (<a href=\"https://gateway-api.sigs.k8s.io/geps/gep-773\">GEP-713</a>) to target specific route rules via the  field in their .</li><li> Simplifying filtering and referencing of route rules in tools such as , , and general-purpose utilities like  and .</li><li><strong>Internal configuration mapping:</strong> Facilitating the generation of internal configurations that reference route rules by name within gateway and mesh implementations.</li></ul><p>This follows the same well-established pattern already adopted for Gateway listeners, Service ports, Pods (and containers),\nand many other Kubernetes resources.</p><p>While the new name field is  (so existing resources remain valid), its use is .\nImplementations are not expected to assign a default value, but they may enforce constraints such as immutability.</p><p>Finally, keep in mind that the <a href=\"https://gateway-api.sigs.k8s.io/geps/gep-995/?h=995#format\">name format</a> is validated,\nand other fields (such as <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/?h=sectionname#sectionname\"></a>)\nmay impose additional, indirect constraints.</p><h2>Experimental channel changes</h2><h3>Enabling external Auth for HTTPRoute</h3><p>Giving Gateway API the ability to enforce authentication and maybe authorization as well at the Gateway or HTTPRoute level has been a highly requested feature for a long time. (See the <a href=\"https://github.com/kubernetes-sigs/gateway-api/issues/1494\">GEP-1494 issue</a> for some background.)</p><p>This Gateway API release adds an Experimental filter in HTTPRoute that tells the Gateway API implementation to call out to an external service to authenticate (and, optionally, authorize) requests.</p><p>This filter is based on the <a href=\"https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/ext_authz_filter#config-http-filters-ext-authz\">Envoy ext_authz API</a>, and allows talking to an Auth service that uses either gRPC or HTTP for its protocol.</p><p>Both methods allow the configuration of what headers to forward to the Auth service, with the HTTP protocol allowing some extra information like a prefix path.</p><p>A HTTP example might look like this (noting that this example requires the Experimental channel to be installed and an implementation that supports External Auth to actually understand the config):</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This allows the backend Auth service to use the supplied headers to make a determination about the authentication for the request.</p><p>When a request is allowed, the external Auth service will respond with a 200 HTTP response code, and optionally extra headers to be included in the request that is forwarded to the backend. When the request is denied, the Auth service will respond with a 403 HTTP response.</p><p>Since the Authorization header is used in many authentication methods, this method can be used to do Basic, Oauth, JWT, and other common authentication and authorization methods.</p><p>Gateway API v1.4.0 introduces a new experimental Mesh resource, which provides a way to configure mesh-wide settings and discover the features supported by a given mesh implementation. This resource is analogous to the Gateway resource and will initially be mainly used for conformance testing, with plans to extend its use to off-cluster Gateways in the future.</p><p>The Mesh resource is cluster-scoped and, as an experimental feature, is named  and resides in the <code>gateway.networking.x-k8s.io</code> API group. A key field is controllerName, which specifies the mesh implementation responsible for the resource. The resource's  stanza indicates whether the mesh implementation has accepted it and lists the features the mesh supports.</p><p>One of the goals of this GEP is to avoid making it more difficult for users to adopt a mesh. To simplify adoption, mesh implementations are expected to create a default Mesh resource upon startup if one with a matching  doesn't already exist. This avoids the need for manual creation of the resource to begin using a mesh.</p><p>The new XMesh API kind, within the gateway.networking.x-k8s.io/v1alpha1 API group,\nprovides a central point for mesh configuration and feature discovery (source).</p><p>A minimal XMesh object specifies the :</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The mesh implementation populates the status field to confirm it has accepted the resource and to list its supported features ( source):</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>Introducing default Gateways</h3><p>For application developers, one common piece of feedback has been the need to explicitly name a parent Gateway for every single north-south Route. While this explicitness prevents ambiguity, it adds friction, especially for developers who just want to expose their application to the outside world without worrying about the underlying infrastructure's naming scheme. To address this, we have introduce the concept of .</p><h4>For application developers: Just \"use the default\"</h4><p>As an application developer, you often don't care about the specific Gateway your traffic flows through, you just want it to work. With this enhancement, you can now create a Route and simply ask it to use a default Gateway.</p><p>This is done by setting the new  field in your Route's .</p><p>Here’s a simple  that uses a default Gateway:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>That's it! No more need to hunt down the correct Gateway name for your environment. Your Route is now a \"defaulted Route.\"</p><h4>For cluster operators: You're still in control</h4><p>This feature doesn't take control away from cluster operators (\"Chihiro\").\nIn fact, they have explicit control over which Gateways can act as a default. A Gateway will only accept these  if it is configured to do so.</p><p>You can also use a ValidatingAdmissionPolicy to either require or even forbid for Routes to rely on a default Gateway.</p><p>As a cluster operator, you can designate a Gateway as a default\nby setting the (new)  field:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Operators can choose to have no default Gateways, or even multiple.</p><h4>How it works and key details</h4><ul><li><p>To maintain a clean, GitOps-friendly workflow, a default Gateway does  modify the  of your Route. Instead, the binding is reflected in the Route's  field. You can always inspect the  stanza of your Route to see exactly which Gateway or Gateways have accepted it. This preserves your original intent and avoids conflicts with CD tools.</p></li><li><p>The design explicitly supports having multiple Gateways designated as defaults within a cluster. When this happens, a defaulted Route will bind to  of them. This enables cluster operators to perform zero-downtime migrations and testing of new default Gateways.</p></li><li><p>You can create a single Route that handles both north-south traffic (traffic entering or leaving the cluster, via a default Gateway) and east-west/mesh traffic (traffic between services within the cluster), by explicitly referencing a Service in .</p></li></ul><p>Default Gateways represent a significant step forward in making the Gateway API simpler and more intuitive for everyday use cases, bridging the gap between the flexibility needed by operators and the simplicity desired by developers.</p><h3>Configuring client certificate validation</h3><p>This release brings updates for configuring client certificate validation, addressing a critical security vulnerability related to connection reuse.\nHTTP connection coalescing is a web performance optimization that allows a client to reuse an existing TLS connection\nfor requests to different domains. While this reduces the overhead of establishing new connections, it introduces a security risk\nin the context of API gateways.\nThe ability to reuse a single TLS connection across multiple Listeners brings the need to introduce shared client certificate\nconfiguration in order to avoid unauthorized access.</p><h4>Why SNI-based mTLS is not the answer</h4><p>One might think that using Server Name Indication (SNI) to differentiate between Listeners would solve this problem.\nHowever, TLS SNI is not a reliable mechanism for enforcing security policies in a connection coalescing scenario.\nA client could use a single TLS connection for multiple peer connections, as long as they are all covered by the same certificate.\nThis means that a client could establish a connection by indicating one peer identity (using SNI), and then reuse that connection\nto access a different virtual host that is listening on the same IP address and port. That reuse, which is controlled by client side\nheuristics, could bypass mutual TLS policies that were specific to the second listener configuration.</p><p>Here's an example to help explain it:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>I have configured a Gateway with two listeners, both having overlapping hostnames.\nMy intention is for the  listener to be accessible only by clients presenting the  certificate.\nIn contrast, the  listener should allow access to a broader audience using any certificate valid for the  domain.</p><p>Consider a scenario where a client initially connects to . The server requests and successfully validates the\n certificate, establishing the connection. Subsequently, the same client wishes to access other sites within this domain,\nsuch as , which is handled by the  listener. Due to connection reuse,\nclients can access  backends without an additional TLS handshake on the existing connection.\nThis process functions as expected.</p><p>However, a critical security vulnerability arises when the order of access is reversed.\nIf a client first connects to  and presents a valid  certificate, the connection is successfully established.\nIf this client then attempts to access , the existing connection's client certificate will not be re-validated.\nThis allows the client to bypass the specific certificate requirement for the  backend, leading to a serious security breach.</p><h4>The solution: per-port TLS configuration</h4><p>The updated Gateway API gains a  field in the  of a Gateway, that allows you to define a default client certificate\nvalidation configuration for all Listeners, and then if needed override it on a per-port basis. This provides a flexible and\npowerful way to manage your TLS policies.</p><p>Here’s a look at the updated API definitions (shown as Go source code):</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h3>Standard GRPCRoute -  field required (technicality)</h3><p>The promotion of GRPCRoute to Standard introduces a minor but technically breaking change regarding the presence of the top-level  field.\nAs part of achieving Standard status, the Gateway API has tightened the OpenAPI schema validation within the GRPCRoute\nCustomResourceDefinition (CRD)\nto explicitly ensure the spec field is required for all GRPCRoute resources.\nThis change enforces stricter conformance to Kubernetes object standards and enhances the resource's stability and predictability.\nWhile it is highly unlikely that users were attempting to define a GRPCRoute without any specification, any existing automation\nor manifests that might have relied on a relaxed interpretation allowing a completely absent  field will now fail validation\nand  be updated to include the  field, even if empty.</p><h3>Experimental CORS support in HTTPRoute - breaking change for  field</h3><p>The Gateway API subproject has introduced a breaking change to the Experimental CORS support in HTTPRoute, concerning the  field\nwithin the CORS policy.\nThis field's definition has been strictly aligned with the upstream CORS specification, which dictates that the corresponding\n<code>Access-Control-Allow-Credentials</code> header must represent a Boolean value.\nPreviously, the implementation might have been overly permissive, potentially accepting non-standard or string representations such as\n due to relaxed schema validation.\nUsers who were configuring CORS rules must now review their manifests and ensure the value for \nstrictly conforms to the new, more restrictive schema.\nAny existing HTTPRoute definitions that do not adhere to this stricter validation will now be rejected by the API server,\nrequiring a configuration update to maintain functionality.</p><h2>Improving the development and usage experience</h2><p>As part of this release, we have improved some of the developer experience workflow:</p><ul><li>Added <a href=\"https://github.com/kubernetes-sigs/kube-api-linter\">Kube API Linter</a> to the CI/CD pipelines, reducing the burden of API reviewers and also reducing the amount of common mistakes.</li><li>Improving the execution time of CRD tests with the usage of <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/envtest\"></a>.</li></ul><p>Additionally, as part of the effort to improve Gateway API usage experience, some efforts were made to remove some ambiguities and some old tech-debts from our documentation website:</p><ul><li>The API reference is now explicit when a field is .</li><li>The GEP (GatewayAPI Enhancement Proposal) navigation bar is automatically generated, reflecting the real status of the enhancements.</li></ul><p>Unlike other Kubernetes APIs, you don't need to upgrade to the latest version of\nKubernetes to get the latest version of Gateway API. As long as you're running\nKubernetes 1.26 or later, you'll be able to get up and running with this version\nof Gateway API.</p><p>As of this writing, seven implementations are already conformant with Gateway API v1.4.0. In alphabetical order:</p><p>Wondering when a feature will be added? There are lots of opportunities to get\ninvolved and help define the future of Kubernetes routing APIs for both ingress\nand service mesh.</p><p>The maintainers would like to thank  who's contributed to Gateway\nAPI, whether in the form of commits to the repo, discussion, ideas, or general\nsupport. We could never have made this kind of progress without the support of\nthis dedicated and active community.</p><h2>Related Kubernetes blog articles</h2>","contentLength":16523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}