{"id":"2k6wXud2N91xpf4PwnSfo4j5WSuTNX5k","title":"Tech News - Last 2 days","displayTitle":"Tech News - Last 2 days","url":"","feedLink":"","is_query":true,"items":[{"title":"In a First, Surgical Robots Learned Tasks By Watching Videos","url":"https://hardware.slashdot.org/story/24/12/30/1349256/in-a-first-surgical-robots-learned-tasks-by-watching-videos?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735574460,"author":"msmash","unread":true,"content":"Speaking of robots, Johns Hopkins University and Stanford University researchers say they trained robots to perform surgical tasks autonomously using video learning, marking a breakthrough in robotic surgery capabilities. \n\nThe robots successfully manipulated needles, tied knots, and sutured wounds independently, demonstrating ability to correct errors like dropped needles without human input. Testing has advanced to full surgeries on animal cadavers. \n\nResearchers aim to address a projected U.S. surgeon shortage of 10,000-20,000 by 2036. The technology builds on decades of robot-assisted surgery, which recorded 876,000 procedures in 2020.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=In+a+First%2C+Surgical+Robots+Learned+Tasks+By+Watching+Videos%3A+https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1349256%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1349256%2Fin-a-first-surgical-robots-learned-tasks-by-watching-videos%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://hardware.slashdot.org/story/24/12/30/1349256/in-a-first-surgical-robots-learned-tasks-by-watching-videos?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564697&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia completes acquisition of AI infrastructure startup Run:ai","url":"https://techcrunch.com/2024/12/30/nvidia-completes-acquisition-of-ai-infrastructure-startup-runai/","date":1735574294,"author":"Kyle Wiggers","unread":true,"content":"<p>Nvidia has completed its acquisition of Run:ai, an Israeli startup that helps manage and optimize AI hardware infrastructure. As part of the merger, Run:ai said its software, which currently only works with Nvidia products, will be open sourced, meaning Nvidia rivals like AMD and Intel will be able to adapt it for their hardware. &#8220;We [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"UK antitrust watchdog launches review of IBM’s HashiCorp takeover","url":"https://techcrunch.com/2024/12/30/uk-antitrust-watchdog-launches-review-of-ibms-hashicorp-takeover/","date":1735573492,"author":"Kyle Wiggers","unread":true,"content":"<p>The Competition and Markets Authority, the U.K.&#8217;s antitrust watchdog, has opened an investigation into whether IBM&#8217;s planned acquisition of cloud software vendor HashiCorp would affect competition. The CMA said Monday it was inviting comment on the merger from interested parties by January 16. The regulator set a provisional February 25 deadline to decide whether to [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia Bets on Robotics To Drive Future Growth","url":"https://hardware.slashdot.org/story/24/12/30/1340245/nvidia-bets-on-robotics-to-drive-future-growth?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735572120,"author":"msmash","unread":true,"content":"An anonymous reader shares a report: Nvidia is betting on robotics as its next big driver of growth, as the world's most valuable semiconductor company faces increasing competition in its core AI chipmaking business. The US tech group, best known for the infrastructure that has underpinned the AI boom, is set to launch its latest generation of compact computers for humanoid robots [non-paywalled link] -- dubbed Jetson Thor -- in the first half of 2025. \n\nNvidia is positioning itself to be the leading platform for what the tech group believes is an imminent robotics revolution. The company sells a \"full stack\" solution, from the layers of software for training AI-powered robots to the chips that go into them. [...] Talla said a shift in the robotics market is being driven by two technological breakthroughs: the explosion of generative AI models and the ability to train robots on these foundational models using simulated environments. The latter has been a particularly significant development as it helps solve what roboticists call the \"Sim-to-Real gap,\" ensuring robots trained in virtual environments can operate effectively in the real world, he said.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Nvidia+Bets+on+Robotics+To+Drive+Future+Growth%3A+https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1340245%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1340245%2Fnvidia-bets-on-robotics-to-drive-future-growth%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://hardware.slashdot.org/story/24/12/30/1340245/nvidia-bets-on-robotics-to-drive-future-growth?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564695&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Backed by a16z and QED, Brazilian startup Carecode puts AI agents to work on healthcare","url":"https://techcrunch.com/2024/12/30/backed-by-a16z-and-qed-brazilian-startup-carecode-puts-ai-agents-to-work-on-healthcare/","date":1735571223,"author":"Anna Heim","unread":true,"content":"<p>AI holds huge promise for healthcare, but not just on the medical side; many startups are convinced machine learning-based systems can do a lot of good on adjacent tasks such as appointment scheduling and confirmations. Brazilian startup Carecode is among these AI believers. It&#8217;s coming out of stealth with an ambition to reduce healthcare costs [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Why China Is Building a Thorium Molten-Salt Reactor","url":"https://spectrum.ieee.org/chinas-thorium-molten-salt-reactor","date":1735570803,"author":"Emily Waltz","unread":true,"content":"<p>China’s demo reactor could breed nuclear fuel from rare earth waste </p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzQxMy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc3Nzk3MzY3OH0.Ry1f7Rt0Q--yMVdfFmvPrkXq8W1khLk8t2lBTemu6co/image.png?width=600","enclosureMime":""},{"title":"Mercedes-backed Volocopter files for bankruptcy","url":"https://techcrunch.com/2024/12/30/mercedes-backed-volocopter-files-for-bankruptcy/","date":1735570669,"author":"Sean O'Kane","unread":true,"content":"<p>German electric air taxi company Volocopter has filed for bankruptcy protection, the latest in a string of similar startups to hit financial turbulence. The company plans to keep operating while it searches for new investors. &#8220;We are ahead of our industry peers in our technological, flight test, and certification progress. That makes us an attractive [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Chess Federation Changes Rules To Allow Jeans Amid Spat; Magnus Carlsen Returns","url":"https://games.slashdot.org/story/24/12/30/1436223/chess-federation-changes-rules-to-allow-jeans-amid-spat-magnus-carlsen-returns?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735569360,"author":"msmash","unread":true,"content":"World chess champion Magnus Carlsen has returned to the International Chess Federation (FIDE) World Rapid and Blitz Championships after new rules allowed players to wear \"elegant\" jeans with jackets. \n\nCarlsen had withdrawn from the New York tournament when officials demanded he change out of jeans he wore after a lunch meeting, threatening him with fines and disqualification. FIDE revised its dress code following the incident, permitting \"appropriate jeans matching the jacket\" as an \"elegant minor deviation\" from standard attire.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Chess+Federation+Changes+Rules+To+Allow+Jeans+Amid+Spat%3B+Magnus+Carlsen+Returns%3A+https%3A%2F%2Fgames.slashdot.org%2Fstory%2F24%2F12%2F30%2F1436223%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fgames.slashdot.org%2Fstory%2F24%2F12%2F30%2F1436223%2Fchess-federation-changes-rules-to-allow-jeans-amid-spat-magnus-carlsen-returns%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://games.slashdot.org/story/24/12/30/1436223/chess-federation-changes-rules-to-allow-jeans-amid-spat-magnus-carlsen-returns?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564755&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Trump Defends Foreign Worker Visas","url":"https://news.slashdot.org/story/24/12/30/0910218/trump-defends-foreign-worker-visas?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735567260,"author":"msmash","unread":true,"content":"President-elect Donald Trump has defended the H-1B visa program for skilled foreign workers. \"I've always liked the visas. I have many H-1B visas on my properties... It's a great program,\" Trump told The New York Post. \n\nHis comments follow recent support for the program from Elon Musk and Vivek Ramaswamy. The H-1B program allows 85,000 skilled workers to immigrate annually, including 20,000 spots for those with U.S. advanced degrees. Trump's businesses have received approval to hire over 2,100 foreign workers since 2008, with about 70 positions through H-1B visas, mostly over a decade ago.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Trump+Defends+Foreign+Worker+Visas%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0910218%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0910218%2Ftrump-defends-foreign-worker-visas%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/30/0910218/trump-defends-foreign-worker-visas?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564539&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Mesa's Terrific Year With Better Vulkan Ray-Tracing, NVK Progress & Same-Day Vulkan 1.4","url":"https://www.phoronix.com/news/Mesa-2024-Highlights","date":1735567220,"author":"Michael Larabel","unread":true,"content":"The open-source Mesa 3D graphics driver had a rather great year with a number of performance optimizations landing, on-time support for Intel Lunar Lake and Battlemage Xe2 graphics, early AMD RDNA4 support, multiple drivers having same-day Vulkan 1.4 support, the continued progress of the open-source NVIDIA NVK Vulkan driver, and much more thanks to the contributions of Intel, AMD, Valve, and other organizations -- even Microsoft's continued merge requests!..","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Top 10 Semiconductor Stories of 2024","url":"https://spectrum.ieee.org/top-semiconductor-stories-2024","date":1735567203,"author":"Samuel K. Moore","unread":true,"content":"<p>Trillion-transistor GPUs, steel-slicing laser chips, particle accelerators, and more</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4Mjg1NS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5MzQ3NDE0OH0.zUCZ-8vA8dnwxNkqZIo5LfGU9Bax-iSKCTfPZoCjmcc/image.jpg?width=600","enclosureMime":""},{"title":"Most Safety Complaints From Plane-Industry Whistleblowers 'Go Nowhere', Risk Retaliation","url":"https://yro.slashdot.org/story/24/12/30/0152249/most-safety-complaints-from-plane-industry-whistleblowers-go-nowhere-risk-retaliation?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735562040,"author":"EditorDavid","unread":true,"content":"America's aerospace industry is overseen by the Federal Aviation Administration (or FAA) &mdash; which also handles safety warnings from the industry's whistleblowers. But the Seattle Times says an analysis of reports to Congress found \"an overwhelmed system delivering underwhelming results for whistleblowers... More than 90% of safety complaints from 2020 through 2023 ended with no violation found by the FAA, while whistleblowers reported them at great personal and professional risk.\"\n\nAside from the FAA's in-house program, employees of Boeing, Spirit and the FAA can report safety hazards to the Office of Special Counsel, which has no FAA ties, or through internal employer complaint programs, such as Boeing's Speak Up and Spirit's Quality 360, to trigger company reviews... In the aftermath of the door-plug blowout over Portland, Boeing specifically asked its employees to use the Speak Up program or the FAA's internal process to report any concerns, according to Boeing spokesperson Jessica Kowal. Both have done a poor job protecting whistleblowers from retaliation, according to a congressionally appointed expert panel... While both were designed to guard against retaliation, critics say they have instead become enablers of it... \n\nA panel of aviation safety experts in February rebuked Boeing's Speak Up program in a report to Congress. Whistleblower advocates criticized Speak Up for commonly outing whistleblowers to the supervisors they're complaining about, exposing them to retaliation. Managers sometimes investigated complaints against themselves. Employees mistrusted the program's promise of anonymity. Collectively, the befuddling maze of whistleblower options sowed \"confusion about reporting systems that may discourage employees from submitting safety concerns,\" according to the expert panel's report.... \n\n[Boeing quality inspector Sam Mohawk, who alleged the 737 MAX line in Renton was losing track of subpar aircraft parts], continues to pursue his FAA claim, originally submitted through Boeing's Speak Up program. Months passed before Boeing addressed Mohawk's complaint. When it did, Mohawk's report was passed to the managers he was complaining about, according to Brian Knowles, Mohawk's South Carolina-based lawyer. \"If you do Speak Up, just know that your report is going to go straight to the guys you're accusing of wrongdoing. They aren't going to say, 'Thanks for speaking up against us,'\" Knowles said. \nThe article includes this quote about the FAA's in-house whistleblower program from Tom Devine, a whistleblower attorney with nearly a half-century of experience across a spectrum of federal agencies, and legal director of the nonprofit Government Accountability Project, which helps whistleblowers navigate the federal system. \"It's been a disaster from the beginning. We tell everyone to avoid it because it's a trap... We've warned whistleblowers not to entrust their rights there.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Most+Safety+Complaints+From+Plane-Industry+Whistleblowers+'Go+Nowhere'%2C+Risk+Retaliation%3A+https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F30%2F0152249%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F30%2F0152249%2Fmost-safety-complaints-from-plane-industry-whistleblowers-go-nowhere-risk-retaliation%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://yro.slashdot.org/story/24/12/30/0152249/most-safety-complaints-from-plane-industry-whistleblowers-go-nowhere-risk-retaliation?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564391&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"RadeonSI Driver Now Uses ACO By Default For Pre-RDNA GPUs","url":"https://www.phoronix.com/news/RadeonSI-ACO-Default-Pre-GFX10","date":1735560480,"author":"Michael Larabel","unread":true,"content":"As a very interesting end-of-year change for Mesa 25.0, AMD is now using the ACO compiler by default for pre-GFX10 (before RDNA / Navi) GPUs with the RadeonSI Gallium3D driver...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Passkey technology is elegant, but it’s most definitely not usable security","url":"https://arstechnica.com/security/2024/12/passkey-technology-is-elegant-but-its-most-definitely-not-usable-security/","date":1735560053,"author":"Dan Goodin","unread":true,"content":"\n              <p>It's that time again, when families and friends gather and implore the more technically inclined among them to troubleshoot problems they're having behind the device screens all around them. One of the most vexing and most common problems is logging into accounts in a way that's both secure and reliable.</p>\n<p>Using the same password everywhere is easy, but in an age of mass data breaches and precision-orchestrated phishing attacks, it's also highly unadvisable. Then again, creating hundreds of unique passwords, storing them securely, and keeping them out of the hands of phishers and database hackers is hard enough for experts, let alone Uncle Charlie, who got his first smartphone only a few years ago. No wonder this problem never goes away.</p>\n<p>Passkeys—the much-talked-about password alternative to passwords that have been widely available for almost two years—was supposed to fix all that. When I wrote about passkeys <a href=\"https://arstechnica.com/information-technology/2023/05/passwordless-google-accounts-are-easier-and-more-secure-than-passwords-heres-why/\">two years ago</a>, I was a big believer. I remain convinced that passkeys mount the steepest hurdle yet for phishers, SIM swappers, database plunderers, and other adversaries trying to hijack accounts. How and why is that?</p><p><a href=\"https://arstechnica.com/security/2024/12/passkey-technology-is-elegant-but-its-most-definitely-not-usable-security/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/security/2024/12/passkey-technology-is-elegant-but-its-most-definitely-not-usable-security/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/passkey-1000x648.jpg","enclosureMime":""},{"title":"Updated Serpent OS Alpha Brings Few Fixes To This Original Linux Distribution","url":"https://www.phoronix.com/news/Serpent-OS-Alpha-Update","date":1735558691,"author":"Michael Larabel","unread":true,"content":"Last week Ikey Doherty's Serpent OS Linux distribution debuted in alpha form while kicking off the new week is updated install media to provide a few fixes for this original from-scratch Linux distribution...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AMD's GPUOpen Vulkan Memory Allocator Now Supports Vulkan 1.4","url":"https://www.phoronix.com/news/Vulkan-Memory-Allocator-3.2","date":1735557990,"author":"Michael Larabel","unread":true,"content":"AMD's GPUOpen team managed to squeeze in a new Vulkan Memory Allocator release into 2024. As a reminder this is a easy to use/integrate Vulkan memory allocation library for both Windows and Linux systems with hopes of making memory allocation and resource creation more easier like with Direct3D 11 and OpenGL...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"xxHash 0.8.3 Brings Runtime Vector Extension Handling For x86/x86_64","url":"https://www.phoronix.com/news/xxHash-0.8.3-Released","date":1735557475,"author":"Michael Larabel","unread":true,"content":"Meta's Yann Collet of Zstd fame is rounding out 2024 by releasing xxHash 0.8.3 as the newest update to this extremely fast non-cryptographic hash algorithm. The xxHash fast hash algorithm pushes for RAM speed limits and with the v0.8.3 update brings more enhancements...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Calo raises $25 million to expand its ready-to-eat meal service beyond the Middle East","url":"https://techcrunch.com/2024/12/30/calo-raises-25-million-to-expand-its-ready-to-eat-meal-service-beyond-middle-east/","date":1735554600,"author":"Ivan Mehta","unread":true,"content":"<p>A business built around increasingly customized ready-to-eat meals has netted Middle Eastern startup Calo a sizeable funding injection as it looks to expand both what it can offer its time-strapped customers and where it delivers its growing range of just-heat-to-eat dishes. The meal delivery market in the Middle East will hit $11.2 billion by 2030, [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AT&T and Verizon say networks are secure after being breached by China-linked Salt Typhoon hackers","url":"https://techcrunch.com/2024/12/30/verizon-says-it-has-secured-its-network-after-breach-by-china-linked-salt-typhoon-group/","date":1735553716,"author":"Carly Page","unread":true,"content":"<p>U.S. telecom giants AT&#38;T and Verizon say they have secured their networks after being targeted by the&#160;China-linked Salt Typhoon cyberespionage group. In a statement given to TechCrunch on Monday, AT&#38;T spokesperson Alexander Byers said the company detects &#8220;no activity by nation-state actors in our networks at this time.&#8221; Verizon spokesperson Richard Young said in an [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Open-Source Platform Is Letting AI Researchers Crack Tough Languages","url":"https://hackernoon.com/new-open-source-platform-is-letting-ai-researchers-crack-tough-languages?source=rss","date":1735553711,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 10 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"5conclusions\">5. Conclusions</h2>\n<p>In this work, we propose a revised approach to NLPre evaluation via benchmarking. This is motivated by the widespread use of the benchmarking technique in other NLP fields on par with the shortcomings of existing NLPre evaluation solutions.</p>\n<p>\\\nWe implement said NLPre benchmarking approach as the online system that evaluates the submitted outcome of an NLPre system and updates the associated leaderboard with the results after the submitter’s approval. The benchmarking system is designed to rank NLPre tools available for a given language in a trustworthy environment.</p>\n<p>\\\nThe endeavour of defining and enhancing the system’s capabilities is conducted concurrently with the effort to create the NLPre benchmark for Polish that encompasses numerous factors, such as tasks not required in English or diverse tagsets. The NLPre-PL benchmark consists of the predefined NLPre tasks, coupled with two reformulated datasets. The NLPre-PL benchmark, therefore, sets the standard for evaluating the performance of the NLPre tools for Polish, which represents a derivative yet important outcome of our research.</p>\n<p>\\\nIn addition to integration into the benchmarking system, NLPre-PL is used to conduct empirical experiments. We perform a robust and extensive comparison of different NLPre methods, including the classical non-neural tools and the modern neural network-based techniques. The results of these experiments on datasets in two tagsets are discussed in detail. The experiments confirm our assumptions that modern architectures obtain better results. Because NLP is a discipline undergoing rapid progress, new NLPre solutions, e.g. multilingual or zero-shot, can be expected in the coming years. These new solutions can be easily tested and compared with the tools evaluated so far in our benchmarking system.</p>\n<p>\\\nFinally, we release the open-source code of the benchmarking system in hopes that this endeavour could be replicated for other languages. To expedite this process, we ensure that the system is fully configurable and language- and tagset-agnostic. The NLPre system, configured for a specified language, can be self-hosted on a chosen server, and the results from the leaderboard are conveniently accessible via an API. We see a potential future application of our system to the UD repository, where for 141 languages, there are currently 245 treebanks with supposedly discrepant versions of the UD tagset.</p>\n<h2 id=\"6appendices\">6. Appendices</h2>\n<h3 id=\"61infrastructureused\">6.1. Infrastructure used</h3>\n<p>We train the models using several types of computational nodes at our disposal, including NVIDIA V100 32GB, NVIDIA GeForce RTX 2080 8GB, NVIDIA GeForce 3070 8GB and Intel Xeon E5-2697 processor. Since we do not perform hyperparameter tuning, this should not impact our results.</p>\n<h3 id=\"62furtherresultsofexperiments\">6.2. Further results of experiments</h3>\n<p>Herein, we present a comprehensive depiction of our experimental findings as they are displayed on the NLPre-PL leaderboard.</p>\n<p>\\\nIn Table 5, we present the full results of the evaluation of the selected models on the Morfeuszbased datasets byName and byType. These results are provided for all available tasks that can be performed on the above-mentioned datasets. As NKJP1M datasets contain no syntantic trees, it is thus impossible to test the dependency parsing task that rely on these trees and measure UAS, LAS, CLAS, MLAS and BLEX.</p>\n<p>\\\nIn Table 6, we present the results of the evaluation of the selected models on the UD-based datasets byName, byType, and PDB. This table contains the results of segmentation, tagging, and lemmatization tasks. Table 7 is a continuation of Table 6 and it contains the results for the same tagset and dataset on the dependency parsing task.</p>\n<p>\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-46830d1.png\" alt=\"Table 5: Benchmark results for the Morfeusz tagset performed on two datasets: NKJP-byType (bT) and NKJP-byName (bN); AA – Aligned Accuracy; F1 – F1 score. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert-base, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\\n\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-6y930da.png\" alt=\"Table 6: Benchmark results for the UD tagset performed on three datasets: NKJP-byType (bT), NKJP-byName (bN), and PDB-UD (PDB) for segmentation, tagging and lemmatization tasks; AA – Aligned Accuracy; F1 – F1 score. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert-base, pl – pl-core-news-lg, H – HerBERT-base.\" /></p>\n<p>\\\n\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-73a30va.png\" alt=\"Table 7: Benchmark results for the UD tagset performed on three datasets: NKJP-byType (bT), NKJP-byName (bN), and PDB-UD (PDB) for the dependency parsing task; AA – Aligned Accuracy; F1 – F1 score. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert-base, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\</p>\n<h2 id=\"7acknowledgements\">7. Acknowledgements</h2>\n<p>This work was supported by the European Regional Development Fund as a part of 2014–2020 Smart Growth Operational Programme, CLARIN — Common Language Resources and Technology Infrastructure (project no. POIR.04.02.00-00C002/19) and DARIAH-PL — Digital Research Infrastructure for the Arts and Humanities (project no. POIR.04.02.00-00-D006/20-0). We gratefully acknowledge Poland’s high-performance computing infrastructure PLGrid (HPC Centers: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2022/015872.</p>\n<h2 id=\"8bibliographicalreferences\">8. Bibliographical References</h2>\n<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135–146.</p>\n<p>\\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.</p>\n<p>\\\nSabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLLX), pages 149–164, New York City. Association for Computational Linguistics.</p>\n<p>\\\nKehai Chen, Tiejun Zhao, Muyun Yang, and Lemao Liu. 2017. Translation prediction with source dependency-based context representation. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1).</p>\n<p>\\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.</p>\n<p>\\\nDick Crouch, Mary Dalrymple, Ronald M. Kaplan, Tracy Holloway King, John Maxwell, and Paula Newman. 2011. XLE Documentation. Palo Alto Research Center.</p>\n<p>\\\nMarie-Catherine de Marneffe, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. 2021. Universal Dependencies. Computational Linguistics, 47(2):255–308.</p>\n<p>\\\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120, Online. Association for Computational Linguistics.</p>\n<p>\\\nAlex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602–610. IJCNN 2005.</p>\n<p>\\\nZhijiang Guo, Yan Zhang, and Wei Lu. 2019. Attention guided graph convolutional networks for relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251, Florence, Italy. Association for Computational Linguistics.</p>\n<p>\\\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th Interna- tional Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4411–4421. PMLR.</p>\n<p>\\\nJungo Kasai, Dan Friedman, Robert Frank, Dragomir Radev, and Owen Rambow. 2019. Syntax-aware neural semantic role labeling with supertags. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 701–709, Minneapolis, Minnesota. Association for Computational Linguistics.</p>\n<p>\\\nDaniel Khashabi, Tushar Khot, Ashish Sabharwal, and Dan Roth. 2018. Question answering as global reasoning over semantic abstractions. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).</p>\n<p>\\\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110– 4124, Online. Association for Computational Linguistics.</p>\n<p>\\\nWitold Kieraś and Marcin Woliński. 2017. Morfeusz 2 – analizator i generator fleksyjny dla języka polskiego. Język Polski, XCVII(1):75–83.</p>\n<p>\\\nMateusz Klimaszewski and Alina Wróblewska. 2021. COMBO: State-of-the-art morphosyntactic analysis. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 50–62, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>\n<p>\\\nRyan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan. Association for Computational Linguistics.</p>\n<p>\\\nInes Montani and Matthew Honnibal. 2022. spaCy: Industrial-Strength Natural Language Processing in Python. Version 3.4.1.</p>\n<p>\\\nRobert Mroczkowski, Piotr Rybak, Alina Wróblewska, and Ireneusz Gawlik. 2021. HerBERT: Efficiently pretrained transformerbased language model for Polish. In Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing, pages 1–10, Kiyv, Ukraine. Association for Computational Linguistics.</p>\n<p>\\\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. 2021a. Trankit: A light-weight transformer-based toolkit for multilingual natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 80–90, Online. Association for Computational Linguistics.</p>\n<p>\\\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. 2021b. Trankit: A light-weight transformer-based toolkit for multilingual natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations.</p>\n<p>\\\nJoakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351–359, Suntec, Singapore. Association for Computational Linguistics.</p>\n<p>\\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>\n<p>\\\nAdrien Pavao, Isabelle Guyon, Anne-Catherine Letournel, Xavier Baró, Hugo Escalante, Sergio Escalera, Tyler Thomas, and Zhen Xu. 2022. Codalab competitions: An open source platform to organize scientific challenges. Technical report, Université Paris-Saclay.</p>\n<p>\\\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020a. AdapterHub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 46–54, Online. Association for Computational Linguistics.</p>\n<p>\\\nJonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020b. MAD-X: An AdapterBased Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.</p>\n<p>\\\nAdam Przepiórkowski, Mirosław Bańko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors. 2012. Narodowy Korpus Języka Polskiego. Wydawnictwo Naukowe PWN, Warsaw.</p>\n<p>\\\nPiotr Przybyła. 2022. LAMBO: Layered Approach to Multi-level BOundary identification.</p>\n<p>\\\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101–108, Online. Association for Computational Linguistics.</p>\n<p>\\\nPiotr Rybak and Alina Wróblewska. 2018. Semisupervised neural system for tagging, parsing and lematization. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 45– 54, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nDevendra Sachan, Yuhao Zhang, Peng Qi, and William L. Hamilton. 2021. Do syntax trees help pre-trained transformers extract information? In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2647– 2661, Online. Association for Computational Linguistics.</p>\n<p>\\\nDjamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie Candito, Jinho D. Choi, Richárd Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepiórkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woliński, Alina Wróblewska, and Eric Villemonte de la Clergerie. 2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146– 182, Seattle, Washington, USA. Association for Computational Linguistics.</p>\n<p>\\\nMilan Straka, Jan Hajič, and Jana Straková. 2016. UDPipe: Trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4290–4297, Portorož, Slovenia. European Language Resources Association (ELRA).</p>\n<p>\\\nMilan Straka and Jana Straková. 2017. Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88–99, Vancouver, Canada. Association for Computational Linguistics.</p>\n<p>\\\nKai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2019. Aspect-level sentiment analysis via convolution over dependency tree. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5679–5688, Hong Kong, China. Association for Computational Linguistics.</p>\n<p>\\\nŁukasz Szałkiewicz and Adam Przepiórkowski. 2012. Anotacja morfoskładniowa. In Adam Przepiórkowski, Mirosław Bańko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors, Narodowy Korpus Języka Polskiego, pages 59– 96. Wydawnictwo Naukowe PWN, Warsaw.</p>\n<p>\\\nShikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, and Partha Talukdar. 2018. RESIDE: Improving distantlysupervised neural relation extraction using side information. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1257–1266, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nYufei Wang, Mark Johnson, Stephen Wan, Yifang Sun, and Wei Wang. 2019. How to best use syntax in semantic role labelling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5338–5343, Florence, Italy. Association for Computational Linguistics.</p>\n<p>\\\nJakub Waszczuk. 2012. Harnessing the crf complexity with domain-specific constraints. the case of morphosyntactic tagging of a highly inflected language. In Proceedings of COLING 2012, pages 2789–2804.</p>\n<p>\\\nJakub Waszczuk, Witold Kieraś, and Marcin Woliński. 2018. Morphosyntactic disambiguation and segmentation for historical polish with graph-based conditional random fields. In International Conference on Text, Speech, and Dialogue, pages 188–196. Springer.</p>\n<p>\\\nMarcin Woliński. 2014. Morfeusz reloaded. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, pages 1106–1111. European Language Resources Association (ELRA).</p>\n<p>\\\nMarcin Woliński. 2019. Automatyczna analiza składnikowa języka polskiego. Wydawnictwa Uniwersytetu Warszawskiego, Warsaw.</p>\n<p>\\\nDaniel Zeman, Jan Hajič, Martin Popel, Martin Potthast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov. 2018. CoNLL 2018 shared task: Multilingual parsing from raw text to Universal Dependencies. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–21, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nDaniel Zeman, Martin Popel, Milan Straka, Jan Hajič, Joakim Nivre, Filip Ginter, Juhani Luotolahti, Sampo Pyysalo, Slav Petrov, Martin Potthast, Francis Tyers, Elena Badmaeva, Memduh Gokirmak, Anna Nedoluzhko, Silvie Cinková, Jan Hajič jr., Jaroslava Hlaváčová, Václava Kettnerová, Zdeňka Urešová, Jenna Kanerva, Stina Ojala, Anna Missilä, Christopher D. Manning, Sebastian Schuster, Siva Reddy, Dima Taji, Nizar Habash, Herman Leung, MarieCatherine de Marneffe, Manuela Sanguinetti, Maria Simi, Hiroshi Kanayama, Valeria de Paiva, Kira Droganova, Héctor Martínez Alonso, Çağrı Çöltekin, Umut Sulubacak, Hans Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg Rehm, Tolga Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde, Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonça, Tatiana Lando, Rattima Nitisaroj, and Josie Li. 2017. CoNLL 2017 shared task: Multilingual parsing from raw text to Universal Dependencies. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, Vancouver, Canada. Association for Computational Linguistics.</p>\n<p>\\\nMeishan Zhang, Zhenghua Li, Guohong Fu, and Min Zhang. 2019. Syntax-enhanced neural machine translation with syntax-aware word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1151–1161, Minneapolis, Minnesota. Association for Computational Linguistics.</p>\n<p>\\\nYuhao Zhang, Peng Qi, and Christopher D. Manning. 2018. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215, Brussels, Belgium. Association for Computational Linguistics.</p>\n<h2 id=\"9languageresourcereferences\">9. Language Resource References</h2>\n<p>Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov. 2019. XLMRoBERTa. Hugging Face.</p>\n<p>\\\nGrave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas. 2018. fastText. Facebook.</p>\n<p>\\\nKłeczek, Dariusz. 2021. Polbert. Hugging Face.</p>\n<p>\\\nLynn, Teresa and Foster, Jennifer and McGuinness, Sarah and Walsh, Abigail and Phelan, Jason and Scannell, Kevin. 2015. Irish Dependency Treebank (UD Irish-IDT). Universal Dependencies Consortium. PID http://hdl.handle.net/11234/1- 4611.</p>\n<p>\\\nMroczkowski, Robert and Rybak, Piotr and Wróblewska, Alina and Gawlik, Ireneusz. 2021. HerBERT. Hugging Face.</p>\n<p>\\\nPrzepiórkowski, Adam and Bańko, Mirosław and Górski, Rafał L. and Lewandowska-Tomaszczyk, Barbara. 2018. National Corpus of Polish. Institute of Computer Science.</p>\n<p>\\\nShen, Mo and McDonald, Ryan and Zeman, Daniel and Qi, Peng. 2019. Chinese Dependency Treebank (UD Chinese-GSD). Universal Dependencies Consortium. PID http://hdl.handle.net/11234/1-4611.</p>\n<p>\\\nWróblewska, Alina. 2018. Polish Dependency Bank (UD Polish-PDB). Universal Dependencies Consortium. PID http://hdl.handle.net/11234/1-5150.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GPT-3 Trips Over Polish Grammar While Classic Tools Hold Their Ground in AI Comparison","url":"https://hackernoon.com/gpt-3-trips-over-polish-grammar-while-classic-tools-hold-their-ground-in-ai-comparison?source=rss","date":1735552815,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 9 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"43results\">4.3. Results</h2>\n<p><strong>Impact of system architecture</strong> We assess the quality of the selected NLPre systems contingent on the NLPre-PL benchmark. In Polish (and most other languages), non-neural NLPre tools are currently not widely developed. We evaluate two of them: Concraft and UDPipe. Although they do not use neural network algorithms to train models, their quality does not significantly differ from the best tested neural systems, especially in terms of segmentation, which UDPipe performs best (Words) or second-best (Sentences) (see Tables 2 and 3). We cannot unequivocally say that the system architecture has a decisive influence on the results, as spaCy models, even transformer-based, output the lowest quality.</p>\n<p>\\\n<strong>Impact of tagset selection</strong> We compare systems trained and tested on data adjusted to two tagsets – the Morfeusz tagset (see Table 2) and the UD tagset (see Table 3). The average scores indicate that only COMBO performs better on Morfeusz-annotated data than on UD data. The performance of Trankit, UDPipe, and Stanza slightly decreases on Morfeusz data. Notably, all spaCy models trained on this dataset record a significant quality drop mainly due to poorly performed morphological analysis, i.e. UFeats values (and thus also the low AllTags values, i.e., matching between UPOS, XPOS, and UFeats). Regarding segmentation, UPOS and XPOS tagging, and lemmatisation, the tagset selection does not negatively affect the results, and the systems perform comparably.</p>\n<p>\\\n<strong>Impact of the size of training data</strong> Intuitively, the size of the training data affects the prediction quality. Considering the data size factor, we compare the average F1 scores of the NLPre systems trained on NKJP1M (see the last row in Table 4) and on PDB-UD (see Table 4), which is two orders of magnitude smaller. The results confirm our intuitive assumptions – there is a difference of 6.21 between the mean F1 scores obtained by the systems trained on the smaller PDB-UD (avg. F1 of 88.16) and those trained on the larger NKJP1M (avg. F1 of 94.37).</p>\n<p>\\\nWhen comparing the performance of individual systems on the smaller PDB-UD dataset, Trankit turns out to be the undisputed winner in all tasks except lemmatisation. However, considering the average performance of all tasks, COMBO and Stanza perform the best.</p>\n<p>\\\nIn alignment with contemporary developments on zero-shot learning, we test the predictive capabilities of GPT-3.5 acquired via the prompting technique (Brown et al., 2020). Despite comprehensive instructions along with the UD tree examples in the prompt, the results are highly unsatisfactory. An error analysis has revealed that 1) the GPT model modifies the input texts (e.g. adds elided words, alters the word’s declension and conjugation, leading also to non-existent words); 2) while parsing questions, it answers them or returns information that they cannot be answered; 3) it replaces Polish words with their foreign equivalents; 4) it outputs graphs with cycles, thus not adhering to UD trees. Even for GPTs, achieving UD-compliant morphosyntactic analysis is challenging when they lack access to training examples. GPT-3.5’s results are not included in the leaderboard.</p>\n<p>\\\n<strong>Impact of split heuristics</strong> As outlined in Section 3.1, NKJP1M has no official split into train, dev, and test subsets. Since intuitively, the type of document can affect text processing, we propose two alternative splits, i.e. byName and byType. We compare the F1 scores for these two splits to verify this hypothesis. For the byName split, the average F1 for tasks and systems is 90.69, and for the byType split, it is 90.56. The difference is negligible, indicating that the document type, and hence the text domain, does not affect the quality of the NLPre tasks. Based on this outcome, we arbitrarily choose the more balanced byType split as binding in the final NLPre-PL benchmarking system. The detailed results of all experiments are in Appendix 6.2.</p>\n<p>\\\n<strong>Inference time</strong> In the context of benchmarking, quality is a fundamental factor. In our case, the best average F1 scores are achieved by COMBO and Stanza, far ahead of spaCy and Concraft. The second crucial issue is the processing time of the evaluated NLPre systems, especially their inference time.[20] We calculate the times in which the systems tokenise, tag and lemmatise the input text.[21] The exception is COMBO with the mandatory parsing module that cannot be disabled. Therefore, its calculations include the parsing time as well. The inference time, corresponding to the number of tokens processed per second, is provided in the last two columns of Tables 2 and 3. On CPU, the fastest systems are spaCy and UDPipe, and the slowest is Concraft. Other systems process one order of magnitude fewer tokens per second than the top ones. On GPU, spaCy is the undisputed winner, followed by Stanza, UDPipe, COMBO and Trankit.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-gt8308f.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-e19304m.png\" alt=\"Figure 2: Pearson correlation coefficients between vectors of F1 scores on Tokens, Sentences, Words, UPOS, XPOS, Lemmas tasks averaged over datasets (excluding PDB-UD) and embeddings.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-soa30or.png\" alt=\"Figure 3: Pearson correlation coefficients between vectors of F1 scores on Tokens, Sentences, Words, UPOS, XPOS, Lemmas tasks averaged over datasets (excluding PDB-UD).\" /></p>\n<p>\\\nPearson’s correlation r suggests that the results are linearly proportional for the same models and different tagsets, which we conclude from the values close to 1 at the intersection of (model<em>i</em>, tagsetud) and (model<em>i</em>,  tagsetnkjp). Even though correlation coefficients are generally high (i.e. r ∈ [0.90, 0.99]) for most pairs (model<em>i</em>,  tagsetud) and (model<em>j,</em> tagsetnkjp), there are noticeable lower values for spaCy, i.e. r ∈ [0.66, 0.78]. We hypothesise that this is due to the non-linear rate of changes between the scores, as all Spearman correlation coefficients exceed 0.89 (i.e. ρ &gt; 0.89).</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-brb303i.png\" alt=\"Figure 4: Dispersion of model performance measured by F1 on the Morfeusz tagset and Sentences, Words, UPOS, XPOS, and Lemmas tasks.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-uic30ll.png\" alt=\"Figure 5: Dispersion of model performance measured by F1 on the UD tagset and Sentences, Words, UPOS, XPOS, and Lemmas tasks.\" /></p>\n<p>\\\nThe results of a more granular analysis of Pearson’s r between vectors of F1 scores for triples (tagseti , modelj , embeddingsk), averaged over datasets, show a strong correlation for the same models, regardless of the tagset and the embedding (see Figure 3). Hence, if a change in the tagset or embedding causes an increase in one task, a proportional increase in remaining tasks is expected.</p>\n<p>\\\nBoxplot charts (see Figures 4 and 5) determine the stability of the model results for a given tagset regardless of dataset and embedding. One box shows the scattering of F1 scores for Tokens, Sentences, Words, UPOS, XPOS, and Lemmas tasks. The shortest COMBO’s box indicates a relatively similar performance of the model across tasks for each triplet (COMBO, embeddingj , datasetk).</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[20] We share a conviction favoured in the NLP community that the training time is slightly less requisite than the inference time since models are trained only once but then constantly reused for predictions. We thus provide inference times.</p>\n<p>\\\n[21] We run tests uniformly on CPU – Intel Xeon Platinum 8268 processor (1 node with 12 cores), and GPU – 2x Tesla V100-SXM2. The machines used to train the models are listed in Appendix 6.1.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Pit GPT-3.5 Against Classic Language Tools in Polish Text Analysis","url":"https://hackernoon.com/researchers-pit-gpt-35-against-classic-language-tools-in-polish-text-analysis?source=rss","date":1735551912,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 8 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"42evaluatedsystems\">4.2. Evaluated systems</h2>\n<p>Based on the NLPre-PL benchmark, we evaluate both well-rooted rule-based disambiguation methods and modern systems based on neural network architectures to enable an informative and thorough comparison of different approaches. We use the most up-to-date versions of available tools at the time of conducting experiments: (1) pipelines of separate tools (Concraft-pl, UDPipe), (2) systems integrating separate models for NLPre tasks (spaCy, Stanza, Trankit), (3) end-to-end systems with a model for all NLPre tasks (COMBO), and large language model GPT-3.5.</p>\n<p>\\\n<strong>Concraft-pl</strong> (Waszczuk, 2012; Waszczuk et al., 2018) [12] is a system for joint morphosyntactic disambiguation and segmentation.[13] It uses Morfeusz morphological analyser (Woliński, 2014; Kieraś and Woliński, 2017) to extract morphological and segmentation equivocates and then disambiguates them using the conditional random fields model. We train the Concraft-pl models with default parameters.</p>\n<p>\\\n<strong>UDPipe</strong> (Straka and Straková, 2017) is a language-agnostic trainable NLPre pipeline.[14] Depending on the task, it uses recurrent neural networks (Graves and Schmidhuber, 2005) in segmentation and tokenization, the average perceptron in tagging and lemmatization, a rule-based approach in multi-word splitting, and a transition-based neural dependency parser. We train the UDPipe models with the default parameters. The dependency parser is trained with the Polish fastText embeddings (Grave et al., 2018).</p>\n<p>\\\n<strong>SpaCy</strong> (Montani and Honnibal, 2022) is an NLP Python library shipped with pretrained pipelines and word vectors for multiple languages.[15] It also supports training the models for tagging and parsing, inter alia. We use spaCy to train pipelines for morphosyntactic analysis with: feed-forward network</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-hv830lq.png\" alt=\"Table 2: Results (F1 scores) and inference time (the number of tokens processed per second) of benchmarking the selected NLPre systems on the Morfeusz tagset averaged by the datasets (byName and byType). The systems are grouped into non-neural and neural by a double horizontal line. Embeddings used in the models are: R – xlmRoBERTa-base, fT – fastText, P – Polbert, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-iy930s3.png\" alt=\"Table 3: Results (F1 scores) and inference time (tokens per second) of benchmarking the selected NLPre systems on the UD tagset averaged by the datasets (byName, byType, and PDB-UD). The systems are grouped into non-neural and neural by a double horizontal line (Concraft is not included because it does not allow data in the UD tagset) Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-4ra30f1.png\" alt=\"Table 4: Results of benchmarking the selected NLPre systems on the smaller PDB-UD dataset. The last row with the mean F1 scores of the models trained on larger NKJP1M data is for reference. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert, pl – pl-core-news-lg, H – HerBERT. The results of GPT-3.5 are greyed out due to their exclusion from display on the leaderboard.\" /></p>\n<p>\\\nbased text encoders with static embeddings (fastText and pl-core-news-lg) or transformer-based encoders with the Polbert embeddings (Kłeczek, 2021), taggers (linear layers with softmax activation on top of the encoders), and transition-based parsers.</p>\n<p>\\\n<strong>Stanza</strong> (Qi et al., 2020) is a language-agnostic, fully neural toolkit offering a modular pipeline for tokenization, multi-word token expansion, lemmatization, tagging, and dependency parsing.[16] It mainly uses recurrent neural networks (Graves and Schmidhuber, 2005) as a base architecture and external word embeddings (fastText). Each module reuses the basic architecture.</p>\n<p>\\\n<strong>Trankit</strong> (Nguyen et al., 2021b) uses a multilingual pre-trained transformer-based language model, XLM-Roberta (Conneau et al., 2019) as the text encoder which is then shared across pipelines for different languages.[17] The resulting model is jointly trained on 90 UD treebanks with a separate adapter (Pfeiffer et al., 2020a,b) for each treebank. Trankit uses a wordpiece-based splitter to exploit contextual information.</p>\n<p>\\\n<strong>COMBO</strong> (Rybak and Wróblewska, 2018; Klimaszewski and Wróblewska, 2021) is a fully neural language-independent NLPre system[18] integrated with the LAMBO tokeniser (Przybyła, 2022). It is an end-to-end system with jointly trained modules for tagging, parsing, and lemmatisation. We train the COMBO models with the pre-trained word embeddings – fastText and HerBERT (Mroczkowski et al., 2021).</p>\n<p>\\\n<strong>GPT-3.5</strong> (Brown et al., 2020) is a large language model, notable for its outstanding performance in NLU tasks. It is a fined-tuned version of the GPT-3 model. GPT-3.5’s architecture is based on a transformer neural network with 12 stacks of decoders blocks with multi-head attention blocks.</p>\n<p>\\\nFor segmentation tasks, we train modules integrated with the tested NLPre systems. The only aberration is in spaCy, where poor segmentation results of the dependency module[19] forced us to use an out-of-the-box sentenciser available in spaCy.</p>\n<p>\\\nFor each model, we initialise training with possibly the most prominent and congruent embedding model available. Virtually all models are capable of fully capitalising from that addition, apart from Concraft and UDPipe. The first does not use embeddings at all, and the latter uses them only for dependency parsing training. If embeddings based on BERT architecture are feasible to use, we select their base versions. This ensures fairness of comparison between NLPre systems, as not all of them support BERT-large embeddings.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[12] Polish is a fusional language for which a two-stage tagging procedure is typically applied: first, a rule-based morphological analyser outputs all morphological interpretations of individual tokens, and then a tagging disambiguator selects the most likely one for each token. The tools implemented in accordance with this procedure are still imminent.</p>\n<p>\\\n[13] https://github.com/kawu/concraft-pl (v2.0)</p>\n<p>\\\n[14] https://ufal.mff.cuni.cz/udpipe (v1)</p>\n<p>\\\n[15] https://github.com/explosion/spaCy (v3.4.1)</p>\n<p>\\\n[16] https://github.com/stanfordnlp/stanza (v1.4.0)</p>\n<p>\\\n[17] https://github.com/nlp-uoregon/trankit (v1.1.1)</p>\n<p>\\\n[18] https://gitlab.clarin-pl.eu/syntactic-tools/combo (v1.0.5)</p>\n<p>\\\n[19] Dependency parsing module is responsible for sentence segmentation in the spaCy implementation.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Learn to Measure AI’s Language Skills","url":"https://hackernoon.com/researchers-learn-to-measure-ais-language-skills?source=rss","date":1735551014,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 7 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"4evaluation\">4. Evaluation</h2>\n<h3 id=\"41evaluationmethodology\">4.1. Evaluation methodology</h3>\n<p>To maintain the de facto standard to NLPre evaluation, we apply the evaluation measures defined for the CoNLL 2018 shared task and implemented in the official evaluation script.[11] In particular, we focus on F1 and <em>AlignedAccuracy</em>, which is similar to F1 but does not consider possible misalignments in tokens, words, or sentences.</p>\n<p>\\\nIn our evaluation process, we follow default training procedures suggested by the authors of the evaluated systems, i.e. we do not conduct any optimal hyperparameter search in favour of leaving the recommended model configuration as-is. We also do not further fine-tune selected models.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[11] https://universaldependencies.org/conll18/conll18<em>ud</em>eval.py</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How Microsoft Made 2024 the Year of Windows on Arm","url":"https://tech.slashdot.org/story/24/12/30/0529253/how-microsoft-made-2024-the-year-of-windows-on-arm?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735550520,"author":"EditorDavid","unread":true,"content":"\"I still can't quite believe that I'm using an Arm-powered Windows laptop every day,\" writes a senior editor at the Verge:\n\nAfter more than a decade of trying to make Windows on Arm a reality, Microsoft and Qualcomm finally nailed it this year with Copilot Plus PCs. These new laptops have excellent battery life and great performance &mdash; and the app compatibility issues that have plagued Windows on Arm are mostly a thing of the past (as long as you're not a gamer). Microsoft wanted 2024 to be \"the year of the AI PC,\" but I think it was very much the year of Windows on Arm... \n\nThe key to Windows on Arm's revival this year was Qualcomm's Snapdragon X Elite processors, which were announced in April. They've provided the type of performance and power efficiency only previously available with Apple's MacBooks and challenged Intel and AMD to do better in the x86 space. After much debate over Microsoft's MacBook Air-beating benchmarks, the reviews rolled in and showed that Windows on Arm was indeed capable of matching and beating Apple's MacBook Air. Qualcomm even hired the \"I'm a Mac\" guy to promote Windows on Arm PCs, showing how confident it was in challenging Apple's laptop dominance. \n\nMicrosoft and Qualcomm also worked closely with developers to make key apps compatible, and it's now very rare to run into an app compatibility issue that can't be solved by a native Arm64 version or Microsoft's improved emulator. Even Google, which previously shunned Windows Phone, has created Arm64 versions of Chrome and Google Drive to support Microsoft's efforts. With developers continually providing native versions of their apps, it makes it a lot easier to switch to a Windows on Arm laptop. The only big exception is gaming, where x86 still reigns supreme for compatibility and performance... \n\nIt's hard not to see 2025 as the year that Windows on Arm continues to eat into the laptop space. A Dell leak revealed Qualcomm is preparing new chips for 2025, and the chip maker has also been rolling out cheaper Arm-based chips to bring laptop prices down. \nThe article acknowledges that both AMD and Intel \"have the key advantage of game compatibility that Windows on Arm is definitely not ready for...\" But \"Given the Windows on Arm gaming situation, a new generation of Nvidia's GPUs could help generate fresh excitement around x86 laptops throughout 2025.\" And \"Nvidia might also be planning to help the Windows on Arm effort. The chip maker has long been rumored to be planning to launch Arm PC chips as soon as 2025... Whatever happens to laptops in 2025, you can guarantee that there's going to be fierce competition between Intel, AMD, and Qualcomm.\" \n\nBut the author still complains about the dedicated Copilot key on his new WIndows-on-Arm laptop. \"While the Copilot experience on Windows has gone through several confusing revisions, it's still a key I accidentally press and then get frustrated when a Copilot window appears.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=How+Microsoft+Made+2024+the+Year+of+Windows+on+Arm%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F0529253%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F0529253%2Fhow-microsoft-made-2024-the-year-of-windows-on-arm%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/30/0529253/how-microsoft-made-2024-the-year-of-windows-on-arm?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564481&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Challenge AI to Tackle the Toughest Parts of Language Processing","url":"https://hackernoon.com/researchers-challenge-ai-to-tackle-the-toughest-parts-of-language-processing?source=rss","date":1735550112,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 6 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"32tasks\">3.2. Tasks</h2>\n<p>The complete set of NLPre tasks was originally curated for evaluating language systems in the CoNLL shared task 2018 (Zeman et al., 2018). These tasks mainly focus on preliminary text processing, such as tokenisation or divulging morphosyntactic features. We follow the CoNLL task choice and include all these tasks in NLPre-PL.</p>\n<p>\\\n<strong>Segmentation</strong> A segmentation task consists in splitting texts into sentences (Sentences), orthographic tokens (Tokens), and syntactic words (Words), the latter being the basic units of morphosyntactic analysis. Segmentation is not a trivial task. In some languages, an orthographic token may be recognised as a multi-word token (multiword for short) combining multiple syntactic words, e.g. in Polish, the token spalibyśmy (Eng. we would sleep) consists of the past participle spali (Eng. slept), the conditional marker by (Eng. would) and the mobile inflection śmy. Since the consistent model of segmentation into words and sentences was used in NKJP1M and PDB-UD, we maintain this data segmentation in NLPre-PL. It is also worth mentioning that the CoNLL format (but not TEI and DAG) allows for annotating orthographic tokens; thus, they are included in the NLPre-PL benchmark.</p>\n<p>\\\n<strong>Tagging</strong> A tagging task is the process of identifying parts of speech (i.e. POS tagging) and possibly morphological features (i.e. morphological analysis) of words. It follows a predefined POS tagset. As mentioned in Section 3.1, two tagsets are used in the NLPre-PL datasets: Morfeusz and UD.</p>\n<p>\\\n<strong>Lemmatisation</strong> Lemmatisation involves predicting canonical forms of syntactic words. Canonical forms are conventionally established identifiers of lexemes (i.e. sets of inflectionally related syntactic words). Since Polish is a fusional language with a large number of inflected words, lemmatisation is an important task, albeit not trivial, e.g. the lemma of kluczy can be either the infinitive kluczyć (Eng. to weave) or the noun klucz (Eng. a key).</p>\n<p>\\\n<strong>Dependency</strong> parsing Dependency parsing is the process of automatically predicting the syntactic structure of an input sentence. A dependency structure is a labelled directed tree with nodes corresponding to syntactic words and edges between these words specifying dependency relations.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Framework Promises to Train AI to Better Understand Hard-to-Grasp Languages Like Polish","url":"https://hackernoon.com/new-framework-promises-to-train-ai-to-better-understand-hard-to-grasp-languages-like-polish?source=rss","date":1735549211,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 5 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"3nlpreplbenchmark\">3. NLPre-PL benchmark</h2>\n<h3 id=\"31datasets\">3.1. Datasets</h3>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ss8303t.png\" alt=\"Table 1: Summary of source datasets (NKJP1M and PDB-UD) and NLPre-PL Datasets (in tokens). Explanations: POS – the part-of-speech tagset; DEP – the dependency schema; Avg. t/s – the average number of tokens per sentence\" /></p>\n<p>\\\n<strong>NKJP1M</strong> (Przepiórkowski et al., 2018) The NKJP1M subcorpus of the Polish National Corpus (Przepiórkowski et al., 2012) is manually annotated according to the NKJP tagset (Szałkiewicz and Przepiórkowski, 2012) and afterwards modified in line with the Morfeusz tagset (Woliński, 2019). This balanced subset of thematic- and genre-diverse texts and transcriptions is used to train Polish POS taggers. NKJP1M is maintained in two formats: TEI[8] and DAG.[9] These two formats are accepted by older NLPre tools but not modern ones. We thus convert NKJP1M to the CoNLL-X format (Buchholz and Marsi, 2006) preserving the original segmentation, POS tags and morphological features (i.e. the Morfeusz tagset), and to the CoNLL-U format10 with UD tags, Morfeusz tags (XPOS) and UD morphological features.</p>\n<p>\\\nSince there is no generally accepted split of NKJP1M into training, development and testing subsets, we uniformly divide NKJP1M in all formats (i.e. DAG, TEI, CoNLL-X and CoNLL-U) pursuant to the formulated splitting heuristics. Each document in the subcorpus contains multiple paragraphs of continuous textual data. To avoid possible information leakage, we treat each such paragraph as an indivisible unit. To ensure that the subsets include paragraphs of varying length, we investigate the distribution over the number of segments in each paragraph. Since it is akin to Gaussian distribution, we decide to not exclude any data, and we divide the paragraphs into K = 10 buckets of roughly similar size and then sample from them with respective ratios of 0.8:0.1:0.1 (corresponding to train, dev, and test subsets). This data selection technique assures similar distribution of segments number per paragraph in three subsets, hereafter byName. For creating our second split, hereafter byType, we consider the type of document a paragraph belongs to. We first group paragraphs into categories equal to the document types, and then we repeat the above-mentioned procedure per category (see the summary of NKJP1M and data splits in Table 1). <strong>PDB-UD</strong> (Wróblewska, 2018) Polish Dependency Bank is the largest collection of Polish sentences manually annotated with dependency trees and afterwards converted into UD representations in line with the UD annotation schema (de Marneffe et al., 2021). PDB-UD slightly correlates with NKJP1M, i.e., a subset of the PDB-UD sentences comes from NKJP1M, and the language-specific tags (XPOS) in PDB-UD match the Morfeusz tagset. PDB-UD is typically used to train NLPre systems for Polish. In NLPre-PL, we use the original PDB-UD data without any modifications and its standard split (see the statistical summary of PDB-UD in Table 1).</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[8] http://nlp.ipipan.waw.pl/TEI4NKJP.</p>\n<p>\\\n[9] https://github.com/kawu/concraft-pl#data-format</p>\n<p>\\\n[10] https://universaldependencies.org/format.html</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Create Plug-and-Play System to Test Language AI Across the Globe","url":"https://hackernoon.com/researchers-create-plug-and-play-system-to-test-language-ai-across-the-globe?source=rss","date":1735548311,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 4 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"23configuration\">2.3. Configuration</h2>\n<p>We acknowledge the need to configure similar evaluation environments for other languages to promote linguistic diversity within the worldwide NLP community and to support local NLP communities working on a particular language. To ensure that, we publish a .yaml file that enables easy management of datasets, tagset, and metrics included in the benchmark. The content of all subpages can be modified using a WYSIWYG editor within the application. This setting ensures quite a low entry level for setting up the platform, with minimal changes required.</p>\n<p>\\\nAs a standard feature, we include pre-defined descriptions for the prevalent NLPre tasks. Those can be modified via either configuration files or the administrator panel. Additionally, we supply a default evaluation script, but users are free to provide their own customised code.</p>\n<p>\\\nTo show the capabilities of the benchmarking system, we set up a prototype for Polish (Figure 1). NLPre-PL is described in detail in Section 3. To support our claim that the system is language agnostic, we set up NLPre-GA for Irish and NLPreZH for Chinese. The choice of those languages is not arbitrary; our objective is to demonstrate the capability of the platform in evaluating diverse languages, including those based on non-Latin scripts. In setting up said benchmarking systems we use existing UDv2.9 treebanks: UD<em>Chinese-GSD (Shen et al., 2019) and UD</em>Irish-IDT (Lynn et al., 2015) and available up-to-date models, trained on these treebanks. The selection of models mirrors the criteria applied in this work regarding the evaluation of Polish, that is: COMBO, Stanza, SpaCy, UDPipe, and Trankit. If the specific model is not available for UDv2.9, we train it from scratch on the datasets linked above.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Web App Lets Researchers Test and Rank Language AI Tools in Real Time","url":"https://hackernoon.com/new-web-app-lets-researchers-test-and-rank-language-ai-tools-in-real-time?source=rss","date":1735547411,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 3 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"22onlinebenchmarkingsystem\">2.2. Online benchmarking system</h2>\n<p>The benchmarking system comprises three main parts: a data repository, a submission and evaluation system, and a leaderboard. The data repository provides descriptions of NLPre tasks, datasets, and evaluation metrics, as well as links to the datasets.</p>\n<p>\\\nThe model submission and evaluation system allows the researchers to evaluate a new model by submitting its predictions for the test sets of raw sentences. It is mandatory to upload predictions for all provided test sets for a given tagset; however, it is possible to participate in an evaluation for only one tagset and only for a selected range of tasks.</p>\n<p>\\\nThe leaderboard is a tabular display of the performance of all submissions with their results for each dataset and tagset. The results for the evaluated model and its rank are displayed in the leaderboard provided the submitter confirms their publication.</p>\n<p>\\\nThe benchmarking system is implemented as a web-based application in Python using Django framework. This framework allows quite an easy implementation of MVC design pattern. Moreover, it offers access to the administrator panel, which can be very useful in the custom configuration of the benchmark. The submission scores are stored in a local SQLite database and the submissions are stored in .zip files in a designated directory. The results from the leaderboard are conveniently accessible via an API.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Build Public Leaderboard for Language Processing Tools","url":"https://hackernoon.com/researchers-build-public-leaderboard-for-language-processing-tools?source=rss","date":1735546515,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 2 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"2nlprebenchmarking\">2. NLPre benchmarking</h2>\n<h3 id=\"21researchconcept\">2.1. Research concept</h3>\n<p>In this study, we introduce a novel adaptation of the benchmarking approach to NLPre. The primary objective is to establish an automated and credible method for evaluating NLPre systems against a provided benchmark and continuously updating their performance ranking on a publicly accessible scoreboard. More specifically, predictions for the benchmark test sets output by NLPre systems and 5 https://nlpre-pl.clarin-pl.eu 6 https://nlpre-zh.clarin-pl.eu 7 https://nlpre-ga.clarin-pl.eu submitted to the benchmarking system are automatically compared against the publicly undisclosed reference dataset. This method effectively prevents result manipulation and ensures fairness of the final assessment. The second important methodological assumption is to enable the ongoing evaluation of new or upgraded NLPre systems to guarantee up-to-date and complete ranking. Consequently, the leaderboard can serve as a reliable point of reference for NLPre system developers.</p>\n<p>\\\nBased on these assumptions, we design and implement the language-centric and tagset-agnostic benchmarking system that enables comprehensive and credible evaluation, constitutes an up-to-date source of information on NLPre progress, and is fully configurable to facilitate building benchmarking systems for multiple languages.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Framework Simplifies Comparison of Language Processing Tools Across Multiple Languages","url":"https://hackernoon.com/new-framework-simplifies-comparison-of-language-processing-tools-across-multiple-languages?source=rss","date":1735545611,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 1 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"abstract\">Abstract</h2>\n<p>With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an extensive evaluation of a variety of Polish NLPre systems. To facilitate the construction of benchmarking environments for other languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full customization of the publicly released source code of the benchmarking system. The links to all the resources (deployed platforms, source code, trained models, datasets etc.) can be found on the project website: https://sites.google.com/view/nlpre-benchmark.</p>\n<p>\\\n<strong>Keywords</strong>: benchmarking, leaderboard, segmentation, POS tagging, dependency parsing, Polish</p>\n<h2 id=\"1introductionandrelatedworks\">1. Introduction and related works</h2>\n<p>Morphosyntactic features predicted by part-ofspeech (POS) taggers and dependency parsers underlie various downstream tasks, including but not limited to sentiment analysis (Sun et al., 2019), relation extraction (Zhang et al., 2018; Vashishth et al., 2018; Guo et al., 2019), semantic role labelling (Wang et al., 2019; Kasai et al., 2019), question answering (Khashabi et al., 2018), or machine translation (Chen et al., 2017; Zhang et al., 2019). These underlying tasks may therefore be referred to as natural language preprocessing (NLPre) tasks, as they precede the advanced NLP tasks. Since the quality of morphosyntactic predictions has a crucial impact on the performance of downstream tasks (Sachan et al., 2021), it is prudent to employ the best existing NLPre tools to predict the proper linguistic features. We are equipped with various NLPre methods, ranging from rule-based tools with hand-crafted grammars (e.g. Crouch et al., 2011), through statistical systems (e.g. Nivre, 2009; McDonald et al., 2005; Straka et al., 2016), neural systems supported by pre-trained language models (e.g. Qi et al., 2020; Nguyen et al., 2021a) to large language models (LLM Ouyang et al., 2022).</p>\n<p>\\\nIn the context of intrinsically evaluating NLPre tools and reporting their performance, a variety of approaches have been proposed, e.g. shared task, performance table, and progress repository. The main goal of a shared task is to comprehensively evaluate participating systems on the released datasets using the carefully defined evaluation methodology. Numerous NLPre shared tasks have been organised so far (e.g. Buchholz and Marsi, 2006; Seddah et al., 2013; Zeman et al., 2017, 2018), and they undoubtedly boosted the development of NLPre. While widely favoured, shared tasks are questionable as a complete and up-todate source of knowledge about NLPre progress. First, they scrutinise only solutions propounded in the current contest and do not include systems participating in the previous editions or possible future ones. Second, as shared tasks are organised sporadically, their results are not revised and may quickly become outdated. Certainly, the datasets released for shared tasks can be reused in experiments involving novel tools. The results of such experiments can be reported in independent scientific publications. Nonetheless, these publications are widely scattered, lacking a centralised platform for systematically tracking the ongoing NLPre progress with respect to a particular language.</p>\n<p>\\\nThe results of a new or upgraded NLPre tool are typically reported in performance tables (e.g. Stanza[1] or Trankit[2]). Such tables provide information about the quality of the tool in preprocessing a set of languages. The performance tables, however, often lack comparison with other systems trained for these particular languages. Additionally, as NL Pre systems may be trained on different dataset releases (e.g. of Universal Dependencies), comparing their performance tables is not conclusive.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-hx830qd.png\" alt=\"Figure 1: Screenshot of the NLPre-PL leaderboard.\" /></p>\n<p>\\\nInformation about trends and progress in NLP research is usually collected in public repositories such as Papers with Code[3] or NLP-progress[4]. These repositories contain a repertoire of datasets for common NLP tasks, e.g. dependency parsing and POS tagging, and rankings of models trained and tested on these datasets. They are open to contributing new datasets and results, which, to ensure their credibility, originate from published and linked scientific papers. However, cutting-edge yet unpublished results of a new or upgraded NLPre system are not eligible to report. NLPre tasks are accompanied by datasets mostly in English, raising the problem of language unrepresentation of the repositories. Last but not least, the Papers with Code repository is prone to abuse. After logging in, one can add new results and link them with irrelevant papers as well as edit existing results. The fraudulent results are publicised immediately.</p>\n<p>\\\nDespite yielding valuable information about the progress in NLPre, the mentioned evaluation approaches also reveal shortcomings, e.g. outdated and incomplete outcomes, lack of cross-system comparison, disregarding some systems, risk of result manipulation and absence of a language-centring perspective.</p>\n<p>\\\nFollowing standard procedures in NLP research, we propose to robustly and fairly evaluate NLPre tools using the benchmarking method that allows for the evaluation of NLP models’ performance and progress. NLP benchmarks are coupled with leaderboards that report and update model performance on the benchmark tasks, e.g. GLUE (Wang et al., 2018), XTREME (Hu et al., 2020), GEM (Gehrmann et al., 2021). The conventional benchmarking approach may be dynamically enhanced, exemplified by the Dynabench platform (Kiela et al., 2021), which enables users to augment the benchmark data by inputting custom examples. This humanand-model-in-the-loop benchmarking scenario appears promising for NLU tasks. Nevertheless, it may not be effective in the case of NLPre, as annotating credible examples of syntactic trees or morphological features requires expert knowledge. Finding multiple experts among casual users can be a serious obstacle, we thus implement our system in tune with the standard benchmarking method.</p>\n<p>\\\nTo our knowledge, benchmarking hasn’t been used to rank NLPre systems, even if it is valuable and desired by the community creating treebanks or designing advanced NLP pipelines. Our NLPre benchmarking approach fills this gap. The proposed online benchmarking system automatically assesses submitted predictions of NLPre systems and publishes their performance ranking on a public scoreboard (see Section 2.2). The system is language-centric and tagset-agnostic, enables comprehensive and credible evaluation and constitutes an up-to-date source of information on NLPre progress for a particular language. Unlike similar platforms, e.g. Codalab (Pavao et al., 2022), the NLPre benchmarking system is fully configurable and easy to set up, allowing users to establish an evaluation environment for any language. Additionally, it can be self-hosted, making it convenient for developers and researchers working with a particular language to have it accessible on a local server.</p>\n<p>\\\nTo justify the use of the benchmarking technique for NLPre tasks, we conduct empirical research in a challenging scenario with Polish as an example language. In the case of Polish, one dominant hurdle arises – the discrepancies between different tagsets, annotation schemes and datasets utilised for training disparate systems preclude their direct comparison. We thus standardise the training and evaluation of NLPre systems on a new performance benchmark for Polish, hereafter NLPre-PL (see Section 3). It consists of a predefined set of NLPre tasks and reformulated versions of existing Polish datasets. Section 4 outlines our robust and reliable evaluation of the selected NLPre systems on the NLPre-PL benchmark. According to our knowledge, no evaluation experiments have been carried out in Polish to compare the performance of off-the-shelf LLMs, neural NLPre systems and established tagging disambiguators due to the lack of a coherent evaluation environment.</p>\n<p>\\\nThis work makes a tripartite contribution encompassing novelty, research, and development underpinned by an open-source ethos. (1) We propose a novel language-oriented benchmarking approach to evaluate and rank NLPre systems. (2) We conduct a scientific evaluation of the proposed approach in the non-trivial Polish language scenario on the assembled NLPre-PL benchmark. (3) We publish online benchmarking platforms for three distinct languages: Polish[5], Chinese[6], and Irish[7], and release the benchmarking system’s source code as open-source.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[1] https://stanfordnlp.github.io/stanza/performance.html (UD v2.8)</p>\n<p>\\\n[2] https://trankit.readthedocs.io/en/latest/performance. html#universal-dependencies-v2-5 (UD v2.5)</p>\n<p>\\\n[3] https://paperswithcode.com</p>\n<p>\\\n[4] http://nlpprogress.com</p>\n<p>\\\n[5] https://nlpre-pl.clarin-pl.eu</p>\n<p>\\\n[6] https://nlpre-zh.clarin-pl.eu</p>\n<p>\\\n[7] https://nlpre-ga.clarin-pl.eu</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The TechBeat: Future-proof Your Marketing With This Guide on Writing for AI Search Engines (12/30/2024)","url":"https://hackernoon.com/12-30-2024-techbeat?source=rss","date":1735542655,"author":"Techbeat","unread":true,"content":"<p>How are you, hacker? \n 🪐<strong>Want to know what's trending right now?:</strong>\n <a href=\"https://hackernoon.com/homepage-has-a-new-baby\">The Techbeat by HackerNoon </a> has got you covered with fresh content from our trending stories of the day! Set email preference <a href=\"https://app.hackernoon.com/profile/email-settings\">here</a>.\n ## <strong><a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\">Future-proof Your Marketing With This Guide on Writing for AI Search Engines</a></strong> <img src=\"https://cdn.hackernoon.com/images/R40xrKHcy9QXU6NDkd58YY2mQOz1-e313j4y.webp\" alt=\"\" />\n By <a href=\"https://hackernoon.com/u/darragh\">@darragh</a> [ 4 Min read ] \n Learn how to write for AI &amp; search engines with actionable tips, examples, &amp; FAQs. Future-proof your content for ChatGPT, Gemini, &amp; Google SEO success! <a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\">Read More.</a></p>\n<h2 id=\"thesneakywaywebbrowsersareidentifyingyouevenwhenyouturnoffcookieshttpshackernooncomthesneakywaywebbrowsersareidentifyingyouevenwhenyouturnoffcookieshttpscdnhackernooncomimageslvbwxpoo4wxh8mnqfbfkapmqkai21d0370tpng\"><strong><a href=\"https://hackernoon.com/the-sneaky-way-web-browsers-are-identifying-you-even-when-you-turn-off-cookies\">The Sneaky Way Web Browsers Are Identifying You (Even When You Turn Off Cookies)</a></strong> <img src=\"https://cdn.hackernoon.com/images/lvbwxpoO4WXH8MNqfBFKapMQkAi2-1d0370t.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/rampageproxies\">@rampageproxies</a> [ 12 Min read ] \n A guide on browser fingerprinting, how it identifies us, fingerprint testing, and what techniques we can use to ensure our browsing is kept anonymous. <a href=\"https://hackernoon.com/the-sneaky-way-web-browsers-are-identifying-you-even-when-you-turn-off-cookies\">Read More.</a></p>\n<h2 id=\"howimaded700amonthwithmyopensourceschedulingtoolhttpshackernooncomhowimaded700amonthwithmyopensourceschedulingtoolhttpscdnhackernooncomimagesfyq3lkdn20xy6a5ptptozsotd5k2ve03427png\"><strong><a href=\"https://hackernoon.com/how-i-made-$700-a-month-with-my-open-source-scheduling-tool\">How I Made $700 a Month With My Open-source Scheduling Tool</a></strong> <img src=\"https://cdn.hackernoon.com/images/Fyq3lKdN20Xy6A5PTptozSOTd5K2-ve03427.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/nevodavid10\">@nevodavid10</a> [ 3 Min read ] \n I built a social media scheduling tool (many exist in the market) and created an open-source version. <a href=\"https://hackernoon.com/how-i-made-$700-a-month-with-my-open-source-scheduling-tool\">Read More.</a></p>\n<h2 id=\"canusdtsurviveeuregulationshttpshackernooncomcanusdtsurviveeuregulationshttpscdnhackernooncomimages10qlzdomrnvcqubjnaav23zzqmu2q1736ispng\"><strong><a href=\"https://hackernoon.com/can-usdt-survive-eu-regulations\">Can USDT Survive EU Regulations?</a></strong> <img src=\"https://cdn.hackernoon.com/images/10QlZDOmrNVcQUBJnAAV23Zzqmu2-q1736is.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/ykalynychenkogeneris\">@ykalynychenkogeneris</a> [ 3 Min read ] \n Coinbase is restricting stablecoins that do not comply with the EU's markets in crypto-assets regulations. <a href=\"https://hackernoon.com/can-usdt-survive-eu-regulations\">Read More.</a></p>\n<h2 id=\"whytheracetoagiishumanitysdefiningmomenthttpshackernooncomwhytheracetoagiishumanitysdefiningmomenthttpscdnhackernooncomimages9sbj6ozmvxoehdxtjjuu75plynp127237v7jpeg\"><strong><a href=\"https://hackernoon.com/why-the-race-to-agi-is-humanitys-defining-moment\">Why the Race to AGI is Humanity's Defining Moment</a></strong> <img src=\"https://cdn.hackernoon.com/images/9SBj6OzMvXOEhDxTjjuu75pLYnp1-27237v7.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/thomascherickal\">@thomascherickal</a> [ 6 Min read ] \n If you ever wondered why AGI is so scary for some scientists, while others are pursing it with passion - well - this article explains everything. <a href=\"https://hackernoon.com/why-the-race-to-agi-is-humanitys-defining-moment\">Read More.</a></p>\n<h2 id=\"googledtrickedopenaiintothinkingitwasaheaditwasnthttpshackernooncomgoogledtrickedopenaiintothinkingitwasaheaditwasnthttpscdnhackernooncomimagesiqkqes3oewdnt1lodv0zmi9jwnz2mx837ysjpeg\"><strong><a href=\"https://hackernoon.com/googled-tricked-openai-into-thinking-it-was-ahead-it-wasnt\">Googled Tricked OpenAI Into Thinking It Was Ahead (It Wasn't)</a></strong> <img src=\"https://cdn.hackernoon.com/images/IqkQes3OEWdNt1lODV0ZMI9jWNZ2-mx837ys.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/hacker661972\">@hacker661972</a> [ 11 Min read ] \n Google has released a cheap, fast and highly capable model to build agentic flows. <a href=\"https://hackernoon.com/googled-tricked-openai-into-thinking-it-was-ahead-it-wasnt\">Read More.</a></p>\n<h2 id=\"everyaipredictionthatcametruein2024andtheonesthatdidnthttpshackernooncomeveryaipredictionthatcametruein2024andtheonesthatdidnthttpscdnhackernooncomimages0qfg9cgg68xnm6wwckdej6kjix92irg39m1jpeg\"><strong><a href=\"https://hackernoon.com/every-ai-prediction-that-came-true-in-2024and-the-ones-that-didnt\">Every AI Prediction That Came True in 2024—and the Ones That Didn't</a></strong> <img src=\"https://cdn.hackernoon.com/images/0qfg9cGG68XNM6wWCkDej6KjiX92-irg39m1.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/morganmsk\">@morganmsk</a> [ 21 Min read ] \n A look back at the key moments that defined AI in 2024. <a href=\"https://hackernoon.com/every-ai-prediction-that-came-true-in-2024and-the-ones-that-didnt\">Read More.</a></p>\n<h2 id=\"blockchainbridgesgapswheretraditionalfinancefailssaysstellardevelopmentfoundationhttpshackernooncomblockchainbridgesgapswheretraditionalfinancefailssaysstellardevelopmentfoundationhttpscdnhackernooncomimagesgjskotsfrcuq2z1o2kablbklw1u29c036cfpng\"><strong><a href=\"https://hackernoon.com/blockchain-bridges-gaps-where-traditional-finance-fails-says-stellar-development-foundation\">Blockchain Bridges Gaps Where Traditional Finance Fails, Says Stellar Development Foundation</a></strong> <img src=\"https://cdn.hackernoon.com/images/gjsKotsFrCUq2Z1o2kabLbKlW1U2-9c036cf.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/terezabizkova\">@terezabizkova</a> [ 11 Min read ] \n Anke Liu and Anna Whitson-Diaz from Stellar Development Foundation share how blockchain bridges gaps in financial inclusion and empowers communities worldwide. <a href=\"https://hackernoon.com/blockchain-bridges-gaps-where-traditional-finance-fails-says-stellar-development-foundation\">Read More.</a></p>\n<h2 id=\"amazonsd845billiongambleclasheswithbroccolifamilysirongripon007httpshackernooncomamazonsd845billiongambleclasheswithbroccolifamilysirongripon007httpscdnhackernooncomimagesntolwvce2kpryow11aq31ts22ky1dn03dlvjpeg\"><strong><a href=\"https://hackernoon.com/amazons-$845-billion-gamble-clashes-with-broccoli-familys-iron-grip-on-007\">Amazon's $8.45 Billion Gamble Clashes with Broccoli Family's Iron Grip on 007</a></strong> <img src=\"https://cdn.hackernoon.com/images/nTolwVCe2KPryOw11aq31tS22Ky1-dn03dlv.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/davidjdeal\">@davidjdeal</a> [ 12 Min read ] \n There's a lot at stake with Amazon's (so far unsuccessful) attempt to create a franchise around the James Bond name. <a href=\"https://hackernoon.com/amazons-$845-billion-gamble-clashes-with-broccoli-familys-iron-grip-on-007\">Read More.</a></p>\n<h2 id=\"canpiratessavedemocracyhttpshackernooncomcanpiratessavedemocracyhttpscdnhackernooncomimages8kftabuye4qdsclufeck1oapnxf28h0349xjpeg\"><strong><a href=\"https://hackernoon.com/can-pirates-save-democracy\">Can Pirates Save Democracy?</a></strong> <img src=\"https://cdn.hackernoon.com/images/8kftaBUYe4QdsCLufeCk1OAPNXF2-8h0349x.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/heinhtetkyaw\">@heinhtetkyaw</a> [ 4 Min read ] \n Pirate parties have demonstrated the potential for cross-ideological collaboration between left-libertarians and right-libertarians. <a href=\"https://hackernoon.com/can-pirates-save-democracy\">Read More.</a></p>\n<h2 id=\"hereswhyhighachieversfeellikefailureshttpshackernooncomhereswhyhighachieversfeellikefailureshttpscdnhackernooncomimagesstandingontopofamountaind0kxvq2p6k7msfxxmb3n6r8mpng\"><strong><a href=\"https://hackernoon.com/heres-why-high-achievers-feel-like-failures\">Here's Why High Achievers Feel Like Failures</a></strong> <img src=\"https://cdn.hackernoon.com/images/standing-on-top-of-a-mountain-d0kxvq2p6k7msfxxmb3n6r8m.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/scottdclary\">@scottdclary</a> [ 11 Min read ] \n The invisible progress paradox is a trap in personal growth. <a href=\"https://hackernoon.com/heres-why-high-achievers-feel-like-failures\">Read More.</a></p>\n<h2 id=\"dohoneypotsstillmatterhttpshackernooncomdohoneypotsstillmatterhttpscdnhackernooncomimagesrhanbxrxjsyoximtykjflecfjyc32f0346vpng\"><strong><a href=\"https://hackernoon.com/do-honeypots-still-matter\">Do Honeypots Still Matter?</a></strong> <img src=\"https://cdn.hackernoon.com/images/RHANbxrXjsYoxIMTyKJFleCFJyC3-2f0346v.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/salkimmich\">@salkimmich</a> [ 7 Min read ] \n In cybersecurity, honeypots are systems or environments designed to lure attackers. The roots of the honeypot idea go much deeper than digital security though. <a href=\"https://hackernoon.com/do-honeypots-still-matter\">Read More.</a></p>\n<h2 id=\"thepublicfearsagibutitshistorymayassuageconcernshttpshackernooncomthepublicfearsagibutitshistorymayassuageconcernshttpscdnhackernooncomimageszgif6tampuhwn5y5hqefuagihw9328534g3jpeg\"><strong><a href=\"https://hackernoon.com/the-public-fears-agi-but-its-history-may-assuage-concerns\">The Public Fears AGI, But Its History May Assuage Concerns </a></strong> <img src=\"https://cdn.hackernoon.com/images/ZGIF6TAmPuhwn5Y5HQEFuaGIhw93-28534g3.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/dansasser\">@dansasser</a> [ 12 Min read ] \n Learn how OpenAI’s o3 model redefines ARC-AGI benchmarks while addressing misconceptions about Artificial General Intelligence and AI's evolving role. <a href=\"https://hackernoon.com/the-public-fears-agi-but-its-history-may-assuage-concerns\">Read More.</a></p>\n<h2 id=\"whatcapitalistsgotwrongaboutthefutureofeducationandwhattheyshouldhavedoneinsteadhttpshackernooncomwhatcapitalistsgotwrongaboutthefutureofeducationandwhattheyshouldhavedoneinsteadhttpscdnhackernooncomimages3gwutajrwqvk90fbpczdqx8nvyj12x0398ipng\"><strong><a href=\"https://hackernoon.com/what-capitalists-got-wrong-about-the-future-of-education-and-what-they-should-have-done-instead\">What Capitalists Got Wrong About the 'Future of Education,' And What They Should Have Done Instead</a></strong> <img src=\"https://cdn.hackernoon.com/images/3GwutAJRWQVk90FbpCzdqx8nvyj1-2x0398i.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/thefrogsociety\">@thefrogsociety</a> [ 17 Min read ] \n The future of work prioritizes AI, tech skills, and efficiency, sidelining diverse talents like artists and caretakers. <a href=\"https://hackernoon.com/what-capitalists-got-wrong-about-the-future-of-education-and-what-they-should-have-done-instead\">Read More.</a></p>\n<h2 id=\"willaiwidenglobalinequalityhttpshackernooncomwillaiwidenglobalinequalityhttpscdnhackernooncomimagesbf5mduzkm2xa5ajgviztmbkqlbz21a033bnjpeg\"><strong><a href=\"https://hackernoon.com/will-ai-widen-global-inequality\">Will AI Widen Global Inequality?</a></strong> <img src=\"https://cdn.hackernoon.com/images/bf5MdUZkm2XA5ajgVIztMBkqLBz2-1a033bn.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/zacamos\">@zacamos</a> [ 5 Min read ] \n AI gives ordinary people lots of computing power — but only if they have the ability and infrastructure to access it. Here's how AI may exacerbate inequalities. <a href=\"https://hackernoon.com/will-ai-widen-global-inequality\">Read More.</a></p>\n<h2 id=\"thesurprisingexperiencethatledmetolaunchmyownproducthttpshackernooncomthesurprisingexperiencethatledmetolaunchmyownproducthttpscdnhackernooncomimagesyvrrwuzcv0dh0icetg9lsokteim2_o32o990ojpeg\"><strong><a href=\"https://hackernoon.com/the-surprising-experience-that-led-me-to-launch-my-own-product\">The Surprising Experience That Led Me to Launch My Own Product</a></strong> <img src=\"https://cdn.hackernoon.com/images/yvRRWuzCv0dH0IcETG9LsoKtEiM2_o32o990o.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/emmanuelozigue_da2j7jog\">@emmanuelozigue_da2j7jog</a> [ 3 Min read ] \n After experiencing firsthand the frustration of having to wait at a barbershop, I decided to take matters in my own hand and build swiftbooked. <a href=\"https://hackernoon.com/the-surprising-experience-that-led-me-to-launch-my-own-product\">Read More.</a></p>\n<h2 id=\"mycryptowalletsetupletsmestayanonymouswhileprotectingmeagainstthefthttpshackernooncommycryptowalletsetupletsmestayanonymouswhileprotectingmeagainstthefthttpscdnhackernooncomimagesfwg2qgsxk8m9zfz7grgymocdagz1a002u51jpeg\"><strong><a href=\"https://hackernoon.com/my-crypto-wallet-setup-lets-me-stay-anonymous-while-protecting-me-against-theft\">My Crypto Wallet Setup Lets Me Stay Anonymous While Protecting Me Against Theft</a></strong> <img src=\"https://cdn.hackernoon.com/images/fwg2QgSXk8M9zFz7grgYmOCDaGz1-a002u51.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/techshinobi\">@techshinobi</a> [ 7 Min read ] \n Learn to set up a cold wallet using Tails, Feather and KeepAssXC. <a href=\"https://hackernoon.com/my-crypto-wallet-setup-lets-me-stay-anonymous-while-protecting-me-against-theft\">Read More.</a></p>\n<h2 id=\"seniorwebdevelopersareusingthese11designpatternstowritesqueakycleanreactcodehttpshackernooncomseniorwebdevelopersareusingthese11designpatternstowritesqueakycleanreactcodehttpscdnhackernooncomimagesypxxsujojbvehoteb0eypeom5do1oy03c2qpng\"><strong><a href=\"https://hackernoon.com/senior-web-developers-are-using-these-11-design-patterns-to-write-squeaky-clean-react-code\">Senior Web Developers Are Using These 11 Design Patterns to Write Squeaky Clean React Code</a></strong> <img src=\"https://cdn.hackernoon.com/images/YpXxsUjOJbVehoteB0eyPeom5do1-oy03c2q.png\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/ilyasseisov\">@ilyasseisov</a> [ 23 Min read ] \n Discover 11 React Design Patterns You Need to Know to improve your code efficiency, scalability, and maintainability. Perfect for all React developers! <a href=\"https://hackernoon.com/senior-web-developers-are-using-these-11-design-patterns-to-write-squeaky-clean-react-code\">Read More.</a></p>\n<h2 id=\"web3recruitersarenowcuttinginterviewsbecausecandidateshaventexperiencedcryptohttpshackernooncomweb3recruitersarenowcuttinginterviewsbecausecandidateshaventexperiencedcryptohttpscdnhackernooncomimages2jqchkrv03exbugklrdzibfm99q2b602sc5jpeg\"><strong><a href=\"https://hackernoon.com/web3-recruiters-are-now-cutting-interviews-because-candidates-havent-experienced-crypto\">Web3 Recruiters Are Now Cutting Interviews Because Candidates Haven't 'Experienced' Crypto</a></strong> <img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-b602sc5.jpeg\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/edwinliavaa\">@edwinliavaa</a> [ 3 Min read ] \n When everyone on your team shares the same background and experiences, it becomes dangerously easy to lose sight of how your product looks to outsiders. <a href=\"https://hackernoon.com/web3-recruiters-are-now-cutting-interviews-because-candidates-havent-experienced-crypto\">Read More.</a></p>\n<h2 id=\"entrepreneursineuropemeettodiscussthefutureofstartupsinadecentralizedworldhttpshackernooncomentrepreneursineuropemeettodiscussthefutureofstartupsinadecentralizedworldhttpscdnhackernooncomimagesgtdtkfy55mbip2rbhfnayxnhqxn2x9c333uwebp\"><strong><a href=\"https://hackernoon.com/entrepreneurs-in-europe-meet-to-discuss-the-future-of-startups-in-a-decentralized-world\">Entrepreneurs in Europe Meet to Discuss the Future of Startups in a Decentralized World</a></strong> <img src=\"https://cdn.hackernoon.com/images/GTDTkFY55mbiP2RBHFNayxNhqXn2-x9c333u.webp\" alt=\"\" /></h2>\n<p>By <a href=\"https://hackernoon.com/u/raysvitla\">@raysvitla</a> [ 4 Min read ] \n Stateless founders are redefining entrepreneurship. Insights from f27.club Lisbon reveal how AI, decentralized tools, and trust build thriving global startups. <a href=\"https://hackernoon.com/entrepreneurs-in-europe-meet-to-discuss-the-future-of-startups-in-a-decentralized-world\">Read More.</a> \n 🧑‍💻 What happened in your world this week? It's been said that <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>, <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,<a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>. Feeling stuck? We got you covered ⬇️⬇️⬇️\n <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.\n See you on Planet Internet! With love, \n The HackerNoon Team ✌️\n <img src=\"https://cdn.hackernoon.com/images/ezgif.com-gif-maker%20(44).gif\" alt=\"\" /></p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI Tools May Soon Manipulate People's Online Decision-Making, Say Researchers","url":"https://slashdot.org/story/24/12/30/0435226/ai-tools-may-soon-manipulate-peoples-online-decision-making-say-researchers?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735538940,"author":"EditorDavid","unread":true,"content":"Slashdot reader SysEngineer shared this report from the Guardian:\n\nAI tools could be used to manipulate online audiences into making decisions &mdash; ranging from what to buy to who to vote for &mdash; according to researchers at the University of Cambridge. The paper highlights an emerging new marketplace for \"digital signals of intent\" &mdash; known as the \"intention economy\" &mdash; where AI assistants understand, forecast and manipulate human intentions and sell that information on to companies who can profit from it. The intention economy is touted by researchers at Cambridge's Leverhulme Centre for the Future of Intelligence (LCFI) as a successor to the attention economy, where social networks keep users hooked on their platforms and serve them adverts. The intention economy involves AI-savvy tech companies selling what they know about your motivations, from plans for a stay in a hotel to opinions on a political candidate, to the highest bidder... \n\nThe study claims that large language models (LLMs), the technology that underpins AI tools such as the ChatGPT chatbot, will be used to \"anticipate and steer\" users based on \"intentional, behavioural and psychological data\"... Advertisers will be able to use generative AI tools to create bespoke online ads, the report claims... AI models will be able to tweak their outputs in response to \"streams of incoming user-generated data\", the study added, citing research showing that models can infer personal information through workaday exchanges and even \"steer\" conversations in order to gain more personal information. \nThe article includes this quote from Dr. Jonnie Penn, an historian of technology at LCFI. \"Unless regulated, the intention economy will treat your motivations as the new currency. It will be a gold rush for those who target, steer and sell human intentions. \n\"We should start to consider the likely impact such a marketplace would have on human aspirations, including free and fair elections, a free press and fair market competition, before we become victims of its unintended consequences.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=AI+Tools+May+Soon+Manipulate+People's+Online+Decision-Making%2C+Say+Researchers%3A+https%3A%2F%2Fslashdot.org%2Fstory%2F24%2F12%2F30%2F0435226%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fslashdot.org%2Fstory%2F24%2F12%2F30%2F0435226%2Fai-tools-may-soon-manipulate-peoples-online-decision-making-say-researchers%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://slashdot.org/story/24/12/30/0435226/ai-tools-may-soon-manipulate-peoples-online-decision-making-say-researchers?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564457&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"When Jimmy Carter Spoke At a Wireless Tradeshow","url":"https://news.slashdot.org/story/24/12/30/0251249/when-jimmy-carter-spoke-at-a-wireless-tradeshow?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735528140,"author":"EditorDavid","unread":true,"content":"Former U.S. President Jimmy Carter has died. Born in 1924, he had just celebrated his 100th birthday on October 1st.\n\n\nIf you want to catch a glimpse of his political charisma, YouTube has a clip of Carter's appearance on \"What's My Line\" when he was still only governor of Georgia. Within five years he'd be president of the United States, serving from 1977 to 1981. \n\nBut it seems like today everyone has a story to tell. More than two decades later, long-time Slashdot reader destinyland saw Jimmy Carter speak in Las Vegas in 2001 on the final day of the CTIA Wireless tradeshow. \"I feel thrilled to be a part of this,\" 77-year-old Carter had said....\n\nCarter applauded the work of \"entrepreneurs and scientists and engineers that are transforming the face of the globe.\" And he noted their technologies could address problems targeted by the Carter Center. \n\nInterrupted by a few cellphone rings, the former President conversed on a stage at the Sands Expo and Venetian Hotel with Tom Wheeler, the president of the wireless communications trade association. Wheeler reminded the audience of Carter's decidedly nontechnical background, discussing An Hour Before Daylight, Carter's memoir about growing up on a farm in Georgia during the Great Depression. \"We were the only family blessed with an outhouse,\" Carter told the crowd. \n\n Wheeler also asked a question many in the technology community could relate to. Carter, he pointed out, had been involuntarily retired. \"What's it feel like?\" The former President told the audience he'd re-focussed his energies into humanitarian efforts through the Carter Center, which is active in providing health services around the world as well as monitoring elections. Carter donated his appearance fee to the Carter Center... \n\n Midway through the hour-long discussion, the former President touted his administration's record of deregulating several industries, including transportation, energy, and communications, saying \"If it hadn't been for that deregulation, this environment in which you all live wouldn't have been possible.\" Carter also shared with the business crowd that it was a belief in free enterprise that made him want to enter politics, drawn from his experiences selling peanuts as a young boy for a dollar a day. \n\nThe audience greeted the former president warmly, giving him a standing ovation both when he took the stage and when he left. Carter joked it was almost enough to make him want to get back into politics. \n\nEveryone has their own opinion. When a friend of mine was in high school, she got to meet Jimmy Carter early in his presidency. He'd seemed unusually kind and good, she said, but remembered her first reaction. \"They're going to eat you alive.\" And yet then, pointing to the humanitarian work he would continue for four decades, she said he was also clearly America's very best ex-president. \n\nAnd the liberal blog Talking Points Memo argues Carter's accomplishments as president are being re-evaluated:\nSome found him to be distinctly unsung, with little attention given to his brokering of peace with the Camp David Accords and emphasis on global human rights. And some just liked him. A serious, intelligent, faithful, deeply honest man who spurned political expediency and burned through hundreds of pages of memos a day, he preached self-restraint, stewardship and commonality to an electorate that cast him off four years later for the glib excesses of Ronald Reagan.... \"People assume that because he wasn't warm and cuddly with Congress that he didn't get much through,\" said John Alter [who wrote the first independent Carter biography in 2020]. \"He signed more legislation in four years than Clinton or Obama did in eight. He has the most prodigious legislative record since World War II, with the exception of Lyndon Johnson.\" \n\nThat record includes, by Alter's count, 14 major pieces of environmental legislation. In one of Carter's more creative moves, he dusted off the 1906 Antiquities Act to keep pristine 56 million acres of Alaskan wilderness. His piecemeal approach, cloaked in distinctly unsexy bills like the 1978 Public Utilities Regulatory Policies Act, planted the seeds for a changing national energy system in the face of climate change. Carter had started underlining passages in scientific journals about what is now the most existential crisis of our time as early as 1971. What's most wrenching about Carter's improvements in energy and environmental policy now is what he wasn't able to accomplish. On his way out of office, he issued a report that included recommendations for cutting carbon emissions &mdash; at exactly the same rate the Paris Climate Accords coalesced behind 35 years later.... \n\nHis Carter Center has virtually eradicated certain devastating diseases on the African continent, part of the work for which he received the Nobel Peace Prize in 2002. He and Rosalynn have also helped build and repair over 4,000 homes for Habitat for Humanity, work that continued well into his 90s. \n\n\n\nI've got my own story. As a young boy I saw Jimmy Carter give a speech in 1977 &mdash; just six months after he'd assumed the presidency. A crowd of teenagers thrilled to see the president gave him a long, loud round of applause. And when it finally died down, Carter said... \n\n\"I wish I got that kind of reception from Congress.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=When+Jimmy+Carter+Spoke+At+a+Wireless+Tradeshow%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0251249%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0251249%2Fwhen-jimmy-carter-spoke-at-a-wireless-tradeshow%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/30/0251249/when-jimmy-carter-spoke-at-a-wireless-tradeshow?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564415&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia’s next move: powering humanoid robots","url":"https://techcrunch.com/2024/12/29/nvidias-next-move-powering-humanoid-robots/","date":1735517747,"author":"Connie Loizos","unread":true,"content":"<p>The chipmaking giant Nvidia is leaning more heavily into robotics in 2025. More specifically, in the first half of the new year, confirms the Financial Times, Nvidia is launching a new generation of compact computers for humanoid robots called Jetson Thor. The move, which was expected, is part of an evolving, years-long strategy. Nvidia doesn’t [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"2024's Ten Top-Grossing Films Were All Sequels or Prequels","url":"https://entertainment.slashdot.org/story/24/12/30/000205/2024s-ten-top-grossing-films-were-all-sequels-or-prequels?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735516980,"author":"EditorDavid","unread":true,"content":"\"Every single one of the top ten box office hits of 2024 was a sequel, a remake... or a prequel,\" writes The Hollywood Reporter. \n\nHere's the list of 2024's top-grossing films published by the movie blog SlashFilm: \n\n10. Beetlejuice Beetlejuice \n9. Venom: The Last Dance\n8. Kung Fu Panda 4\n7. Godzilla x Kong: The New Empire \n6. Wicked\n5. Dune: Part Two \n4. Moana 2\n3. Despicable Me 4\n2. Deadpool &amp; Wolverine \n1. Inside Out 2 \n\n2024 was the year Godzilla celebrated its 70th year as a franchise &mdash; but it wasn't the only long-running franchise. \"When the Marvel Cinematic Universe went R-rated with Deadpool &amp; Wolverine... it was literally more successful than any other R-rated movie in history,\" SlashFilm points out, while Venom: The Last Dance was the year's 9th highest-earner. (But several other big superhero movies flopped and \"the misses outweighed the hits this year, while DC sat it out entirely as the world waits for Superman to usher in James Gunn's new DC Universe.\") \n\nThey also marvel that Wicked earned $572 million after opening on the same day as Ridley Scott's Gladiator II.... \n\nBut in the end SlashFilm describes 2024 as \"a banner year for animation,\" with computer-animated movies filling four of the top ten spots (Kung Fu Panda 4, Moana 2, Despicable Me 4, and Inside Out 2). And another interesting trend? Though the world flocked to Tim Burton's first sequel to Beetlejuice after 36 years, Warner Bros. was, \"at one point, pushing for Beetlejuice 2 to go directly to streaming on Max.\" And Disney original had the same idea for Moana 2, leading SlashFilm to conclude that 2024's box office \"should be the death of the big direct-to-streaming movie.\" SlashFilm notes that Disney also sent several Pixar originals to Disney+ between 2020 and 2022, which \"did immeasurable damage to the brand, something that even CEO Bob Iger has acknowledged.\" And then after a theatrical debut Pixar's Inside Out 2 became \"the eighth biggest movie ever at the box office, with $1.698 billion to its name\" &mdash; and the highest-grossing animated film ever made. \n\n\nAnd Dune: Part Two?\n\nDenis Villeneuve accomplished nothing shy of a miracle with 2021's \"Dune,\" an adaptation of Frank Herbert's cherished sci-fi novel that was faithful to the material, massive in scale, but still felt like an auteur film... The only downside? 2021 was a terrible time to release a movie, particularly a Warner Bros. movie, as all of the studio's films were going to HBO Max the same day they hit theaters. Yet, \"Dune\" made $400 million in its original run, which was enough to justify a sequel. Evidently, the audience for this franchise grew exponentially in the years before \"Dune: Part Two\" hit theaters in early March... All told, Villeneuve's sweeping, epic sequel pulled in $714.4 million worldwide, all while garnering tons of acclaim once again. Also, not for nothing, Villeneuve got it made for less than $200 million... \n\n\nWithout \"Dune: Part Two\" making what it made, the box office might have been in truly dire shape. As a relatively dead April and very weak May followed, this overperformance helped keep theaters afloat until greener pastures arrived in the back half of the year. The Spice must flow, as it were. \n\n\n The Hollywood Reporter offers another take on the significance of 2024:\n\nTotal domestic box office revenue appears to be heading toward around $8 billion, down from 2023's exhilarating post-COVID turnaround of $9 billion, but the National Association of Theatre Owners prefers to accentuate the positive, attributing the dip to a shortage of product due to the labor strikes and taking encouragement from the renewal of the movie habit... \n\nInterestingly, or thankfully, the cinematic universes of Marvel, DC, and Star Wars failed to expand: except for Deadpool &amp; Wolverine, not one of the huge hits came from a comic book franchise or a galaxy far, far away. \nThe article then complains about people using their phones during the movie for texting, talking, and photographing the movie itself. (Though it applauds a PSA against the practice in which Deadpool and Wolverine \"delivered the message in laudably blunt terms.\") \nAnd on Wikipedia, Deadpool &amp; Wolverine and Dune: Part Two were the eighth and 23rd most popular articles of 2024.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=2024's+Ten+Top-Grossing+Films+Were+All+Sequels+or+Prequels%3A+https%3A%2F%2Fentertainment.slashdot.org%2Fstory%2F24%2F12%2F30%2F000205%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fentertainment.slashdot.org%2Fstory%2F24%2F12%2F30%2F000205%2F2024s-ten-top-grossing-films-were-all-sequels-or-prequels%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://entertainment.slashdot.org/story/24/12/30/000205/2024s-ten-top-grossing-films-were-all-sequels-or-prequels?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564331&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Can Money Buy You a Longer Life?","url":"https://science.slashdot.org/story/24/12/29/2252216/can-money-buy-you-a-longer-life?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735513080,"author":"EditorDavid","unread":true,"content":"An anonymous reader shared this report from the Wall Street Journal:\n\n\nThe rich get richer &mdash; and older. People with high salaries and net worth tend to live longer lives, research shows. Once Americans make it to their late 50s, the wealthiest 10% live to a median age of around 86 years, roughly 14 years longer than the least wealthy 10%, according to a study published earlier this year in JAMA Internal Medicine. People with more money can afford healthier food, more healthcare and homes in safer, less-polluted neighborhoods, says Kathryn Himmelstein, a co-author of the study and a medical director at the Boston Public Health Commission. \n\nThough you can't add more months or years to your online shopping cart yet, health and aging researchers say there are ways to spend money to improve your chances of living longer. They suggest favoring purchases that help you track your health, stay active and reduce stress. \"We know the things that help us age better, and everyone's always disappointed when you tell them,\" says Andrew Scott, director of economics at the Ellison Institute of Technology in Oxford, England. \"Eat less and eat better, sleep more, exercise more and spend time with friends....\" But certain gadgets and luxuries can be worth the cost, some researchers say. Devices such as the Apple Watch and Oura Ring can instill healthy habits and catch worrying patterns that might emerge between annual checkups, says Joe Coughlin, the director of the MIT AgeLab... Coughlin says he once went to the emergency room because his Apple Watch detected a spike in his heart rate that he hadn't noticed himself. \n\n\"For the superwealthy, suddenly living longer and living better has become the new prestige,\" Coughlin says. Higher incomes correlate with longer lives, but there are diminishing returns. Each successive jump in pay is linked to smaller boosts in longevity, a 2016 study from the research group Opportunity Insights found... A key to the relationship between income and longevity is that money doesn't just buy stuff that helps you live longer. It also buys time and reduces stress. \"If you've got a nice place to live and you don't have to worry about food on the table, you have the mental head space and resources to prioritize your health,\" says Steven Woolf, a professor at Virginia Commonwealth University School of Medicine... Moreover, many lower-income jobs are more physically taxing and more prone to workplace accidents and exposure to harmful substances. \n\nThe article also includes examples of spending that promotes health, including things like home gym equipment and even swing-dancing lessons. \nBut it also adds that \"plenty of things that are good for you don't come with a bill, such as going on a walk or minimizing screen time before bedtime.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Can+Money+Buy+You+a+Longer+Life%3F%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2252216%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2252216%2Fcan-money-buy-you-a-longer-life%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/2252216/can-money-buy-you-a-longer-life?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564297&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI data centers could be ‘distorting’ the US power grid","url":"https://techcrunch.com/2024/12/29/ai-data-centers-could-be-distorting-the-us-power-grid/","date":1735511697,"author":"Anthony Ha","unread":true,"content":"<p>The proliferation of data centers aiming to meet the computational needs of AI could be bad news for the US power grid, according to a new report in Bloomberg. Using the 1 million residential sensors tracked by Whisker Labs, along with market intelligence data from DC Byte, Bloomberg found that more than half of the [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How Bias in Medical AI Affects Diagnoses Across Different Groups","url":"https://hackernoon.com/how-bias-in-medical-ai-affects-diagnoses-across-different-groups?source=rss","date":1735509612,"author":"Demographic","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<ol>\n<li><p><a href=\"https://hackernoon.com/preview/nBblhY4GavOD7ClMUBnG\">Abstract and Introduction</a> </p></li>\n<li><p><a href=\"http://hackernoon.com/preview/Q4LyJJ5e4TJ4BzjlgyN1\">Related work</a> </p></li>\n<li><p>Methods</p>\n<p><a href=\"http://hackernoon.com/preview/sm7wDCdG5lz0AUrDBp3M\">3.1 Positive-sum fairness</a></p>\n<p><a href=\"http://hackernoon.com/preview/QC7oPOQnmKaxm2ayCciG\">3.2 Application</a></p></li>\n<li><p><a href=\"http://hackernoon.com/preview/1EyAXdyx1D3kezijKFs0\">Experiments</a></p>\n<p><a href=\"http://hackernoon.com/preview/Bfgncv3APejMq3Q2PpSK\">4.1 Initial results</a></p>\n<p><a href=\"https://hackernoon.com/preview/XVQopaF8Sls4BQ1YYZqk\">4.2 Positive-sum fairness</a> </p></li>\n<li><p><a href=\"https://hackernoon.com/preview/gBzBsQJmHMvmTsUFxYhC\">Conclusion and References</a></p></li>\n</ol>\n<h2 id=\"2relatedwork\">2 Related work</h2>\n<p>Bias is commonly identified in medical image analysis applications [38,40]. For instance [6], a CNN trained on brain MRI resulted in a significant difference between ethnicities. Seyyed-Kalantari et al. [32] observed that minorities received higher rates of algorithmic underdiagnosis. Zong et al. [40] assessed bias mitigation algorithms inand out-of-distribution settings. The experiments demonstrated the wide existence of bias in AI-based medical imaging classifiers and none of the bias mitigation algorithms was able to prevent this.</p>\n<p>\\\nDifferent definitions of fairness are used:</p>\n<p>\\\n– <strong>Individual fairness</strong> [25] requires that similar individuals should be treated equally and thus have similar predictions. For example, a model should have comparable diagnosis on two similar X-Ray images. </p>\n<p>\\\n– <strong>Group fairness</strong> requires equal performance on sub-groups divided based on sensitive attributes (e.g., race, sex, and age). Common group fairness metrics are demographic parity [8], equal odds [12] and predictive rate parity or sufficiency [21]. </p>\n<p>\\\n– <strong>Minimax fairness</strong> [5] seeks to ensure that the worst-off group is treated as fairly as possible, reducing the most severe negative impacts of a decision or system.</p>\n<p>\\\nThese definitions have pros and cons [36]. Individual fairness relies on the choice of the distance metric, which requires expert input. In minimax fairness, the ideal solution is difficult to compute and the degree of unfairness relies heavily on the choice of the set of models. Group fairness metrics are easy to implement and understand, but are not always adapted to the problem nor compatible with one another [2,18]. And even though prior work has broadened the group fairness notion by adding other normative choices than strict equality [1], none of the proposed metrics prevent the harm that could be brought to each subgroup’s performance individually or to the whole population’s benefit.</p>\n<p>\\\nAs mentioned in the introduction, similarly to [24,34,27,26], we believe that medical AI is different from other domains in that each improvement can save lives. Therefore, increasing disparities to achieve the best performance possible for each demographic subgroup and for the population as a whole could be justified. Previous research has shown that images themselves could carry demographic encodings [10,9]. E.g., Yang et al. [39] investigate the utilization of demographic encodings by analyzing the use of demographic shortcuts for disease classification. Two papers [41,11] examine the relevance of explicitly using sensitive attributes in fair classification systems for non-medical problems. They compare different models which leverage sensitive attributes with a model which is not trained on any sensitive attribute.</p>\n<p>\\</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Samia Belhadj∗, Lunit Inc., Seoul, Republic of Korea (samia.belhadj@lunit.io);</p>\n<p>(2) Sanguk Park [0009 −0005 −0538 −5522]*, Lunit Inc., Seoul, Republic of Korea (tony.superb@lunit.io);</p>\n<p>(3) Ambika Seth, Lunit Inc., Seoul, Republic of Korea (ambika.seth@lunit.io);</p>\n<p>(4) Hesham Dar [0009 −0003 −6458 −2097], Lunit Inc., Seoul, Republic of Korea (heshamdar@lunit.io);</p>\n<p>(5) Thijs Kooi [0009 −0003 −6458 −2097], Kooi, Lunit Inc., Seoul, Republic of Korea (tkooi@lunit.io). </p>\n<p>:::</p>\n<hr />\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2409.19940\">available on arxiv</a> under CC BY-NC-SA 4.0 license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Are We Better Prepared Now for Another Pandemic?","url":"https://science.slashdot.org/story/24/12/29/2152207/are-we-better-prepared-now-for-another-pandemic?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735509540,"author":"EditorDavid","unread":true,"content":"When it comes to the possibility of a bird flu outbreak, America's Centers for Disease Control recently issued a statement that the risk to the public \"remains low.\" \n\n\nBut even in the event of a worst-case scenario, New York magazine believes \"We may be more equipped for another pandemic than you think...\"\n\nIn 2023, more than half of people surveyed said that their lives had not returned to normal since the COVID outbreak, and a surprising number &mdash; 47 percent &mdash; said they now believe their lives will never return to normal. \n\nBut do we really know how a new pandemic would go and how we would handle it? Things are different this time &mdash; and in ways that aren't all bad. Unlike with COVID in the spring of 2020, millions of doses of bird-flu vaccines at various stages of testing sit in government stockpiles, and more are on the way. There are also already tests that work, though these are not broadly available to the public... Recent research suggests that we might actually manage a second pandemic better than we would believe. Despite all the noise to the contrary, a June poll by Harvard's School of Public Health says that Americans overall think the government responses to COVID &mdash; asking people to wear masks, pausing indoor dining, requiring health-care workers to get vaccinated &mdash; were all good ideas. Although the media tends to paint school closures as radically unpopular, only 44 percent of respondents said they currently think the shutdowns were a mistake. \n\nA growing body of research also suggests that many Americans feel stronger for what we endured during the most extreme days of COVID. Counter to what we like to say about our friends and neighbors and children, the challenge of the pandemic may have benefited some people's mental health. One study found that \"children entering the pandemic with clinically meaningful mental-health problems experienced notable improvements in their mental health.\" (Turns out there's one thing worse than shutting down an American school and that's having to attend it.) \n\nThe article also points out that \"There is no real information\" on the likelihood of a bird-flu virus even crossing over into humans. \n\nAnd of course, \"COVID still kills, with a body count just shy of 50,000 Americans in 2024, and it feels like a stretch to say that Americans are particularly concerned.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Are+We+Better+Prepared+Now+for+Another+Pandemic%3F%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2152207%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2152207%2Fare-we-better-prepared-now-for-another-pandemic%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/2152207/are-we-better-prepared-now-for-another-pandemic?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564255&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Linux 6.13-rc5 Released To Cap Off Linus Torvalds' Birthday Week","url":"https://www.phoronix.com/news/Linux-6.13-rc5-Released","date":1735507830,"author":"Michael Larabel","unread":true,"content":"The holiday between Christmas and New Year's is... Linus Torvalds' birthday on 28 December. Capping off the Linux creator's 55th birthday week is the Linux 6.13-rc5 kernel release...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Exploring Positive-Sum Fairness in Medical AI","url":"https://hackernoon.com/exploring-positive-sum-fairness-in-medical-ai?source=rss","date":1735507816,"author":"Demographic","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Samia Belhadj∗, Lunit Inc., Seoul, Republic of Korea (samia.belhadj@lunit.io);</p>\n<p>(2) Sanguk Park [0009 −0005 −0538 −5522]*, Lunit Inc., Seoul, Republic of Korea (tony.superb@lunit.io);</p>\n<p>(3) Ambika Seth, Lunit Inc., Seoul, Republic of Korea (ambika.seth@lunit.io);</p>\n<p>(4) Hesham Dar [0009 −0003 −6458 −2097], Lunit Inc., Seoul, Republic of Korea (heshamdar@lunit.io);</p>\n<p>(5) Thijs Kooi [0009 −0003 −6458 −2097], Kooi, Lunit Inc., Seoul, Republic of Korea (tkooi@lunit.io). </p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<ol>\n<li><p><a href=\"https://hackernoon.com/preview/nBblhY4GavOD7ClMUBnG\">Abstract and Introduction</a> </p></li>\n<li><p><a href=\"http://hackernoon.com/preview/Q4LyJJ5e4TJ4BzjlgyN1\">Related work</a> </p></li>\n<li><p>Methods</p>\n<p><a href=\"http://hackernoon.com/preview/sm7wDCdG5lz0AUrDBp3M\">3.1 Positive-sum fairness</a></p>\n<p><a href=\"http://hackernoon.com/preview/QC7oPOQnmKaxm2ayCciG\">3.2 Application</a></p></li>\n<li><p><a href=\"http://hackernoon.com/preview/1EyAXdyx1D3kezijKFs0\">Experiments</a></p>\n<p><a href=\"http://hackernoon.com/preview/Bfgncv3APejMq3Q2PpSK\">4.1 Initial results</a></p>\n<p><a href=\"https://hackernoon.com/preview/XVQopaF8Sls4BQ1YYZqk\">4.2 Positive-sum fairness</a> </p></li>\n<li><p><a href=\"https://hackernoon.com/preview/gBzBsQJmHMvmTsUFxYhC\">Conclusion and References</a></p></li>\n</ol>\n<p>\\\n<strong>Abstract</strong>. Fairness in medical AI is increasingly recognized as a crucial aspect of healthcare delivery. While most of the prior work done on fairness emphasizes the importance of equal performance, we argue that decreases in fairness can be either harmful or non-harmful, depending on the type of change and how sensitive attributes are used. To this end, we introduce the notion of positive-sum fairness, which states that an increase in performance that results in a larger group disparity is acceptable as long as it does not come at the cost of individual subgroup performance. This allows sensitive attributes correlated with the disease to be used to increase performance without compromising on fairness.</p>\n<p>\\\nWe illustrate this idea by comparing four CNN models that make di fferent use of the race attribute in the training phase. The results show that removing all demographic encodings from the images helps close the gap in performance between the different subgroups, whereas leveraging the race attribute as a model’s input increases the overall performance while widening the disparities between subgroups. These larger gaps are then put in perspective of the collective benefit through our notion of positive-sum fairness to distinguish harmful from non harmful disparities.</p>\n<h2 id=\"1introduction\">1 Introduction</h2>\n<p>Medical imaging plays a critical role in diagnosis, treatment planning, and monitoring patient progress. However, the reliability of medical imaging algorithms is not uniformly distributed across different demographic groups, raising concerns about fairness and potential biases in the results. Fairness in medical imaging most often refers to the equitable treatment of patients from diverse demographic backgrounds, regardless of their gender, race, ethnicity, or other characteristics sensitive to discrimination [19,38].</p>\n<p>\\\nThis equitable treatment is often interpreted as a similar performance across different demographic subgroups. When applied to domains like credit card scoring or AI-powered recruiting, ignoring all sensitive attributes and prioritizing a similar performance across the different demographic subgroups is an acceptable approach. However, in the medical field, demographic attributes are important clinical factors which radiologists and clinicians often take into consideration as they can have a strong impact on their diagnoses and can guide them to consider specific tests or treatments based on the patient’s demographic profile. The prevalence of diseases can be correlated to demographic attributes. For example, studies have shown that breast cancer has a higher incidence among Ashkenazi Jewish women [37,30]. And, due to historical and social disparities as well as different physiological features across demographic subgroups, the difficulty level of medical tasks is not uniformly distributed. For this reason, even collecting more or more diverse data does not necessarily produce equal performance across demographic subgroups as the best achievable result is not the same for each of them [27]. In a domain where each improvement can save lives, it is hard to disregard the benefit of the population as a whole for the sake of decreasing the disparities between subgroups.</p>\n<p>\\\nPetersen et al. [26] examined various types of demographic invariance in medical imaging AI, highlighting why they can be undesirable and stressing the need for better fairness assessments and mitigation techniques in this field. Several fairness measures suffer from degradation in the overall performance by penalizing the performance of an AI system for groups that it performs better on, in order to achieve parity with groups it performs worse on, which is referred to as “levelling down” [24]. While we are aware of papers suggesting training methods which aim to maximize the benefit of each subgroup (Berk Ustun [34], for instance, suggested debiasing methods following the ethical principles of beneficence (“do the best”) and non-maleficence (“do not harm”) [35] in regards to fairness), and methods which improve fairness by understanding and mitigating the demographic encodings present in images [39,3], we could not find any fairness evaluation framework or definition which allows to compare different models from the prism of harmful and non harmful disparities.</p>\n<p>\\\nWe, therefore, introduce the notion of positive-sum fairness: when looking at a situation where we have an initial model and are looking at the trade-off between fairness and performance while trying to improve it, inequitable performance can be acceptable as long as it does not come at the expense of other subgroups and allows a higher overall performance to be achieved. Specifically, we argue that differences in performance can be harmful and non-harmful. We consider a disparity harmful if it comes at the cost of the overall performance or if improving the overall performance is achieved by decreasing performance on any protected subgroup. A difference in performance across protected subgroups is considered non-harmful if, by improving an AI system’s performance, we exacerbate the disparities between subgroups without negatively impacting any specific subgroup. This main idea is summarized in figure 1.</p>\n<p>\\\nWe compare the positive-sum fairness framework with a more traditional group fairness definition, which is the largest disparity in performance across subgroups. We show that some models, while increasing this disparity, actually improve the performance of each subgroup individually and that other models which decrease the disparity (\"improving fairness\" from a classic point of view) are harming some subgroups to achieve it.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/null-gz034p3.png\" alt=\"Fig. 1: We investigate fairness of AI models and introduce the concept of ’positivesum fairness’ to differentiate harmful and non-harmful disparities. Graph a) shows the performance of an initial model per protected groups. b) shows the performance of an updated model with a higher overall performance but a lower fairness, under its standard definition, as indicated by the larger difference between the most and least advantaged groups and therefore could be rejected on the basis of fairness. c) shows the same updated model as b) however it shows the performance difference per group compared to the initial model. In this positive-sum framing we see that none of the groups had a reduction in performance and therefore the increased performance in Race C did not come at the cost of performance in any other group.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2409.19940\">available on arxiv</a> under CC BY-NC-SA 4.0 license.</p>\n<p>:::</p>\n<p>* These authors contributed equally to this work</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"India’s mobile payments dilemma","url":"https://techcrunch.com/2024/12/29/indias-mobile-payments-dilemma/","date":1735507256,"author":"Manish Singh","unread":true,"content":"<p>India&#8217;s payments regulator is set to decide as early as Monday whether to curb the dominance of Walmart&#8217;s PhonePe and Google in the nation&#8217;s fast-growing mobile payments market, a move that could reshape how its billion-plus population moves money. The decision centers on UPI, or Unified Payments Interface, a network backed by more than 50 [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Benchmarking The AMD INVLPGB Linux Kernel Patches For Better Performance","url":"https://www.phoronix.com/review/amd-invlpgb-linux","date":1735506920,"author":"Michael Larabel","unread":true,"content":"Last weekend a Meta engineer posted Linux kernel patches to make use of the AMD INVLPGB instruction for broadcast TLB invalidation. The Linux kernel can in turn invalidate TLB entries on remote CPUs without needing to send IPIs and without having to wait for remote CPUs to handle those interrupts. Synthetic benchmarks shown in that patch series were very promising and thus I carried out some benchmarking over the holidays of this AMD INVLPGB support for the Linux kernel.","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"KDE Amarok 3.2 Music Player Released With Initial Qt6/KF6 Support","url":"https://www.phoronix.com/news/KDE-Amarok-3.2-Released","date":1735506000,"author":"Michael Larabel","unread":true,"content":"Back in April was the release of the Amarok 3.0 music player for KDE after a six year hiatus and their first version ported to using the Qt5 toolkit and KDE Frameworks 5. Now in ending out 2024, the Amarok team has released an updated version of this open-source music player that provides initial support for the Qt6 toolkit and KDE Frameworks 6...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Finland Finds Drag Marks Near Broken Undersea Cable. Russia's 'Shadow Fleet' Suspected","url":"https://tech.slashdot.org/story/24/12/29/2055221/finland-finds-drag-marks-near-broken-undersea-cable-russias-shadow-fleet-suspected?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735505880,"author":"EditorDavid","unread":true,"content":"Reuters reports:\n\nFinnish police said on Sunday they had found tracks that drag on for dozens of kilometres along the bottom of the Baltic Sea where a tanker carrying Russian oil is suspected of breaking a power line and four telecoms cables with its anchor... A break in the 658 megawatt (MW) Estlink 2 power cable between Finland and Estonia occurred at midday on Wednesday, leaving only the 358 MW Estlink 1 linking the two countries, grid operators said. They said Estlink 2 might not be back in service before August. \n\nIn an interesting twist, the New York Times reports that the ship \"bears all the hallmarks of vessels belonging to Russia's shadow fleet, officials said, and had embarked from a Russian port shortly before the cables were cut.\"\n\nIf confirmed, it would be the first known instance of a shadow fleet vessel being used to intentionally sabotage critical infrastructure in Europe &mdash; and, officials and experts said, a clear escalation by Russia in its conflict with the West... NATO's general secretary, Mark Rutte, responding to requests from the leaders of Finland and Estonia, both member nations, said the Atlantic alliance would \"enhance\" its military presence in the Baltic Sea... \n\nSince Russia began assembling its fleet, the number of shadow vessels traversing the oceans has grown by hundreds and now makes up 17 percent of the total global oil tanker fleet... Nearly 70 percent of Russia's oil is being transported by shadow tankers, according to an analysis published in October by the Kyiv School of Economics Institute, a research organization based in Ukraine... The authorities in Finland are still investigating whether the \"Eagle S\" engaged in a criminal act. But the sheer size of the shadow fleet might have made using some of these vessels for sabotage irresistible to Russia, [said Elisabeth Braw, a senior fellow at the Atlantic Council who has researched and written about shadow fleets]... \n\nWhile it's still not certain that this week's cable cutting was done intentionally, the Baltic Sea, for a number of reasons, is an ideal arena to carry out sabotage operations. It is relatively shallow and is crisscrossed with essential undersea cables and pipelines that provide energy, as well as internet and phone services, to a number of European countries that are NATO members. Russia has relatively unfettered access to the sea from several ports, and its commercial vessels, protected by international maritime law, can move around international waters largely unmolested... The suspicions that Russia was using shadow vessels for more than just escaping sanctions existed before this week's cable cutting. Last April, the head of Sweden's Navy told a local news outlet that there was evidence such ships were being used to conduct signals intelligence on behalf of Russia and that some fishing vessels had been spotted with antennas and masts not normally seen on commercial vessels. Since the war began, there has also been an uptick in suspicious episodes resulting in damage to critical undersea infrastructure... \n\n\nHours after Finland's energy grid operator alerted the police that an undersea power cable was damaged on Wednesday, Finnish officers descended by helicopter to the ship's deck and took over the bridge, preventing the vessel from sailing farther. By Friday, it remained at anchor in the Gulf of Finland, guarded by a Finnish Defense Forces missile boat and a Border Guard patrol vessel. \n\nThe cable incident happened just weeks after the EU issued new sanctions targetting Russia's shadow fleet, Euronews reports. \"A handful of Chinese companies suspected of enabling Russia's production of drones are also blacklisted as part of the agreement, a diplomat told Euronews.\"\nThe \"shadow fleet\" has been accused of deceptive practices, including transmitting falsified data and turning off their transporters to become invisible to satellite systems, and conducting multiple ship-to-ship transfers to conceal the origin of the oil barrels...<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Finland+Finds+Drag+Marks+Near+Broken+Undersea+Cable.++Russia's+'Shadow+Fleet'+Suspected%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F29%2F2055221%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F29%2F2055221%2Ffinland-finds-drag-marks-near-broken-undersea-cable-russias-shadow-fleet-suspected%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/29/2055221/finland-finds-drag-marks-near-broken-undersea-cable-russias-shadow-fleet-suspected?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564225&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Crypto industry groups sue IRS over broker reporting rule","url":"https://techcrunch.com/2024/12/29/crypto-industry-groups-sue-irs-over-broker-reporting-rule/","date":1735504854,"author":"Anthony Ha","unread":true,"content":"<p>Three crypto industry groups — the DeFi Education Fund, the Blockchain Association, and the Texas Blockchain Council — are suing the Internal Revenue Service to block new regulations that require decentralized finance (DeFi) entities to report customer information. The IRS has been finalizing crypto tax regulations as part of the Biden Administration’s Infrastructure Investment and [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"'Y2K Seems Like a Joke Now, But in 1999 People Were Freaking Out'","url":"https://it.slashdot.org/story/24/12/29/195226/y2k-seems-like-a-joke-now-but-in-1999-people-were-freaking-out?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735502220,"author":"EditorDavid","unread":true,"content":"NPR remembers when the world \"prepared for the impending global meltdown\" that might've been, on December 31, 1999 &mdash; and the possible bug known as Y2K:\n\nThe Clinton administration said that preparing the U.S. for Y2K was probably \"the single largest technology management challenge in history.\" The bug threatened a cascade of potential disruptions &mdash; blackouts, medical equipment failures, banks shutting down, travel screeching to a halt &mdash; if the systems and software that helped keep society functioning no longer knew what year it was... Computer specialist and grassroots organizer Paloma O'Riley compared the scale and urgency of Y2K prep to telling somebody to change out a rivet on the Golden Gate Bridge. Changing out just one rivet is simple, but \"if you suddenly tell this person he now has to change out all the rivets on the bridge and he has only 24 hours to do it in &mdash; that's a problem,\" O'Riley told reporter Jason Beaubien in 1998.... \n\nThe date switchover rattled a swath of vital tech, including Wall Street trading systems, power plants and tools used in air traffic control. The Federal Aviation Administration put its systems through stress tests and mock scenarios as 2000 drew closer. \"Twenty-three million lines of code in the air traffic control system did seem a little more daunting, I will say, than I had probably anticipated,\" FAA Administrator Jane Garvey told NPR in 1998. Ultimately there were no systemwide aviation breakdowns, but airlines were put on a Y2K alert.... \n\nSome financial analysts remained skeptical Y2K would come and go with minimal disruption. But by November 1999 the Federal Reserve said it was confident the U.S. economy would weather the big switch. \"Federal banking agencies have been visited and inspected. Every bank in the United States, which includes probably 9,000 to 10,000 institutions, over 99% received a satisfactory rating,\" Fed Board Governor Edward Kelley said at the time. \n\nThe article also remembers a California programmer who bought a mobile home, a propane generator, and a year's supply of dehydrated food. (They were also considering buying a handgun &mdash; and converting his bank savings into gold, silver, and cash.) And \"Dozens of communities across the U.S. formed Y2K preparedness groups to stave off unnecessary panic...\" \n\nBut the article concludes that \"the aggressive planning and recalibration paid off. Humanity passed into the year 2000 without pandemonium...\" \n\nAnd \"People like Jack Pentes of Charlotte, N.C., were left to figure out what to do with their emergency stockpiles.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Y2K+Seems+Like+a+Joke+Now%2C+But+in+1999+People+Were+Freaking+Out'%3A+https%3A%2F%2Fit.slashdot.org%2Fstory%2F24%2F12%2F29%2F195226%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fit.slashdot.org%2Fstory%2F24%2F12%2F29%2F195226%2Fy2k-seems-like-a-joke-now-but-in-1999-people-were-freaking-out%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://it.slashdot.org/story/24/12/29/195226/y2k-seems-like-a-joke-now-but-in-1999-people-were-freaking-out?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564159&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"LLaVA-Phi: Limitations and What You Can Expect in the Future","url":"https://hackernoon.com/llava-phi-limitations-and-what-you-can-expect-in-the-future?source=rss","date":1735499714,"author":"Writings, Papers and Blogs on Text Models","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"https://hackernoon.com/preview/0xvVYI3OvJLvwRUYFnhV\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Jo6taCeMb5To49SvqmUy\">2. Related Work</a></p>\n<p><a href=\"https://hackernoon.com/preview/8re4SLMd5eLulCS8byQf\">3. LLaVA-Phi and 3.1. Training</a></p>\n<p><a href=\"https://hackernoon.com/preview/XmeEoU4AZVnfg5r6ymbU\">3.2. Qualitative Results</a></p>\n<p><a href=\"https://hackernoon.com/preview/bk1VFYQ8qOylIqzav05R\">4. Experiments</a></p>\n<p><a href=\"https://hackernoon.com/preview/WwxpDAUyfNGjRGKmxiF4\">5. Conclusion, Limitation, and Future Works and References</a></p>\n<h2 id=\"5conclusionlimitationandfutureworks\">5. Conclusion, Limitation, and Future Works</h2>\n<p>We introduce LLaVA-Phi, a vision language assistant developed using the compact language model Phi-2. Our work demonstrates that such small vision-language models can perform effectively on standard benchmarks when combined with the LLaVA training methodology and a select dataset of high-quality data. The primary goal of our project is to aid the community in creating lightweight, multi-modal models capable of vision-language reasoning, optimized for operation on edge devices. This innovation paves the way for deploying multi-modal assistants in time-sensitive applications, such as robotics [35, 38].</p>\n<p>\\\n<strong>Limitations.</strong> Given that Phi-2 utilizes the codegenmono [29] tokenizer and our model has not been specifically fine-tuned for following multilingual instructions, our LLaVA-Phi architecture is unable to process instructions in multiple languages, including Chinese.</p>\n<p>\\\n<strong>Future Works.</strong> As language models have become significantly smaller in size compared to traditional vision-language models, they have become more accessible and affordable for the research community to explore fundamental concepts in vision-language integration. In future work, we plan to examine the impact of the size of the visual encoder and refine the training strategies for small language models, including approaches like direct preference optimization and RLHF, among other techniques. These efforts aim to further reduce model size while enhancing performance.</p>\n<h2 id=\"references\">References</h2>\n<p>[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022. 1</p>\n<p>\\\n[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 3</p>\n<p>\\\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3</p>\n<p>\\\n[4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 3</p>\n<p>\\\n[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 1</p>\n<p>\\\n[6] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. 2, 3, 4</p>\n<p>\\\n[7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1</p>\n<p>\\\n[8] W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang, B Li, P Fung, and S Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. 1, 3, 4</p>\n<p>\\\n[9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. 3</p>\n<p>\\\n[10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 3, 4</p>\n<p>\\\n[11] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 3</p>\n<p>\\\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017. 3, 4</p>\n<p>\\\n[13] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth ´ Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. 1</p>\n<p>\\\n[14] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608–3617, 2018. 3, 4</p>\n<p>\\\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3</p>\n<p>\\\n[16] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700–6709, 2019. 3</p>\n<p>\\\n[17] H Laurenc¸on, L Saulnier, L Tronchon, S Bekman, A Singh, A Lozhkov, T Wang, S Karamcheti, A Rush, and D Kiela. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527, 2023. 3, 4</p>\n<p>\\\n[18] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 3</p>\n<p>\\\n[19] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. 3</p>\n<p>\\\n[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 3</p>\n<p>\\\n[21] Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie ´ Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. 1</p>\n<p>\\\n[22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. 3, 4</p>\n<p>\\\n[23] Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: achieving¿ 80% on gsm8k with small language models. arXiv preprint arXiv:2312.09241, 2023. 1</p>\n<p>\\\n[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 3</p>\n<p>\\\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 3</p>\n<p>\\\n[26] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023. 1</p>\n<p>\\\n[27] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 3, 4</p>\n<p>\\\n[28] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:2507–2521, 2022. 1, 3, 4</p>\n<p>\\\n[29] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022. 4</p>\n<p>\\\n[30] OpenAI. Gpt-4 technical report. arXiv preprint, 2023. 1</p>\n<p>\\\n[31] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317–8326, 2019. 3, 4</p>\n<p>\\\n[32] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 1</p>\n<p>\\\n[33] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 2, 3</p>\n<p>\\\n[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1</p>\n<p>\\\n[35] Junjie Wen, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, et al. Object-centric instruction augmentation for robotic manipulation. arXiv preprint arXiv:2401.02814, 2024. 4</p>\n<p>\\\n[36] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 3, 4</p>\n<p>\\\n[37] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. arXiv preprint, page arXiv:2304.10592, 2023. 1, 3</p>\n<p>\\\n[38] Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, et al. Language-conditioned robotic manipulation with fast and slow thinking. arXiv preprint arXiv:2401.04181, 2024. 4</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2401.02330\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GPS Is Broken, And It's Holding Tech Back","url":"https://hackernoon.com/gps-is-broken-and-its-holding-tech-back?source=rss","date":1735498819,"author":"Jack Borie","unread":true,"content":"<p>In a world dominated by connectivity, we rely on GPS for everything from navigating city streets to tracking the arrival of our food delivery. But most of us don’t give much thought to how it all works—until it doesn’t. Whether it’s your Uber driver getting lost in a crowded urban area, your delivery package arriving late, or an autonomous vehicle encountering a signal disruption, the vulnerabilities of GPS are becoming clearer.</p>\n<p>\\\nAs we move toward a more connected and automated future, the limitations of GPS are not just an inconvenience; they’re a roadblock to progress. Luckily, a new wave of GPS alternatives is coming, and the impact will be felt by businesses, consumers, and investors alike.</p>\n<p>\\\n<strong>GPS: CRITICAL BUT VULNERABLE</strong></p>\n<p>GPS has served us well, but it has limitations. In both military and commercial applications, GPS signals are vulnerable to jamming, spoofing, and interference, leading to inaccuracies and inefficiencies. For everyday people, this could mean anything from a map taking you to the wrong location to a self-driving car losing its bearings in a tunnel. In dense urban environments, GPS struggles to provide the accuracy needed for precise navigation, especially when signals are blocked by tall buildings or adverse weather.</p>\n<p>\\\nAs we become more dependent on real-time location services in our daily lives—whether through ride-sharing apps, smart home deliveries, or even personal fitness tracking—these vulnerabilities can create real-world frustrations. And as businesses grow increasingly automated and logistics-driven, the stakes become even higher.</p>\n<p>\\\n<strong>THE RISE OF GPS ALTERNATIVES</strong></p>\n<p>The good news? The next generation of navigation technologies is emerging, offering more accurate, secure, and reliable alternatives to GPS. These technologies are not only set to transform industries but will also dramatically improve the consumer experience.</p>\n<p>\\\nU.S. tech company AstraNav has invented a software-based method to provide devices and vehicles with precise location data without relying on GPS, internet, WiFi, or cellular signals. Their patented technology leverages the Earth’s own magnetic fields instead of satellite signals or network connectivity for navigation, tracking, and other location intelligence services. For consumers, this would give turn-by-turn directions indoors, underground, or in places where GPS signals are traditionally weak or non-existent. Imagine navigating a massive shopping mall or a subway system without losing your way because of poor GPS signals. Or, think about how your phone could guide you through multi-story buildings or crowded areas with pinpoint precision. Governments can also deploy a layer of secure, reliable location services as a backup to satellite constellations, which are notoriously vulnerable to jamming and spoofing.</p>\n<p>\\\nAnother exciting development comes from the world of quantum mechanics. Researchers at Imperial College London are working on the “quantum compass,” a navigation system that doesn’t rely on external signals. Using subatomic particles, this technology can determine location even in underground tunnels or underwater. This could revolutionize industries like mining, oil and gas, and urban infrastructure, but it also has massive implications for everyday users. Consumers could soon have devices that work flawlessly in environments where GPS fails—think deep in a subway system or a rural area far from cell towers.</p>\n<p>\\\nThen there’s Xona Space Systems, which is developing a satellite constellation known as Pulsar©. This system aims to provide centimeter-level accuracy, far exceeding what GPS can offer today. For consumers and businesses alike, this level of accuracy will have immediate benefits. Think about the rise of autonomous delivery drones or driverless taxis—these services will depend on near-perfect geolocation to operate safely and efficiently. The Pulsar system could ensure that your next grocery delivery arrives at your doorstep with precision, no matter how dense or remote your neighborhood may be.</p>\n<p>\\\n<strong>WHY IT MATTERS FOR CONSUMERS, BUSINESSES, AND INVESTORS</strong></p>\n<p>What’s exciting about these developments is that they’re not just about improving navigation for businesses—they represent a critical shift in how companies can secure their operations and ensure business continuity. As a business leader, it’s essential to understand that this isn’t just about benefits; it’s about staying ahead of potential disruptions and vulnerabilities.</p>\n<p>\\\nAs our reliance on location-based services grows—with applications in autonomous vehicles, drone deliveries, and smart cities—the risks associated with outdated GPS systems are also rising. What happens if your logistics operations face interference, or your autonomous systems lose accuracy due to signal jamming? Emerging alternatives provide new layers of security, precision, and reliability.</p>\n<p>\\\nTo prepare, business leaders need to start by reassessing their current dependencies on GPS and similar systems. Here’s what you can do:</p>\n<ul>\n<li><p><strong>Invest In Resilient Navigation Technologies</strong>: Look into alternatives like M-GPS®, quantum navigation, or advanced satellite systems that offer more reliable and secure services.</p></li>\n<li><p><strong>Conduct Risk Assessments</strong>: Evaluate the vulnerabilities in your current operations tied to GPS and location-based services, especially in environments prone to signal disruption.</p></li>\n<li><p><strong>Diversify Your Navigation Infrastructure</strong>: Ensure you have backup systems in place that can handle failures or disruptions in traditional GPS.</p></li>\n<li><p><strong>Collaborate With Tech Providers</strong>: Stay in close contact with providers of these emerging technologies to ensure you’re integrating the latest advancements as they become available.</p>\n<p>\\</p></li>\n</ul>\n<p>By taking proactive steps today, businesses can ensure they’re not just reacting to changes in navigation technologies but leading with solutions that secure their future operations.</p>\n<p>\\\n<strong>A NEW ERA IN NAVIGATION</strong></p>\n<p>The next decade will witness a radical transformation in how we navigate both the physical and digital worlds. For businesses, consumers, and investors, now is the time to start paying attention to these emerging technologies. The companies developing GPS alternatives like M-GPS® and quantum compasses will shape the future of location services and geospatial technology.</p>\n<p>\\\nThis isn’t just about a better GPS—it’s about creating a world where our devices work seamlessly, no matter where we are. Whether you’re an investor looking for the next big thing, a business leader seeking an edge in logistics, or a consumer frustrated by your GPS cutting out at the worst possible moment, the future of navigation is coming fast, and it’s worth keeping your eyes on the horizon.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"LLaVA-Phi: Qualitative Results - Take A Look At Its Remarkable Generelization Capabilities","url":"https://hackernoon.com/llava-phi-qualitative-results-take-a-look-at-its-remarkable-generelization-capabilities?source=rss","date":1735498816,"author":"Writings, Papers and Blogs on Text Models","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"https://hackernoon.com/preview/0xvVYI3OvJLvwRUYFnhV\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Jo6taCeMb5To49SvqmUy\">2. Related Work</a></p>\n<p><a href=\"https://hackernoon.com/preview/8re4SLMd5eLulCS8byQf\">3. LLaVA-Phi and 3.1. Training</a></p>\n<p><a href=\"https://hackernoon.com/preview/XmeEoU4AZVnfg5r6ymbU\">3.2. Qualitative Results</a></p>\n<p><a href=\"https://hackernoon.com/preview/bk1VFYQ8qOylIqzav05R\">4. Experiments</a></p>\n<p><a href=\"https://hackernoon.com/preview/WwxpDAUyfNGjRGKmxiF4\">5. Conclusion, Limitation, and Future Works and References</a></p>\n<h2 id=\"32qualitativeresults\">3.2. Qualitative Results</h2>\n<p>We present several examples that demonstrate the remarkable generalization capabilities of LLaVA-Phi, comparing its outputs with those of the LLaVA-1.5-13B models. In Figure 1, a meme is displayed, and we ask the visionlanguage assistant to explain why this meme is considered humorous. While LLaVA-1.5-13B provides a reasonable interpretation based on the image, LLaVA-Phi’s response is more empathetic, highlighting the humor by associating the dog’s ’laid-back demeanor’ with the ’stress or fatigue’ typically associated with a ’new workweek’.</p>\n<p>\\\nIn the second example, we instructed the model to generate Python code for converting an Excel table into a bar chart, as illustrated in Figure 2. LLaVA-1.5-13B generated a simplistic code snippet that only reads the table and prints it, diverging from the instructions to create a plot. In contrast, LLaVA-Phi accurately comprehended the task, providing instructions to read the table, add a title and labels, and correctly plot the bar chart using matplotlib. We believe this enhanced code generation capability stems from Phi-2, which was pre-trained on a large corpus of code snippets and is primarily used for code generation.</p>\n<p>\\\nThe third challenge involves solving a simple math problem, requiring the model to accurately recognize text through OCR and then perform the necessary mathematical computations, as shown in Figure 3. LLaVA-1.5-13B, while providing a step-by-step computation based on the image, incorrectly recognized the numbers and mathematical symbols. In contrast, our proposed LLaVA-Phi, without providing a chain-of-thought reasoning, still produces the correct answer. Our quantitative results on ScienceQA further confirm that LLaVA-Phi excels in these types of questionanswering tasks.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2401.02330\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"BTC vs. BTC—What Saifedean Ammous' Upcoming Book Gets Right (And Wrong!)","url":"https://hackernoon.com/btc-vs-btcwhat-saifedean-ammous-upcoming-book-gets-right-and-wrong?source=rss","date":1735498811,"author":"M-Marvin Ken","unread":true,"content":"<p>\\\nDr. Saifedean Ammous has authored the most read Bitcoin book in the world - <strong>The Bitcoin Standard</strong>.</p>\n<p>\\\nNow, he is preparing to unleash his dragons onto the fictional scene with <strong>The Gold Standard</strong>.</p>\n<p>\\\nFor more information about this next book, check out his <a href=\"https://x.com/saifedean/status/1866463783385755677\">tweet</a>. He provides the first 4 chapters free at his website and the book is available for pre-order, to be availed in March 2025.</p>\n<p>\\\nBut March 2025 is too far away for me :sweat_smile:.</p>\n<p>\\\nSo here I go, pitting BTC against BTC.</p>\n<p>\\</p>\n<h2 id=\"1thepollutionproblem\">1. The Pollution Problem</h2>\n<p><img src=\"https://cdn.hackernoon.com/images/EHUYZRLLAUbgArGtd43kEIxTOTS2-fr03m2d.jpeg\" alt=\"Aeroplanes polluting a high-tech BTC city\" /></p>\n<p>\\\n\\\nI’ll start right off the bat by stating the obvious.</p>\n<p>\\\nThe Blériot Transportation Company (BTC) and its Aeroplane Revolution, will quickly pollute the world at levels that would dwarf the factory pollution of Britain in its 19th century Industrial Revolution.</p>\n<p>\\\nAt peak Aeroplane Revolution, I cannot fathom the noise, especially in very busy capital-clearing areas.</p>\n<p>\\\nToday, <a href=\"https://www.oag.com/airline-frequency-and-capacity-statistics\">more than 100,000 flights</a> are taken every day. Considering an average of 100 passengers per flight, more than 10 million people are flying every day.</p>\n<p>But that is all in our imperfect fiat world.</p>\n<p>\\\nHow about if we also had gold clearance services by a 100-year-old BTC system, running concurrently?</p>\n<p>\\\nWell, people perform payments more frequently than they do flights, so there would be more than 100,000 <em>golden flights</em> happening daily as well.</p>\n<p>Granted. I would be happier and wealthier.</p>\n<p>But would those planes be so good for our climate problems and respiratory health?</p>\n<p>I don’t think so.</p>\n<p>\\\nAnd in the unlikely event of a golden plane crash, gold bars falling from the sky add complications to the entire debacle. </p>\n<p>Also, some crazy person would be motivated to shoot one of these golden birds down.</p>\n<p>\\\nIf Satoshi ever surfaced to challenge that BTC system, inviting it to upgrade to Bitcoin, I believe he would be welcomed with open arms.</p>\n<p>Bitcoin is highly energy efficient compared to the Blériot Transportation Company. With next to no pollution. Nor sleep lost over whether any pirate will shoot the plane down.</p>\n<p>Bitcoin keys, which is all you need, are always safe on a piece of paper in the drawer.</p>\n<p>\\</p>\n<h2 id=\"2nexttonoscammers\">2. Next to no Scammers</h2>\n<p>\\\nI’ll give this to the world of the Blériot Transportation Company. I really do not see how scammers, with keyboard-warrior fingers and only underwear, would rob any self-dignified man or woman of their BTC gold.</p>\n<p>\\\nThat world would have more tough hombres. Men and Women alike.</p>\n<p>\\\nHere’s the thing about gold: it inspires confidence in a way that comparative digital wealth does not.</p>\n<p>At least, that’s what all the pirate stories I have read tell me.</p>\n<p>Even the term ‘Spanish Conquistador’ has a fierceness that ‘Bitcoin Maximalist’ just doesn’t match.</p>\n<p>\\\nTo seal this deal, look at all these movie titles on gold:</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EHUYZRLLAUbgArGtd43kEIxTOTS2-7q13mq6.jpeg\" alt=\"\" /></p>\n<p>All created in a world <em>without</em> the Blériot Transportation Company.</p>\n<p>\\\nNow look at movies about bitcoin, crypto and digital money.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EHUYZRLLAUbgArGtd43kEIxTOTS2-3223mwq.jpeg\" alt=\"\" /></p>\n<p>\\\n3 search terms, but still, not much could be found.</p>\n<p>\\\nI guess that settles it.</p>\n<p>\\\nYou also won’t get the below muscle mass if all your wealth can easily be carried around on a flash drive.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EHUYZRLLAUbgArGtd43kEIxTOTS2-id33mva.jpeg\" alt=\"\" /></p>\n<p>On a BTC gold standard, we would definitely be back on the moon by now.</p>\n<p>\\\nWith some gold bars in tow.</p>\n<p>\\\n\\\n\\\n\\\n\\\n\\\n\\\n\\\n\\\n\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"'Did Anything Good Happen in 2024? Actually, Yes!'","url":"https://science.slashdot.org/story/24/12/29/030248/did-anything-good-happen-in-2024-actually-yes?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735498620,"author":"EditorDavid","unread":true,"content":"The Washington Post shares some good news from 2024:\n\nResearchers were able to detect a significant dip in atmospheric levels of hydrochlorofluorocarbons &mdash; harmful gases that deplete the ozone layer &mdash; for the first time, almost 30 years after countries first agreed to phase out the chemicals. \n\nA new satellite launched in March to track and publicly reveal the biggest methane polluters in the oil and gas industry &mdash; an important step in tackling the greenhouse gas that accounts for almost a third of global warming. The NASA/Carbon Mapper satellite, which measures CO2 and methane emissions, also launched, providing detailed images from individual oil and gas facilities across the world. \n\nBack on Earth, the world's largest plant for pulling carbon dioxide out of the atmosphere opened in Iceland. Norway became the first country to have more electric than gas-powered vehicles, while one Japanese island began using a new generation of batteries to help stockpile massive amounts of clean electricity. \n\nThere were also small but important victories for animal conservation. The Iberian lynx, a European wildcat once on the brink of extinction, is no longer classed as an \"endangered\" species &mdash; in what experts have hailed as the \"greatest recovery of a cat species ever achieved through conservation....\" \n\nDespite a large number of powerful tornadoes to hit the United States in early 2024, the death tolls were fortunately not as high as meteorologists feared, in part due to improved forecasting technology. \n\nThe article also notes America's Food and Drug Administration approved a new therapy which uses a patients' own cells to attack skin cancer for adults for whom surgery isn't an option. \"Experts said the decision could open the door to similar treatments for far more common cancers.\" \n\nAnd one more inspiring story from 2024: 105-year-old Virginia Hislop, of Yakima, Washington received her master's degree from Stanford University...<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status='Did+Anything+Good+Happen+in+2024%3F++Actually%2C+Yes!'%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F030248%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F030248%2Fdid-anything-good-happen-in-2024-actually-yes%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/030248/did-anything-good-happen-in-2024-actually-yes?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23563775&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"LLaVA-Phi: How We Rigorously Evaluated It Using an Extensive Array of Academic Benchmarks","url":"https://hackernoon.com/llava-phi-how-we-rigorously-evaluated-it-using-an-extensive-array-of-academic-benchmarks?source=rss","date":1735496111,"author":"Writings, Papers and Blogs on Text Models","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"https://hackernoon.com/preview/0xvVYI3OvJLvwRUYFnhV\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Jo6taCeMb5To49SvqmUy\">2. Related Work</a></p>\n<p><a href=\"https://hackernoon.com/preview/8re4SLMd5eLulCS8byQf\">3. LLaVA-Phi and 3.1. Training</a></p>\n<p><a href=\"https://hackernoon.com/preview/XmeEoU4AZVnfg5r6ymbU\">3.2. Qualitative Results</a></p>\n<p><a href=\"https://hackernoon.com/preview/bk1VFYQ8qOylIqzav05R\">4. Experiments</a></p>\n<p><a href=\"https://hackernoon.com/preview/WwxpDAUyfNGjRGKmxiF4\">5. Conclusion, Limitation, and Future Works and References</a></p>\n<h2 id=\"4experiments\">4. Experiments</h2>\n<p>We rigorously evaluated LLaVA-Phi using an extensive array of academic benchmarks specifically designed for multi-modal models. These included tests for general question-answering such as VQA-v2 [12], VizWizQA [14], ScienceQA [28], and TextQA [31], as well as more specialized assessments like POPE [22] for evaluating object hallucination, and MME [10], MMBench [27], and MMVet [36] for a comprehensive evaluation of diverse multi-modal abilities, such as visual understanding and visual commonsense reasoning.</p>\n<p>\\\nThese benchmarks are meticulously structured to challenge and scrutinize complex multi-modal tasks. We benchmarked LLaVA-Phi against a variety of state-of-the-art, large vision-language models, as detailed in Table 1. It is important to note that both our method and LLaVA1.5 utilize the same publicly available datasets for pre-training and visual instruction fine-tuning.</p>\n<p>\\\nOur model demonstrated a capacity for visual-based question-answering, surpassing many existing large multimodal models. Remarkably, LLaVA-Phi outperformed models that use 7B-parameter or larger Large Language Models (LLMs) as their backbone, such as IDEFICS [17] and InstructBLIP [8]. A particularly notable achievement was our model’s best performance on ScienceQA [28]. We attribute this success to the Phi-2 language model, which has been specifically trained on code generation and mathematical corpora, thereby enhancing our multi-modal model’s prowess in math-based question-answering.</p>\n<p>\\\nIn the comprehensive multi-modal benchmark of MMBench [27], LLaVA-Phi showed significantly superior performance compared to many existing 7B-LLM-based vision-language models. For example, our model outperformed Otter by 11.5% and InstructBLIP by 23.8%. This underscores the effectiveness of LLaVA-Phi in handling complex multi-modal tasks, reinforcing the potential of smaller, more efficient models in the rapidly evolving field of multi-modal models.</p>\n<p>\\\nWe also compared to MobileVLM [6], a concurrent work that builds up an efficient vision-language model. Across all five benchmarks, our LLaVA-Phi consistently outperforms their method. It’s important to note that the margins of lead are modest, with the exception of ScienceQA. We attribute this performance disparity primarily to the differences in the pretraining stages of the language models.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2401.02330\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Yichen Zhu, Midea Group;</p>\n<p>(2) Minjie Zhu, Midea Group and East China Normal University;</p>\n<p>(3) Ning Liu, Midea Group;</p>\n<p>(4) Zhicai Ou, Midea Group;</p>\n<p>(5) Xiaofeng Mou, Midea Group.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Evaluating vLLM With Basic Sampling","url":"https://hackernoon.com/evaluating-vllm-with-basic-sampling?source=rss","date":1735488913,"author":"Writings, Papers and Blogs on Text Models","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/UZfnXsE8xpuPm7xJGzSd\">Abstract and 1 Introduction</a></p>\n<p><a href=\"https://hackernoon.com/preview/Joq6DOP9CkD0S4F8MX1r\">2 Background and 2.1 Transformer-Based Large Language Models</a></p>\n<p><a href=\"http://hackernoon.com/preview/25g0m9nrxol991ZY9qci\">2.2 LLM Service & Autoregressive Generation</a></p>\n<p><a href=\"http://hackernoon.com/preview/IQ9Fd8hlh5MpHVMFA27O\">2.3 Batching Techniques for LLMs</a></p>\n<p><a href=\"https://hackernoon.com/preview/ytSMq2pxVtKIRC7iS3kK\">3 Memory Challenges in LLM Serving</a></p>\n<p><a href=\"http://hackernoon.com/preview/6BZmg60VAii9DNx0L1qc\">3.1 Memory Management in Existing Systems</a></p>\n<p><a href=\"http://hackernoon.com/preview/ZzGN1QRkg16N6zN7hCJi\">4 Method and 4.1 PagedAttention</a></p>\n<p><a href=\"http://hackernoon.com/preview/4Da8juHWnURn6g4urrl4\">4.2 KV Cache Manager</a></p>\n<p><a href=\"http://hackernoon.com/preview/GieTLdxmIGxSHDideqV5\">4.3 Decoding with PagedAttention and vLLM</a></p>\n<p><a href=\"http://hackernoon.com/preview/7g3isP8BzgyEsTasNUr2\">4.4 Application to Other Decoding Scenarios</a></p>\n<p><a href=\"http://hackernoon.com/preview/TEjiBAga2TDQ1ZZp5Vbg\">4.5 Scheduling and Preemption</a></p>\n<p><a href=\"http://hackernoon.com/preview/OjYLeVPi5jM2NSWKvipy\">4.6 Distributed Execution</a></p>\n<p><a href=\"http://hackernoon.com/preview/yqADDmfsCggYCcSdA6Nj\">5 Implementation</a></p>\n<p><a href=\"http://hackernoon.com/preview/S6sF02nTOzlXKZE1iQcP\">6 Evaluation and 6.1 Experimental Setup</a></p>\n<p><a href=\"http://hackernoon.com/preview/K3TeA7MNOi0g142ICu5W\">6.2 Basic Sampling</a></p>\n<p><a href=\"http://hackernoon.com/preview/cJxKQHExtKQrburnndpo\">6.3 Parallel Sampling and Beam Search</a></p>\n<p><a href=\"http://hackernoon.com/preview/JEOm7WvIfRfxEjepPqXW\">6.4 Shared prefix</a></p>\n<p><a href=\"http://hackernoon.com/preview/DWk5D3KHLfKhdQC02ayZ\">6.5 Chatbot</a></p>\n<p><a href=\"http://hackernoon.com/preview/l7VvLlkEDSBJ5GzMlwJf\">7 Ablation Studies</a></p>\n<p><a href=\"http://hackernoon.com/preview/PJb0S41IDQAbwYp0e6RZ\">8 Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/5d2sL9hRMUBNmVCZWMBC\">9 Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/HL77hmYOoM9MPB5fKgmq\">10 Conclusion, Acknowledgement and References</a></p>\n<h2 id=\"62basicsampling\">6.2 Basic Sampling</h2>\n<p>We evaluate the performance of vLLM with basic sampling (one sample per request) on three models and two datasets. The first row of Fig. 12 shows the results on the ShareGPT dataset. The curves illustrate that as the request rate increases, the latency initially increases at a gradual pace but then suddenly explodes. This can be attributed to the fact that when the request rate surpasses the capacity of the serving system, the queue length continues to grow infinitely and so does the latency of the requests.</p>\n<p>\\\nOn the ShareGPT dataset, vLLM can sustain 1.7×–2.7× higher request rates compared to Orca (Oracle) and 2.7×–8× compared to Orca (Max), while maintaining similar latencies. This is because vLLM’s PagedAttention can efficiently manage the memory usage and thus enable batching more requests than Orca. For example, as shown in Fig. 13a, for OPT-13B vLLM processes 2.2× more requests at the same time than Orca (Oracle) and 4.3× more requests than Orca (Max). Compared to FasterTransformer, vLLM can sustain up to 22× higher request rates, as FasterTransformer does not utilize a fine-grained scheduling mechanism and inefficiently manages the memory like Orca (Max).</p>\n<p>\\\nThe second row of Fig. 12 and Fig. 13b shows the results on the Alpaca dataset, which follows a similar trend to the ShareGPT dataset. One exception is Fig. 12 (f), where vLLM’s advantage over Orca (Oracle) and Orca (Pow2) is less pronounced. This is because the model and server configuration for OPT-175B (Table 1) allows for large GPU memory space available to store KV cache, while the Alpaca dataset has short sequences. In this setup, Orca (Oracle) and Orca (Pow2) can also batch a large number of requests despite the inefficiencies in their memory management. As a result, the performance of the systems becomes compute-bound rather than memory-bound.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ql830zk.png\" alt=\"Figure 15. Average amount of memory saving from sharing KV blocks, when serving OPT-13B for the Alpaca trace.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2309.06180\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Woosuk Kwon, UC Berkeley with Equal contribution;</p>\n<p>(2) Zhuohan Li, UC Berkeley with Equal contribution;</p>\n<p>(3) Siyuan Zhuang, UC Berkeley;</p>\n<p>(4) Ying Sheng, UC Berkeley and Stanford University;</p>\n<p>(5) Lianmin Zheng, UC Berkeley;</p>\n<p>(6) Cody Hao Yu, Independent Researcher;</p>\n<p>(7) Cody Hao Yu, Independent Researcher;</p>\n<p>(8) Joseph E. Gonzalez, UC Berkeley;</p>\n<p>(9) Hao Zhang, UC San Diego;</p>\n<p>(10) Ion Stoica, UC Berkeley.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The HackerNoon Newsletter: Heres Why High Achievers Feel Like Failures (12/29/2024)","url":"https://hackernoon.com/12-29-2024-newsletter?source=rss","date":1735488338,"author":"Noonification","unread":true,"content":"\n              \n        <p><strong>How are you, hacker?</strong></p>\n        <br />\n        <p>🪐 What’s happening in tech today, December 29, 2024?</p>\n        <br />\n        <p>\n          The\n          <a href=\"https://hackernoon.com/noonification\" target=\"_blank\" rel=\"noopener\"> HackerNoon Newsletter</a>\n          brings the HackerNoon \n          <a href=\"https://hackernoon.com\" target=\"_blank\" rel=\"noopener\">homepage</a>\n          straight to your inbox.\n          <a href=\"https://hackernoon.com/on-this-day\" target=\"_blank\" rel=\"noopener\">On this day,</a>\n          \n            <strong>The HMS Warrior is Launched</strong> in 1860,  <strong>The Apex of the Japanese Asset Price Bubble Occured</strong> in 1989,  <strong> The UK settled its Anglo-American, post WWII loan debt</strong> in 2006, \n          \n          and  we present you with these top quality stories. \n          \n            From \n        <a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\" class=\"eventTitle\"><strong>Future-proof Your Marketing With This Guide on Writing for AI Search Engines</strong></a>\n       to \n        <a href=\"https://hackernoon.com/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum\" class=\"eventTitle\"><strong>Data Availability Or: How Rollups Learned To Stop Worrying And Love Ethereum</strong></a>,\n       let’s dive right in.\n          \n        </p>\n      \n              \n          <h2><a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">RootstockCollective In-Depth: Empowering Bitcoin Builders</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/InxBRjRIs6M1kdhuWcyNHiiUrxm1-j5034qi.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/rootstock_io\">@rootstock_io</a> [ 7 Min read ] Empowering Bitcoin builders with RootstockCollective DAO: Rewarding innovation, stakers, and developers in the Bitcoin sidechain ecosystem. <a href=\"https://hackernoon.com/rootstockcollective-in-depth-empowering-bitcoin-builders\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Join Lumoz zkVerifier Node Mining and Share 2.5 Billion MOZ Rewards</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/Wls6TtjOLGMbl8aKwQlIbcyfjQF2-cx03ymq.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 4 Min read ] Lumoz Node Network  MOZ staking are live! Join the zkVerifier network, stake MOZ, or run nodes to share 25% of $90M in rewards. Act now! <a href=\"https://hackernoon.com/join-lumoz-zkverifier-node-mining-and-share-25-billion-moz-rewards\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\">Future-proof Your Marketing With This Guide on Writing for AI Search Engines</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/R40xrKHcy9QXU6NDkd58YY2mQOz1-e313j4y.webp\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/darragh\">@darragh</a> [ 4 Min read ] Learn how to write for AI  search engines with actionable tips, examples,  FAQs. Future-proof your content for ChatGPT, Gemini,  Google SEO success! <a href=\"https://hackernoon.com/future-proof-your-marketing-with-this-guide-on-writing-for-ai-search-engines\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/the-sneaky-way-web-browsers-are-identifying-you-even-when-you-turn-off-cookies\">The Sneaky Way Web Browsers Are Identifying You (Even When You Turn Off Cookies)</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/lvbwxpoO4WXH8MNqfBFKapMQkAi2-1d0370t.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/rampageproxies\">@rampageproxies</a> [ 12 Min read ] A guide on browser fingerprinting, how it identifies us, fingerprint testing, and what techniques we can use to ensure our browsing is kept anonymous. <a href=\"https://hackernoon.com/the-sneaky-way-web-browsers-are-identifying-you-even-when-you-turn-off-cookies\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Harnessing Shared Security For Secure Cross-Chain Interoperability</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-l903680.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 47 Min read ] A deep dive on shared security and the role of shared security infrastructure in building robust and secure cross-chain interoperability solutions for users. <a href=\"https://hackernoon.com/harnessing-shared-security-for-secure-cross-chain-interoperability\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum\">Data Availability Or: How Rollups Learned To Stop Worrying And Love Ethereum</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/WSQJfCSXOxWphTNQ7sneVvhdWGu1-hd036k1.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 27 Min read ] Data availability is a critical component of scaling Ethereum. Learn how and why Layer 2 rollups use Ethereum for data availability—and why this matters.  <a href=\"https://hackernoon.com/data-availability-or-how-rollups-learned-to-stop-worrying-and-love-ethereum\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/heres-why-high-achievers-feel-like-failures\">Heres Why High Achievers Feel Like Failures</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/standing-on-top-of-a-mountain-d0kxvq2p6k7msfxxmb3n6r8m.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/scottdclary\">@scottdclary</a> [ 11 Min read ] The invisible progress paradox is a trap in personal growth. <a href=\"https://hackernoon.com/heres-why-high-achievers-feel-like-failures\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/goodbye-passwords-hello-passkeys-the-future-of-authentication\">Goodbye Passwords, Hello Passkeys: The Future of Authentication </a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/security-symbol-on-a-laptop-screen-e9ol03dr3i89katj0uwpomzi.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/radioactive\">@radioactive</a> [ 6 Min read ] Discover how passkeys revolutionize online authentication.  <a href=\"https://hackernoon.com/goodbye-passwords-hello-passkeys-the-future-of-authentication\">Read More.</a></p>\n        \n              \n        <br />\n        <p>🧑‍💻 What happened in your world this week?</p>\n        <p>\n          It's been said that\n          <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>.\n          Feeling stuck? We got you covered ⬇️⬇️⬇️\n        </p>\n        <br />\n        <p>\n          <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n        </p>\n        <br />\n        <p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>\n        <br />\n        <p><img src=\"https://cdn.hackernoon.com/images/the-hackernoon-newsletter-footer.png\" alt /></p>\n      \n            ","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Kdenlive Preparing For An Exciting 2025 With Background Removal Tool & More","url":"https://www.phoronix.com/news/Kdenlive-Background-Removal-25","date":1735485314,"author":"Michael Larabel","unread":true,"content":"The KDE app Kdenlive that is a very popular and featureful open-source video editor is preparing for an exciting 2025...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GPS Spoofing Attacks Are Dangerously Misleading Airliners","url":"https://spectrum.ieee.org/gps-spoofing-2670499105","date":1735484403,"author":"Margo Anderson","unread":true,"content":"<p>Electronic warfare is taking a perilous turn into civilian airspace</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzgwMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0ODQyNTA2NX0.5iWnSeL_UGfAyzZ6VJZNQQrm6u3ONzKQipV7JL7Ylhg/image.png?width=600","enclosureMime":""},{"title":"Africa’s newest fintech unicorns are winning by keeping their feet on the ground","url":"https://techcrunch.com/2024/12/29/africas-fintech-unicorns-blend-digital-banking-and-physical-touchpoints/","date":1735484400,"author":"Tage Kene-Okafor","unread":true,"content":"<p>Africa’s tech ecosystem just got a boost of attention, with South Africa’s TymeBank and Nigeria’s Moniepoint both raising funds in recent weeks at valuations of over $1 billion and joining the coveted unicorn pantheon. But those valuations don’t just reflect investor confidence. They signal the success they’ve had in taking disruptive fintech models originally developed [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Faster USB Performance For xHCI DbC Coming With Linux 6.14 Plus A 10 Year Old Bug Fixed","url":"https://www.phoronix.com/news/Faster-USB-xHCI-DbC-Linux-6.14","date":1735483524,"author":"Michael Larabel","unread":true,"content":"Thanks to work from Intel engineers, the upcoming Linux 6.14 kernel cycle will feature faster USB xHCI DbC performance for debug performance and a few other missing xHCI bits being addressed. Plus there is a fix for a rare 10 year old USB bug report...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Top 7 Robotics Stories of 2024","url":"https://spectrum.ieee.org/top-robotics-stories-2024","date":1735480802,"author":"Evan Ackerman","unread":true,"content":"<p>A new Atlas, Figure's bonkers funding round, and the last voyage of a helicopter on Mars</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzY0OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NTQ4MzExMX0.3bca1xoiyaOOgYmcqPZ7K3Izu2FimblAkRoTgP96l7M/image.jpg?width=600","enclosureMime":""},{"title":"Permira’s Brian Ruder talks AI, Squarespace acquisition, and the value of co-leadership","url":"https://techcrunch.com/2024/12/29/permiras-brian-ruder-talks-ai-squarespace-acquisition-and-the-value-of-co-leadership/","date":1735480800,"author":"Paul Sawers","unread":true,"content":"<p>It has been a busy year in the private equity realm, with countless big-money acquisitions unfolding. The take-private space specifically has seen some sizable transactions, with private equity firms spearheading more than a dozen billion-dollar deals for public tech companies. London-headquartered Permira was a key protagonist, joining Blackstone to acquire European online classifieds group Adevinta [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Ubuntu's Great Year From 24.04 LTS To Focusing More On Performance Optimizations","url":"https://www.phoronix.com/news/Ubuntu-2024-Great-Year","date":1735473669,"author":"Michael Larabel","unread":true,"content":"From my independent monitoring, Ubuntu Linux had a pretty great year. Ubuntu 24.04 LTS shipped and has been well received across enterprises, Canonical engineers have been focusing more on performance optimizations for Ubuntu, and there has been other interesting changes like their new commitment to always ship the latest upstream Linux kernel version as of Ubuntu release time. Plus they have continued with various GNOME desktop improvements, Ubuntu on servers continues with steady traction, and all-around was a pretty exciting year for the Ubuntu camp...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Fish Shell Outlines Their Successes & Challenges Going From C++ To Rust","url":"https://www.phoronix.com/news/Fish-Shell-Rust-Challenges","date":1735473213,"author":"Michael Larabel","unread":true,"content":"Earlier this month the Fish Shell 4.0 went into beta with the C++ code ported to Rust. Now with most of the Fish Shell code transitioned to Rust, the project put out a blog post this weekend outlining the successes and challenges they have encountered in porting their large C++ codebase to Rust...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Apple DWI Backlight Linux Driver Updated For Various iPhones, iPods & iPads","url":"https://www.phoronix.com/news/Apple-DWI-Backlight-Linux-v4","date":1735472940,"author":"Michael Larabel","unread":true,"content":"While Linux 6.13 is introducing basic support for various Apple iPads and iPhones using A-series SoCs, the support is just that: basic. Various feature limitations remain for those dreaming over the prospects of running Linux on older Apple mobile devices. One of various feature limitations remaining are around backlight control for different models and for that there is the Apple DWI backlight driver for Linux that continues to be hacked on...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Linux 6.13-rc5 To See Fix For Intel TDX CoCo VMs Potentially Leaking Decrypted Memory","url":"https://www.phoronix.com/news/Linux-6.13-Fixing-TDX-CoCo-Leak","date":1735471447,"author":"Michael Larabel","unread":true,"content":"The x86 fixes pull request was sent out this morning ahead of the Linux 6.13-rc5 kernel being released later today. Both x86 fixes this week pertain to Intel bits: a self-test issue on upcoming Intel FRED (Flexible Return and Event Delivery) systems and also an issue of Intel TDX confidential computing VM guests potentially leaking decrypted memory within the unrecoverable error handling...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Google CEO says AI model Gemini will be the company’s ‘biggest focus’ in 2025","url":"https://techcrunch.com/2024/12/28/google-ceo-says-ai-model-gemini-will-the-companys-biggest-focus-in-2025/","date":1735423241,"author":"Anthony Ha","unread":true,"content":"<p>CEO Sundar Pichai reportedly told Google employees that 2025 will be a “critical” year for the company. CNBC reports that it obtained audio from a December 18 strategy meeting where Pichai and other executives put on ugly holiday sweaters and laid out their priorities for the coming year. “I think 2025 will be critical,” Pichai [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Trump asks Supreme Court to pause imminent TikTok ban","url":"https://techcrunch.com/2024/12/28/trump-asks-supreme-court-to-pause-imminent-tiktok-ban/","date":1735405241,"author":"Anthony Ha","unread":true,"content":"<p>Attorneys representing President-elect Donald Trump have asked the Supreme Court to pause a law that would force TikTok-owner ByteDance to sell the short-form video app or see it banned from the United States. If the app isn’t sold, the ban is set to take effect in just a few weeks, on January 19. ByteDance is [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"NVIDIA Made Great Strides With Their Open-Source Kernel Code & Wayland Support In 2024","url":"https://www.phoronix.com/news/NVIDIA-2024-Linux-Highlights","date":1735403820,"author":"Michael Larabel","unread":true,"content":"This year NVIDIA's official Linux graphics driver enjoyed much more robust Wayland support, their open-source kernel modules have matured greatly and are now being used by default, and their proprietary Vulkan and OpenGL drivers remain in good standing for performant Linux gaming and workstation graphics. NVIDIA's Linux driver stack had a rather great year...","flags":null,"enclosureUrl":"","enclosureMime":""}]}