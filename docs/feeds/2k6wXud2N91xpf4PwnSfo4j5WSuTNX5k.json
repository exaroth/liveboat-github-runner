{"id":"2k6wXud2N91xpf4PwnSfo4j5WSuTNX5k","title":"Tech News - Last 2 days","displayTitle":"Tech News - Last 2 days","url":"","feedLink":"","is_query":true,"items":[{"title":"Mercedes-backed Volocopter Files for Bankruptcy","url":"https://tech.slashdot.org/story/24/12/30/1555238/mercedes-backed-volocopter-files-for-bankruptcy?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735581720,"author":"msmash","unread":true,"content":"German electric air taxi company Volocopter has filed for bankruptcy protection, the latest in a string of similar startups to hit financial turbulence. From a report: Volocopter is one of the more well-funded electric air taxi startups, having raised hundreds of millions of dollars over nearly a decade with backing from major automakers like Germany's Mercedes-Benz and China's Geely.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Mercedes-backed+Volocopter+Files+for+Bankruptcy%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F1555238%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F1555238%2Fmercedes-backed-volocopter-files-for-bankruptcy%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/30/1555238/mercedes-backed-volocopter-files-for-bankruptcy?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564825&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Intel's Linux Performance Optimizations Continue Paying Off For AMD EPYC","url":"https://www.phoronix.com/review/epyc-genoa-2year-linux","date":1735579800,"author":"Michael Larabel","unread":true,"content":"As part of my end-of-year benchmarking and various historical comparisons, over the holidays I was curious to take a look at how the mature AMD EPYC 9004 \"Genoa\" performance has evolved over the past two years under Linux. Going off benchmarks I ran back at the end of 2022 on the same AMD Titanite EPYC reference server platform for two EPYC 9654 Genoa processors, I repeated the same tests using the newest releases of Intel Clear Linux and Ubuntu Linux for seeing how the performance has evolved.","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Google CEO Warns of High Stakes in 2025 AI Race","url":"https://tech.slashdot.org/story/24/12/30/1113226/google-ceo-warns-of-high-stakes-in-2025-ai-race?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735579200,"author":"msmash","unread":true,"content":"Google CEO Sundar Pichai has warned employees the company faces critical challenges in 2025 as it races to catch up in AI amid rising competition and regulatory scrutiny. \"The stakes are high,\" Pichai said at a strategy meeting, details of which were reported by CNBC. \"I think it's really important we internalize the urgency of this moment, and need to move faster as a company. The stakes are high. These are disruptive moments. In 2025, we need to be relentlessly focused on unlocking the benefits of this technology and solve real user problems.\" \n\nThe meeting revealed employee concerns about ChatGPT \"becoming synonymous to AI the same way Google is to search.\" In response, DeepMind co-founder Demis Hassabis outlined plans to \"turbo charge\" Google's Gemini app, which executives hope will become their next product to reach 500 million users. Pichai showed a chart positioning Gemini 1.5 ahead of OpenAI's GPT, though he expects \"some back and forth\" in 2025. The report adds: [Pichai] acknowledged that Google has had to play catchup. \"In history, you don't always need to be first but you have to execute well and really be the best in class as a product,\" he said. \"I think that's what 2025 is all about.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Google+CEO+Warns+of+High+Stakes+in+2025+AI+Race%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F1113226%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F1113226%2Fgoogle-ceo-warns-of-high-stakes-in-2025-ai-race%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/30/1113226/google-ceo-warns-of-high-stakes-in-2025-ai-race?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564595&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Apple TV+ will be free to stream this weekend","url":"https://techcrunch.com/2024/12/30/apple-tv-will-be-free-to-stream-this-weekend/","date":1735578709,"author":"Aisha Malik","unread":true,"content":"<p>Apple is allowing anyone to access its Apple TV+ streaming service for free this weekend, from January 3 to 5. The company made the announcement on Monday alongside a short video featuring its most popular TV shows, including &#8220;Severance,&#8221; &#8220;Slow Horses,&#8221; &#8220;Shrinking,&#8221; and more. It&#8217;s worth noting that this is the first time that Apple [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Bench to be acquired after abruptly shutting down","url":"https://techcrunch.com/2024/12/30/bench-to-be-acquired-after-abruptly-shutting-down/","date":1735577728,"author":"Charles Rollet","unread":true,"content":"<p>Bench, the VC-backed accounting startup that left thousands of customers locked out of their accounts after it suddenly shut down last week, will be acquired by Employer.com for an undisclosed price in a last-minute deal, TechCrunch has exclusively learned. The San Francisco-based HR tech company Employer.com focuses on payroll and onboarding, in contrast to Bench, [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Virtual Try-On Technology: What You Should Know","url":"https://hackernoon.com/virtual-try-on-technology-what-you-should-know?source=rss","date":1735576836,"author":"BoxHero","unread":true,"content":"<p>\\\n<em>What’s your personal color?</em></p>\n<p>\\\nYou’ve probably been asked this question a lot lately. Or maybe you’ve even asked yourself at some point.</p>\n<p>\\\nThere’s a good reason why <strong><a href=\"https://theconceptwardrobe.com/colour-analysis-comprehensive-guides/what-is-color-analysis\">color analysis</a></strong> has been receiving so much hype these days. People want to make sure their next wardrobe addition perfectly matches their seasonal color palette.</p>\n<p>\\n While consulting a color analysis expert can cost a few hundred dollars, many are turning to free, AI-powered tools like TikTok filters and apps to figure it out on their own.</p>\n<p>\\\nWe’re not here to debate whether in-person color analysis is worth the money or not. Instead, let’s explore how business owners can make use of this trend to improve and personalize their customers’ online shopping experience. \\n  \\n How? Through <strong><strong><a href=\"https://www.forbes.com/sites/lelalondon/2021/05/20/virtual-try-on-is-more-than-a-pandemic-trendand-these-brands-are-reaping-the-rewards/\">virtual try-ons (VTO)</a></strong>.</strong></p>\n<p>\\\nAs the name suggests, this digital solution allows shoppers to try on accessories, makeup, and clothing virtually without visiting the physical store. <em>Want to know if a dress flatters your body type? Or if a lipstick shade complements your skin tone?</em> All it takes is a mobile device.</p>\n<p>\\</p>\n<hr />\n<p>\\</p>\n<h3 id=\"inthisarticlewellwalkyouthrougheverythingyouneedtoknowaboutusingvtotechnology\"><strong>In this article, we’ll walk you through everything you need to know about using VTO technology.</strong></h3>\n<p>Here’s what we’ll cover:</p>\n<p>\\</p>\n<ul>\n<li>E-commerce and the cost of returns</li>\n<li>What is VTO and how it works</li>\n<li>The cost of adopting VTO</li>\n<li>Biometric data privacy concerns</li>\n<li>A step-by-step guide to using VTO tools</li>\n</ul>\n<p>\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-n8134lx.jpeg\" alt=\"A customer virtually trying on a pair of shoes using his phone before making a purchase online. \" /></p>\n<p>\\</p>\n<h2 id=\"thereturndilemmaabilliondollarissue\"><strong>The Return Dilemma: A Billion-Dollar Issue</strong></h2>\n<p>The rise of VTO technology in e-commerce didn’t happen by chance. It emerged in response to the surge in online shopping during the COVID-19 pandemic and the growing need to address high return rates.</p>\n<p>\\\nE-commerce sales are projected to reach <strong><a href=\"https://www.statista.com/statistics/272391/us-retail-e-commerce-sales-forecast/\">$1.9 trillion</a></strong> by 2029. Despite the promising positive outlook, business owners continue to face a significant number of returns, especially in fashion retail.</p>\n<p>\\\nA <strong><a href=\"https://nrf.com/research/2023-consumer-returns-retail-industry\">2023 report</a></strong> by the National Retail Federation revealed that merchandise returns reached $743 billion last year, with online purchases contributing significantly at $247 billion.</p>\n<p>\\</p>\n<h3 id=\"whatreturnsmeanforshoppers\">What Returns Mean for Shoppers</h3>\n<p>For some customers, the return process can be a frustrating and time-consuming experience. Aside from the disappointment of receiving an item that doesn’t meet expectations, returning a product often involves multiple steps: submitting a request, waiting for approval, and physically dropping off the item at a designated location.</p>\n<p>\\\nDelays, miscommunications, and confusing policies only add to the hassle, leaving shoppers dissatisfied and less likely to make future purchases.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-tj234ny.jpeg\" alt=\"Returns can be a frustrating and time-consuming process, leaving customers dissatisfied and less likely to shop again.\" /></p>\n<p>\\</p>\n<h3 id=\"thepriceofreturnsforretailers\"><strong>The Price of Returns for Retailers</strong></h3>\n<p>The cost of processing a return can reach up to <strong><a href=\"https://www.easyship.com/resources/ecommerce-returns\">65%</a></strong> of the original selling price, posing a significant challenge for businesses. Items bought online are significantly more likely to be returned compared to those purchased in traditional brick-and-mortar stores. Additionally, many customers say they’re <strong><a href=\"https://www.invespcro.com/blog/ecommerce-product-return-rate-statistics/\">more likely to shop again with a retailer</a></strong> if the return process is simple and if free return shipping is offered.</p>\n<p>\\\nThese challenges (i.e., high return costs and growing customer expectations) can be addressed with VTO technology. VTO helps customers visualize how items will fit or look, minimizing the likelihood of returns caused by dissatisfaction with appearance or fit.</p>\n<p>\\\nAs <strong><a href=\"https://www.businessinsider.com/artificial-intelligence-trends-retail-virtual-try-on-2023-10\">Alexander Berend</a></strong>, CEO of the AI-imaging software company Anthropics Technology, said:</p>\n<p>\\</p>\n<blockquote>\n  <p><em>\"There's a few different reasons why people return clothes… People try it on and then they just don't like how it looks on them, or it could be that it doesn't fit, or it could be that you don't like the feel of the clothes. <strong>Virtual try-on</strong> really goes to those first two in a big way.\"</em></p>\n</blockquote>\n<p>\\\n\\</p>\n<p>:::tip\n<strong>Did You Know?</strong> You can easily <strong>manage order returns and refunds</strong> with <strong>BoxHero</strong>! Simply add return details directly into the <strong>Purchase & Sales</strong> feature for easy tracking and management. Here’s our <strong><a href=\"https://www.boxhero.io/en/blog/update-returns-and-refunds\">guide</a></strong> to get started.</p>\n<p>:::</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-mx035r5.jpeg\" alt=\"Managing refunds with BoxHero's Returns Feature\" /></p>\n<p><a href=\"https://apps.apple.com/in/app/inventory-management-boxhero/id1325512157?embedable=true\">https://apps.apple.com/in/app/inventory-management-boxhero/id1325512157?embedable=true</a></p>\n<p>\\</p>\n<h2 id=\"understandingvtoanoverview\"><strong>Understanding VTO: An Overview</strong></h2>\n<p>Also known as <em>‘virtual fitting’ or ‘digital try-on,’</em> VTO has been adopted in the e-commerce landscape for quite some time now, especially <strong><a href=\"https://www.washingtonpost.com/business/2020/07/09/virtual-try-ons-are-replacing-fitting-rooms-during-pandemic/\">during the COVID-19 pandemic</a></strong>. It’s widely used across industries like cosmetics, jewelry, apparel, and footwear to provide customers with a more interactive shopping experience.</p>\n<p>\\</p>\n<p>:::info\n<strong><a href=\"https://www.sephora.sg/pages/virtual-artist?srsltid=AfmBOoqSmgLG8tki4jnmqfaQ_4HoP_Yex0iw0WlLPK0ab7LETqKhZYfb\">Sephora’s Virtual Artist</a></strong> analyzes consumers' face shapes and skin tones to deliver personalized makeup recommendations, creating a tailored experience for each shopper.</p>\n<p>:::</p>\n<p>\\\nThis technology enables customers to visualize products from the brand’s catalog on themselves using a smartphone or any camera-equipped device. By overlaying digital versions of items like clothing or accessories onto a real-time image of the user, customers can move around and see if the product looks good on them from different angles.</p>\n<p>\\\nSome tools even allow users to <strong>upload photos or videos</strong>, using these as models to virtually ‘try on’ items like sunglasses, earrings, or outfits.</p>\n<p>\\</p>\n<h3 id=\"howvtotechnologyworks\"><strong>How VTO Technology Works</strong></h3>\n<p>VTO brings together a powerful mix of machine learning, artificial intelligence (AI), augmented reality (AR), and 3D modeling and animation. <strong><a href=\"https://www.leewayhertz.com/ai-enabled-virtual-try-on/#overview-ai-try-ons\">Here’s how it works</a></strong>:</p>\n<p>\\</p>\n<ul>\n<li>The user takes or uploads a photo or video of themselves.</li>\n<li>Computer vision analyzes the image to create a detailed digital model, identifying key features like body measurements and facial landmarks.</li>\n<li>Virtual clothing or accessories are then adjusted to fit the digital model, tailored to size, style, and user preferences.</li>\n<li>Finally, the user sees a realistic image or video of how the product would look on them.</li>\n</ul>\n<p>\\</p>\n<p>:::info\nFor example, <strong><a href=\"https://www.warbyparker.com/app\">Warby Parker</a></strong>’s VTO technology makes it possible for customers to try on glasses virtually, providing recommendations for the best frame width and even measuring pupillary distance for a perfect fit.</p>\n<p>:::</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-4i4341x.jpeg\" alt=\"Warby Parker’s VTO page\" /></p>\n<p>\\\n\\</p>\n<h2 id=\"exploringvtopriceeffectivenessandmore\"><strong>Exploring VTO: Price, Effectiveness, and More</strong></h2>\n<p>VTO’s <strong><a href=\"https://viewit3d.com/virtual-try-on-vs-traditional-shopping-roi-comparison/\">upfront costs</a></strong> cover <em>software development,</em> <em>3D scanning equipment</em>, <em>annual cloud storage</em>, <em>installation on the e-commerce site,</em> and <em>product photography</em>. Overall, the initial investment could range from <strong>$95,000</strong> to <strong>$470,000</strong>.</p>\n<p>\\\nThe good news is, retailers don’t have to build from scratch. There are many VTO software tools that can be integrated seamlessly with retailers’ e-commerce platforms, including:</p>\n<p>\\</p>\n<ul>\n<li><strong><strong><a href=\"https://wanna.fashion/\">WANNA</a></strong>:</strong> It delivers cutting-edge 3D and AR technologies for fashion items such as bags, footwear, jewelry, and clothing. WANNA has been used by FARFECTCH, Dolce &amp; Gabbana, Valentino, and Balenciaga.</li>\n</ul>\n<p>\\</p>\n<blockquote>\n  <p><em>“AR and Virtual Try-On is a transformational opportunity for the luxury sector, bridging the gap between physical and digital, evolving e-commerce from 2D flat into 3D immersive and personalized. At FARFETCH, we have seen great results across the shopping funnel with outstanding feature adoption, product engagement, and significantly higher sales for Virtual Try-On-enabled products.”</em></p>\n  <p>\\\n  - <strong><a href=\"https://wanna.fashion/farfetchapp\">Filipa Neto</a></strong>, Head of Open Innovation, FARFETCH</p>\n</blockquote>\n<p>\\\n\\</p>\n<ul>\n<li><p><strong><strong><a href=\"https://business.zyler.com/\">Zyler</a></strong>:</strong> It offers a digital dressing room tool that makes browsing more fun. Zyler also gives size recommendations and personalized emails for the retailers. It’s trusted by brands like Moss, John Lewis, M&amp;S, and Larusmiani.</p>\n<p>\\</p></li>\n<li><p><strong><strong><a href=\"https://www.reactivereality.com/\">PICTOFit:</a></strong></strong> An end-to-end platform for VTO, this solution allows users to make <strong><a href=\"https://www.gartner.com/en/information-technology/glossary/avatar\">avatars</a></strong> and to try on garments that fit their body type and style. It also offers virtual mix-and-match, size visualization, and layered outfit styling.</p></li>\n</ul>\n<p>\\</p>\n<ul>\n<li><strong><strong><a href=\"https://www.makeupar.com/business\">Perfect Corp.</a></strong>:</strong> This tool goes beyond VTO with AI analysis tools, such as a skin scanner for skincare recommendations and aesthetic simulation to assist medical clinics in presenting treatments to their patients. It supports makeup, eyewear, accessories, and more.</li>\n</ul>\n<p>\\\nThe starting price for these VTO software ranges from around <strong>$100 to $1,000</strong>, making them fairly accessible. Also, when it comes to VTO, the accuracy depends on the underlying technology, so make sure you choose the software that meets your e-commerce and financial requirements.</p>\n<p>\\</p>\n<h3 id=\"isvtoreallyeffective\"><strong>Is VTO really effective?</strong></h3>\n<p>A recent <strong><a href=\"https://koasas.kaist.ac.kr/bitstream/10203/278837/1/000584809900001.pdf\">study</a></strong> shows that VTO technology increases average sales per customer while reducing return rates by over 27%. It also keeps customers engaged longer, proving its value in improving the online shopping experience.</p>\n<p>\\\n\\</p>\n<p>:::info\n<strong>Did You Know?</strong> <strong><a href=\"https://www.makeupar.com/business/successstory/colgate\">Colgate</a></strong>, using Perfect Corp.’s VTO solution, gives consumers real-time visualization of teeth whitening. This resulted in a 40% increase in consumers clicking “Buy Now” versus those who are not using the AR-powered experience.</p>\n<p>:::</p>\n<p>\\</p>\n<blockquote>\n  <p><em>“If a company claims using their product will produce X number of shades of improvement, what does that mean to a consumer? The consumer is left wondering what each shade of whitening would look like… Our AR-Powered Teeth Whitening Virtual Experience, based on our extensive research and clinical results, shows consumers a realistic representation of what our products can do for their own teeth. This is what consumers want and need.”</em></p>\n  <p>\\\n  <strong>[-]()<a href=\"https://www.makeupar.com/business/successstory/colgate\">Gary Binstock</a></strong>, Director of Technology, Strategic Innovation &amp; Technology Alliances of Colgate-Palmolive</p>\n</blockquote>\n<p>\\\n\\</p>\n<h3 id=\"whatdocustomerssay\"><strong>What do customers say?</strong></h3>\n<p>AR is becoming a must-have for businesses as younger generations embrace it in their shopping experiences. A <strong><a href=\"https://www.dynata.com/content/GCT_The-New-Experience-Economy.pdf\">2022 Dynata survey</a></strong> revealed that most Gen-Z and Millennial online shoppers want to use AR to try on makeup or experiment with new hair colors, and are interested in virtually trying on clothes, shoes, and accessories.</p>\n<p>\\\nThis shows that Millennials and Gen-Z, the <strong><a href=\"https://www.statista.com/chart/33296/estimated-consumer-class-spending-by-region-generation/\">largest spending demographic</a></strong>, are driving the demand for AR-powered shopping experiences.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-i0534mv.jpeg\" alt=\"A customer browsing clothing options on her phone, ready to make an online purchase.\" /></p>\n<p>\\\n\\</p>\n<h2 id=\"thehiddenrisksofvtobiometricprivacyandlegalcompliance\"><strong>The Hidden Risks of VTO: Biometric Privacy and Legal Compliance</strong></h2>\n<p>VTO tools collect biometric identifiers such as eye color, retina, and facial geometry. These unique physical features can be stolen and used by hackers to impersonate users and gain unauthorized access to devices and personal accounts.</p>\n<p>\\\nThe Biometric Information Privacy Act (BIPA) has led to legal challenges for <strong><a href=\"https://www.duanemorris.com/articles/bipa_data_risks_high_tech_fashion.html\">companies</a></strong> using VTO technology. <strong><a href=\"https://law.justia.com/cases/federal/district-courts/illinois/ilndce/1:2022cv01988/414288/40/\">Estee Lauder</a></strong>, <strong><a href=\"https://law.justia.com/cases/federal/district-courts/new-york/nysdce/1:2022cv02944/578061/36/\">Louis Vuitton</a></strong>, and <strong><a href=\"https://law.justia.com/cases/federal/district-courts/illinois/ilndce/1:2022cv04633/418741/48/\">Christian Dior</a></strong> were among those that got embroiled in the biometric privacy issue by their customers.</p>\n<p>\\\nSome retailers argue that BIPA doesn’t apply to them because they don’t store biometric data. However, VTO uses AI to analyze customer preferences and biometric profiles to suggest products. This process typically involves collecting and temporarily storing data, which may trigger compliance requirements under BIPA and other privacy laws.</p>\n<p>\\\nUnder BIPA, companies must take the following steps to avoid legal pitfalls:</p>\n<p>\\</p>\n<ol>\n<li>Inform individuals in writing that they’re collecting biometric data.</li>\n<li>State the purpose of collection and the retention period.</li>\n<li>Get the consumer’s written consent before collecting the data.</li>\n</ol>\n<p>\\\nSo, what steps should businesses take to protect their customers’ information and mitigate risks? <strong><a href=\"https://www.purduegloballawschool.edu/blog/news/virtual-try-on-technologies\">Purdue Global Law School</a></strong> cited the following tips:</p>\n<p>\\</p>\n<ul>\n<li><p>Clearly disclose any biometric data collection in your <strong>terms and conditions</strong>.</p></li>\n<li><p>If you need to store biometric information, make sure your policies and practices comply with BIPA and other privacy laws.</p></li>\n<li><p>Consider using <strong>popup notifications</strong> or <strong>‘clickwrap’ agreements</strong>that require customers to explicitly agree before using VTO tools. \\n </p>\n<p>\\</p>\n<p><img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-s1634w4.jpeg\" alt=\"VTO technology collect biometric data such as facial geometry of the user, which can be used and stolen by hackers to impersonate the customers.&nbsp;\" /></p></li>\n</ul>\n<p>\\\n\\</p>\n<hr />\n<h2 id=\"vtomadesimpleaquickguidetogettingstarted\"><strong>VTO Made Simple: A Quick Guide to Getting Started</strong></h2>\n<p>Now that we’ve covered the basics of VTO, the costs, and legal considerations, here’s a straightforward guide on how to adopt this technology.</p>\n<p>\\\n<strong>Step 1: Know Your Goals</strong></p>\n<p>Start by identifying why you want to implement VTO. Is it to reduce return rates, increase customer confidence, or improve conversion rates? Take a look at your current challenges, such as high return rates or low customer engagement, and consider conducting a quick survey to see whether your customers are interested in using try-on tools during their shopping experience.</p>\n<p>\\\n<strong>Step 2: Choose the Right VTO Solution</strong></p>\n<p>Again, you don’t need to develop your own VTO system, as it requires a higher upfront cost. Research <strong><a href=\"https://www.glamar.io/blog/best-virtual-try-it-on#20-best-try-it-on-tools-in-2025\">VTO software options</a></strong> and choose what aligns with your business needs. What AR functions do you need? How much can you allot for this technology? Will it be easy to integrate with your current platform? Consider all these factors before choosing your solution.</p>\n<p>\\n <strong>Step 3: Update Terms and Conditions / Ensure Legal Compliance</strong></p>\n<p>Inform customers why their data is collected, how it will be used, and how long it will be retained. Obtain consent from customers before collecting biometric information.</p>\n<p>\\\n<strong>Step 4: Prepare Your Inventory</strong></p>\n<p>High-quality images or 3D models of your products are essential for VTO functionality. A well-organized inventory system will ensure a smooth setup process.</p>\n<p>\\</p>\n<p>:::tip\n<strong>Pro Tip:</strong> Use tools like <strong>BoxHero</strong> to streamline inventory management alongside your VTO solution. <strong>BoxHero’s cloud-based inventory solution</strong> offers powerful features that can be synced with your VTO software.</p>\n<ul>\n<li>The <strong><a href=\"https://www.boxhero.io/en/blog/barcode-generator?ref=boxhero-en.ghost.io\">Barcode</a></strong> feature lets you generate, design, and print your barcode labels for your inventories. Plus, you can add high-quality photos for easy identification of your items.</li>\n<li>You can set custom <strong><a href=\"https://www.boxhero.io/en/blog/taking-advantage-of-the-attributes-feature-of-boxhero?ref=boxhero-en.ghost.io\">attributes</a></strong> to easily categorize products by size, color, style, or other specifications.</li>\n<li>With <strong><a href=\"https://www.boxhero.io/en/features/sales?ref=boxhero-en.ghost.io\">Purchase and Sales</a></strong>, you can manage your entire order system—from invoicing to shipping—on a single platform.</li>\n</ul>\n<p>:::</p>\n<p>\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-hv135bm.jpeg\" alt=\"BoxHero's Custom Attributes under Item Details\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-5c2351r.jpeg\" alt=\"Creating a Purchase Order (P.O.) through BoxHero's Purchase & Sales feature\" /></p>\n<p>\\\n\\\n\\\n<strong>Step 5: Test Your VTO Technology</strong></p>\n<p>Work closely with developers to incorporate the VTO software into your e-commerce platform. Test the user interface yourself. Is it intuitive and easy to use? Does it provide a realistic and accurate representation of products? Are the sizing and alignment of overlays accurate when applied to images and videos?</p>\n<p>\\\n<strong>Step 6: Promote Your VTO Feature</strong></p>\n<p>Once your VTO is ready, let your customers know! Launch a marketing campaign to showcase the benefits of your new tool, such as a personalized shopping experience. Highlight the feature in your emails, social media posts, and website banners. Also, you could ask the customers for feedback or reviews about their experience with the technology.</p>\n<p>\\\n<strong>Step 7: Monitor Performance and Feedback</strong></p>\n<p>Take a look at your performance to evaluate the impact of VTO on your business. Use analytics to measure key indicators such as <em>changes in return rates, increase in sales and customer satisfaction, and the time spent on your platform using the VTO tool.</em></p>\n<p>\\\nCompare data before and after rolling out VTO to determine if it’s meeting your primary goals. Don’t forget to collect ongoing customer feedback to identify areas for improvement.</p>\n<p>\\</p>\n<p>:::tip\nWith BoxHero’s <strong><a href=\"https://www.boxhero.io/en/features/analytics\">Analytics</a></strong> feature, you’ll get to see insights into inventory trends and sales performance. You can easily compare data from before and after implementing VTO to measure its effectiveness. Try <strong>BoxHero</strong> today and make smarter, data-driven decisions to maximize your VTO.</p>\n<p>:::</p>\n<p>\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/EWjOBlQqPvcBJuT560iDq0AKI3q2-9s335m1.jpeg\" alt=\"BoxHero’s Analytics Feature for Inventory Insights\" /></p>\n<p>\\\n\\</p>\n<hr />\n<h2 id=\"makingthemostofvtowithyourinventory\"><strong>Making the Most of VTO with Your Inventory</strong></h2>\n<p>VTO tools provide an immersive and personalized shopping experience for consumers. By simply uploading a photo or video, customers can virtually try on clothes, accessories, eyewear, and even footwear—all without stepping into a physical store. Powered by a combination of AR, AI, and 3D modeling, VTO technology has proven to reduce return rates and improve purchase decisions.</p>\n<p>\\\nHowever, like any technology, VTO comes with challenges, particularly around biometric data privacy. To address this, make sure to obtain clear consent from shoppers and inform them about how long their data will be retained (if applicable). These steps can help you avoid potential legal issues and build trust with your customers.</p>\n<p>\\\nTo maximize the effectiveness of VTO, a good inventory management solution is essential. With <strong><a href=\"https://play.google.com/store/apps/details?id=com.bgpworks.boxhero&hl=en&gl=US\">BoxHero</a></strong>, you can organize and track your stock in real-time to ensure accurate availability for your VTO system.</p>\n<p>\\\nTry <strong>BoxHero</strong> today and take your VTO-equipped business to the next level!</p>\n<p><a href=\"https://www.boxhero.io/en?embedable=true\">https://www.boxhero.io/en?embedable=true</a></p>\n<p>\\n  \\n </p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia Open-Sources Run:ai, the Software It Acquired For $700 Million","url":"https://news.slashdot.org/story/24/12/30/1420230/nvidia-open-sources-runai-the-software-it-acquired-for-700-million?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735576800,"author":"msmash","unread":true,"content":"Nvidia has completed its acquisition of Run:ai, a provider of GPU cloud orchestration software for AI workloads, and announced plans to open-source the platform. The deal, valued at $700 million, brings the Israel-based startup under Nvidia's umbrella after their collaboration since 2020. \n\nRun:ai's software helps enterprises manage and schedule Nvidia GPU resources for AI applications across cloud and on-premises environments. Founded in 2018, the company's platform currently supports only Nvidia GPUs, but open-sourcing will enable expansion to other AI ecosystems, according to founders Omri Geller and Ronen Dar. The acquisition strengthens Nvidia's software portfolio as the company, now valued at $3.56 trillion, expands beyond its core graphics chip business into AI infrastructure management.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Nvidia+Open-Sources+Run%3Aai%2C+the+Software+It+Acquired+For+%24700+Million%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F1420230%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F1420230%2Fnvidia-open-sources-runai-the-software-it-acquired-for-700-million%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/30/1420230/nvidia-open-sources-runai-the-software-it-acquired-for-700-million?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564739&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Why the Beyoncé Bowl Matters to Connected TV and Live Sports","url":"https://hackernoon.com/why-the-beyonce-bowl-matters-to-connected-tv-and-live-sports?source=rss","date":1735576217,"author":"David Deal","unread":true,"content":"<p>Live sports is a massive growth opportunity for New Hollywood streaming businesses who dominate connected TV. The global sports online live video streaming market was valued at&nbsp;<strong><a href=\"https://www.verifiedmarketresearch.com/product/sports-online-live-video-streaming-market/?t\">$27.93 billion in 2024</a></strong>&nbsp;and is projected to grow to $133.98 billion by 2031, with a compound annual growth rate of 24.64%. in 2024, digital sports viewers were expected to&nbsp;<strong><a href=\"https://www.sportsvideo.org/2024/12/23/op-ed-the-future-of-sports-streaming-prioritizing-fan-engagement/?t\">surpass linear TV viewers</a></strong>&nbsp;for the first time, with 105.3 million digital viewers compared to 85.7 million on traditional platforms. The major streaming brands, including Amazon, Apple, Netflix, Peacock, and YouTube, are locked in a knock-down, drag-out battle to capture live sports viewers and attract more advertising revenue. And the NFL is the coveted prize in this fight. The NFL is one of the world’s most powerful brands.&nbsp;Nielsen says the&nbsp;<strong><a href=\"https://www.sportspromedia.com/news/nfl-us-tv-most-watched-broadcasts-top-100-2023-super-bowl-viewership/?ref=hackernoon.com#:~:text=National%20Football%20League%20(NFL)%20games,top%20ten%20being%20playoff%20games.\">NFL accounted for a 93 of the 100 most-watched TV broadcasts in 2023</a></strong>. The NFL has sensed and responded to the rise of CTV, agreeing to air games with a variety of streaming companies, among them Amazon Prime, Paramount+, Peacock, and YouTube TV.</p>\n<p>\\\nWhich brings us to the epic Beyoncé Bowl on Christmas Day 2024, aka Beyoncé’s halftime performance during Netflix NFL Christmas Gameday. For the first time, Netflix hosted not one but two NFL games on Christmas Day, featuring matchups between the Kansas City Chiefs and Pittsburgh Steelers followed by the Houston Texans hosting the Baltimore Ravens. Netflix’s exclusive livestreams of two Christmas Day NFL games each reached an&nbsp;<strong><a href=\"https://about.netflix.com/en/news/netflix-nfl-christmas-gameday-reaches-65-million-us-viewers\">average of more than 24 million U.S. viewers</a></strong>, making them&nbsp;<strong><a href=\"https://www.nbcnews.com/sports/nfl/pair-christmas-nfl-games-netflix-set-streaming-records-rcna185507\">the most streamed NFL games in U.S. history</a></strong>&nbsp;– and comparable to numbers for a on average, in the U.S. for both games, according to Nielsen figures provided by Netflix. This is in line for a typical NFL game on linear TV.</p>\n<p>\\\nNice numbers. But Beyoncé beat them both. Her halftime performance during the Texans/Ravens game drew 27 million viewers, and no wonder. Both the NFL games were boring, lopsided contests. But Queen Bey lit up connected TV and the digital world in general with the debut performance of her 2024 genre-bending album&nbsp;<em>Cowboy Carter</em>. This matters because the appearance of Beyoncé elevated a sporting event to a level of cultural relevance as the Super Bowl is capable of doing – and raised the stakes for the intersection of sports and entertainment on connected TV.</p>\n<p>\\</p>\n<h2 id=\"thebeyoncbowlnbspwinsbig\"><strong>The #BeyoncéBowl</strong>&nbsp;<strong>Wins Big</strong></h2>\n<p>\\\nBy any measure, her 13-minute appearance was a success. According to Netflix,&nbsp;#BeyoncéBowl&nbsp;rose to the #1 worldwide trend on X immediately as her performance kicked off, eclipsing #Christmas. Following her performance, Netflix occupied 10 of the top 12 trending topics on X in the United States.</p>\n<p>\\\nBut the numbers don’t capture the significance of her performance. She elevated live sports on connected TV to another level of cultural relevance. This is important because when any brand, including Netflix and the NFL, achieves, cultural relevance, they connect with (and even shape) the values, beliefs, and behaviors of their audiences. And when a brand does that, they lock in loyalty.</p>\n<p>\\\nBeyoncé knows how to elevate a sporting event to a higher level of cultural significance. She did so in 2016 with her performance of the riveting “Formation,” a politically-charged statement of identity at the Super Bowl 50 Halftime Show. She did it again on Christmas Day when her&nbsp;<em>Cowboy Carter</em>&nbsp;setlist drew attention to the rich heritage of Black country music as the album itself did on its release in 2024. In the wake of her performance, news media like&nbsp;<em><strong><a href=\"https://www.theguardian.com/music/2024/dec/26/beyonce-netflix-nfl-halftime-performance-review-ravens-vs-texans\">The Guardian</a></strong></em>,&nbsp;<em><strong><a href=\"https://www.nytimes.com/2024/12/25/arts/music/beyonce-performance-halftime-show-christmas-netflix.html\">The New York Times</a></strong></em>, and&nbsp;<em><strong><a href=\"https://theweek.com/culture-life/music/2024-black-country-artists\">The Week</a></strong></em>&nbsp;published thoughtful analyses of Black country.</p>\n<p>\\\nSocial media blew up as she performed a setlist that included “16 Carriages,” “YA YA,” Texas Hold ‘Em” and more.&nbsp;<em>Cowboy Carter</em>&nbsp;is a much talked about and celebrated artistic achievement that blends country and Americana with elements of blues, folk, soul, and hip-hop, reflecting her Texan roots and the rich tapestry of Southern music. Although it’s not a country album, per se, the country influences have made a profound statement on the inclusivity and evolution of American music genres.</p>\n<p>\\\nThose influences were on display during the halftime show. Beyoncé surrounded herself with musical guests including Post Malone, Shaboozey, and her daughter Blue Ivy. But the most important inclusion were the Black country women who made contributions to&nbsp;<em>Cowboy Carter</em>: Brittney Spencer, Reyna Roberts, Tiera Kennedy, and Tanner Adell. They sang along with her interpretation of the Beatles’ “Blackbird” (fashioned as “Blackbiird” on&nbsp;<em>Cowboy Carter</em>) as they did on the album.</p>\n<p>\\\nThe choice of “Blackbiird” was significant: Paul McCartney wrote the original “Blackbird” in 1968 as a response to the civil rights movement in the United States. Inspired by the racial tensions and struggles for equality during that time, McCartney has explained that the “blackbird” in the song symbolizes a Black woman or person enduring oppression but holding hope for change. For Beyoncé to include the song at an NFL game and to include the Black country artists who sang on its album version was a cultural statement of purpose and artistry.</p>\n<p>\\\nFor Netflix and the NFL, the Beyoncé halftime show ups the ante for what a sporting event and artist can accomplish together especially on Christmas Day. On linear TV, Christmas Day broadcasts have not featured performers of such global stature. While the NBA’s Christmas Day games often included celebrity cameos in games or promotional material, these moments paled in comparison to Beyoncé’s multi-million-dollar production. By incorporating a performer of Beyoncé’s caliber, Netflix and the NFL upped the ante for the streaming industry in particular. Their collaboration underlines how entertainment and sports are converging in ways that redefine what viewers can expect not only from holiday programming but sports programming.</p>\n<p>\\</p>\n<h2 id=\"advertisingimplicationsfornetflix\"><strong>Advertising Implications for Netflix</strong></h2>\n<p>\\\nThe success of Netflix NFL Christmas Gameday will also help boost Netflix’s already successful advertising business. Advertising for the NFL games appeared across both the ad-supported and ad-free subscription tiers.&nbsp;<strong><a href=\"https://www.marketingdive.com/news/netflix-nfl-christmas-fanduel-verizon-70-million-ad-tier/732639/?t\">Ad inventory sold out in November</a></strong>. For advertisers, this ensured access to Netflix’s full NFL audience, creating a unified and highly valuable advertising opportunity that maximizes reach.</p>\n<p>\\\nAnalysts estimate that the two NFL Christmas Day games could&nbsp;<strong><a href=\"https://finance.yahoo.com/news/netflix-bets-big-nfl-christmas-140510061.html\">generate $150 million in advertising revenue</a></strong>, effectively covering Netflix’s $150 million investment in the broadcast rights. This not only offsets costs but also demonstrates the financial viability of live sports as a revenue stream for Netflix.</p>\n<p>\\\nFuture NFL broadcasts will give Netflix an opportunity to attract premium advertisers who are eager to reach large, engaged audiences. Netflix can use its advanced data analytics capabilities to provide highly targeted advertising options. Oh, and Netflix and Beyoncé are not done yet. They have a three-project, $60 million deal signed in 2019. The Netflix NFL Christmas Gameday was the second initiative (the first was a 2019 concert documentary,&nbsp;“Homecoming: A Film by Beyoncé”). The two are partnering on a documentary of the #BeyoncéBowl, too, which will keep the moment in the public eye.</p>\n<p>\\</p>\n<h2 id=\"theamazonadvantage\"><strong>The Amazon Advantage</strong></h2>\n<p>\\\nYou can be sure Netflix’s competitors are figuring out how to answer the Netflix challenge, especially Amazon, which achieved some history of its own by streaming NFL Black Friday games in 2023 and 2024. Amazon has its work cut out. The 2024 NFL Black Friday game, streamed exclusively on Amazon Prime Video, achieved an average viewership of&nbsp;<strong><a href=\"https://thedesk.net/2024/12/black-friday-amazon-ratings-chiefs-raiders/?t\">13.51 million</a></strong>, far less than Netflix NFL Christmas Gameday although a 41% increase compared to the 2023 inaugural Black Friday game. But Amazon has something Netflix lacks: online commerce during live sports and a more interactive and interesting viewing experience to complement the action on the field. Expect Amazon to lean into its strength while attempting to engineer its own entertainment moment.</p>\n<p>\\</p>\n<p>:::info\n<strong>Bottom line</strong>: the stage is set for sports and entertainment to dominate New Hollywood and connected TV in 2025.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"ZeroShape: Presenting Our Architecture for Shape Reconstruction","url":"https://hackernoon.com/zeroshape-presenting-our-architecture-for-shape-reconstruction?source=rss","date":1735576210,"author":"The FewShot Prompting Publication","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/gH1qdz0Dt3mBaMdl9jIS\">Abstract and 1 Introduction</a></p>\n<p><a href=\"http://hackernoon.com/preview/aSTNB03lgvkkjZHFRzoC\">2. Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/0scn6XiQ0BREvFGk0oL5\">3. Method and 3.1. Architecture</a></p>\n<p><a href=\"http://hackernoon.com/preview/Vr0Ad2am2GFeyaIOE6UW\">3.2. Loss and 3.3. Implementation Details</a></p>\n<p>4. Data Curation</p>\n<p><a href=\"http://hackernoon.com/preview/WTlB97rfr7YUwBrbOghS\">4.1. Training Dataset</a></p>\n<p><a href=\"http://hackernoon.com/preview/VGlrNJDi5CmpdgwVsiiv\">4.2. Evaluation Benchmark</a></p>\n<p><a href=\"http://hackernoon.com/preview/dvWHPgbXsIEpl6swOYnq\">5. Experiments and 5.1. Metrics</a></p>\n<p><a href=\"http://hackernoon.com/preview/Ytl9ckAbfos5VTv52Ioe\">5.2. Baselines</a></p>\n<p><a href=\"https://hackernoon.com/preview/Yhpn26VKOWgPM3XYLplp\">5.3. Comparison to SOTA Methods</a></p>\n<p><a href=\"https://hackernoon.com/preview/Xnc0atAoxT52eG6wEtYy\">5.4. Qualitative Results and 5.5. Ablation Study</a></p>\n<p><a href=\"http://hackernoon.com/preview/wpHCZGTcgp2kyN0axX87\">6. Limitations and Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/16wpDr6a2WTKTkQDEUVB\">7. Conclusion and References</a></p>\n<p>\\\n<a href=\"http://hackernoon.com/preview/1C9qGmkudTc6oJpaV0oH\">A. Additional Qualitative Comparison</a></p>\n<p><a href=\"http://hackernoon.com/preview/Of5RJEdBN6rofK2JSsNx\">B. Inference on AI-generated Images</a></p>\n<p><a href=\"http://hackernoon.com/preview/04TDA2TsrsVxYurzi2VT\">C. Data Curation Details</a></p>\n<h2 id=\"3method\">3. Method</h2>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-6z830ru.png\" alt=\"\" /></p>\n<h3 id=\"31architecture\">3.1. Architecture</h3>\n<p>We now present our architecture (see Fig. 3) for shape reconstruction. Our architecture is based on two established practices from prior works in this field: 1) usage of intermediate geometric representation [33, 56, 64, 67, 70] and 2) explicit reasoning with spatial feature maps [5, 63, 68]. Specifically, our model consists of three submodules: a depth and camera estimator, a geometric unprojection unit and a projection-guided shape reconstructor.</p>\n<p>\\\n<strong>Depth and camera estimator.</strong> We propose to estimate the 3D visible object surface as an intermediate representation. To infer the full shape of an object, one must understand the visible surface—not only because the visible surface is often a large part of the full surface, but also because an accurate visible surface facilitates geometric reasoning of the full object reconstruction. This is because cues for reconstruction that allow for generalization, such as symmetry, curvature, and repetition, can be more effectively detected and leveraged in the 3D space. For example, if an object is symmetric, then accurately inferring the 3D symmetry planes from a partial 3D surface is much easier than from 2D RGB or relative depth.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-zea30x0.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ac9302t.png\" alt=\"Figure 3. Overview of our model. Our consists of three modules: a depth and camera estimator, a geometric unprojection unit and a projection-guided shape reconstructor. The depth and camera estimator predicts the depth and camera intrinsics from the input image with a DPT backbone. The geometric unprojection unit converts the depth and intrinsics estimation into a normalized 3D visible surface, which is parameterized by a three-channel projection map. The shape reconstructor finally reconstructs the full occupancy field by fetching localized information from projection map through cross attention.\" /></p>\n<p>\\\nWe use a view-centric coordinate system, because prior works show that view-centric learning is beneficial to generalization [55, 56]. Therefore the camera coordinate frame is the “world” coordinate frame for shape reconstruction, which means that only the camera intrinsics matrix is required to unproject pixels to 3D. Note that unprojection is fully differentiable w.r.t. D and K, so we can easily use it as a module in an end-to-end learning-based model. Additionally, the projection maps are foreground-segmented, and the represented visible surface is normalized in the 3D space to be zero-mean and unit-scale before being fed into the next module.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-9ub30a9.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-t3c303s.png\" alt=\"Figure 4. Effect of Intrinsics. Unprojecting an accurate depth map into a 3D surface surface with erroneous intrinsics leads to skewed shape with wrong 3D aspect ratio.\" /></p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2312.14198\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Zixuan Huang, University of Illinois at Urbana-Champaign and both authors contributed equally to this work;</p>\n<p>(2) Stefan Stojanov, Georgia Institute of Technology and both authors contributed equally to this work;</p>\n<p>(3) Anh Thai, Georgia Institute of Technology;</p>\n<p>(4) Varun Jampani, Stability AI;</p>\n<p>(5) James M. Rehg, University of Illinois at Urbana-Champaign.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Cloudflare Makes Open-Source h3i For HTTP/3 Testing & Debugging","url":"https://www.phoronix.com/news/Cloudflare-Open-Source-h3i","date":1735576064,"author":"Michael Larabel","unread":true,"content":"Cloudflare is ending 2024 by announcing a new open-source project: h3i for low-level HTTP/3 testing and debugging...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"ZeroShape: Related Work to Get You Caught Up","url":"https://hackernoon.com/zeroshape-related-work-to-get-you-caught-up?source=rss","date":1735575310,"author":"The FewShot Prompting Publication","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/gH1qdz0Dt3mBaMdl9jIS\">Abstract and 1 Introduction</a></p>\n<p><a href=\"http://hackernoon.com/preview/aSTNB03lgvkkjZHFRzoC\">2. Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/0scn6XiQ0BREvFGk0oL5\">3. Method and 3.1. Architecture</a></p>\n<p><a href=\"http://hackernoon.com/preview/Vr0Ad2am2GFeyaIOE6UW\">3.2. Loss and 3.3. Implementation Details</a></p>\n<p>4. Data Curation</p>\n<p><a href=\"http://hackernoon.com/preview/WTlB97rfr7YUwBrbOghS\">4.1. Training Dataset</a></p>\n<p><a href=\"http://hackernoon.com/preview/VGlrNJDi5CmpdgwVsiiv\">4.2. Evaluation Benchmark</a></p>\n<p><a href=\"http://hackernoon.com/preview/dvWHPgbXsIEpl6swOYnq\">5. Experiments and 5.1. Metrics</a></p>\n<p><a href=\"http://hackernoon.com/preview/Ytl9ckAbfos5VTv52Ioe\">5.2. Baselines</a></p>\n<p><a href=\"https://hackernoon.com/preview/Yhpn26VKOWgPM3XYLplp\">5.3. Comparison to SOTA Methods</a></p>\n<p><a href=\"https://hackernoon.com/preview/Xnc0atAoxT52eG6wEtYy\">5.4. Qualitative Results and 5.5. Ablation Study</a></p>\n<p><a href=\"http://hackernoon.com/preview/wpHCZGTcgp2kyN0axX87\">6. Limitations and Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/16wpDr6a2WTKTkQDEUVB\">7. Conclusion and References</a></p>\n<p>\\\n<a href=\"http://hackernoon.com/preview/1C9qGmkudTc6oJpaV0oH\">A. Additional Qualitative Comparison</a></p>\n<p><a href=\"http://hackernoon.com/preview/Of5RJEdBN6rofK2JSsNx\">B. Inference on AI-generated Images</a></p>\n<p><a href=\"http://hackernoon.com/preview/04TDA2TsrsVxYurzi2VT\">C. Data Curation Details</a></p>\n<h2 id=\"2relatedwork\">2. Related Work</h2>\n<p>Estimating the 3D shape of an object from a single is a complex inverse problem: while the shape of the visible object can be estimated from shading, estimating the shape of the occluded portion requires prior knowledge about object geometry. This is one of the marvels of human perception and achieving it computationally is a major goal for our field. We review regression and generative methods for this task. Regression-based Methods. </p>\n<p>\\\nThese works investigate different ways to represent 3D object shapes and the architectures to produce them from a single image, e.g., meshes [23, 58, 62] or implicit representations like discrete [8, 54] or continuous [35, 39] occupancy, signed distance fields [21, 56, 68], point clouds [2, 14], or sets of parametric surfaces [15, 69]. A major limitation of these works is the limited generalization beyond the categories of the training set. </p>\n<p>\\\nThe improvements of decomposing the problem into predicting the depth and then estimating the complete shape [50, 56, 64, 67, 70], and representing 3D in a viewer centered rather than object centered reference frame [50, 56, 70] allowed for improved zero-shot generalization. Most architectures follow an encoder/decoder design, where the encoder produces a feature map from which the decoder predicts the 3D shape. </p>\n<p>\\\nWhile early works produced a single feature vector for the entire image, it was later identified that using local features from a 2D feature map improved the detail of the predicted shapes [58, 68] and improved generalization to unseen categories [5, 67]. This culminated with the current state-of-the art regression method, MCC [63], which takes an RGB-D image as input, and uses a transformer-based encoder-decoder setup to produce a “shell occupancy” prediction [4]. Our approach incorporates all these prior findings for improved generalization, and builds upon them with a new module for estimating the visible shape of the object that estimates depth and camera intrinsics, which is processed with a cross attention-based decoder to produce an occupancy prediction.</p>\n<p>\\\n<strong>3D Generative Methods</strong> This category of methods does zero-shot 3D shape reconstruction using a learned 3D generative prior, where the 3D generation is conditioned on one or few input images. Given image or text conditioning, early work [65] used GANs to generate voxels, whereas more recent works use diffusion models to generate point clouds [37], or function parameters for implicit 3D representations [22]. Another related type of generative framing is conditional view synthesis. </p>\n<p>\\\nWorks in this direction fine-tune 2D generative models [31], or train them from scratch [60, 71], to synthesize novel views conditioned on single images and viewing angles. This results in an implicit 3D prior, from which a 3D shape can then be extracted by fitting a 3D neural representation to the synthesized images, or predicting its parameters [30].</p>\n<p>\\\n<strong>3D from 2D Generative Models</strong> There have been efforts to use the real-world 2D image prior from text-to-2D generative models [1, 44, 48, 49] to reconstruct 3D shape from a single image. This category of works [11, 34, 53] often uses techniques such as the SDS loss [40, 57] and generates 3D assets from images by optimizing for each object separately. The prolonged optimization time prevents these works from being evaluated at scale or applied in many realworld applications. Orthogonal to the optimization-based approaches, we focus on learning a 3D shape prior that generalizes across instance. We do not perform any perinstance optimization at test time.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2312.14198\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[4] Traditionally occupancy is formulated as predicting whether a point in 3D is inside/outside a watertight mesh, whereas MCC predicts whether it is within an ε wide shell representing the surface of the object.</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Zixuan Huang, University of Illinois at Urbana-Champaign and both authors contributed equally to this work;</p>\n<p>(2) Stefan Stojanov, Georgia Institute of Technology and both authors contributed equally to this work;</p>\n<p>(3) Anh Thai, Georgia Institute of Technology;</p>\n<p>(4) Varun Jampani, Stability AI;</p>\n<p>(5) James M. Rehg, University of Illinois at Urbana-Champaign.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The HackerNoon Newsletter: GPS Is Broken, And Its Holding Tech Back (12/30/2024)","url":"https://hackernoon.com/12-30-2024-newsletter?source=rss","date":1735574675,"author":"Noonification","unread":true,"content":"\n              \n        <p><strong>How are you, hacker?</strong></p>\n        <br />\n        <p>🪐 What’s happening in tech today, December 30, 2024?</p>\n        <br />\n        <p>\n          The\n          <a href=\"https://hackernoon.com/noonification\" target=\"_blank\" rel=\"noopener\"> HackerNoon Newsletter</a>\n          brings the HackerNoon \n          <a href=\"https://hackernoon.com\" target=\"_blank\" rel=\"noopener\">homepage</a>\n          straight to your inbox.\n          <a href=\"https://hackernoon.com/on-this-day\" target=\"_blank\" rel=\"noopener\">On this day,</a>\n          \n            <strong>The Farthest South in the Antarctica is Reached</strong> in 1902,  <strong>The Union of Soviet Socialist Republics (USSR) is formed</strong> in 1922, \n          \n          and  we present you with these top quality stories. \n          \n            From \n        <a href=\"https://hackernoon.com/gps-is-broken-and-its-holding-tech-back\" class=\"eventTitle\"><strong>GPS Is Broken, And Its Holding Tech Back</strong></a>\n       to \n        <a href=\"https://hackernoon.com/building-vertical-saas-in-an-old-school-industry-ai-that-fits-not-fights\" class=\"eventTitle\"><strong>Building Vertical SaaS in an Old School Industry: AI That Fits, Not Fights</strong></a>,\n       let’s dive right in.\n          \n        </p>\n      \n              \n          <h2><a href=\"https://hackernoon.com/building-vertical-saas-in-an-old-school-industry-ai-that-fits-not-fights\">Building Vertical SaaS in an Old School Industry: AI That Fits, Not Fights</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/rdXndhSmJSgxnJiSxZffRETRibf1-3x33n13.png\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/shangyan\">@shangyan</a> [ 7 Min read ] Building Vertical SaaS in an Old School Industry: AI That Fits, Not Fights <a href=\"https://hackernoon.com/building-vertical-saas-in-an-old-school-industry-ai-that-fits-not-fights\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/new-framework-simplifies-comparison-of-language-processing-tools-across-multiple-languages\">New Framework Simplifies Comparison of Language Processing Tools Across Multiple Languages</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-ir02t68.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/morphology\">@morphology</a> [ 6 Min read ] Researchers in Poland have developed an open-source tool that improves the evaluation and comparison of AI used in natural language preprocessing. <a href=\"https://hackernoon.com/new-framework-simplifies-comparison-of-language-processing-tools-across-multiple-languages\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/gps-is-broken-and-its-holding-tech-back\">GPS Is Broken, And Its Holding Tech Back</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/2jqChkrv03exBUgkLrDzIbfM99q2-ay02sge.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/jackborie\">@jackborie</a> [ 4 Min read ] GPS signals are vulnerable to jamming, spoofing, and interference, leading to inaccuracies and inefficiencies. <a href=\"https://hackernoon.com/gps-is-broken-and-its-holding-tech-back\">Read More.</a></p>\n        \n          <h2><a href=\"https://hackernoon.com/btc-vs-btcwhat-saifedean-ammous-upcoming-book-gets-right-and-wrong\">BTC vs. BTC—What Saifedean Ammous Upcoming Book Gets Right (And Wrong!)</a></h2>\n          <p><img src=\"https://cdn.hackernoon.com/images/EHUYZRLLAUbgArGtd43kEIxTOTS2-vn43mo2.jpeg\" alt /> </p>\n          <br />\n          <p>By <a href=\"https://hackernoon.com/u/maken8\">@maken8</a> [ 3 Min read ] If gold could be moved around like Bitcoin, using aeroplanes, this would pollute the planet more. But it would make us tougher humans.  <a href=\"https://hackernoon.com/btc-vs-btcwhat-saifedean-ammous-upcoming-book-gets-right-and-wrong\">Read More.</a></p>\n        \n              \n        <br />\n        <p>🧑‍💻 What happened in your world this week?</p>\n        <p>\n          It's been said that\n          <a href=\"https://hackernoon.com/developers-the-why-and-how-to-writing-technical-articles-54e824789ef6\">writing can help consolidate technical knowledge</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\">establish credibility</a>,\n          <a href=\"https://hackernoon.com/how-can-non-writers-become-effective-bloggers-1pq32wd\"> and contribute to emerging community standards</a>.\n          Feeling stuck? We got you covered ⬇️⬇️⬇️\n        </p>\n        <br />\n        <p>\n          <a href=\"https://app.hackernoon.com/mobile/lZx3fmlPdlPJpVBIdble\">ANSWER THESE GREATEST INTERVIEW QUESTIONS OF ALL TIME</a>\n        </p>\n        <br />\n        <p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>\n        <br />\n        <p><img src=\"https://cdn.hackernoon.com/images/the-hackernoon-newsletter-footer.png\" alt /></p>\n      \n            ","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"In a First, Surgical Robots Learned Tasks By Watching Videos","url":"https://hardware.slashdot.org/story/24/12/30/1349256/in-a-first-surgical-robots-learned-tasks-by-watching-videos?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735574460,"author":"msmash","unread":true,"content":"Speaking of robots, Johns Hopkins University and Stanford University researchers say they trained robots to perform surgical tasks autonomously using video learning, marking a breakthrough in robotic surgery capabilities. \n\nThe robots successfully manipulated needles, tied knots, and sutured wounds independently, demonstrating ability to correct errors like dropped needles without human input. Testing has advanced to full surgeries on animal cadavers. \n\nResearchers aim to address a projected U.S. surgeon shortage of 10,000-20,000 by 2036. The technology builds on decades of robot-assisted surgery, which recorded 876,000 procedures in 2020.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=In+a+First%2C+Surgical+Robots+Learned+Tasks+By+Watching+Videos%3A+https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1349256%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1349256%2Fin-a-first-surgical-robots-learned-tasks-by-watching-videos%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://hardware.slashdot.org/story/24/12/30/1349256/in-a-first-surgical-robots-learned-tasks-by-watching-videos?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564697&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Introducing ZeroShape: A Strong Regression-Based Zero-Shot 3D Shape Reconstruction Method","url":"https://hackernoon.com/introducing-zeroshape-a-strong-regression-based-zero-shot-3d-shape-reconstruction-method?source=rss","date":1735574421,"author":"The FewShot Prompting Publication","unread":true,"content":"<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/gH1qdz0Dt3mBaMdl9jIS\">Abstract and 1 Introduction</a></p>\n<p><a href=\"http://hackernoon.com/preview/aSTNB03lgvkkjZHFRzoC\">2. Related Work</a></p>\n<p><a href=\"http://hackernoon.com/preview/0scn6XiQ0BREvFGk0oL5\">3. Method and 3.1. Architecture</a></p>\n<p><a href=\"http://hackernoon.com/preview/Vr0Ad2am2GFeyaIOE6UW\">3.2. Loss and 3.3. Implementation Details</a></p>\n<p>4. Data Curation</p>\n<p><a href=\"http://hackernoon.com/preview/WTlB97rfr7YUwBrbOghS\">4.1. Training Dataset</a></p>\n<p><a href=\"http://hackernoon.com/preview/VGlrNJDi5CmpdgwVsiiv\">4.2. Evaluation Benchmark</a></p>\n<p><a href=\"http://hackernoon.com/preview/dvWHPgbXsIEpl6swOYnq\">5. Experiments and 5.1. Metrics</a></p>\n<p><a href=\"http://hackernoon.com/preview/Ytl9ckAbfos5VTv52Ioe\">5.2. Baselines</a></p>\n<p><a href=\"https://hackernoon.com/preview/Yhpn26VKOWgPM3XYLplp\">5.3. Comparison to SOTA Methods</a></p>\n<p><a href=\"https://hackernoon.com/preview/Xnc0atAoxT52eG6wEtYy\">5.4. Qualitative Results and 5.5. Ablation Study</a></p>\n<p><a href=\"http://hackernoon.com/preview/wpHCZGTcgp2kyN0axX87\">6. Limitations and Discussion</a></p>\n<p><a href=\"http://hackernoon.com/preview/16wpDr6a2WTKTkQDEUVB\">7. Conclusion and References</a></p>\n<p>\\\n<a href=\"http://hackernoon.com/preview/1C9qGmkudTc6oJpaV0oH\">A. Additional Qualitative Comparison</a></p>\n<p><a href=\"http://hackernoon.com/preview/Of5RJEdBN6rofK2JSsNx\">B. Inference on AI-generated Images</a></p>\n<p><a href=\"http://hackernoon.com/preview/04TDA2TsrsVxYurzi2VT\">C. Data Curation Details</a></p>\n<h2 id=\"abstract\">Abstract</h2>\n<p><em>We study the problem of single-image zero-shot 3D shape reconstruction. Recent works learn zero-shot shape reconstruction through generative modeling of 3D assets, but these models are computationally expensive at train and inference time. In contrast, the traditional approach to this problem is regression-based, where deterministic models are trained to directly regress the object shape. Such regression methods possess much higher computational efficiency than generative methods. This raises a natural question: is generative modeling necessary for high performance, or conversely, are regression-based approaches still competitive?</em></p>\n<p>\\\n<em>To answer this, we design a strong regression-based model, called ZeroShape, based on the converging findings in this field and a novel insight. We also curate a large real-world evaluation benchmark, with objects from three different real-world 3D datasets. This evaluation benchmark is more diverse and an order of magnitude larger than what prior works use to quantitatively evaluate their models, aiming at reducing the evaluation variance in our field. We show that ZeroShape not only achieves superior performance over state-of-the-art methods, but also demonstrates significantly higher computational and data efficiency.</em>[1]</p>\n<h2 id=\"1introduction\">1. Introduction</h2>\n<p>Inferring the properties of individual objects such as their category or 3D shape is a fundamental task in computer vision. The ultimate goal is to do this accurately for any object, generally referred to as zero-shot generalization. </p>\n<p>\\\nFor machine learning methods, this means high accuracy on data distributions that may be significantly different from the training set, such as images of novel types of objects like machine parts or images from uncommon visual contexts like underwater imagery. An object representation capable of zero-shot generalization, therefore, needs to accurately capture the visual properties that are shared across all</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-va830q6.png\" alt=\"Figure 1. We outperform SOTA methods for zero-shot 3D shape reconstruction, while having faster inference time and less training data. Circle size indicates the number of 3D assets used for training, with biggest being 3M[2]. F-Score with threshold 0.05 is averaged over Octroc3D [51], Pix3D [52] and OmniObject3D [66].\" /></p>\n<p>\\\nobjects in the world—an extremely ambitious goal.</p>\n<p>\\\nRecent work in computer vision has taken the broader challenge of zero-shot generalization head-on, with impressive developments for 2D vision tasks like segmentation [26, 42], visual question answering [3, 27], image generation [1, 48, 49], and in training general vision representations that can be easily adapted for any vision task [38, 43]. This progress has largely been enabled by increasing model size and scaling training dataset size to the order of tens to hundreds of millions of images.</p>\n<p>\\\nThese developments have inspired efforts which aim at zero-shot generalization for single image 3D object shape reconstruction [22, 30, 31, 37]. This is a classical and famously ill-posed problem, with important applications like virtual object placement in scenes in AR and object manipulation in robotics. </p>\n<p>\\\nThese works aim to learn a “zero-shot 3D shape prior” by relying on generative diffusion models for 3D point clouds [37], NeRFs [22], or for 2D images fine tuned for novel-view synthesis [30, 31], enabled by millionscale 3D data curation efforts such as Objaverse [9, 10]. While these methods show impressive zero-shot generalization ability, it comes at a great compute cost due to large model parameter counts and the inference-time sampling required by diffusion models. </p>\n<p>\\\nUsing expensive generative modeling for zero-shot 3D shape from single images diverges from the approach of early deep learning-based works on this task [50, 56, 63, 67, 70]. These works define the task as a 3D occupancy or signed distance regression problem and predict the shape of objects in a single forward pass. This raises a natural question: is generative modeling necessary for high performance at learning zeroshot 3D shape prior, or conversely, can a regression-based approach still be competitive?</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-wh930xe.png\" alt=\"Figure 2. ZeroShape reconstructions from in-the-wild images. Our method produces detailed and accurate object reconstructions from single-view images on a diverse set of objects.\" /></p>\n<p>\\\nIn this work, we find that regression approaches are indeed competitive if designed carefully, and computationally more efficient by a large margin compared to the generative counterparts. We propose ZeroShape: a regressionbased 3D shape reconstruction approach that achieves stateof-the-art zero-shot generalization, trained entirely on synthetic data, while requiring a fraction of the compute and data budget of prior work (see Fig. 1). We build our model upon key ingredients that facilitate generalization based on prior works: 1) usage of intermediate geometric representation (e.g. depth) [33, 56, 64, 67, 70], 2) explicit reasoning with local features [5, 63, 68]. </p>\n<p>\\\nSpecifically, we decompose the reconstruction into estimating the shape of the visible portion of the object, and then predicting the complete 3D object shape based on this initial prediction. The accurate estimation of the visible 3D surface is enabled by a joint modeling of camera intrinsics and depth, which we find to be essential for high accuracy.</p>\n<p>\\\nAnother thrust of our work is a large benchmark for evaluating zero-shot reconstruction performance. The 3D vision community is working on developing a zero-shot 3D shape prior, but what is the correct way to evaluate our progress? Currently we lack a well-defined benchmark, which has lead to well-curated qualitative results and small scale quantitative results[3] on different datasets across different papers. This makes it difficult to track progress and identify directions for future research. </p>\n<p>\\\nTo resolve this and standardize evaluation, we develop a protocol based on data generated from existing datasets of 3D object assets. Our benchmark includes thousands of common objects from hundreds of different categories and multiple data sources. We consider real images paired with 3D meshes [51, 52], and also generate photorealistic renders of 3D object scans [66]. Our large scale quantitative evaluation provides a rigorous perspective on the current state-of-the-art.</p>\n<p>\\\nIn summary, our contributions are:</p>\n<p>\\\n• ZeroShape: A regression-based zero-shot 3D shape reconstruction method with state-of-the-art performance at a fraction of the compute and data budget of prior work.</p>\n<p>\\\n• A unified large-scale evaluation benchmark for zero-shot 3D shape reconstruction, generated by standardized processing and rendering of existing 3D datasets.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2312.14198\">available on arxiv</a> under CC BY 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[1] Project website at: https://zixuanh.com/projects/zeroshape.html</p>\n<p>\\\n[2] We use 3M as a reference value. Point-E [37] and Shape-E [22] state a dataset size of “several million”.</p>\n<p>\\\n[3] On the order of hundreds of objects from tens of categories at best, to just a few dozen objects at worst.</p>\n<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Zixuan Huang, University of Illinois at Urbana-Champaign and both authors contributed equally to this work;</p>\n<p>(2) Stefan Stojanov, Georgia Institute of Technology and both authors contributed equally to this work;</p>\n<p>(3) Anh Thai, Georgia Institute of Technology;</p>\n<p>(4) Varun Jampani, Stability AI;</p>\n<p>(5) James M. Rehg, University of Illinois at Urbana-Champaign.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Building Vertical SaaS in an Old School Industry: AI That Fits, Not Fights","url":"https://hackernoon.com/building-vertical-saas-in-an-old-school-industry-ai-that-fits-not-fights?source=rss","date":1735574417,"author":"Shangyan","unread":true,"content":"<p>\\\nThe journey into building AI-driven SaaS took us into uncharted waters, especially once we realized how badly these old-school industries needed it. In <a href=\"https://hackernoon.com/building-vertical-saas-in-an-old-school-industry-what-not-to-do\">part 1 of this series</a>, I talked about the roadblocks we hit in building our first ERP product. Now, let’s dive into how AI helped us pivot and unlock new opportunities in an industry that's still trying to do things the old-fashioned way.</p>\n<p>\\</p>\n<h2 id=\"whereitallstarted\">Where It All Started</h2>\n<p>When we kicked off, e-commerce seemed like the obvious solution for distributors. We built tools to modernize their outdated order-entry processes, but we quickly hit a wall—chefs still preferred picking up the phone to call suppliers. They wouldn't budge from their old-school ways, and no sleek online system was going to change that overnight. We needed to meet them where they were.</p>\n<p>\\\nSo we went back to the drawing board and started calling up all the operators served by our supplier customers – everyone from active e-commerce app users, churned users, to chefs who vehemently opposed the idea of an app – to ask what they really thought. As it turned out, ordering from an app wasn’t the 10x-better experience we envisioned over calling/texting/emailing, unless we could somehow give them live visibility into product availability and delivery status, a problem we deemed too difficult to solve with a software product alone.</p>\n<p>\\\nAt the same time, we know wholesalers are still drowning in hours of manual order entry every night. How do we automate away all this tedious work? That’s when it clicked – large language models are a perfect fit to handle such workflows with unstructured data. AI dramatically changes the cost equation so we can automate operations that were previously too messy to – it’s the missing piece that may finally bring a technologically underserved industry like food distribution into the modern era. It’s truly a paradigm shift: around <a href=\"https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data\">80%</a> of the world’s data is unstructured.</p>\n<p>\\\nSo, we shifted gears. Rather than pushing both suppliers and operators to adopt fully digital workflows, we built AI-powered tools like Butter’s AI Order Assistant that complemented their existing process.</p>\n<p>\\</p>\n<h2 id=\"makingthepivotwithaimorethanpromises\">Making the Pivot with AI (More than Promises)</h2>\n<p>We learned pretty quickly that the success of AI wasn’t about building a “sexier” product — it was about making sure it actually completes user tasks. The AI Order Assistant didn’t ask chefs or distributors to reinvent their process. It adapted to what they already knew, slotting right into their workflows.</p>\n<p>\\\nThat’s all it takes. By building AI capable of processing natural-language orders (think voice commands or texts), we made the process easier, not harder. And because it was an add-on, not a full system replacement, distributors were quick to adopt it. Within weeks of launching, dozens of suppliers and ERP partners expressed interest. They saw it as an easy upgrade without the headaches that typically come with \"digital transformation.\"</p>\n<p>\\\nWhen a client onboards, we connect with their order desk email &amp; voicemail inbox, and automatically start converting incoming client orders into structured purchase order entries, leveraging each customer’s order guide content as well as their historical order patterns (read from their ERPs). For the first time ever, knowledge of a chef’s preferences was finally transferred from sales rep Joey’s head to a digital system — when a chef simply orders “2 cases of shrimp,” the system can accurately understand if they mean “4-6 Tiger Shrimp Frozen,” not “16-20 EZ Peel Shrimp,” or the 80 other shrimp product variations sold.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/rdXndhSmJSgxnJiSxZffRETRibf1-e903n8n.png\" alt=\"AI Orders interface, where supplier users can review and quickly modify AI model suggestions\" /></p>\n<p>\\\nKnowing that the AI suggestions won’t be 100% perfect, we conducted extensive UX interviews and ensured users can easily correct the model output so it gets it right the next time. Critically, we made sure everything can be performed with keyboard input alone, since they heavily rely on similar hotkeys in ERP systems to enter hundreds of orders. Users loved the experience and quickly jumped on board. The end result? Order processing time got reduced by over 96%, and supplier clients could reduce back-office staff count, or uplevel them to perform higher-value tasks like quality control and customer relations management.</p>\n<p>\\\nAfter Butter was acquired by GrubMarket, we took the AI Order Assistant model and scaled it into GrubAssist. This tool sits atop existing ERPs, providing natural-language business intelligence and analytics. It integrates painlessly into what the food industry knows and uses. And as anyone familiar with warehouse receiving or BOH knows, uninterrupted workflows are the only things keeping people sane.</p>\n<p>\\\nTakeaway: <strong>Start with an AI solution that fits into existing workflows, without overhauling everything. Ease of integration and familiarity are key to faster adoption.</strong></p>\n<p>\\</p>\n<h2 id=\"lessonsfrombuildingllmproduct\">Lessons from Building LLM Product</h2>\n<ol>\n<li><p><strong>Design around tech limits.</strong> LLMs are powerful but still maturing; they can lag or miss the mark on reliability. Clever design can hide some of the technical shortcomings. For example, since restaurants/retailers place their orders for the next day ahead of time, we can afford to process them in the background (before the supplier staff comes to work in the early mornings), opting for models with greater reasoning capacity while sacrificing some speed.</p>\n<p>\\</p></li>\n<li><p><strong>Speed before perfection…</strong> In early stages, don’t get bogged down in finding the “perfect” model. Use what’ll get you to market first. Simple techniques like RAG work surprisingly well if you give it the right context. If built the right way, AI-infused products automatically get better on their own when the underlying foundation model improves.</p>\n<p>\\</p></li>\n<li><p><strong>…but nail the foundation.</strong> Experimentation needs flexibility. Create a modular architecture so you can swap out models or features, and integrate clear, quantifiable in-product feedback systems — “building by vibes” doesn’t cut it. Your architecture should give you a solid iteration speed advantage.</p>\n<p>\\</p></li>\n<li><p><strong>Your interface can make (or break) your product.</strong> Even with a \"perfect\" model, start with the assumption that 20% of a task will need a human in the loop to QC. Make this interaction as simple and intuitive as possible, or you’ll lose user buy-in fast. The more you empower the human in the loop, the more you can leverage them to improve your product.</p>\n<p>\\</p></li>\n<li><p><strong>Capture unstructured knowledge.</strong> In old school industries, vital knowledge isn’t digitized—it's in people’s heads. If customer preferences exist only in Joey the Sales Rep’s mind, create an interface to capture it. These insights strengthen and differentiate your model, giving it a constantly evolving data advantage.</p>\n<p>\\</p></li>\n<li><p><strong>Feedback loops drive accuracy.</strong> Engineering takes your AI product far, but feedback takes it further. Provide a seamless way for users to input feedback directly within the product, and combine this with a tuning engine to drive more accurate, contextually relevant outputs.</p></li>\n</ol>\n<p>\\</p>\n<h2 id=\"thesecretofworkingwithlegacysystems\">The Secret of Working with Legacy Systems</h2>\n<p>Here’s the hard truth: no matter how great your AI solution is, you still need those old-school ERP players to want to integrate with you. It’s not enough to develop cutting-edge AI if it can’t communicate with the systems that distributors already rely on. And when you step in like you’re trying to replace them, you become impossible to work with.</p>\n<p>\\\nIn our case, we needed legacy ERPs to enable integrations through methods like EDI (Electronic Data Interchange) or SFTP file exchange. These legacy systems are deeply embedded, and convincing (then architecting) them to connect with new AI tools isn’t always easy. But we found a sweet spot by offering an add-on that actually improves their existing product, encouraging clients to stick to their existing infrastructure, while getting all the benefits of AI. The magic was in showing both the business and their infrastructure provider how our AI was a net positive, without making them feel like they needed to scrap everything or lose a partnership. Don’t get cut out by overlooking the existing network and overplaying your hand.</p>\n<p>\\\nThat said, the golden window for this kind of integration is closing fast. AI expertise is spreading, and even the slower, old-school service providers are getting into the game. You'll need to act quickly, find your angle, and work with the existing players.</p>\n<p>\\\nFor industry incumbents, beware of new software solutions that take an <a href=\"https://www.tidemarkcap.com/vskp-chapter/control-points-patterns-2024\">integrate and surround</a> approach. These are products that provide a fully self-contained business unit (e.g. field sales) and shift the cost/revenue equation significantly in their favor. Understanding these dynamics early on is key to choosing the right partners.</p>\n<p>\\\nTakeaway: <strong>Work alongside legacy systems, showing clear benefits and enhancements that don't force a complete overhaul. Help them see the value of a low-risk, high-reward addition.</strong></p>\n<p>\\</p>\n<h2 id=\"takeawaysforthefuture\">Takeaways for the Future</h2>\n<p>These traditional sectors that have relied on unstructured data—like handwritten logs and audio records—are finally accessible to modern tech solutions, thanks to LLMs. Vertical SaaS is quickly becoming more viable across these industries, and it’s tempting to apply AI to everything.</p>\n<p>\\\nStill, remember that AI’s success doesn’t come down to the tech alone. The key challenge still lies in achieving product-market fit. While AI breakthroughs open doors, they don’t change the fundamentals of product development. Start by developing a clear understanding of your users and their needs; the technology will follow.</p>\n<p>\\\nLooking back, the biggest lesson we learned was that AI succeeds best when it fits into existing processes, not when it tries to upend them. The question is, who's going to seize the opportunity before it’s gone? There's a little more to the story. In part 3, I’ll explore how combining a rollup strategy with AI creates a winning formula for SaaS in traditional industries.</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia completes acquisition of AI infrastructure startup Run:ai","url":"https://techcrunch.com/2024/12/30/nvidia-completes-acquisition-of-ai-infrastructure-startup-runai/","date":1735574294,"author":"Kyle Wiggers","unread":true,"content":"<p>Nvidia has completed its acquisition of Run:ai, an Israeli startup that helps manage and optimize AI hardware infrastructure. As part of the merger, Run:ai said its software, which currently only works with Nvidia products, will be open sourced, meaning Nvidia rivals like AMD and Intel will be able to adapt it for their hardware. &#8220;We [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"UK antitrust watchdog launches review of IBM’s HashiCorp takeover","url":"https://techcrunch.com/2024/12/30/uk-antitrust-watchdog-launches-review-of-ibms-hashicorp-takeover/","date":1735573492,"author":"Kyle Wiggers","unread":true,"content":"<p>The Competition and Markets Authority (CMA), the U.K.&#8217;s antitrust watchdog, has opened an investigation into whether IBM&#8217;s planned acquisition of cloud software vendor HashiCorp would affect competition. The CMA said Monday it was inviting comment on the merger from interested parties by January 16. The regulator set a provisional February 25 deadline to decide whether [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Cineflicks To Launch First-Ever Watch-to-Earn Streaming Platform With Rewards For Viewers","url":"https://hackernoon.com/cineflicks-to-launch-first-ever-watch-to-earn-streaming-platform-with-rewards-for-viewers?source=rss","date":1735572442,"author":"BTCWire","unread":true,"content":"<p>Cineflicks, an upcoming OTT streaming platform, is set to redefine the entertainment landscape by launching a revolutionary “watch-to-earn” model that rewards users with cryptocurrency tokens for every minute of content consumed. </p>\n<p>\\\nAiming to go beyond traditional subscription-based services, Cineflicks provides users with not only diverse and engaging content but also the opportunity to earn CNF Tokens, creating a unique streaming experience that integrates digital finance with entertainment.</p>\n<p>\\\nBy accumulating CNF Tokens through viewing time, users unlock real-world value in their entertainment habits, setting Cineflicks apart in the crowded OTT market. These tokens can be retained as digital assets or converted into fiat currency, offering viewers a financial return on their screen time.</p>\n<p>\\</p>\n<blockquote>\n  <p>“With Cineflicks, users are empowered to turn their viewing time into a genuine earning opportunity,” said Thomas Caddick, CEO at Cineflicks. “We’re transforming the streaming experience into a dynamic, rewarding ecosystem that brings together entertainment and the fast-growing world of digital assets.”</p>\n</blockquote>\n<p>\\\nThe platform’s launch will be preceded by a presale of CNF Tokens, allowing early supporters to invest and gain a stake in Cineflicks’ future. This presale not only provides early investors with access to CNF Tokens at a pre-market rate but also builds an engaged community around the app, helping to generate excitement for the full launch.</p>\n<p>\\\nCineflicks aims to foster a vibrant user base from the start, with a unique community-based approach that blends entertainment and finance.</p>\n<p>\\\nUpon launch, Cineflicks will feature a rich catalogue of popular movies, TV shows, and exclusive content, carefully curated to appeal to a broad audience.</p>\n<p>\\\nThe platform’s intuitive interface, accessible across multiple devices, ensures users enjoy a seamless experience while earning rewards. Cineflicks also plans regular content updates, delivering fresh and diverse entertainment options to maintain viewer engagement.</p>\n<p>\\\nThe “watch-to-earn” system reimagines audience engagement by rewarding users based on time spent viewing rather than relying solely on subscription fees. Cineflicks’ blockchain-based model offers secure, transparent transactions and a decentralised structure, appealing to both casual viewers and tech-savvy users interested in the benefits of cryptocurrency.</p>\n<p>\\\nBy combining the appeal of streaming with the potential of digital assets, Cineflicks brings a fresh perspective to the OTT space, providing both immediate entertainment and a stake in a growing digital ecosystem.</p>\n<p>\\\nCineflicks’ launch has already sparked interest among entertainment and digital finance communities, as its innovative model may inspire a shift in OTT viewer dynamics. </p>\n<p>\\\nAs the platform prepares for its app launch and token presale, industry observers are watching closely to see how this unique approach will impact the streaming landscape. Cineflicks is poised to be a pioneering force in OTT by turning screen time into an investment, reshaping how audiences view and interact with content in the digital age.</p>\n<p>\\\nAs the new year approaches, Cineflicks is gearing up to usher in a new era for streaming with its exciting Cineflicks Ambassadors Club campaign. </p>\n<p>\\\nTo celebrate the upcoming launch of their groundbreaking “Watch to Earn” streaming platform, Cineflicks is offering an incredible $5,000 USDT and 20 million CNF tokens in prizes, totaling over $2 million in rewards.</p>\n<p>\\\nCineflicks is inviting users to join the Cineflicks Ambassadors Club and be part of the next big revolution in the entertainment industry. With the app launch just around the corner, Cineflicks is ready to change the way users experience entertainment and rewards, making 2025 the year of endless possibilities.</p>\n<h3 id=\"aboutcineflicks\">About Cineflicks</h3>\n<p>Cineflicks is the world’s first incentivised streaming OTT platform, combining entertainment with crypto rewards. With a vast library of content across multiple genres, Cineflicks offers a unique experience where viewers earn CNF tokens for every hour they watch.</p>\n<p>\\\nOur platform features personalised recommendations, multi-device access, and Web3 integration for secure, transparent rewards. We’re redefining the future of streaming by making screen time profitable, fun, and rewarding for everyone. Dive into the world of Cineflicks — where watching is more than entertainment; it’s an investment in your enjoyment!</p>\n<p><strong><a href=\"https://cineflicks.org/\">Website</a></strong> I <strong><a href=\"https://x.com/cineflicksorg\">Twitter</a></strong> I <strong><a href=\"https://t.me/cineflickschat\">Telegram</a></strong> I <strong><a href=\"https://www.instagram.com/cineflicksorg/\">Instagram</a></strong> I <strong><a href=\"https://discord.gg/zM9JchKNDg\">Discord</a></strong></p>\n<p>:::tip\nThis story was distributed as a release by Btcwire under HackerNoon’s Business Blogging Program. Learn more about the program&nbsp;<strong><a href=\"https://business.hackernoon.com/business-blogging?ref=hackernoon.com\">here</a></strong></p>\n<p>:::</p>\n<p>\\\n\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia Bets on Robotics To Drive Future Growth","url":"https://hardware.slashdot.org/story/24/12/30/1340245/nvidia-bets-on-robotics-to-drive-future-growth?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735572120,"author":"msmash","unread":true,"content":"An anonymous reader shares a report: Nvidia is betting on robotics as its next big driver of growth, as the world's most valuable semiconductor company faces increasing competition in its core AI chipmaking business. The US tech group, best known for the infrastructure that has underpinned the AI boom, is set to launch its latest generation of compact computers for humanoid robots [non-paywalled link] -- dubbed Jetson Thor -- in the first half of 2025. \n\nNvidia is positioning itself to be the leading platform for what the tech group believes is an imminent robotics revolution. The company sells a \"full stack\" solution, from the layers of software for training AI-powered robots to the chips that go into them. [...] Talla said a shift in the robotics market is being driven by two technological breakthroughs: the explosion of generative AI models and the ability to train robots on these foundational models using simulated environments. The latter has been a particularly significant development as it helps solve what roboticists call the \"Sim-to-Real gap,\" ensuring robots trained in virtual environments can operate effectively in the real world, he said.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Nvidia+Bets+on+Robotics+To+Drive+Future+Growth%3A+https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1340245%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fhardware.slashdot.org%2Fstory%2F24%2F12%2F30%2F1340245%2Fnvidia-bets-on-robotics-to-drive-future-growth%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://hardware.slashdot.org/story/24/12/30/1340245/nvidia-bets-on-robotics-to-drive-future-growth?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564695&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Backed by a16z and QED, Brazilian startup Carecode puts AI agents to work on healthcare","url":"https://techcrunch.com/2024/12/30/backed-by-a16z-and-qed-brazilian-startup-carecode-puts-ai-agents-to-work-on-healthcare/","date":1735571223,"author":"Anna Heim","unread":true,"content":"<p>AI holds huge promise for healthcare, but not just on the medical side; many startups are convinced machine learning-based systems can do a lot of good on adjacent tasks such as appointment scheduling and confirmations. Brazilian startup Carecode is among these AI believers. It&#8217;s coming out of stealth with an ambition to reduce healthcare costs [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"MEXC Elevates Trading Experience With Zero-Fee Futures And Exclusive Rewards For SVIP Traders","url":"https://hackernoon.com/mexc-elevates-trading-experience-with-zero-fee-futures-and-exclusive-rewards-for-svip-traders?source=rss","date":1735570854,"author":"BTCWire","unread":true,"content":"<p>Seychelles, December 30, 2024 — MEXC, a leading global cryptocurrency exchange, announces the launch of its exclusive SVIP New Year Trading Fee Promotion, designed to empower sophisticated traders with zero-fee Futures trading and exclusive rewards.</p>\n<p>\\\nThis latest initiative underscores MEXC's commitment to providing value-added services that cater to the evolving needs of the diverse crypto investor community.</p>\n<p>\\\nWith smart money increasingly driving market trends, high-volume traders play a pivotal role in shaping market dynamics. MEXC's SVIP program meets the needs of these sophisticated traders by providing access to industry-leading liquidity, a broad selection of trending tokens, and exclusive benefits.</p>\n<p>\\\nBy securing SVIP status, traders can elevate their trading experience to new heights in 2025, benefiting from the lowest fees in the industry.</p>\n<h3 id=\"exclusivefeestructureformexcsvips\">Exclusive Fee Structure for MEXC SVIPs</h3>\n<p>MEXC's SVIP program offers the lowest fees in the industry:</p>\n<p>\\</p>\n<ul>\n<li>Futures: 60% discount on both Maker and Taker fees</li>\n<li>Spot: 60% discount on both Maker and Taker fees</li>\n</ul>\n<p>\\\nHigh-volume traders can benefit from even lower fees as they reach specific trading milestones:</p>\n<p>\\</p>\n<ul>\n<li>During the SVIP experience period, users who achieve a cumulative Futures trading volume of 1 billion USDT will continue to enjoy the exclusive fee structure.</li>\n<li>Upon reaching a cumulative Futures trading volume of 5 billion USDT, Taker fees will be further reduced to 0.005%, ensuring even more fee discounts for frequent traders.</li>\n</ul>\n<p>\\\nTo further incentivize active participation, the first 50 SVIP traders to achieve a cumulative Futures trading volume of 1 billion USDT between December 23, 2024 (UTC) and January 31, 2025 (UTC) will receive a luxurious Louis Vuitton card holder, adding a touch of elegance to their trading journey.</p>\n<h3 id=\"howtoapplyforsvip\">How to Apply for SVIP</h3>\n<p>Becoming an SVIP is simple and rewarding:</p>\n<ol>\n<li>Visit the <strong><a href=\"https://www.mexc.com/VIPProgram\">event page</a></strong> to view details and prepare the required documentation.</li>\n<li>Submit proof of a cumulative trading volume of at least 10 million USDT (Spot and Futures combined) on other platforms within the past 30 days.</li>\n<li>Upon approval, start enjoying the exclusive benefits of MEXC SVIP status immediately.</li>\n</ol>\n<h3 id=\"aneweraforcryptotradersmexcsvisionfor2025\">A New Era for Crypto Traders: MEXC's Vision for 2025</h3>\n<p>The launch of the SVIP New Year Trading Fee Promotion is part of MEXC's broader strategy to continually innovate and set the standard for premium services in the cryptocurrency industry.</p>\n<p>\\\nBy offering low fees, exclusive rewards, and personalized benefits, MEXC is committed to supporting high-net-worth traders and positioning itself as the platform of choice for sophisticated traders seeking a seamless and rewarding experience.</p>\n<p>\\\nAs the global leader in cryptocurrency exchange services, MEXC continues to prioritize innovation, security, and a user-first approach. Serving over 30 million users in 170+ countries, MEXC remains dedicated to providing a secure, efficient, and accessible platform for traders at every level.</p>\n<h3 id=\"aboutmexc\">About MEXC</h3>\n<p>Founded in 2018,<a href=\"https://www.mexc.com/\"> </a><strong><a href=\"https://www.mexc.com/\">MEXC</a></strong> is committed to being \"Your Easiest Way to Crypto.\" Serving over 30 million users across 170+ countries, MEXC is known for its broad selection of trending tokens, frequent airdrop opportunities, and low trading fees. </p>\n<p>\\\nOur user-friendly platform is designed to support both new traders and experienced investors, offering secure and efficient access to digital assets. MEXC prioritizes simplicity and innovation, making crypto trading more accessible and rewarding.</p>\n<p><strong><a href=\"https://www.mexc.com/\">Official Website</a></strong>｜ <strong><a href=\"https://twitter.com/MEXC_Official\">X </a></strong>｜<a href=\"https://t.me/MEXCEnglish\"> </a><strong><a href=\"https://t.me/MEXCEnglish\">Telegram</a></strong> ｜<strong><a href=\"https://www.mexc.com/learn/article/17827791509543\">Sign up on MEXC</a></strong> ｜<strong><a href=\"https://www.mexc.com/VIPProgram\">Become an SVIP</a></strong></p>\n<p>:::tip\nThis story was distributed as a release by Btcwire under HackerNoon’s Business Blogging Program. Learn more about the program&nbsp;<strong><a href=\"https://business.hackernoon.com/business-blogging?ref=hackernoon.com\">here</a></strong></p>\n<p>:::</p>\n<p>\\\n \\n  \\n </p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Meet Fastex: HackerNoon Company of the Week","url":"https://hackernoon.com/meet-fastex-hackernoon-company-of-the-week?source=rss","date":1735570811,"author":"Company of the Week","unread":true,"content":"<p>We are back with another&nbsp;<strong><a href=\"https://hackernoon.com/u/companyoftheweek\">Company of the Week</a></strong>&nbsp;feature! Every week, we share an awesome tech brand from our&nbsp;<strong><a href=\"https://hackernoon.com/companies\">tech company database</a></strong>, making their evergreen mark on the internet. This unique HackerNoon database ranks S&amp;P 500 companies and top startups of the year alike.</p>\n<p>\\\nThis week, we are proud to present <strong><a href=\"https://hackernoon.com/company/fastex\">Fastex</a></strong>, the all-encompassing ecosystem redefining the Web3 experience.</p>\n<p>\\\nFastex is a web 3.0 solution-based ecosystem built to deliver multiple centralized and decentralized solutions for advanced technological experience; it also powers <a href=\"https://www.ftnft.com/?ref=hackernoon.com\">ftNFT</a>,&nbsp;a leading NFT platform comprising a marketplace and four phygital spaces operating within the Fastex Web3 ecosystem.</p>\n<p>\\</p>\n<p>:::tip\n<a href=\"https://techcompanynewspage.paperform.co/\">Want to be featured on HackerNoon’s Company of the Week? Request Your Tech Company Page on HackerNoon</a>!</p>\n<p>:::</p>\n<hr />\n<p><img src=\"https://cdn.hackernoon.com/images/hQ098u52DzPm2Y4UITQcQXtLRAk2-v493xcc.jpeg\" alt=\"\" /></p>\n<p>\\</p>\n<hr />\n<h2 id=\"fastexhackernoontargetedads\">Fastex &lt;&gt; HackerNoon Targeted Ads</h2>\n<p>Fastex recently partnered with HackerNoon to announce the <a href=\"https://hackernoon.com/announcing-the-ftnft-yocerebrum-awards-volume-3-eden-of-innovation-and-creativity\">third iteration</a> of its <a href=\"https://www.ftnftinternationalawards.com/?utm_source=nftevening&utm_medium=pr&utm_campaign=awards&ref=hackernoon.com\">ftNFT YoCerebrum Awards</a>. The event invited all</p>\n<p>NFT enthusiasts to participate, nominate, and celebrate the pinnacle of NFT achievements. The event was not just about recognition but also about fostering a vibrant community where creativity and intellect converged.</p>\n<p>\\\nThe awards began with a nomination and public voting process in October 2024, culminating in a prestigious ceremony and afterparty on Nov. 14. <a href=\"https://hackernoon.com/bahamut-music-art-and-media-winners-announced-in-ftnft-yocerebrum-awards-2024-malta\">Winners</a> were determined through public blockchain votes, <a href=\"http://ftnft.com/?ref=hackernoon.com\">ftNFT.com</a> marketplace data, and <a href=\"https://www.ftnftinternationalawards.com/jury?ref=hackernoon.com\">jury</a> considerations and received a fabulous Bahamut Trophy and 2024 FTN Tokens in prizes.</p>\n<p>\\</p>\n<blockquote>\n  <p><strong><em><a href=\"https://calendly.com/hackernoon\">Learn How You Can Advertise to Your Specific Niche on HackerNoon</a></em></strong></p>\n</blockquote>\n<p>\\</p>\n<h2 id=\"meetfastexfunfacts\">Meet Fastex: #FunFacts</h2>\n<p>HackerNoon was surprised to learn that it too was nominated in the <a href=\"https://www.ftnftinternationalawards.com/?utm_source=nftevening&utm_medium=pr&utm_campaign=awards&ref=hackernoon.com\">ftNFT YoCerebrum Awards</a> in the <a href=\"https://hackernoon.com/bahamut-music-art-and-media-winners-announced-in-ftnft-yocerebrum-awards-2024-malta\">Best Web 3.0 Media Coverage</a>. The announcement, quite literally, came out of nowhere and was a proud moment for the entire team here at HackerNoon.</p>\n<p><a href=\"https://hackernoon.com/bahamut-music-art-and-media-winners-announced-in-ftnft-yocerebrum-awards-2024-malta?embedable=true\">https://hackernoon.com/bahamut-music-art-and-media-winners-announced-in-ftnft-yocerebrum-awards-2024-malta?embedable=true</a></p>\n<p>\\\nAnd then, like magic, it was announced that HackerNoon won the award! Yup, you read it right, HackerNoon received the  award of the <a href=\"https://hackernoon.com/bahamut-music-art-and-media-winners-announced-in-ftnft-yocerebrum-awards-2024-malta\">Best Web 3.0 Media Coverage</a> during the ftNFT International Awards held on Nov 14th, 2024, in Malta. :partying<em>face::partying</em>face::partying<em>face::partying</em>face:</p>\n<p>\\\nNothing could have made us happier! 😏</p>\n<p>\\</p>\n<blockquote>\n  <p><strong><a href=\"https://business.hackernoon.com/business-blogging\">Share Your Company's Story via HackerNoon</a></strong></p>\n</blockquote>\n<p>\\\n\\\nThat's all this week, folks! Stay Creative, Stay Iconic.</p>\n<p>The HackerNoon Team</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Why China Is Building a Thorium Molten-Salt Reactor","url":"https://spectrum.ieee.org/chinas-thorium-molten-salt-reactor","date":1735570803,"author":"Emily Waltz","unread":true,"content":"<p>China’s demo reactor could breed nuclear fuel from rare earth waste </p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzQxMy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc3Nzk3MzY3OH0.Ry1f7Rt0Q--yMVdfFmvPrkXq8W1khLk8t2lBTemu6co/image.png?width=600","enclosureMime":""},{"title":"Mercedes-backed Volocopter files for bankruptcy","url":"https://techcrunch.com/2024/12/30/mercedes-backed-volocopter-files-for-bankruptcy/","date":1735570669,"author":"Sean O'Kane","unread":true,"content":"<p>German electric air taxi company Volocopter has filed for bankruptcy protection, the latest in a string of similar startups to hit financial turbulence. The company plans to keep operating while it searches for new investors. &#8220;We are ahead of our industry peers in our technological, flight test, and certification progress. That makes us an attractive [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Chess Federation Changes Rules To Allow Jeans Amid Spat; Magnus Carlsen Returns","url":"https://games.slashdot.org/story/24/12/30/1436223/chess-federation-changes-rules-to-allow-jeans-amid-spat-magnus-carlsen-returns?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735569360,"author":"msmash","unread":true,"content":"World chess champion Magnus Carlsen has returned to the International Chess Federation (FIDE) World Rapid and Blitz Championships after new rules allowed players to wear \"elegant\" jeans with jackets. \n\nCarlsen had withdrawn from the New York tournament when officials demanded he change out of jeans he wore after a lunch meeting, threatening him with fines and disqualification. FIDE revised its dress code following the incident, permitting \"appropriate jeans matching the jacket\" as an \"elegant minor deviation\" from standard attire.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Chess+Federation+Changes+Rules+To+Allow+Jeans+Amid+Spat%3B+Magnus+Carlsen+Returns%3A+https%3A%2F%2Fgames.slashdot.org%2Fstory%2F24%2F12%2F30%2F1436223%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fgames.slashdot.org%2Fstory%2F24%2F12%2F30%2F1436223%2Fchess-federation-changes-rules-to-allow-jeans-amid-spat-magnus-carlsen-returns%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://games.slashdot.org/story/24/12/30/1436223/chess-federation-changes-rules-to-allow-jeans-amid-spat-magnus-carlsen-returns?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564755&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Trump Defends Foreign Worker Visas","url":"https://news.slashdot.org/story/24/12/30/0910218/trump-defends-foreign-worker-visas?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735567260,"author":"msmash","unread":true,"content":"President-elect Donald Trump has defended the H-1B visa program for skilled foreign workers. \"I've always liked the visas. I have many H-1B visas on my properties... It's a great program,\" Trump told The New York Post. \n\nHis comments follow recent support for the program from Elon Musk and Vivek Ramaswamy. The H-1B program allows 85,000 skilled workers to immigrate annually, including 20,000 spots for those with U.S. advanced degrees. Trump's businesses have received approval to hire over 2,100 foreign workers since 2008, with about 70 positions through H-1B visas, mostly over a decade ago.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Trump+Defends+Foreign+Worker+Visas%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0910218%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0910218%2Ftrump-defends-foreign-worker-visas%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/30/0910218/trump-defends-foreign-worker-visas?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564539&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Mesa's Terrific Year With Better Vulkan Ray-Tracing, NVK Progress & Same-Day Vulkan 1.4","url":"https://www.phoronix.com/news/Mesa-2024-Highlights","date":1735567220,"author":"Michael Larabel","unread":true,"content":"The open-source Mesa 3D graphics driver had a rather great year with a number of performance optimizations landing, on-time support for Intel Lunar Lake and Battlemage Xe2 graphics, early AMD RDNA4 support, multiple drivers having same-day Vulkan 1.4 support, the continued progress of the open-source NVIDIA NVK Vulkan driver, and much more thanks to the contributions of Intel, AMD, Valve, and other organizations -- even Microsoft's continued merge requests!..","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Top 10 Semiconductor Stories of 2024","url":"https://spectrum.ieee.org/top-semiconductor-stories-2024","date":1735567203,"author":"Samuel K. Moore","unread":true,"content":"<p>Trillion-transistor GPUs, steel-slicing laser chips, particle accelerators, and more</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4Mjg1NS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5MzQ3NDE0OH0.zUCZ-8vA8dnwxNkqZIo5LfGU9Bax-iSKCTfPZoCjmcc/image.jpg?width=600","enclosureMime":""},{"title":"Most Safety Complaints From Plane-Industry Whistleblowers 'Go Nowhere', Risk Retaliation","url":"https://yro.slashdot.org/story/24/12/30/0152249/most-safety-complaints-from-plane-industry-whistleblowers-go-nowhere-risk-retaliation?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735562040,"author":"EditorDavid","unread":true,"content":"America's aerospace industry is overseen by the Federal Aviation Administration (or FAA) &mdash; which also handles safety warnings from the industry's whistleblowers. But the Seattle Times says an analysis of reports to Congress found \"an overwhelmed system delivering underwhelming results for whistleblowers... More than 90% of safety complaints from 2020 through 2023 ended with no violation found by the FAA, while whistleblowers reported them at great personal and professional risk.\"\n\nAside from the FAA's in-house program, employees of Boeing, Spirit and the FAA can report safety hazards to the Office of Special Counsel, which has no FAA ties, or through internal employer complaint programs, such as Boeing's Speak Up and Spirit's Quality 360, to trigger company reviews... In the aftermath of the door-plug blowout over Portland, Boeing specifically asked its employees to use the Speak Up program or the FAA's internal process to report any concerns, according to Boeing spokesperson Jessica Kowal. Both have done a poor job protecting whistleblowers from retaliation, according to a congressionally appointed expert panel... While both were designed to guard against retaliation, critics say they have instead become enablers of it... \n\nA panel of aviation safety experts in February rebuked Boeing's Speak Up program in a report to Congress. Whistleblower advocates criticized Speak Up for commonly outing whistleblowers to the supervisors they're complaining about, exposing them to retaliation. Managers sometimes investigated complaints against themselves. Employees mistrusted the program's promise of anonymity. Collectively, the befuddling maze of whistleblower options sowed \"confusion about reporting systems that may discourage employees from submitting safety concerns,\" according to the expert panel's report.... \n\n[Boeing quality inspector Sam Mohawk, who alleged the 737 MAX line in Renton was losing track of subpar aircraft parts], continues to pursue his FAA claim, originally submitted through Boeing's Speak Up program. Months passed before Boeing addressed Mohawk's complaint. When it did, Mohawk's report was passed to the managers he was complaining about, according to Brian Knowles, Mohawk's South Carolina-based lawyer. \"If you do Speak Up, just know that your report is going to go straight to the guys you're accusing of wrongdoing. They aren't going to say, 'Thanks for speaking up against us,'\" Knowles said. \nThe article includes this quote about the FAA's in-house whistleblower program from Tom Devine, a whistleblower attorney with nearly a half-century of experience across a spectrum of federal agencies, and legal director of the nonprofit Government Accountability Project, which helps whistleblowers navigate the federal system. \"It's been a disaster from the beginning. We tell everyone to avoid it because it's a trap... We've warned whistleblowers not to entrust their rights there.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Most+Safety+Complaints+From+Plane-Industry+Whistleblowers+'Go+Nowhere'%2C+Risk+Retaliation%3A+https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F30%2F0152249%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fyro.slashdot.org%2Fstory%2F24%2F12%2F30%2F0152249%2Fmost-safety-complaints-from-plane-industry-whistleblowers-go-nowhere-risk-retaliation%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://yro.slashdot.org/story/24/12/30/0152249/most-safety-complaints-from-plane-industry-whistleblowers-go-nowhere-risk-retaliation?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564391&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"RadeonSI Driver Now Uses ACO By Default For Pre-RDNA GPUs","url":"https://www.phoronix.com/news/RadeonSI-ACO-Default-Pre-GFX10","date":1735560480,"author":"Michael Larabel","unread":true,"content":"As a very interesting end-of-year change for Mesa 25.0, AMD is now using the ACO compiler by default for pre-GFX10 (before RDNA / Navi) GPUs with the RadeonSI Gallium3D driver...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Passkey technology is elegant, but it’s most definitely not usable security","url":"https://arstechnica.com/security/2024/12/passkey-technology-is-elegant-but-its-most-definitely-not-usable-security/","date":1735560053,"author":"Dan Goodin","unread":true,"content":"\n              <p>It's that time again, when families and friends gather and implore the more technically inclined among them to troubleshoot problems they're having behind the device screens all around them. One of the most vexing and most common problems is logging into accounts in a way that's both secure and reliable.</p>\n<p>Using the same password everywhere is easy, but in an age of mass data breaches and precision-orchestrated phishing attacks, it's also highly unadvisable. Then again, creating hundreds of unique passwords, storing them securely, and keeping them out of the hands of phishers and database hackers is hard enough for experts, let alone Uncle Charlie, who got his first smartphone only a few years ago. No wonder this problem never goes away.</p>\n<p>Passkeys—the much-talked-about password alternative to passwords that have been widely available for almost two years—was supposed to fix all that. When I wrote about passkeys <a href=\"https://arstechnica.com/information-technology/2023/05/passwordless-google-accounts-are-easier-and-more-secure-than-passwords-heres-why/\">two years ago</a>, I was a big believer. I remain convinced that passkeys mount the steepest hurdle yet for phishers, SIM swappers, database plunderers, and other adversaries trying to hijack accounts. How and why is that?</p><p><a href=\"https://arstechnica.com/security/2024/12/passkey-technology-is-elegant-but-its-most-definitely-not-usable-security/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/security/2024/12/passkey-technology-is-elegant-but-its-most-definitely-not-usable-security/#comments\">Comments</a></p>\n\n            ","flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/12/passkey-1000x648.jpg","enclosureMime":""},{"title":"Updated Serpent OS Alpha Brings Few Fixes To This Original Linux Distribution","url":"https://www.phoronix.com/news/Serpent-OS-Alpha-Update","date":1735558691,"author":"Michael Larabel","unread":true,"content":"Last week Ikey Doherty's Serpent OS Linux distribution debuted in alpha form while kicking off the new week is updated install media to provide a few fixes for this original from-scratch Linux distribution...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AMD's GPUOpen Vulkan Memory Allocator Now Supports Vulkan 1.4","url":"https://www.phoronix.com/news/Vulkan-Memory-Allocator-3.2","date":1735557990,"author":"Michael Larabel","unread":true,"content":"AMD's GPUOpen team managed to squeeze in a new Vulkan Memory Allocator release into 2024. As a reminder this is a easy to use/integrate Vulkan memory allocation library for both Windows and Linux systems with hopes of making memory allocation and resource creation more easier like with Direct3D 11 and OpenGL...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"xxHash 0.8.3 Brings Runtime Vector Extension Handling For x86/x86_64","url":"https://www.phoronix.com/news/xxHash-0.8.3-Released","date":1735557475,"author":"Michael Larabel","unread":true,"content":"Meta's Yann Collet of Zstd fame is rounding out 2024 by releasing xxHash 0.8.3 as the newest update to this extremely fast non-cryptographic hash algorithm. The xxHash fast hash algorithm pushes for RAM speed limits and with the v0.8.3 update brings more enhancements...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Calo raises $25 million to expand its ready-to-eat meal service beyond the Middle East","url":"https://techcrunch.com/2024/12/30/calo-raises-25-million-to-expand-its-ready-to-eat-meal-service-beyond-middle-east/","date":1735554600,"author":"Ivan Mehta","unread":true,"content":"<p>A business built around increasingly customized ready-to-eat meals has netted Middle Eastern startup Calo a sizeable funding injection as it looks to expand both what it can offer its time-strapped customers and where it delivers its growing range of just-heat-to-eat dishes. The meal delivery market in the Middle East will hit $11.2 billion by 2030, [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AT&T and Verizon say networks are secure after being breached by China-linked Salt Typhoon hackers","url":"https://techcrunch.com/2024/12/30/verizon-says-it-has-secured-its-network-after-breach-by-china-linked-salt-typhoon-group/","date":1735553716,"author":"Carly Page","unread":true,"content":"<p>U.S. telecom giants AT&#38;T and Verizon say they have secured their networks after being targeted by the&#160;China-linked Salt Typhoon cyberespionage group. In a statement given to TechCrunch on Monday, AT&#38;T spokesperson Alexander Byers said the company detects &#8220;no activity by nation-state actors in our networks at this time.&#8221; Verizon spokesperson Richard Young said in an [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Open-Source Platform Is Letting AI Researchers Crack Tough Languages","url":"https://hackernoon.com/new-open-source-platform-is-letting-ai-researchers-crack-tough-languages?source=rss","date":1735553711,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 10 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"5conclusions\">5. Conclusions</h2>\n<p>In this work, we propose a revised approach to NLPre evaluation via benchmarking. This is motivated by the widespread use of the benchmarking technique in other NLP fields on par with the shortcomings of existing NLPre evaluation solutions.</p>\n<p>\\\nWe implement said NLPre benchmarking approach as the online system that evaluates the submitted outcome of an NLPre system and updates the associated leaderboard with the results after the submitter’s approval. The benchmarking system is designed to rank NLPre tools available for a given language in a trustworthy environment.</p>\n<p>\\\nThe endeavour of defining and enhancing the system’s capabilities is conducted concurrently with the effort to create the NLPre benchmark for Polish that encompasses numerous factors, such as tasks not required in English or diverse tagsets. The NLPre-PL benchmark consists of the predefined NLPre tasks, coupled with two reformulated datasets. The NLPre-PL benchmark, therefore, sets the standard for evaluating the performance of the NLPre tools for Polish, which represents a derivative yet important outcome of our research.</p>\n<p>\\\nIn addition to integration into the benchmarking system, NLPre-PL is used to conduct empirical experiments. We perform a robust and extensive comparison of different NLPre methods, including the classical non-neural tools and the modern neural network-based techniques. The results of these experiments on datasets in two tagsets are discussed in detail. The experiments confirm our assumptions that modern architectures obtain better results. Because NLP is a discipline undergoing rapid progress, new NLPre solutions, e.g. multilingual or zero-shot, can be expected in the coming years. These new solutions can be easily tested and compared with the tools evaluated so far in our benchmarking system.</p>\n<p>\\\nFinally, we release the open-source code of the benchmarking system in hopes that this endeavour could be replicated for other languages. To expedite this process, we ensure that the system is fully configurable and language- and tagset-agnostic. The NLPre system, configured for a specified language, can be self-hosted on a chosen server, and the results from the leaderboard are conveniently accessible via an API. We see a potential future application of our system to the UD repository, where for 141 languages, there are currently 245 treebanks with supposedly discrepant versions of the UD tagset.</p>\n<h2 id=\"6appendices\">6. Appendices</h2>\n<h3 id=\"61infrastructureused\">6.1. Infrastructure used</h3>\n<p>We train the models using several types of computational nodes at our disposal, including NVIDIA V100 32GB, NVIDIA GeForce RTX 2080 8GB, NVIDIA GeForce 3070 8GB and Intel Xeon E5-2697 processor. Since we do not perform hyperparameter tuning, this should not impact our results.</p>\n<h3 id=\"62furtherresultsofexperiments\">6.2. Further results of experiments</h3>\n<p>Herein, we present a comprehensive depiction of our experimental findings as they are displayed on the NLPre-PL leaderboard.</p>\n<p>\\\nIn Table 5, we present the full results of the evaluation of the selected models on the Morfeuszbased datasets byName and byType. These results are provided for all available tasks that can be performed on the above-mentioned datasets. As NKJP1M datasets contain no syntantic trees, it is thus impossible to test the dependency parsing task that rely on these trees and measure UAS, LAS, CLAS, MLAS and BLEX.</p>\n<p>\\\nIn Table 6, we present the results of the evaluation of the selected models on the UD-based datasets byName, byType, and PDB. This table contains the results of segmentation, tagging, and lemmatization tasks. Table 7 is a continuation of Table 6 and it contains the results for the same tagset and dataset on the dependency parsing task.</p>\n<p>\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-46830d1.png\" alt=\"Table 5: Benchmark results for the Morfeusz tagset performed on two datasets: NKJP-byType (bT) and NKJP-byName (bN); AA – Aligned Accuracy; F1 – F1 score. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert-base, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\\n\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-6y930da.png\" alt=\"Table 6: Benchmark results for the UD tagset performed on three datasets: NKJP-byType (bT), NKJP-byName (bN), and PDB-UD (PDB) for segmentation, tagging and lemmatization tasks; AA – Aligned Accuracy; F1 – F1 score. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert-base, pl – pl-core-news-lg, H – HerBERT-base.\" /></p>\n<p>\\\n\\\n\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-73a30va.png\" alt=\"Table 7: Benchmark results for the UD tagset performed on three datasets: NKJP-byType (bT), NKJP-byName (bN), and PDB-UD (PDB) for the dependency parsing task; AA – Aligned Accuracy; F1 – F1 score. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert-base, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\</p>\n<h2 id=\"7acknowledgements\">7. Acknowledgements</h2>\n<p>This work was supported by the European Regional Development Fund as a part of 2014–2020 Smart Growth Operational Programme, CLARIN — Common Language Resources and Technology Infrastructure (project no. POIR.04.02.00-00C002/19) and DARIAH-PL — Digital Research Infrastructure for the Arts and Humanities (project no. POIR.04.02.00-00-D006/20-0). We gratefully acknowledge Poland’s high-performance computing infrastructure PLGrid (HPC Centers: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2022/015872.</p>\n<h2 id=\"8bibliographicalreferences\">8. Bibliographical References</h2>\n<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135–146.</p>\n<p>\\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.</p>\n<p>\\\nSabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLLX), pages 149–164, New York City. Association for Computational Linguistics.</p>\n<p>\\\nKehai Chen, Tiejun Zhao, Muyun Yang, and Lemao Liu. 2017. Translation prediction with source dependency-based context representation. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1).</p>\n<p>\\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online. Association for Computational Linguistics.</p>\n<p>\\\nDick Crouch, Mary Dalrymple, Ronald M. Kaplan, Tracy Holloway King, John Maxwell, and Paula Newman. 2011. XLE Documentation. Palo Alto Research Center.</p>\n<p>\\\nMarie-Catherine de Marneffe, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. 2021. Universal Dependencies. Computational Linguistics, 47(2):255–308.</p>\n<p>\\\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120, Online. Association for Computational Linguistics.</p>\n<p>\\\nAlex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602–610. IJCNN 2005.</p>\n<p>\\\nZhijiang Guo, Yan Zhang, and Wei Lu. 2019. Attention guided graph convolutional networks for relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 241–251, Florence, Italy. Association for Computational Linguistics.</p>\n<p>\\\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th Interna- tional Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4411–4421. PMLR.</p>\n<p>\\\nJungo Kasai, Dan Friedman, Robert Frank, Dragomir Radev, and Owen Rambow. 2019. Syntax-aware neural semantic role labeling with supertags. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 701–709, Minneapolis, Minnesota. Association for Computational Linguistics.</p>\n<p>\\\nDaniel Khashabi, Tushar Khot, Ashish Sabharwal, and Dan Roth. 2018. Question answering as global reasoning over semantic abstractions. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).</p>\n<p>\\\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4110– 4124, Online. Association for Computational Linguistics.</p>\n<p>\\\nWitold Kieraś and Marcin Woliński. 2017. Morfeusz 2 – analizator i generator fleksyjny dla języka polskiego. Język Polski, XCVII(1):75–83.</p>\n<p>\\\nMateusz Klimaszewski and Alina Wróblewska. 2021. COMBO: State-of-the-art morphosyntactic analysis. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 50–62, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>\n<p>\\\nRyan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 91–98, Ann Arbor, Michigan. Association for Computational Linguistics.</p>\n<p>\\\nInes Montani and Matthew Honnibal. 2022. spaCy: Industrial-Strength Natural Language Processing in Python. Version 3.4.1.</p>\n<p>\\\nRobert Mroczkowski, Piotr Rybak, Alina Wróblewska, and Ireneusz Gawlik. 2021. HerBERT: Efficiently pretrained transformerbased language model for Polish. In Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing, pages 1–10, Kiyv, Ukraine. Association for Computational Linguistics.</p>\n<p>\\\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. 2021a. Trankit: A light-weight transformer-based toolkit for multilingual natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 80–90, Online. Association for Computational Linguistics.</p>\n<p>\\\nMinh Van Nguyen, Viet Dac Lai, Amir Pouran Ben Veyseh, and Thien Huu Nguyen. 2021b. Trankit: A light-weight transformer-based toolkit for multilingual natural language processing. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations.</p>\n<p>\\\nJoakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351–359, Suntec, Singapore. Association for Computational Linguistics.</p>\n<p>\\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.</p>\n<p>\\\nAdrien Pavao, Isabelle Guyon, Anne-Catherine Letournel, Xavier Baró, Hugo Escalante, Sergio Escalera, Tyler Thomas, and Zhen Xu. 2022. Codalab competitions: An open source platform to organize scientific challenges. Technical report, Université Paris-Saclay.</p>\n<p>\\\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020a. AdapterHub: A framework for adapting transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 46–54, Online. Association for Computational Linguistics.</p>\n<p>\\\nJonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020b. MAD-X: An AdapterBased Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654–7673, Online. Association for Computational Linguistics.</p>\n<p>\\\nAdam Przepiórkowski, Mirosław Bańko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors. 2012. Narodowy Korpus Języka Polskiego. Wydawnictwo Naukowe PWN, Warsaw.</p>\n<p>\\\nPiotr Przybyła. 2022. LAMBO: Layered Approach to Multi-level BOundary identification.</p>\n<p>\\\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101–108, Online. Association for Computational Linguistics.</p>\n<p>\\\nPiotr Rybak and Alina Wróblewska. 2018. Semisupervised neural system for tagging, parsing and lematization. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 45– 54, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nDevendra Sachan, Yuhao Zhang, Peng Qi, and William L. Hamilton. 2021. Do syntax trees help pre-trained transformers extract information? In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2647– 2661, Online. Association for Computational Linguistics.</p>\n<p>\\\nDjamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie Candito, Jinho D. Choi, Richárd Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepiórkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woliński, Alina Wróblewska, and Eric Villemonte de la Clergerie. 2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146– 182, Seattle, Washington, USA. Association for Computational Linguistics.</p>\n<p>\\\nMilan Straka, Jan Hajič, and Jana Straková. 2016. UDPipe: Trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), pages 4290–4297, Portorož, Slovenia. European Language Resources Association (ELRA).</p>\n<p>\\\nMilan Straka and Jana Straková. 2017. Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88–99, Vancouver, Canada. Association for Computational Linguistics.</p>\n<p>\\\nKai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu. 2019. Aspect-level sentiment analysis via convolution over dependency tree. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5679–5688, Hong Kong, China. Association for Computational Linguistics.</p>\n<p>\\\nŁukasz Szałkiewicz and Adam Przepiórkowski. 2012. Anotacja morfoskładniowa. In Adam Przepiórkowski, Mirosław Bańko, Rafał L. Górski, and Barbara Lewandowska-Tomaszczyk, editors, Narodowy Korpus Języka Polskiego, pages 59– 96. Wydawnictwo Naukowe PWN, Warsaw.</p>\n<p>\\\nShikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, and Partha Talukdar. 2018. RESIDE: Improving distantlysupervised neural relation extraction using side information. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1257–1266, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nYufei Wang, Mark Johnson, Stephen Wan, Yifang Sun, and Wei Wang. 2019. How to best use syntax in semantic role labelling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5338–5343, Florence, Italy. Association for Computational Linguistics.</p>\n<p>\\\nJakub Waszczuk. 2012. Harnessing the crf complexity with domain-specific constraints. the case of morphosyntactic tagging of a highly inflected language. In Proceedings of COLING 2012, pages 2789–2804.</p>\n<p>\\\nJakub Waszczuk, Witold Kieraś, and Marcin Woliński. 2018. Morphosyntactic disambiguation and segmentation for historical polish with graph-based conditional random fields. In International Conference on Text, Speech, and Dialogue, pages 188–196. Springer.</p>\n<p>\\\nMarcin Woliński. 2014. Morfeusz reloaded. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, pages 1106–1111. European Language Resources Association (ELRA).</p>\n<p>\\\nMarcin Woliński. 2019. Automatyczna analiza składnikowa języka polskiego. Wydawnictwa Uniwersytetu Warszawskiego, Warsaw.</p>\n<p>\\\nDaniel Zeman, Jan Hajič, Martin Popel, Martin Potthast, Milan Straka, Filip Ginter, Joakim Nivre, and Slav Petrov. 2018. CoNLL 2018 shared task: Multilingual parsing from raw text to Universal Dependencies. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–21, Brussels, Belgium. Association for Computational Linguistics.</p>\n<p>\\\nDaniel Zeman, Martin Popel, Milan Straka, Jan Hajič, Joakim Nivre, Filip Ginter, Juhani Luotolahti, Sampo Pyysalo, Slav Petrov, Martin Potthast, Francis Tyers, Elena Badmaeva, Memduh Gokirmak, Anna Nedoluzhko, Silvie Cinková, Jan Hajič jr., Jaroslava Hlaváčová, Václava Kettnerová, Zdeňka Urešová, Jenna Kanerva, Stina Ojala, Anna Missilä, Christopher D. Manning, Sebastian Schuster, Siva Reddy, Dima Taji, Nizar Habash, Herman Leung, MarieCatherine de Marneffe, Manuela Sanguinetti, Maria Simi, Hiroshi Kanayama, Valeria de Paiva, Kira Droganova, Héctor Martínez Alonso, Çağrı Çöltekin, Umut Sulubacak, Hans Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg Rehm, Tolga Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde, Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonça, Tatiana Lando, Rattima Nitisaroj, and Josie Li. 2017. CoNLL 2017 shared task: Multilingual parsing from raw text to Universal Dependencies. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, Vancouver, Canada. Association for Computational Linguistics.</p>\n<p>\\\nMeishan Zhang, Zhenghua Li, Guohong Fu, and Min Zhang. 2019. Syntax-enhanced neural machine translation with syntax-aware word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1151–1161, Minneapolis, Minnesota. Association for Computational Linguistics.</p>\n<p>\\\nYuhao Zhang, Peng Qi, and Christopher D. Manning. 2018. Graph convolution over pruned dependency trees improves relation extraction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215, Brussels, Belgium. Association for Computational Linguistics.</p>\n<h2 id=\"9languageresourcereferences\">9. Language Resource References</h2>\n<p>Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov. 2019. XLMRoBERTa. Hugging Face.</p>\n<p>\\\nGrave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas. 2018. fastText. Facebook.</p>\n<p>\\\nKłeczek, Dariusz. 2021. Polbert. Hugging Face.</p>\n<p>\\\nLynn, Teresa and Foster, Jennifer and McGuinness, Sarah and Walsh, Abigail and Phelan, Jason and Scannell, Kevin. 2015. Irish Dependency Treebank (UD Irish-IDT). Universal Dependencies Consortium. PID http://hdl.handle.net/11234/1- 4611.</p>\n<p>\\\nMroczkowski, Robert and Rybak, Piotr and Wróblewska, Alina and Gawlik, Ireneusz. 2021. HerBERT. Hugging Face.</p>\n<p>\\\nPrzepiórkowski, Adam and Bańko, Mirosław and Górski, Rafał L. and Lewandowska-Tomaszczyk, Barbara. 2018. National Corpus of Polish. Institute of Computer Science.</p>\n<p>\\\nShen, Mo and McDonald, Ryan and Zeman, Daniel and Qi, Peng. 2019. Chinese Dependency Treebank (UD Chinese-GSD). Universal Dependencies Consortium. PID http://hdl.handle.net/11234/1-4611.</p>\n<p>\\\nWróblewska, Alina. 2018. Polish Dependency Bank (UD Polish-PDB). Universal Dependencies Consortium. PID http://hdl.handle.net/11234/1-5150.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GPT-3 Trips Over Polish Grammar While Classic Tools Hold Their Ground in AI Comparison","url":"https://hackernoon.com/gpt-3-trips-over-polish-grammar-while-classic-tools-hold-their-ground-in-ai-comparison?source=rss","date":1735552815,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 9 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"43results\">4.3. Results</h2>\n<p><strong>Impact of system architecture</strong> We assess the quality of the selected NLPre systems contingent on the NLPre-PL benchmark. In Polish (and most other languages), non-neural NLPre tools are currently not widely developed. We evaluate two of them: Concraft and UDPipe. Although they do not use neural network algorithms to train models, their quality does not significantly differ from the best tested neural systems, especially in terms of segmentation, which UDPipe performs best (Words) or second-best (Sentences) (see Tables 2 and 3). We cannot unequivocally say that the system architecture has a decisive influence on the results, as spaCy models, even transformer-based, output the lowest quality.</p>\n<p>\\\n<strong>Impact of tagset selection</strong> We compare systems trained and tested on data adjusted to two tagsets – the Morfeusz tagset (see Table 2) and the UD tagset (see Table 3). The average scores indicate that only COMBO performs better on Morfeusz-annotated data than on UD data. The performance of Trankit, UDPipe, and Stanza slightly decreases on Morfeusz data. Notably, all spaCy models trained on this dataset record a significant quality drop mainly due to poorly performed morphological analysis, i.e. UFeats values (and thus also the low AllTags values, i.e., matching between UPOS, XPOS, and UFeats). Regarding segmentation, UPOS and XPOS tagging, and lemmatisation, the tagset selection does not negatively affect the results, and the systems perform comparably.</p>\n<p>\\\n<strong>Impact of the size of training data</strong> Intuitively, the size of the training data affects the prediction quality. Considering the data size factor, we compare the average F1 scores of the NLPre systems trained on NKJP1M (see the last row in Table 4) and on PDB-UD (see Table 4), which is two orders of magnitude smaller. The results confirm our intuitive assumptions – there is a difference of 6.21 between the mean F1 scores obtained by the systems trained on the smaller PDB-UD (avg. F1 of 88.16) and those trained on the larger NKJP1M (avg. F1 of 94.37).</p>\n<p>\\\nWhen comparing the performance of individual systems on the smaller PDB-UD dataset, Trankit turns out to be the undisputed winner in all tasks except lemmatisation. However, considering the average performance of all tasks, COMBO and Stanza perform the best.</p>\n<p>\\\nIn alignment with contemporary developments on zero-shot learning, we test the predictive capabilities of GPT-3.5 acquired via the prompting technique (Brown et al., 2020). Despite comprehensive instructions along with the UD tree examples in the prompt, the results are highly unsatisfactory. An error analysis has revealed that 1) the GPT model modifies the input texts (e.g. adds elided words, alters the word’s declension and conjugation, leading also to non-existent words); 2) while parsing questions, it answers them or returns information that they cannot be answered; 3) it replaces Polish words with their foreign equivalents; 4) it outputs graphs with cycles, thus not adhering to UD trees. Even for GPTs, achieving UD-compliant morphosyntactic analysis is challenging when they lack access to training examples. GPT-3.5’s results are not included in the leaderboard.</p>\n<p>\\\n<strong>Impact of split heuristics</strong> As outlined in Section 3.1, NKJP1M has no official split into train, dev, and test subsets. Since intuitively, the type of document can affect text processing, we propose two alternative splits, i.e. byName and byType. We compare the F1 scores for these two splits to verify this hypothesis. For the byName split, the average F1 for tasks and systems is 90.69, and for the byType split, it is 90.56. The difference is negligible, indicating that the document type, and hence the text domain, does not affect the quality of the NLPre tasks. Based on this outcome, we arbitrarily choose the more balanced byType split as binding in the final NLPre-PL benchmarking system. The detailed results of all experiments are in Appendix 6.2.</p>\n<p>\\\n<strong>Inference time</strong> In the context of benchmarking, quality is a fundamental factor. In our case, the best average F1 scores are achieved by COMBO and Stanza, far ahead of spaCy and Concraft. The second crucial issue is the processing time of the evaluated NLPre systems, especially their inference time.[20] We calculate the times in which the systems tokenise, tag and lemmatise the input text.[21] The exception is COMBO with the mandatory parsing module that cannot be disabled. Therefore, its calculations include the parsing time as well. The inference time, corresponding to the number of tokens processed per second, is provided in the last two columns of Tables 2 and 3. On CPU, the fastest systems are spaCy and UDPipe, and the slowest is Concraft. Other systems process one order of magnitude fewer tokens per second than the top ones. On GPU, spaCy is the undisputed winner, followed by Stanza, UDPipe, COMBO and Trankit.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-gt8308f.png\" alt=\"\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-e19304m.png\" alt=\"Figure 2: Pearson correlation coefficients between vectors of F1 scores on Tokens, Sentences, Words, UPOS, XPOS, Lemmas tasks averaged over datasets (excluding PDB-UD) and embeddings.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-soa30or.png\" alt=\"Figure 3: Pearson correlation coefficients between vectors of F1 scores on Tokens, Sentences, Words, UPOS, XPOS, Lemmas tasks averaged over datasets (excluding PDB-UD).\" /></p>\n<p>\\\nPearson’s correlation r suggests that the results are linearly proportional for the same models and different tagsets, which we conclude from the values close to 1 at the intersection of (model<em>i</em>, tagsetud) and (model<em>i</em>,  tagsetnkjp). Even though correlation coefficients are generally high (i.e. r ∈ [0.90, 0.99]) for most pairs (model<em>i</em>,  tagsetud) and (model<em>j,</em> tagsetnkjp), there are noticeable lower values for spaCy, i.e. r ∈ [0.66, 0.78]. We hypothesise that this is due to the non-linear rate of changes between the scores, as all Spearman correlation coefficients exceed 0.89 (i.e. ρ &gt; 0.89).</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-brb303i.png\" alt=\"Figure 4: Dispersion of model performance measured by F1 on the Morfeusz tagset and Sentences, Words, UPOS, XPOS, and Lemmas tasks.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-uic30ll.png\" alt=\"Figure 5: Dispersion of model performance measured by F1 on the UD tagset and Sentences, Words, UPOS, XPOS, and Lemmas tasks.\" /></p>\n<p>\\\nThe results of a more granular analysis of Pearson’s r between vectors of F1 scores for triples (tagseti , modelj , embeddingsk), averaged over datasets, show a strong correlation for the same models, regardless of the tagset and the embedding (see Figure 3). Hence, if a change in the tagset or embedding causes an increase in one task, a proportional increase in remaining tasks is expected.</p>\n<p>\\\nBoxplot charts (see Figures 4 and 5) determine the stability of the model results for a given tagset regardless of dataset and embedding. One box shows the scattering of F1 scores for Tokens, Sentences, Words, UPOS, XPOS, and Lemmas tasks. The shortest COMBO’s box indicates a relatively similar performance of the model across tasks for each triplet (COMBO, embeddingj , datasetk).</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[20] We share a conviction favoured in the NLP community that the training time is slightly less requisite than the inference time since models are trained only once but then constantly reused for predictions. We thus provide inference times.</p>\n<p>\\\n[21] We run tests uniformly on CPU – Intel Xeon Platinum 8268 processor (1 node with 12 cores), and GPU – 2x Tesla V100-SXM2. The machines used to train the models are listed in Appendix 6.1.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Pit GPT-3.5 Against Classic Language Tools in Polish Text Analysis","url":"https://hackernoon.com/researchers-pit-gpt-35-against-classic-language-tools-in-polish-text-analysis?source=rss","date":1735551912,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 8 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"42evaluatedsystems\">4.2. Evaluated systems</h2>\n<p>Based on the NLPre-PL benchmark, we evaluate both well-rooted rule-based disambiguation methods and modern systems based on neural network architectures to enable an informative and thorough comparison of different approaches. We use the most up-to-date versions of available tools at the time of conducting experiments: (1) pipelines of separate tools (Concraft-pl, UDPipe), (2) systems integrating separate models for NLPre tasks (spaCy, Stanza, Trankit), (3) end-to-end systems with a model for all NLPre tasks (COMBO), and large language model GPT-3.5.</p>\n<p>\\\n<strong>Concraft-pl</strong> (Waszczuk, 2012; Waszczuk et al., 2018) [12] is a system for joint morphosyntactic disambiguation and segmentation.[13] It uses Morfeusz morphological analyser (Woliński, 2014; Kieraś and Woliński, 2017) to extract morphological and segmentation equivocates and then disambiguates them using the conditional random fields model. We train the Concraft-pl models with default parameters.</p>\n<p>\\\n<strong>UDPipe</strong> (Straka and Straková, 2017) is a language-agnostic trainable NLPre pipeline.[14] Depending on the task, it uses recurrent neural networks (Graves and Schmidhuber, 2005) in segmentation and tokenization, the average perceptron in tagging and lemmatization, a rule-based approach in multi-word splitting, and a transition-based neural dependency parser. We train the UDPipe models with the default parameters. The dependency parser is trained with the Polish fastText embeddings (Grave et al., 2018).</p>\n<p>\\\n<strong>SpaCy</strong> (Montani and Honnibal, 2022) is an NLP Python library shipped with pretrained pipelines and word vectors for multiple languages.[15] It also supports training the models for tagging and parsing, inter alia. We use spaCy to train pipelines for morphosyntactic analysis with: feed-forward network</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-hv830lq.png\" alt=\"Table 2: Results (F1 scores) and inference time (the number of tokens processed per second) of benchmarking the selected NLPre systems on the Morfeusz tagset averaged by the datasets (byName and byType). The systems are grouped into non-neural and neural by a double horizontal line. Embeddings used in the models are: R – xlmRoBERTa-base, fT – fastText, P – Polbert, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-iy930s3.png\" alt=\"Table 3: Results (F1 scores) and inference time (tokens per second) of benchmarking the selected NLPre systems on the UD tagset averaged by the datasets (byName, byType, and PDB-UD). The systems are grouped into non-neural and neural by a double horizontal line (Concraft is not included because it does not allow data in the UD tagset) Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert, pl – pl-core-news-lg, H – HerBERT.\" /></p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-4ra30f1.png\" alt=\"Table 4: Results of benchmarking the selected NLPre systems on the smaller PDB-UD dataset. The last row with the mean F1 scores of the models trained on larger NKJP1M data is for reference. Embeddings used in the models are: R – xlm-RoBERTa-base, fT – fastText, P – Polbert, pl – pl-core-news-lg, H – HerBERT. The results of GPT-3.5 are greyed out due to their exclusion from display on the leaderboard.\" /></p>\n<p>\\\nbased text encoders with static embeddings (fastText and pl-core-news-lg) or transformer-based encoders with the Polbert embeddings (Kłeczek, 2021), taggers (linear layers with softmax activation on top of the encoders), and transition-based parsers.</p>\n<p>\\\n<strong>Stanza</strong> (Qi et al., 2020) is a language-agnostic, fully neural toolkit offering a modular pipeline for tokenization, multi-word token expansion, lemmatization, tagging, and dependency parsing.[16] It mainly uses recurrent neural networks (Graves and Schmidhuber, 2005) as a base architecture and external word embeddings (fastText). Each module reuses the basic architecture.</p>\n<p>\\\n<strong>Trankit</strong> (Nguyen et al., 2021b) uses a multilingual pre-trained transformer-based language model, XLM-Roberta (Conneau et al., 2019) as the text encoder which is then shared across pipelines for different languages.[17] The resulting model is jointly trained on 90 UD treebanks with a separate adapter (Pfeiffer et al., 2020a,b) for each treebank. Trankit uses a wordpiece-based splitter to exploit contextual information.</p>\n<p>\\\n<strong>COMBO</strong> (Rybak and Wróblewska, 2018; Klimaszewski and Wróblewska, 2021) is a fully neural language-independent NLPre system[18] integrated with the LAMBO tokeniser (Przybyła, 2022). It is an end-to-end system with jointly trained modules for tagging, parsing, and lemmatisation. We train the COMBO models with the pre-trained word embeddings – fastText and HerBERT (Mroczkowski et al., 2021).</p>\n<p>\\\n<strong>GPT-3.5</strong> (Brown et al., 2020) is a large language model, notable for its outstanding performance in NLU tasks. It is a fined-tuned version of the GPT-3 model. GPT-3.5’s architecture is based on a transformer neural network with 12 stacks of decoders blocks with multi-head attention blocks.</p>\n<p>\\\nFor segmentation tasks, we train modules integrated with the tested NLPre systems. The only aberration is in spaCy, where poor segmentation results of the dependency module[19] forced us to use an out-of-the-box sentenciser available in spaCy.</p>\n<p>\\\nFor each model, we initialise training with possibly the most prominent and congruent embedding model available. Virtually all models are capable of fully capitalising from that addition, apart from Concraft and UDPipe. The first does not use embeddings at all, and the latter uses them only for dependency parsing training. If embeddings based on BERT architecture are feasible to use, we select their base versions. This ensures fairness of comparison between NLPre systems, as not all of them support BERT-large embeddings.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[12] Polish is a fusional language for which a two-stage tagging procedure is typically applied: first, a rule-based morphological analyser outputs all morphological interpretations of individual tokens, and then a tagging disambiguator selects the most likely one for each token. The tools implemented in accordance with this procedure are still imminent.</p>\n<p>\\\n[13] https://github.com/kawu/concraft-pl (v2.0)</p>\n<p>\\\n[14] https://ufal.mff.cuni.cz/udpipe (v1)</p>\n<p>\\\n[15] https://github.com/explosion/spaCy (v3.4.1)</p>\n<p>\\\n[16] https://github.com/stanfordnlp/stanza (v1.4.0)</p>\n<p>\\\n[17] https://github.com/nlp-uoregon/trankit (v1.1.1)</p>\n<p>\\\n[18] https://gitlab.clarin-pl.eu/syntactic-tools/combo (v1.0.5)</p>\n<p>\\\n[19] Dependency parsing module is responsible for sentence segmentation in the spaCy implementation.</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Learn to Measure AI’s Language Skills","url":"https://hackernoon.com/researchers-learn-to-measure-ais-language-skills?source=rss","date":1735551014,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 7 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"4evaluation\">4. Evaluation</h2>\n<h3 id=\"41evaluationmethodology\">4.1. Evaluation methodology</h3>\n<p>To maintain the de facto standard to NLPre evaluation, we apply the evaluation measures defined for the CoNLL 2018 shared task and implemented in the official evaluation script.[11] In particular, we focus on F1 and <em>AlignedAccuracy</em>, which is similar to F1 but does not consider possible misalignments in tokens, words, or sentences.</p>\n<p>\\\nIn our evaluation process, we follow default training procedures suggested by the authors of the evaluated systems, i.e. we do not conduct any optimal hyperparameter search in favour of leaving the recommended model configuration as-is. We also do not further fine-tune selected models.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[11] https://universaldependencies.org/conll18/conll18<em>ud</em>eval.py</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"How Microsoft Made 2024 the Year of Windows on Arm","url":"https://tech.slashdot.org/story/24/12/30/0529253/how-microsoft-made-2024-the-year-of-windows-on-arm?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735550520,"author":"EditorDavid","unread":true,"content":"\"I still can't quite believe that I'm using an Arm-powered Windows laptop every day,\" writes a senior editor at the Verge:\n\nAfter more than a decade of trying to make Windows on Arm a reality, Microsoft and Qualcomm finally nailed it this year with Copilot Plus PCs. These new laptops have excellent battery life and great performance &mdash; and the app compatibility issues that have plagued Windows on Arm are mostly a thing of the past (as long as you're not a gamer). Microsoft wanted 2024 to be \"the year of the AI PC,\" but I think it was very much the year of Windows on Arm... \n\nThe key to Windows on Arm's revival this year was Qualcomm's Snapdragon X Elite processors, which were announced in April. They've provided the type of performance and power efficiency only previously available with Apple's MacBooks and challenged Intel and AMD to do better in the x86 space. After much debate over Microsoft's MacBook Air-beating benchmarks, the reviews rolled in and showed that Windows on Arm was indeed capable of matching and beating Apple's MacBook Air. Qualcomm even hired the \"I'm a Mac\" guy to promote Windows on Arm PCs, showing how confident it was in challenging Apple's laptop dominance. \n\nMicrosoft and Qualcomm also worked closely with developers to make key apps compatible, and it's now very rare to run into an app compatibility issue that can't be solved by a native Arm64 version or Microsoft's improved emulator. Even Google, which previously shunned Windows Phone, has created Arm64 versions of Chrome and Google Drive to support Microsoft's efforts. With developers continually providing native versions of their apps, it makes it a lot easier to switch to a Windows on Arm laptop. The only big exception is gaming, where x86 still reigns supreme for compatibility and performance... \n\nIt's hard not to see 2025 as the year that Windows on Arm continues to eat into the laptop space. A Dell leak revealed Qualcomm is preparing new chips for 2025, and the chip maker has also been rolling out cheaper Arm-based chips to bring laptop prices down. \nThe article acknowledges that both AMD and Intel \"have the key advantage of game compatibility that Windows on Arm is definitely not ready for...\" But \"Given the Windows on Arm gaming situation, a new generation of Nvidia's GPUs could help generate fresh excitement around x86 laptops throughout 2025.\" And \"Nvidia might also be planning to help the Windows on Arm effort. The chip maker has long been rumored to be planning to launch Arm PC chips as soon as 2025... Whatever happens to laptops in 2025, you can guarantee that there's going to be fierce competition between Intel, AMD, and Qualcomm.\" \n\nBut the author still complains about the dedicated Copilot key on his new WIndows-on-Arm laptop. \"While the Copilot experience on Windows has gone through several confusing revisions, it's still a key I accidentally press and then get frustrated when a Copilot window appears.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=How+Microsoft+Made+2024+the+Year+of+Windows+on+Arm%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F0529253%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F30%2F0529253%2Fhow-microsoft-made-2024-the-year-of-windows-on-arm%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/30/0529253/how-microsoft-made-2024-the-year-of-windows-on-arm?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564481&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Challenge AI to Tackle the Toughest Parts of Language Processing","url":"https://hackernoon.com/researchers-challenge-ai-to-tackle-the-toughest-parts-of-language-processing?source=rss","date":1735550112,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 6 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"32tasks\">3.2. Tasks</h2>\n<p>The complete set of NLPre tasks was originally curated for evaluating language systems in the CoNLL shared task 2018 (Zeman et al., 2018). These tasks mainly focus on preliminary text processing, such as tokenisation or divulging morphosyntactic features. We follow the CoNLL task choice and include all these tasks in NLPre-PL.</p>\n<p>\\\n<strong>Segmentation</strong> A segmentation task consists in splitting texts into sentences (Sentences), orthographic tokens (Tokens), and syntactic words (Words), the latter being the basic units of morphosyntactic analysis. Segmentation is not a trivial task. In some languages, an orthographic token may be recognised as a multi-word token (multiword for short) combining multiple syntactic words, e.g. in Polish, the token spalibyśmy (Eng. we would sleep) consists of the past participle spali (Eng. slept), the conditional marker by (Eng. would) and the mobile inflection śmy. Since the consistent model of segmentation into words and sentences was used in NKJP1M and PDB-UD, we maintain this data segmentation in NLPre-PL. It is also worth mentioning that the CoNLL format (but not TEI and DAG) allows for annotating orthographic tokens; thus, they are included in the NLPre-PL benchmark.</p>\n<p>\\\n<strong>Tagging</strong> A tagging task is the process of identifying parts of speech (i.e. POS tagging) and possibly morphological features (i.e. morphological analysis) of words. It follows a predefined POS tagset. As mentioned in Section 3.1, two tagsets are used in the NLPre-PL datasets: Morfeusz and UD.</p>\n<p>\\\n<strong>Lemmatisation</strong> Lemmatisation involves predicting canonical forms of syntactic words. Canonical forms are conventionally established identifiers of lexemes (i.e. sets of inflectionally related syntactic words). Since Polish is a fusional language with a large number of inflected words, lemmatisation is an important task, albeit not trivial, e.g. the lemma of kluczy can be either the infinitive kluczyć (Eng. to weave) or the noun klucz (Eng. a key).</p>\n<p>\\\n<strong>Dependency</strong> parsing Dependency parsing is the process of automatically predicting the syntactic structure of an input sentence. A dependency structure is a labelled directed tree with nodes corresponding to syntactic words and edges between these words specifying dependency relations.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Framework Promises to Train AI to Better Understand Hard-to-Grasp Languages Like Polish","url":"https://hackernoon.com/new-framework-promises-to-train-ai-to-better-understand-hard-to-grasp-languages-like-polish?source=rss","date":1735549211,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 5 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"3nlpreplbenchmark\">3. NLPre-PL benchmark</h2>\n<h3 id=\"31datasets\">3.1. Datasets</h3>\n<p><img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-ss8303t.png\" alt=\"Table 1: Summary of source datasets (NKJP1M and PDB-UD) and NLPre-PL Datasets (in tokens). Explanations: POS – the part-of-speech tagset; DEP – the dependency schema; Avg. t/s – the average number of tokens per sentence\" /></p>\n<p>\\\n<strong>NKJP1M</strong> (Przepiórkowski et al., 2018) The NKJP1M subcorpus of the Polish National Corpus (Przepiórkowski et al., 2012) is manually annotated according to the NKJP tagset (Szałkiewicz and Przepiórkowski, 2012) and afterwards modified in line with the Morfeusz tagset (Woliński, 2019). This balanced subset of thematic- and genre-diverse texts and transcriptions is used to train Polish POS taggers. NKJP1M is maintained in two formats: TEI[8] and DAG.[9] These two formats are accepted by older NLPre tools but not modern ones. We thus convert NKJP1M to the CoNLL-X format (Buchholz and Marsi, 2006) preserving the original segmentation, POS tags and morphological features (i.e. the Morfeusz tagset), and to the CoNLL-U format10 with UD tags, Morfeusz tags (XPOS) and UD morphological features.</p>\n<p>\\\nSince there is no generally accepted split of NKJP1M into training, development and testing subsets, we uniformly divide NKJP1M in all formats (i.e. DAG, TEI, CoNLL-X and CoNLL-U) pursuant to the formulated splitting heuristics. Each document in the subcorpus contains multiple paragraphs of continuous textual data. To avoid possible information leakage, we treat each such paragraph as an indivisible unit. To ensure that the subsets include paragraphs of varying length, we investigate the distribution over the number of segments in each paragraph. Since it is akin to Gaussian distribution, we decide to not exclude any data, and we divide the paragraphs into K = 10 buckets of roughly similar size and then sample from them with respective ratios of 0.8:0.1:0.1 (corresponding to train, dev, and test subsets). This data selection technique assures similar distribution of segments number per paragraph in three subsets, hereafter byName. For creating our second split, hereafter byType, we consider the type of document a paragraph belongs to. We first group paragraphs into categories equal to the document types, and then we repeat the above-mentioned procedure per category (see the summary of NKJP1M and data splits in Table 1). <strong>PDB-UD</strong> (Wróblewska, 2018) Polish Dependency Bank is the largest collection of Polish sentences manually annotated with dependency trees and afterwards converted into UD representations in line with the UD annotation schema (de Marneffe et al., 2021). PDB-UD slightly correlates with NKJP1M, i.e., a subset of the PDB-UD sentences comes from NKJP1M, and the language-specific tags (XPOS) in PDB-UD match the Morfeusz tagset. PDB-UD is typically used to train NLPre systems for Polish. In NLPre-PL, we use the original PDB-UD data without any modifications and its standard split (see the statistical summary of PDB-UD in Table 1).</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[8] http://nlp.ipipan.waw.pl/TEI4NKJP.</p>\n<p>\\\n[9] https://github.com/kawu/concraft-pl#data-format</p>\n<p>\\\n[10] https://universaldependencies.org/format.html</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Create Plug-and-Play System to Test Language AI Across the Globe","url":"https://hackernoon.com/researchers-create-plug-and-play-system-to-test-language-ai-across-the-globe?source=rss","date":1735548311,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 4 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"23configuration\">2.3. Configuration</h2>\n<p>We acknowledge the need to configure similar evaluation environments for other languages to promote linguistic diversity within the worldwide NLP community and to support local NLP communities working on a particular language. To ensure that, we publish a .yaml file that enables easy management of datasets, tagset, and metrics included in the benchmark. The content of all subpages can be modified using a WYSIWYG editor within the application. This setting ensures quite a low entry level for setting up the platform, with minimal changes required.</p>\n<p>\\\nAs a standard feature, we include pre-defined descriptions for the prevalent NLPre tasks. Those can be modified via either configuration files or the administrator panel. Additionally, we supply a default evaluation script, but users are free to provide their own customised code.</p>\n<p>\\\nTo show the capabilities of the benchmarking system, we set up a prototype for Polish (Figure 1). NLPre-PL is described in detail in Section 3. To support our claim that the system is language agnostic, we set up NLPre-GA for Irish and NLPreZH for Chinese. The choice of those languages is not arbitrary; our objective is to demonstrate the capability of the platform in evaluating diverse languages, including those based on non-Latin scripts. In setting up said benchmarking systems we use existing UDv2.9 treebanks: UD<em>Chinese-GSD (Shen et al., 2019) and UD</em>Irish-IDT (Lynn et al., 2015) and available up-to-date models, trained on these treebanks. The selection of models mirrors the criteria applied in this work regarding the evaluation of Polish, that is: COMBO, Stanza, SpaCy, UDPipe, and Trankit. If the specific model is not available for UDv2.9, we train it from scratch on the datasets linked above.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Web App Lets Researchers Test and Rank Language AI Tools in Real Time","url":"https://hackernoon.com/new-web-app-lets-researchers-test-and-rank-language-ai-tools-in-real-time?source=rss","date":1735547411,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 3 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"22onlinebenchmarkingsystem\">2.2. Online benchmarking system</h2>\n<p>The benchmarking system comprises three main parts: a data repository, a submission and evaluation system, and a leaderboard. The data repository provides descriptions of NLPre tasks, datasets, and evaluation metrics, as well as links to the datasets.</p>\n<p>\\\nThe model submission and evaluation system allows the researchers to evaluate a new model by submitting its predictions for the test sets of raw sentences. It is mandatory to upload predictions for all provided test sets for a given tagset; however, it is possible to participate in an evaluation for only one tagset and only for a selected range of tasks.</p>\n<p>\\\nThe leaderboard is a tabular display of the performance of all submissions with their results for each dataset and tagset. The results for the evaluated model and its rank are displayed in the leaderboard provided the submitter confirms their publication.</p>\n<p>\\\nThe benchmarking system is implemented as a web-based application in Python using Django framework. This framework allows quite an easy implementation of MVC design pattern. Moreover, it offers access to the administrator panel, which can be very useful in the custom configuration of the benchmark. The submission scores are stored in a local SQLite database and the submissions are stored in .zip files in a designated directory. The results from the leaderboard are conveniently accessible via an API.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Researchers Build Public Leaderboard for Language Processing Tools","url":"https://hackernoon.com/researchers-build-public-leaderboard-for-language-processing-tools?source=rss","date":1735546515,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 2 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"2nlprebenchmarking\">2. NLPre benchmarking</h2>\n<h3 id=\"21researchconcept\">2.1. Research concept</h3>\n<p>In this study, we introduce a novel adaptation of the benchmarking approach to NLPre. The primary objective is to establish an automated and credible method for evaluating NLPre systems against a provided benchmark and continuously updating their performance ranking on a publicly accessible scoreboard. More specifically, predictions for the benchmark test sets output by NLPre systems and 5 https://nlpre-pl.clarin-pl.eu 6 https://nlpre-zh.clarin-pl.eu 7 https://nlpre-ga.clarin-pl.eu submitted to the benchmarking system are automatically compared against the publicly undisclosed reference dataset. This method effectively prevents result manipulation and ensures fairness of the final assessment. The second important methodological assumption is to enable the ongoing evaluation of new or upgraded NLPre systems to guarantee up-to-date and complete ranking. Consequently, the leaderboard can serve as a reliable point of reference for NLPre system developers.</p>\n<p>\\\nBased on these assumptions, we design and implement the language-centric and tagset-agnostic benchmarking system that enables comprehensive and credible evaluation, constitutes an up-to-date source of information on NLPre progress, and is fully configurable to facilitate building benchmarking systems for multiple languages.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<p>\\</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"New Framework Simplifies Comparison of Language Processing Tools Across Multiple Languages","url":"https://hackernoon.com/new-framework-simplifies-comparison-of-language-processing-tools-across-multiple-languages?source=rss","date":1735545611,"author":"Morphology","unread":true,"content":"<p>:::info\n<strong>Authors:</strong></p>\n<p>(1) Martyna Wiącek, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(2) Piotr Rybak, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(3) Łukasz Pszenny, Institute of Computer Science, Polish Academy of Sciences;</p>\n<p>(4) Alina Wróblewska, Institute of Computer Science, Polish Academy of Sciences.</p>\n<p>:::</p>\n<p>:::tip\n<strong><em>Editor's note: This is Part 1 of 10 of a study on improving the evaluation and comparison of tools used in natural language preprocessing. Read the rest below.</em></strong></p>\n<p>:::</p>\n<h2 id=\"tableoflinks\">Table of Links</h2>\n<p><a href=\"http://hackernoon.com/preview/bno8PQ98dnRvbWqmDZxb\">Abstract and 1. Introduction and related works</a></p>\n<ol start=\"2\">\n<li>NLPre benchmarking</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/xCJbu5gXIv6zBAcu5auW\">2.1. Research concept</a></p>\n<p><a href=\"http://hackernoon.com/preview/EXMz9HrlJooLIyu7ODzx\">2.2. Online benchmarking system</a></p>\n<p><a href=\"http://hackernoon.com/preview/UyWaaHMwxn7o4zSm8xLN\">2.3. Configuration</a></p>\n<ol start=\"3\">\n<li>NLPre-PL benchmark</li>\n</ol>\n<p><a href=\"https://hackernoon.com/preview/zXpZnA8L9NTMUPTCkab9\">3.1. Datasets</a></p>\n<p><a href=\"http://hackernoon.com/preview/12WG6PdUY2ibjsLEMRCv\">3.2. Tasks</a></p>\n<ol start=\"4\">\n<li>Evaluation</li>\n</ol>\n<p><a href=\"http://hackernoon.com/preview/AwftrNalbqqQOJUA5saB\">4.1. Evaluation methodology</a></p>\n<p><a href=\"http://hackernoon.com/preview/QYdzSKSpD7i4Sfzht4QG\">4.2. Evaluated systems</a></p>\n<p><a href=\"https://hackernoon.com/preview/INjYMrFLBAhohaOcMy0e\">4.3. Results</a></p>\n<ol start=\"5\">\n<li><a href=\"http://hackernoon.com/preview/BiHsvrAaR5fw8TFmY3ol\">Conclusions</a></li>\n</ol>\n<ul>\n<li>Appendices</li>\n<li>Acknowledgements</li>\n<li>Bibliographical References</li>\n<li>Language Resource References</li>\n</ul>\n<h2 id=\"abstract\">Abstract</h2>\n<p>With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an extensive evaluation of a variety of Polish NLPre systems. To facilitate the construction of benchmarking environments for other languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full customization of the publicly released source code of the benchmarking system. The links to all the resources (deployed platforms, source code, trained models, datasets etc.) can be found on the project website: https://sites.google.com/view/nlpre-benchmark.</p>\n<p>\\\n<strong>Keywords</strong>: benchmarking, leaderboard, segmentation, POS tagging, dependency parsing, Polish</p>\n<h2 id=\"1introductionandrelatedworks\">1. Introduction and related works</h2>\n<p>Morphosyntactic features predicted by part-ofspeech (POS) taggers and dependency parsers underlie various downstream tasks, including but not limited to sentiment analysis (Sun et al., 2019), relation extraction (Zhang et al., 2018; Vashishth et al., 2018; Guo et al., 2019), semantic role labelling (Wang et al., 2019; Kasai et al., 2019), question answering (Khashabi et al., 2018), or machine translation (Chen et al., 2017; Zhang et al., 2019). These underlying tasks may therefore be referred to as natural language preprocessing (NLPre) tasks, as they precede the advanced NLP tasks. Since the quality of morphosyntactic predictions has a crucial impact on the performance of downstream tasks (Sachan et al., 2021), it is prudent to employ the best existing NLPre tools to predict the proper linguistic features. We are equipped with various NLPre methods, ranging from rule-based tools with hand-crafted grammars (e.g. Crouch et al., 2011), through statistical systems (e.g. Nivre, 2009; McDonald et al., 2005; Straka et al., 2016), neural systems supported by pre-trained language models (e.g. Qi et al., 2020; Nguyen et al., 2021a) to large language models (LLM Ouyang et al., 2022).</p>\n<p>\\\nIn the context of intrinsically evaluating NLPre tools and reporting their performance, a variety of approaches have been proposed, e.g. shared task, performance table, and progress repository. The main goal of a shared task is to comprehensively evaluate participating systems on the released datasets using the carefully defined evaluation methodology. Numerous NLPre shared tasks have been organised so far (e.g. Buchholz and Marsi, 2006; Seddah et al., 2013; Zeman et al., 2017, 2018), and they undoubtedly boosted the development of NLPre. While widely favoured, shared tasks are questionable as a complete and up-todate source of knowledge about NLPre progress. First, they scrutinise only solutions propounded in the current contest and do not include systems participating in the previous editions or possible future ones. Second, as shared tasks are organised sporadically, their results are not revised and may quickly become outdated. Certainly, the datasets released for shared tasks can be reused in experiments involving novel tools. The results of such experiments can be reported in independent scientific publications. Nonetheless, these publications are widely scattered, lacking a centralised platform for systematically tracking the ongoing NLPre progress with respect to a particular language.</p>\n<p>\\\nThe results of a new or upgraded NLPre tool are typically reported in performance tables (e.g. Stanza[1] or Trankit[2]). Such tables provide information about the quality of the tool in preprocessing a set of languages. The performance tables, however, often lack comparison with other systems trained for these particular languages. Additionally, as NL Pre systems may be trained on different dataset releases (e.g. of Universal Dependencies), comparing their performance tables is not conclusive.</p>\n<p>\\\n <img src=\"https://cdn.hackernoon.com/images/fWZa4tUiBGemnqQfBGgCPf9594N2-hx830qd.png\" alt=\"Figure 1: Screenshot of the NLPre-PL leaderboard.\" /></p>\n<p>\\\nInformation about trends and progress in NLP research is usually collected in public repositories such as Papers with Code[3] or NLP-progress[4]. These repositories contain a repertoire of datasets for common NLP tasks, e.g. dependency parsing and POS tagging, and rankings of models trained and tested on these datasets. They are open to contributing new datasets and results, which, to ensure their credibility, originate from published and linked scientific papers. However, cutting-edge yet unpublished results of a new or upgraded NLPre system are not eligible to report. NLPre tasks are accompanied by datasets mostly in English, raising the problem of language unrepresentation of the repositories. Last but not least, the Papers with Code repository is prone to abuse. After logging in, one can add new results and link them with irrelevant papers as well as edit existing results. The fraudulent results are publicised immediately.</p>\n<p>\\\nDespite yielding valuable information about the progress in NLPre, the mentioned evaluation approaches also reveal shortcomings, e.g. outdated and incomplete outcomes, lack of cross-system comparison, disregarding some systems, risk of result manipulation and absence of a language-centring perspective.</p>\n<p>\\\nFollowing standard procedures in NLP research, we propose to robustly and fairly evaluate NLPre tools using the benchmarking method that allows for the evaluation of NLP models’ performance and progress. NLP benchmarks are coupled with leaderboards that report and update model performance on the benchmark tasks, e.g. GLUE (Wang et al., 2018), XTREME (Hu et al., 2020), GEM (Gehrmann et al., 2021). The conventional benchmarking approach may be dynamically enhanced, exemplified by the Dynabench platform (Kiela et al., 2021), which enables users to augment the benchmark data by inputting custom examples. This humanand-model-in-the-loop benchmarking scenario appears promising for NLU tasks. Nevertheless, it may not be effective in the case of NLPre, as annotating credible examples of syntactic trees or morphological features requires expert knowledge. Finding multiple experts among casual users can be a serious obstacle, we thus implement our system in tune with the standard benchmarking method.</p>\n<p>\\\nTo our knowledge, benchmarking hasn’t been used to rank NLPre systems, even if it is valuable and desired by the community creating treebanks or designing advanced NLP pipelines. Our NLPre benchmarking approach fills this gap. The proposed online benchmarking system automatically assesses submitted predictions of NLPre systems and publishes their performance ranking on a public scoreboard (see Section 2.2). The system is language-centric and tagset-agnostic, enables comprehensive and credible evaluation and constitutes an up-to-date source of information on NLPre progress for a particular language. Unlike similar platforms, e.g. Codalab (Pavao et al., 2022), the NLPre benchmarking system is fully configurable and easy to set up, allowing users to establish an evaluation environment for any language. Additionally, it can be self-hosted, making it convenient for developers and researchers working with a particular language to have it accessible on a local server.</p>\n<p>\\\nTo justify the use of the benchmarking technique for NLPre tasks, we conduct empirical research in a challenging scenario with Polish as an example language. In the case of Polish, one dominant hurdle arises – the discrepancies between different tagsets, annotation schemes and datasets utilised for training disparate systems preclude their direct comparison. We thus standardise the training and evaluation of NLPre systems on a new performance benchmark for Polish, hereafter NLPre-PL (see Section 3). It consists of a predefined set of NLPre tasks and reformulated versions of existing Polish datasets. Section 4 outlines our robust and reliable evaluation of the selected NLPre systems on the NLPre-PL benchmark. According to our knowledge, no evaluation experiments have been carried out in Polish to compare the performance of off-the-shelf LLMs, neural NLPre systems and established tagging disambiguators due to the lack of a coherent evaluation environment.</p>\n<p>\\\nThis work makes a tripartite contribution encompassing novelty, research, and development underpinned by an open-source ethos. (1) We propose a novel language-oriented benchmarking approach to evaluate and rank NLPre systems. (2) We conduct a scientific evaluation of the proposed approach in the non-trivial Polish language scenario on the assembled NLPre-PL benchmark. (3) We publish online benchmarking platforms for three distinct languages: Polish[5], Chinese[6], and Irish[7], and release the benchmarking system’s source code as open-source.</p>\n<p>\\</p>\n<p>:::info\nThis paper is <a href=\"https://arxiv.org/abs/2403.04507\">available on arxiv</a> under CC BY-NC-SA 4.0 DEED license.</p>\n<p>:::</p>\n<hr />\n<p>[1] https://stanfordnlp.github.io/stanza/performance.html (UD v2.8)</p>\n<p>\\\n[2] https://trankit.readthedocs.io/en/latest/performance. html#universal-dependencies-v2-5 (UD v2.5)</p>\n<p>\\\n[3] https://paperswithcode.com</p>\n<p>\\\n[4] http://nlpprogress.com</p>\n<p>\\\n[5] https://nlpre-pl.clarin-pl.eu</p>\n<p>\\\n[6] https://nlpre-zh.clarin-pl.eu</p>\n<p>\\\n[7] https://nlpre-ga.clarin-pl.eu</p>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI Tools May Soon Manipulate People's Online Decision-Making, Say Researchers","url":"https://slashdot.org/story/24/12/30/0435226/ai-tools-may-soon-manipulate-peoples-online-decision-making-say-researchers?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735538940,"author":"EditorDavid","unread":true,"content":"Slashdot reader SysEngineer shared this report from the Guardian:\n\nAI tools could be used to manipulate online audiences into making decisions &mdash; ranging from what to buy to who to vote for &mdash; according to researchers at the University of Cambridge. The paper highlights an emerging new marketplace for \"digital signals of intent\" &mdash; known as the \"intention economy\" &mdash; where AI assistants understand, forecast and manipulate human intentions and sell that information on to companies who can profit from it. The intention economy is touted by researchers at Cambridge's Leverhulme Centre for the Future of Intelligence (LCFI) as a successor to the attention economy, where social networks keep users hooked on their platforms and serve them adverts. The intention economy involves AI-savvy tech companies selling what they know about your motivations, from plans for a stay in a hotel to opinions on a political candidate, to the highest bidder... \n\nThe study claims that large language models (LLMs), the technology that underpins AI tools such as the ChatGPT chatbot, will be used to \"anticipate and steer\" users based on \"intentional, behavioural and psychological data\"... Advertisers will be able to use generative AI tools to create bespoke online ads, the report claims... AI models will be able to tweak their outputs in response to \"streams of incoming user-generated data\", the study added, citing research showing that models can infer personal information through workaday exchanges and even \"steer\" conversations in order to gain more personal information. \nThe article includes this quote from Dr. Jonnie Penn, an historian of technology at LCFI. \"Unless regulated, the intention economy will treat your motivations as the new currency. It will be a gold rush for those who target, steer and sell human intentions. \n\"We should start to consider the likely impact such a marketplace would have on human aspirations, including free and fair elections, a free press and fair market competition, before we become victims of its unintended consequences.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=AI+Tools+May+Soon+Manipulate+People's+Online+Decision-Making%2C+Say+Researchers%3A+https%3A%2F%2Fslashdot.org%2Fstory%2F24%2F12%2F30%2F0435226%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fslashdot.org%2Fstory%2F24%2F12%2F30%2F0435226%2Fai-tools-may-soon-manipulate-peoples-online-decision-making-say-researchers%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://slashdot.org/story/24/12/30/0435226/ai-tools-may-soon-manipulate-peoples-online-decision-making-say-researchers?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564457&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"When Jimmy Carter Spoke At a Wireless Tradeshow","url":"https://news.slashdot.org/story/24/12/30/0251249/when-jimmy-carter-spoke-at-a-wireless-tradeshow?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735528140,"author":"EditorDavid","unread":true,"content":"Former U.S. President Jimmy Carter has died. Born in 1924, he had just celebrated his 100th birthday on October 1st.\n\n\nIf you want to catch a glimpse of his political charisma, YouTube has a clip of Carter's appearance on \"What's My Line\" when he was still only governor of Georgia. Within five years he'd be president of the United States, serving from 1977 to 1981. \n\nBut it seems like today everyone has a story to tell. More than two decades later, long-time Slashdot reader destinyland saw Jimmy Carter speak in Las Vegas in 2001 on the final day of the CTIA Wireless tradeshow. \"I feel thrilled to be a part of this,\" 77-year-old Carter had said....\n\nCarter applauded the work of \"entrepreneurs and scientists and engineers that are transforming the face of the globe.\" And he noted their technologies could address problems targeted by the Carter Center. \n\nInterrupted by a few cellphone rings, the former President conversed on a stage at the Sands Expo and Venetian Hotel with Tom Wheeler, the president of the wireless communications trade association. Wheeler reminded the audience of Carter's decidedly nontechnical background, discussing An Hour Before Daylight, Carter's memoir about growing up on a farm in Georgia during the Great Depression. \"We were the only family blessed with an outhouse,\" Carter told the crowd. \n\n Wheeler also asked a question many in the technology community could relate to. Carter, he pointed out, had been involuntarily retired. \"What's it feel like?\" The former President told the audience he'd re-focussed his energies into humanitarian efforts through the Carter Center, which is active in providing health services around the world as well as monitoring elections. Carter donated his appearance fee to the Carter Center... \n\n Midway through the hour-long discussion, the former President touted his administration's record of deregulating several industries, including transportation, energy, and communications, saying \"If it hadn't been for that deregulation, this environment in which you all live wouldn't have been possible.\" Carter also shared with the business crowd that it was a belief in free enterprise that made him want to enter politics, drawn from his experiences selling peanuts as a young boy for a dollar a day. \n\nThe audience greeted the former president warmly, giving him a standing ovation both when he took the stage and when he left. Carter joked it was almost enough to make him want to get back into politics. \n\nEveryone has their own opinion. When a friend of mine was in high school, she got to meet Jimmy Carter early in his presidency. He'd seemed unusually kind and good, she said, but remembered her first reaction. \"They're going to eat you alive.\" And yet then, pointing to the humanitarian work he would continue for four decades, she said he was also clearly America's very best ex-president. \n\nAnd the liberal blog Talking Points Memo argues Carter's accomplishments as president are being re-evaluated:\nSome found him to be distinctly unsung, with little attention given to his brokering of peace with the Camp David Accords and emphasis on global human rights. And some just liked him. A serious, intelligent, faithful, deeply honest man who spurned political expediency and burned through hundreds of pages of memos a day, he preached self-restraint, stewardship and commonality to an electorate that cast him off four years later for the glib excesses of Ronald Reagan.... \"People assume that because he wasn't warm and cuddly with Congress that he didn't get much through,\" said John Alter [who wrote the first independent Carter biography in 2020]. \"He signed more legislation in four years than Clinton or Obama did in eight. He has the most prodigious legislative record since World War II, with the exception of Lyndon Johnson.\" \n\nThat record includes, by Alter's count, 14 major pieces of environmental legislation. In one of Carter's more creative moves, he dusted off the 1906 Antiquities Act to keep pristine 56 million acres of Alaskan wilderness. His piecemeal approach, cloaked in distinctly unsexy bills like the 1978 Public Utilities Regulatory Policies Act, planted the seeds for a changing national energy system in the face of climate change. Carter had started underlining passages in scientific journals about what is now the most existential crisis of our time as early as 1971. What's most wrenching about Carter's improvements in energy and environmental policy now is what he wasn't able to accomplish. On his way out of office, he issued a report that included recommendations for cutting carbon emissions &mdash; at exactly the same rate the Paris Climate Accords coalesced behind 35 years later.... \n\nHis Carter Center has virtually eradicated certain devastating diseases on the African continent, part of the work for which he received the Nobel Peace Prize in 2002. He and Rosalynn have also helped build and repair over 4,000 homes for Habitat for Humanity, work that continued well into his 90s. \n\n\n\nI've got my own story. As a young boy I saw Jimmy Carter give a speech in 1977 &mdash; just six months after he'd assumed the presidency. A crowd of teenagers thrilled to see the president gave him a long, loud round of applause. And when it finally died down, Carter said... \n\n\"I wish I got that kind of reception from Congress.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=When+Jimmy+Carter+Spoke+At+a+Wireless+Tradeshow%3A+https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0251249%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fnews.slashdot.org%2Fstory%2F24%2F12%2F30%2F0251249%2Fwhen-jimmy-carter-spoke-at-a-wireless-tradeshow%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://news.slashdot.org/story/24/12/30/0251249/when-jimmy-carter-spoke-at-a-wireless-tradeshow?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564415&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Nvidia’s next move: Powering humanoid robots","url":"https://techcrunch.com/2024/12/29/nvidias-next-move-powering-humanoid-robots/","date":1735517747,"author":"Connie Loizos","unread":true,"content":"<p>The chipmaking giant Nvidia is leaning more heavily into robotics in 2025. More specifically, in the first half of the new year, confirms the Financial Times, Nvidia is launching a new generation of compact computers for humanoid robots called Jetson Thor. The move, which was expected, is part of an evolving, years-long strategy. Nvidia doesn’t [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"2024's Ten Top-Grossing Films Were All Sequels or Prequels","url":"https://entertainment.slashdot.org/story/24/12/30/000205/2024s-ten-top-grossing-films-were-all-sequels-or-prequels?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735516980,"author":"EditorDavid","unread":true,"content":"\"Every single one of the top ten box office hits of 2024 was a sequel, a remake... or a prequel,\" writes The Hollywood Reporter. \n\nHere's the list of 2024's top-grossing films published by the movie blog SlashFilm: \n\n10. Beetlejuice Beetlejuice \n9. Venom: The Last Dance\n8. Kung Fu Panda 4\n7. Godzilla x Kong: The New Empire \n6. Wicked\n5. Dune: Part Two \n4. Moana 2\n3. Despicable Me 4\n2. Deadpool &amp; Wolverine \n1. Inside Out 2 \n\n2024 was the year Godzilla celebrated its 70th year as a franchise &mdash; but it wasn't the only long-running franchise. \"When the Marvel Cinematic Universe went R-rated with Deadpool &amp; Wolverine... it was literally more successful than any other R-rated movie in history,\" SlashFilm points out, while Venom: The Last Dance was the year's 9th highest-earner. (But several other big superhero movies flopped and \"the misses outweighed the hits this year, while DC sat it out entirely as the world waits for Superman to usher in James Gunn's new DC Universe.\") \n\nThey also marvel that Wicked earned $572 million after opening on the same day as Ridley Scott's Gladiator II.... \n\nBut in the end SlashFilm describes 2024 as \"a banner year for animation,\" with computer-animated movies filling four of the top ten spots (Kung Fu Panda 4, Moana 2, Despicable Me 4, and Inside Out 2). And another interesting trend? Though the world flocked to Tim Burton's first sequel to Beetlejuice after 36 years, Warner Bros. was, \"at one point, pushing for Beetlejuice 2 to go directly to streaming on Max.\" And Disney original had the same idea for Moana 2, leading SlashFilm to conclude that 2024's box office \"should be the death of the big direct-to-streaming movie.\" SlashFilm notes that Disney also sent several Pixar originals to Disney+ between 2020 and 2022, which \"did immeasurable damage to the brand, something that even CEO Bob Iger has acknowledged.\" And then after a theatrical debut Pixar's Inside Out 2 became \"the eighth biggest movie ever at the box office, with $1.698 billion to its name\" &mdash; and the highest-grossing animated film ever made. \n\n\nAnd Dune: Part Two?\n\nDenis Villeneuve accomplished nothing shy of a miracle with 2021's \"Dune,\" an adaptation of Frank Herbert's cherished sci-fi novel that was faithful to the material, massive in scale, but still felt like an auteur film... The only downside? 2021 was a terrible time to release a movie, particularly a Warner Bros. movie, as all of the studio's films were going to HBO Max the same day they hit theaters. Yet, \"Dune\" made $400 million in its original run, which was enough to justify a sequel. Evidently, the audience for this franchise grew exponentially in the years before \"Dune: Part Two\" hit theaters in early March... All told, Villeneuve's sweeping, epic sequel pulled in $714.4 million worldwide, all while garnering tons of acclaim once again. Also, not for nothing, Villeneuve got it made for less than $200 million... \n\n\nWithout \"Dune: Part Two\" making what it made, the box office might have been in truly dire shape. As a relatively dead April and very weak May followed, this overperformance helped keep theaters afloat until greener pastures arrived in the back half of the year. The Spice must flow, as it were. \n\n\n The Hollywood Reporter offers another take on the significance of 2024:\n\nTotal domestic box office revenue appears to be heading toward around $8 billion, down from 2023's exhilarating post-COVID turnaround of $9 billion, but the National Association of Theatre Owners prefers to accentuate the positive, attributing the dip to a shortage of product due to the labor strikes and taking encouragement from the renewal of the movie habit... \n\nInterestingly, or thankfully, the cinematic universes of Marvel, DC, and Star Wars failed to expand: except for Deadpool &amp; Wolverine, not one of the huge hits came from a comic book franchise or a galaxy far, far away. \nThe article then complains about people using their phones during the movie for texting, talking, and photographing the movie itself. (Though it applauds a PSA against the practice in which Deadpool and Wolverine \"delivered the message in laudably blunt terms.\") \nAnd on Wikipedia, Deadpool &amp; Wolverine and Dune: Part Two were the eighth and 23rd most popular articles of 2024.<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=2024's+Ten+Top-Grossing+Films+Were+All+Sequels+or+Prequels%3A+https%3A%2F%2Fentertainment.slashdot.org%2Fstory%2F24%2F12%2F30%2F000205%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fentertainment.slashdot.org%2Fstory%2F24%2F12%2F30%2F000205%2F2024s-ten-top-grossing-films-were-all-sequels-or-prequels%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://entertainment.slashdot.org/story/24/12/30/000205/2024s-ten-top-grossing-films-were-all-sequels-or-prequels?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564331&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Can Money Buy You a Longer Life?","url":"https://science.slashdot.org/story/24/12/29/2252216/can-money-buy-you-a-longer-life?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735513080,"author":"EditorDavid","unread":true,"content":"An anonymous reader shared this report from the Wall Street Journal:\n\n\nThe rich get richer &mdash; and older. People with high salaries and net worth tend to live longer lives, research shows. Once Americans make it to their late 50s, the wealthiest 10% live to a median age of around 86 years, roughly 14 years longer than the least wealthy 10%, according to a study published earlier this year in JAMA Internal Medicine. People with more money can afford healthier food, more healthcare and homes in safer, less-polluted neighborhoods, says Kathryn Himmelstein, a co-author of the study and a medical director at the Boston Public Health Commission. \n\nThough you can't add more months or years to your online shopping cart yet, health and aging researchers say there are ways to spend money to improve your chances of living longer. They suggest favoring purchases that help you track your health, stay active and reduce stress. \"We know the things that help us age better, and everyone's always disappointed when you tell them,\" says Andrew Scott, director of economics at the Ellison Institute of Technology in Oxford, England. \"Eat less and eat better, sleep more, exercise more and spend time with friends....\" But certain gadgets and luxuries can be worth the cost, some researchers say. Devices such as the Apple Watch and Oura Ring can instill healthy habits and catch worrying patterns that might emerge between annual checkups, says Joe Coughlin, the director of the MIT AgeLab... Coughlin says he once went to the emergency room because his Apple Watch detected a spike in his heart rate that he hadn't noticed himself. \n\n\"For the superwealthy, suddenly living longer and living better has become the new prestige,\" Coughlin says. Higher incomes correlate with longer lives, but there are diminishing returns. Each successive jump in pay is linked to smaller boosts in longevity, a 2016 study from the research group Opportunity Insights found... A key to the relationship between income and longevity is that money doesn't just buy stuff that helps you live longer. It also buys time and reduces stress. \"If you've got a nice place to live and you don't have to worry about food on the table, you have the mental head space and resources to prioritize your health,\" says Steven Woolf, a professor at Virginia Commonwealth University School of Medicine... Moreover, many lower-income jobs are more physically taxing and more prone to workplace accidents and exposure to harmful substances. \n\nThe article also includes examples of spending that promotes health, including things like home gym equipment and even swing-dancing lessons. \nBut it also adds that \"plenty of things that are good for you don't come with a bill, such as going on a walk or minimizing screen time before bedtime.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Can+Money+Buy+You+a+Longer+Life%3F%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2252216%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2252216%2Fcan-money-buy-you-a-longer-life%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/2252216/can-money-buy-you-a-longer-life?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564297&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"AI data centers could be ‘distorting’ the US power grid","url":"https://techcrunch.com/2024/12/29/ai-data-centers-could-be-distorting-the-us-power-grid/","date":1735511697,"author":"Anthony Ha","unread":true,"content":"<p>The proliferation of data centers aiming to meet the computational needs of AI could be bad news for the U.S. power grid, according to a new report in Bloomberg. Using the 1 million residential sensors tracked by Whisker Labs, along with market intelligence data from DC Byte, Bloomberg found that more than half of the [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Are We Better Prepared Now for Another Pandemic?","url":"https://science.slashdot.org/story/24/12/29/2152207/are-we-better-prepared-now-for-another-pandemic?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735509540,"author":"EditorDavid","unread":true,"content":"When it comes to the possibility of a bird flu outbreak, America's Centers for Disease Control recently issued a statement that the risk to the public \"remains low.\" \n\n\nBut even in the event of a worst-case scenario, New York magazine believes \"We may be more equipped for another pandemic than you think...\"\n\nIn 2023, more than half of people surveyed said that their lives had not returned to normal since the COVID outbreak, and a surprising number &mdash; 47 percent &mdash; said they now believe their lives will never return to normal. \n\nBut do we really know how a new pandemic would go and how we would handle it? Things are different this time &mdash; and in ways that aren't all bad. Unlike with COVID in the spring of 2020, millions of doses of bird-flu vaccines at various stages of testing sit in government stockpiles, and more are on the way. There are also already tests that work, though these are not broadly available to the public... Recent research suggests that we might actually manage a second pandemic better than we would believe. Despite all the noise to the contrary, a June poll by Harvard's School of Public Health says that Americans overall think the government responses to COVID &mdash; asking people to wear masks, pausing indoor dining, requiring health-care workers to get vaccinated &mdash; were all good ideas. Although the media tends to paint school closures as radically unpopular, only 44 percent of respondents said they currently think the shutdowns were a mistake. \n\nA growing body of research also suggests that many Americans feel stronger for what we endured during the most extreme days of COVID. Counter to what we like to say about our friends and neighbors and children, the challenge of the pandemic may have benefited some people's mental health. One study found that \"children entering the pandemic with clinically meaningful mental-health problems experienced notable improvements in their mental health.\" (Turns out there's one thing worse than shutting down an American school and that's having to attend it.) \n\nThe article also points out that \"There is no real information\" on the likelihood of a bird-flu virus even crossing over into humans. \n\nAnd of course, \"COVID still kills, with a body count just shy of 50,000 Americans in 2024, and it feels like a stretch to say that Americans are particularly concerned.\"<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Are+We+Better+Prepared+Now+for+Another+Pandemic%3F%3A+https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2152207%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fscience.slashdot.org%2Fstory%2F24%2F12%2F29%2F2152207%2Fare-we-better-prepared-now-for-another-pandemic%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://science.slashdot.org/story/24/12/29/2152207/are-we-better-prepared-now-for-another-pandemic?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564255&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Linux 6.13-rc5 Released To Cap Off Linus Torvalds' Birthday Week","url":"https://www.phoronix.com/news/Linux-6.13-rc5-Released","date":1735507830,"author":"Michael Larabel","unread":true,"content":"The holiday between Christmas and New Year's is... Linus Torvalds' birthday on 28 December. Capping off the Linux creator's 55th birthday week is the Linux 6.13-rc5 kernel release...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"India’s mobile payments dilemma","url":"https://techcrunch.com/2024/12/29/indias-mobile-payments-dilemma/","date":1735507256,"author":"Manish Singh","unread":true,"content":"<p>India&#8217;s payments regulator is set to decide as early as Monday whether to curb the dominance of Walmart&#8217;s PhonePe and Google in the nation&#8217;s fast-growing mobile payments market, a move that could reshape how its billion-plus population moves money. The decision centers on UPI, or Unified Payments Interface, a network backed by more than 50 [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Benchmarking The AMD INVLPGB Linux Kernel Patches For Better Performance","url":"https://www.phoronix.com/review/amd-invlpgb-linux","date":1735506920,"author":"Michael Larabel","unread":true,"content":"Last weekend a Meta engineer posted Linux kernel patches to make use of the AMD INVLPGB instruction for broadcast TLB invalidation. The Linux kernel can in turn invalidate TLB entries on remote CPUs without needing to send IPIs and without having to wait for remote CPUs to handle those interrupts. Synthetic benchmarks shown in that patch series were very promising and thus I carried out some benchmarking over the holidays of this AMD INVLPGB support for the Linux kernel.","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"KDE Amarok 3.2 Music Player Released With Initial Qt6/KF6 Support","url":"https://www.phoronix.com/news/KDE-Amarok-3.2-Released","date":1735506000,"author":"Michael Larabel","unread":true,"content":"Back in April was the release of the Amarok 3.0 music player for KDE after a six year hiatus and their first version ported to using the Qt5 toolkit and KDE Frameworks 5. Now in ending out 2024, the Amarok team has released an updated version of this open-source music player that provides initial support for the Qt6 toolkit and KDE Frameworks 6...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Finland Finds Drag Marks Near Broken Undersea Cable. Russia's 'Shadow Fleet' Suspected","url":"https://tech.slashdot.org/story/24/12/29/2055221/finland-finds-drag-marks-near-broken-undersea-cable-russias-shadow-fleet-suspected?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1735505880,"author":"EditorDavid","unread":true,"content":"Reuters reports:\n\nFinnish police said on Sunday they had found tracks that drag on for dozens of kilometres along the bottom of the Baltic Sea where a tanker carrying Russian oil is suspected of breaking a power line and four telecoms cables with its anchor... A break in the 658 megawatt (MW) Estlink 2 power cable between Finland and Estonia occurred at midday on Wednesday, leaving only the 358 MW Estlink 1 linking the two countries, grid operators said. They said Estlink 2 might not be back in service before August. \n\nIn an interesting twist, the New York Times reports that the ship \"bears all the hallmarks of vessels belonging to Russia's shadow fleet, officials said, and had embarked from a Russian port shortly before the cables were cut.\"\n\nIf confirmed, it would be the first known instance of a shadow fleet vessel being used to intentionally sabotage critical infrastructure in Europe &mdash; and, officials and experts said, a clear escalation by Russia in its conflict with the West... NATO's general secretary, Mark Rutte, responding to requests from the leaders of Finland and Estonia, both member nations, said the Atlantic alliance would \"enhance\" its military presence in the Baltic Sea... \n\nSince Russia began assembling its fleet, the number of shadow vessels traversing the oceans has grown by hundreds and now makes up 17 percent of the total global oil tanker fleet... Nearly 70 percent of Russia's oil is being transported by shadow tankers, according to an analysis published in October by the Kyiv School of Economics Institute, a research organization based in Ukraine... The authorities in Finland are still investigating whether the \"Eagle S\" engaged in a criminal act. But the sheer size of the shadow fleet might have made using some of these vessels for sabotage irresistible to Russia, [said Elisabeth Braw, a senior fellow at the Atlantic Council who has researched and written about shadow fleets]... \n\nWhile it's still not certain that this week's cable cutting was done intentionally, the Baltic Sea, for a number of reasons, is an ideal arena to carry out sabotage operations. It is relatively shallow and is crisscrossed with essential undersea cables and pipelines that provide energy, as well as internet and phone services, to a number of European countries that are NATO members. Russia has relatively unfettered access to the sea from several ports, and its commercial vessels, protected by international maritime law, can move around international waters largely unmolested... The suspicions that Russia was using shadow vessels for more than just escaping sanctions existed before this week's cable cutting. Last April, the head of Sweden's Navy told a local news outlet that there was evidence such ships were being used to conduct signals intelligence on behalf of Russia and that some fishing vessels had been spotted with antennas and masts not normally seen on commercial vessels. Since the war began, there has also been an uptick in suspicious episodes resulting in damage to critical undersea infrastructure... \n\n\nHours after Finland's energy grid operator alerted the police that an undersea power cable was damaged on Wednesday, Finnish officers descended by helicopter to the ship's deck and took over the bridge, preventing the vessel from sailing farther. By Friday, it remained at anchor in the Gulf of Finland, guarded by a Finnish Defense Forces missile boat and a Border Guard patrol vessel. \n\nThe cable incident happened just weeks after the EU issued new sanctions targetting Russia's shadow fleet, Euronews reports. \"A handful of Chinese companies suspected of enabling Russia's production of drones are also blacklisted as part of the agreement, a diplomat told Euronews.\"\nThe \"shadow fleet\" has been accused of deceptive practices, including transmitting falsified data and turning off their transporters to become invisible to satellite systems, and conducting multiple ship-to-ship transfers to conceal the origin of the oil barrels...<p><div class=\"share_submission\" style=\"position:relative;\">\n<a class=\"slashpop\" href=\"http://twitter.com/home?status=Finland+Finds+Drag+Marks+Near+Broken+Undersea+Cable.++Russia's+'Shadow+Fleet'+Suspected%3A+https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F29%2F2055221%2F%3Futm_source%3Dtwitter%26utm_medium%3Dtwitter\"><img src=\"https://a.fsdn.com/sd/twitter_icon_large.png\"></a>\n<a class=\"slashpop\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Ftech.slashdot.org%2Fstory%2F24%2F12%2F29%2F2055221%2Ffinland-finds-drag-marks-near-broken-undersea-cable-russias-shadow-fleet-suspected%3Futm_source%3Dslashdot%26utm_medium%3Dfacebook\"><img src=\"https://a.fsdn.com/sd/facebook_icon_large.png\"></a>\n\n\n\n</div></p><p><a href=\"https://tech.slashdot.org/story/24/12/29/2055221/finland-finds-drag-marks-near-broken-undersea-cable-russias-shadow-fleet-suspected?utm_source=rss1.0moreanon&amp;utm_medium=feed\">Read more of this story</a> at Slashdot.</p><iframe src=\"https://slashdot.org/slashdot-it.pl?op=discuss&amp;id=23564225&amp;smallembed=1\" style=\"height: 300px; width: 100%; border: none;\"></iframe>","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Crypto industry groups sue IRS over broker reporting rule","url":"https://techcrunch.com/2024/12/29/crypto-industry-groups-sue-irs-over-broker-reporting-rule/","date":1735504854,"author":"Anthony Ha","unread":true,"content":"<p>Three crypto industry groups — the DeFi Education Fund, the Blockchain Association, and the Texas Blockchain Council — are suing the Internal Revenue Service to block new regulations that require decentralized finance (DeFi) entities to report customer information. The IRS has been finalizing crypto tax regulations as part of the Biden Administration’s Infrastructure Investment and [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Kdenlive Preparing For An Exciting 2025 With Background Removal Tool & More","url":"https://www.phoronix.com/news/Kdenlive-Background-Removal-25","date":1735485314,"author":"Michael Larabel","unread":true,"content":"The KDE app Kdenlive that is a very popular and featureful open-source video editor is preparing for an exciting 2025...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"GPS Spoofing Attacks Are Dangerously Misleading Airliners","url":"https://spectrum.ieee.org/gps-spoofing-2670499105","date":1735484403,"author":"Margo Anderson","unread":true,"content":"<p>Electronic warfare is taking a perilous turn into civilian airspace</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzgwMi9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0ODQyNTA2NX0.5iWnSeL_UGfAyzZ6VJZNQQrm6u3ONzKQipV7JL7Ylhg/image.png?width=600","enclosureMime":""},{"title":"Africa’s newest fintech unicorns are winning by keeping their feet on the ground","url":"https://techcrunch.com/2024/12/29/africas-fintech-unicorns-blend-digital-banking-and-physical-touchpoints/","date":1735484400,"author":"Tage Kene-Okafor","unread":true,"content":"<p>Africa’s tech ecosystem just got a boost of attention, with South Africa’s TymeBank and Nigeria’s Moniepoint both raising funds in recent weeks at valuations of over $1 billion and joining the coveted unicorn pantheon. But those valuations don’t just reflect investor confidence. They signal the success they’ve had in taking disruptive fintech models originally developed [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Faster USB Performance For xHCI DbC Coming With Linux 6.14 Plus A 10 Year Old Bug Fixed","url":"https://www.phoronix.com/news/Faster-USB-xHCI-DbC-Linux-6.14","date":1735483524,"author":"Michael Larabel","unread":true,"content":"Thanks to work from Intel engineers, the upcoming Linux 6.14 kernel cycle will feature faster USB xHCI DbC performance for debug performance and a few other missing xHCI bits being addressed. Plus there is a fix for a rare 10 year old USB bug report...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"The Top 7 Robotics Stories of 2024","url":"https://spectrum.ieee.org/top-robotics-stories-2024","date":1735480802,"author":"Evan Ackerman","unread":true,"content":"<p>A new Atlas, Figure's bonkers funding round, and the last voyage of a helicopter on Mars</p>","flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NTM4MzY0OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NTQ4MzExMX0.3bca1xoiyaOOgYmcqPZ7K3Izu2FimblAkRoTgP96l7M/image.jpg?width=600","enclosureMime":""},{"title":"Permira’s Brian Ruder talks AI, Squarespace acquisition, and the value of co-leadership","url":"https://techcrunch.com/2024/12/29/permiras-brian-ruder-talks-ai-squarespace-acquisition-and-the-value-of-co-leadership/","date":1735480800,"author":"Paul Sawers","unread":true,"content":"<p>It has been a busy year in the private equity realm, with countless big-money acquisitions unfolding. The take-private space specifically has seen some sizable transactions, with private equity firms spearheading more than a dozen billion-dollar deals for public tech companies. London-headquartered Permira was a key protagonist, joining Blackstone to acquire European online classifieds group Adevinta [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Ubuntu's Great Year From 24.04 LTS To Focusing More On Performance Optimizations","url":"https://www.phoronix.com/news/Ubuntu-2024-Great-Year","date":1735473669,"author":"Michael Larabel","unread":true,"content":"From my independent monitoring, Ubuntu Linux had a pretty great year. Ubuntu 24.04 LTS shipped and has been well received across enterprises, Canonical engineers have been focusing more on performance optimizations for Ubuntu, and there has been other interesting changes like their new commitment to always ship the latest upstream Linux kernel version as of Ubuntu release time. Plus they have continued with various GNOME desktop improvements, Ubuntu on servers continues with steady traction, and all-around was a pretty exciting year for the Ubuntu camp...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Fish Shell Outlines Their Successes & Challenges Going From C++ To Rust","url":"https://www.phoronix.com/news/Fish-Shell-Rust-Challenges","date":1735473213,"author":"Michael Larabel","unread":true,"content":"Earlier this month the Fish Shell 4.0 went into beta with the C++ code ported to Rust. Now with most of the Fish Shell code transitioned to Rust, the project put out a blog post this weekend outlining the successes and challenges they have encountered in porting their large C++ codebase to Rust...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Apple DWI Backlight Linux Driver Updated For Various iPhones, iPods & iPads","url":"https://www.phoronix.com/news/Apple-DWI-Backlight-Linux-v4","date":1735472940,"author":"Michael Larabel","unread":true,"content":"While Linux 6.13 is introducing basic support for various Apple iPads and iPhones using A-series SoCs, the support is just that: basic. Various feature limitations remain for those dreaming over the prospects of running Linux on older Apple mobile devices. One of various feature limitations remaining are around backlight control for different models and for that there is the Apple DWI backlight driver for Linux that continues to be hacked on...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Linux 6.13-rc5 To See Fix For Intel TDX CoCo VMs Potentially Leaking Decrypted Memory","url":"https://www.phoronix.com/news/Linux-6.13-Fixing-TDX-CoCo-Leak","date":1735471447,"author":"Michael Larabel","unread":true,"content":"The x86 fixes pull request was sent out this morning ahead of the Linux 6.13-rc5 kernel being released later today. Both x86 fixes this week pertain to Intel bits: a self-test issue on upcoming Intel FRED (Flexible Return and Event Delivery) systems and also an issue of Intel TDX confidential computing VM guests potentially leaking decrypted memory within the unrecoverable error handling...","flags":null,"enclosureUrl":"","enclosureMime":""},{"title":"Google CEO says AI model Gemini will be the company’s ‘biggest focus’ in 2025","url":"https://techcrunch.com/2024/12/28/google-ceo-says-ai-model-gemini-will-the-companys-biggest-focus-in-2025/","date":1735423241,"author":"Anthony Ha","unread":true,"content":"<p>CEO Sundar Pichai reportedly told Google employees that 2025 will be a “critical” year for the company. CNBC reports that it obtained audio from a December 18 strategy meeting where Pichai and other executives put on ugly holiday sweaters and laid out their priorities for the coming year. “I think 2025 will be critical,” Pichai [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>\n","flags":null,"enclosureUrl":"","enclosureMime":""}]}