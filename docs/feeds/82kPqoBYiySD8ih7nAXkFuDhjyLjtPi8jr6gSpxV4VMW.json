{"id":"82kPqoBYiySD8ih7nAXkFuDhjyLjtPi8jr6gSpxV4VMW","title":"The Go Blog","displayTitle":"Dev - Golang Blog","url":"http://blog.golang.org/feed.atom","feedLink":"http://blog.golang.org/feed.atom","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":2,"items":[{"title":"Container-aware GOMAXPROCS","url":"https://go.dev/blog/container-aware-gomaxprocs","date":1755648000,"author":"Michael Pratt and Carlos Amedee","guid":136,"unread":true,"content":"<p>Go 1.25 includes new container-aware  defaults, providing more sensible default behavior for many container workloads, avoiding throttling that can impact tail latency, and improving Go’s out-of-the-box production-readiness.\nIn this post, we will dive into how Go schedules goroutines, how that scheduling interacts with container-level CPU controls, and how Go can perform better with awareness of container CPU controls.</p><p>One of Go’s strengths is its built-in and easy-to-use concurrency via goroutines.\nFrom a semantic perspective, goroutines appear very similar to operating system threads, enabling us to write simple, blocking code.\nOn the other hand, goroutines are more lightweight than operating system threads, making it much cheaper to create and destroy them on the fly.</p><p>While a Go implementation could map each goroutine to a dedicated operating system thread, Go keeps goroutines lightweight with a runtime scheduler that makes threads fungible.\nAny Go-managed thread can run any goroutine, so creating a new goroutine doesn’t require creating a new thread, and waking a goroutine doesn’t necessarily require waking another thread.</p><p>That said, along with a scheduler comes scheduling questions.\nFor example, exactly how many threads should we use to run goroutines?\nIf 1,000 goroutines are runnable, should we schedule them on 1,000 different threads?</p><p>This is where <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> comes in.\nSemantically,  tells the Go runtime the “available parallelism” that Go should use.\nIn more concrete terms,  is the maximum number of threads to use for running goroutines at once.</p><p>So, if  and there are 1,000 runnable goroutines, Go will use 8 threads to run 8 goroutines at a time.\nOften, goroutines run for a very short time and then block, at which point Go will switch to running another goroutine on that same thread.\nGo will also preempt goroutines that don’t block on their own, ensuring all goroutines get a chance to run.</p><p>From Go 1.5 through Go 1.24,  defaulted to the total number of CPU cores on the machine.\nNote that in this post, “core” more precisely means “logical CPU.”\nFor example, a machine with 4 physical CPUs with hyperthreading has 8 logical CPUs.</p><p>This typically makes a good default for “available parallelism” because it naturally matches the available parallelism of the hardware.\nThat is, if there are 8 cores and Go runs more than 8 threads at a time, the operating system will have to multiplex these threads onto the 8 cores, much like how Go multiplexes goroutines onto threads.\nThis extra layer of scheduling is not always a problem, but it is unnecessary overhead.</p><p>Another of Go’s core strengths is the convenience of deploying applications via a container, and managing the number of cores Go uses is especially important when deploying an application within a container orchestration platform.\nContainer orchestration platforms like <a href=\"https://kubernetes.io/\" rel=\"noreferrer\" target=\"_blank\">Kubernetes</a> take a set of machine resources and schedule containers within the available resources based on requested resources.\nPacking as many containers as possible within a cluster’s resources requires the platform to be able to predict the resource usage of each scheduled container.\nWe want Go to adhere to the resource utilization constraints that the container orchestration platform sets.</p><p>Let’s explore the effects of the  setting in the context of Kubernetes, as an example.\nPlatforms like Kubernetes provide a mechanism to limit the resources consumed by a container.\nKubernetes has the concept of CPU resource limits, which signal to the underlying operating system how many core resources a specific container or set of containers will be allocated.\nSetting a CPU limit translates to the creation of a Linux <a href=\"https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu\" rel=\"noreferrer\" target=\"_blank\">control group</a> CPU bandwidth limit.</p><p>Before Go 1.25, Go was unaware of CPU limits set by orchestration platforms.\nInstead, it would set  to the number of cores on the machine it was deployed to.\nIf there was a CPU limit in place, the application may try to use far more CPU than allowed by the limit.\nTo prevent an application from exceeding its limit, the Linux kernel will <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">throttle</a> the application.</p><p>Throttling is a blunt mechanism for restricting containers that would otherwise exceed their CPU limit: it completely pauses application execution for the remainder of the throttling period.\nThe throttling period is typically 100ms, so throttling can cause substantial tail latency impact compared to the softer scheduling multiplexing effects of a lower  setting.\nEven if the application never has much parallelism, tasks performed by the Go runtime—such as garbage collection—can still cause CPU spikes that trigger throttling.</p><p>We want Go to provide efficient and reliable defaults when possible, so in Go 1.25, we have made  take into account its container environment by default.\nIf a Go process is running inside a container with a CPU limit,  will default to the CPU limit if it is less than the core count.</p><p>Container orchestration systems may adjust container CPU limits on the fly, so Go 1.25 will also periodically check the CPU limit and adjust  automatically if it changes.</p><p>Both of these defaults only apply if  is otherwise unspecified.\nSetting the  environment variable or calling  continues to behave as before.\nThe <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> documentation covers the details of the new behavior.</p><h2>Slightly different models</h2><p>Both  and a container CPU limit place a limit on the maximum amount of CPU the process can use, but their models are subtly different.</p><p> is a parallelism limit.\nIf  Go will never run more than 8 goroutines at a time.</p><p>By contrast, CPU limits are a throughput limit.\nThat is, they limit the total CPU time used in some period of wall time.\nThe default period is 100ms.\nSo an “8 CPU limit” is actually a limit of 800ms of CPU time every 100ms of wall time.</p><p>This limit could be filled by running 8 threads continuously for the entire 100ms, which is equivalent to .\nOn the other hand, the limit could also be filled by running 16 threads for 50ms each, with each thread being idle or blocked for the other 50ms.</p><p>In other words, a CPU limit doesn’t limit the total number of CPUs the container can run on.\nIt only limits total CPU time.</p><p>Most applications have fairly consistent CPU usage across 100ms periods, so the new  default is a pretty good match to the CPU limit, and certainly better than the total core count!\nHowever, it is worth noting that particularly spiky workloads may see a latency increase from this change due to  preventing short-lived spikes of additional threads beyond the CPU limit average.</p><p>In addition, since CPU limits are a throughput limit, they can have a fractional component (e.g., 2.5 CPU).\nOn the other hand,  must be a positive integer.\nThus, Go must round the limit to a valid  value.\nGo always rounds up to enable use of the full CPU limit.</p><p>Go’s new  default is based on the container’s CPU limit, but container orchestration systems also provide a “CPU request” control.\nWhile the CPU limit specifies the maximum CPU a container may use, the CPU request specifies the minimum CPU guaranteed to be available to the container at all times.</p><p>It is common to create containers with a CPU request but no CPU limit, as this allows containers to utilize machine CPU resources beyond the CPU request that would otherwise be idle due to lack of load from other containers.\nUnfortunately, this means that Go cannot set  based on the CPU request, which would prevent utilization of additional idle resources.</p><p>Containers with a CPU request are still <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">constrained</a> when exceeding their request if the machine is busy.\nThe weight-based constraint of exceeding requests is “softer” than the hard period-based throttling of CPU limits, but CPU spikes from high  can still have an adverse impact on application behavior.</p><h2>Should I set a CPU limit?</h2><p>We have learned about the problems caused by having  too high, and that setting a container CPU limit allows Go to automatically set an appropriate , so an obvious next step is to wonder whether all containers should set a CPU limit.</p><p>While that may be good advice to automatically get a reasonable  defaults, there are many other factors to consider when deciding whether to set a CPU limit, such as prioritizing utilization of idle resources by avoiding limits vs prioritizing predictable latency by setting limits.</p><p>The worst behaviors from a mismatch between  and effective CPU limits occur when  is significantly higher than the effective CPU limit.\nFor example, a small container receiving 2 CPUs running on a 128 core machine.\nThese are the cases where it is most valuable to consider setting an explicit CPU limit, or, alternatively, explicitly setting .</p><p>Go 1.25 provides more sensible default behavior for many container workloads by setting  based on container CPU limits.\nDoing so avoids throttling that can impact tail latency, improves efficiency, and generally tries to ensure Go is production-ready out-of-the-box.\nYou can get the new defaults simply by setting the Go version to 1.25.0 or higher in your .</p><p>Thanks to everyone in the community that contributed to the <a href=\"https://go.dev/issue/33803\">long</a><a href=\"https://go.dev/issue/73193\">discussions</a> that made this a reality, and in particular to feedback from the maintainers of <a href=\"https://pkg.go.dev/go.uber.org/automaxprocs\" rel=\"noreferrer\" target=\"_blank\"></a> from Uber, which has long provided similar behavior to its users.</p>","contentLength":9210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.25 is released","url":"https://go.dev/blog/go1.25","date":1754956800,"author":"Dmitri Shuralyov, on behalf of the Go team","guid":135,"unread":true,"content":"<p>Today the Go team is pleased to release Go 1.25.\nYou can find its binary archives and installers on the <a href=\"https://go.dev/dl/\">download page</a>.</p><p>Please refer to the <a href=\"https://go.dev/doc/go1.25\">Go 1.25 Release Notes</a> for the complete list\nof additions, changes and improvements in Go 1.25.</p><p>Over the next few weeks, follow-up blog posts will cover some of the topics\nrelevant to Go 1.25 in more detail. Check back in later to read those posts.</p><p>Thanks to everyone who contributed to this release by writing code, filing bugs,\ntrying out experimental additions, sharing feedback, and testing the release candidates.\nYour efforts helped make Go 1.25 as stable as possible.\nAs always, if you notice any problems, please <a href=\"https://go.dev/issue/new\">file an issue</a>.</p><p>We hope you enjoy using the new release!</p>","contentLength":710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","go"]}