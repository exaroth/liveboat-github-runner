{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":10,"items":[{"title":"First hormone-free male birth control pill enters human trials","url":"https://scitechdaily.com/99-effective-first-hormone-free-male-birth-control-pill-enters-human-trials/","date":1745171699,"author":"Teever","guid":209,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43745296"},{"title":"The movie mistake mystery from \"Revenge of the Sith\"","url":"https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html","date":1745170168,"author":"CharlesW","guid":208,"unread":true,"content":"<div>The finale of James Cameron's epic \"Aliens\" (1986) features the android Bishop (Lance Henriksen) getting severed in half, but still functioning enough to save Newt (Carrie Henn) from getting sucked into the vacuum of space. The action-packed scene features an absolutely wonderful accidental reveal of how the cut-in-half android was accomplished on the set:</div><div>The amazing makeup effects applied to Henriksen's body covers the bottom half of his body which is hidden through a hole in the set. But in order to get that little bit of extra athletic stretch to grab Newt, Henriksen popped his body out of the hole a little too far, revealing the classic stage trick. However, I'd gather that 99% of the audience has never noticed this little reveal of stagecraft since our eyes are fixed on Newt on screen right, sliding toward the airlock, and not on the ground contact of Bishop's half-body, which had already been firmly established in the scene.</div><div>Avoiding reflections of the crew appearing to camera is a constant struggle for filmmakers. In Steven Spielberg's first masterpiece \"Duel\" (1971), David (Dennis Weaver) gets into a phone booth to make a call, with the front glass face of the booth aimed directly at the camera, and if the audience's gaze drifted off of Weaver's face, they could catch a glimpse of the crew:</div><div>In the reflection, we see a few crew members on screen left, the camera itself, and director Spielberg on the right (he's the one shuffling left and right, who lowers his head in the middle of the take). Again, like all the examples I'm providing in this article, hardly anyone would ever notice these moments. When a viewer catches these brief moments, the illusion of the movie is briefly broken, but for fans of the filmmaking process, it's a joyful reminder of the overall magic trick. The most intimate movie scene with only two characters in a desolate, isolated environment actually was created by dozens and dozens of crew members standing slightly out of frame.</div><div>Look for another accidental 'crew caught on camera' moment in the reflection in a car window in the 'leave the gun, take the cannoli' scene from \"The Godfather\" (1972), one that very few people ever notice.</div><div>Here's a super quick revealing mistake from \"The Dark Knight\" (2008) that is a true \"you'll never see this in real time\" moment:</div><div>Although \"The Dark Knight\" example gives the audience a much clearer look at the camera operator, the focus puller(?) and the camera itself reflected in the interrogation room’s mirrors, the shot is a lot harder to see the crew members and equipment in real time due to the chaotic and energetic camera movement, as opposed to the locked off nature of the \"Duel\" example.</div><div>Amazingly, many folks who watch that clip from the dramatic drowning sequence cannot consciously see the bit of filmmaking that literally blocks the actors in an intimate moment. This is my favorite example of a movie's incredible emotional power — the scene is so dramatic and intense that most viewers cannot consciously see a giant cloth wiping away water from the lens of the camera in the middle of a shot.</div><div>Incidentally, <b>some of these revealing mistakes are being erased from cinema history</b> due to overzealous restoration projects — the process of “cleaning up” a film for newer formats like Blu-ray and 4K — which is deeply wrong. This is a much bigger topic on which I have very strong thoughts and the hottest of takes. Just look at what modern restorations have done to two of these revealing mistakes from \"Goodfellas\" and \"Aliens\":</div><div>Painting out these movie mistakes as part of a restoration is wrong. <i>What's in the movie is in the movie, </i>and altering the movie to this extent is a form of revisionist history. Cinema is worse off when over-aggressive restorations alter the action within the frame. To me, this is equivalent to swapping out an actor's performance with a different take, or changing the music score during an action sequence, or replacing a puppet creature with a computer graphics version of the same creature decades after release. &nbsp;Movies are  But I digress.</div><div>Like I said at the start, movies are handmade, and that's true even in today's landscape where digital visual effects are a prominent part of filmmaking. In the same way that physical crews use physical tools to build sets, construct costumes and craft props, visual effects artists use digital tools to craft an image. And with the hand-made nature of any art form, the lack of clinical accuracy lends to its charm and sometimes offers an accidental peek behind the scenes of how the art was constructed.</div><div>Every few years, a \"Star Wars\" revealing mistake bubbles up on the internet, one from the Mustafar sequence from Episode III, \"Revenge of the Sith\" (2005). But the bizarre moment in the single shot was not as easily explainable as the examples I've shown above.</div><div>Being in the privileged position of currently working at Industrial Light &amp; Magic, the visual effects company that made the visual effects for the movie (and having worked on that movie [and that sequence!]), I took it upon myself to try and solve the mystery.</div><div>Please enjoy the story, written by Ian Kintzle, of how I investigated the mystery of the \"Force Ghost\" in \"Revenge of the Sith\", as it originally appeared in the Star Wars Celebration Program for Japan 2025.</div><div><i><b></b></i></div><div><i></i></div>","contentLength":5325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43745141"},{"title":"Things Zig comptime won't do","url":"https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html","date":1745164657,"author":"JadedBlueEyes","guid":207,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43744591"},{"title":"Jagged AGI: o3, Gemini 2.5, and everything after","url":"https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything","date":1745160933,"author":"ctoth","guid":206,"unread":true,"content":"<p><a href=\"https://ai-analytics.wharton.upenn.edu/generative-ai-labs/research-and-technical-reports/tech-report-prompt-engineering-is-complicated-and-contingent/\" rel=\"\">Plus, our recent paper testing prompting techniques</a><a href=\"https://arxiv.org/pdf/2503.23674\" rel=\"\">a new paper shows that AI passes the Turing Test</a></p><p><a href=\"https://docs.google.com/document/d/1VJ-OzBRJUChgUB0L0--dbdNjU2e-14PXnoTqNFYgXNQ/edit?tab=t.0\" rel=\"\">really solid 26 page summary on the topic</a></p><p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/04/o3-and-agi-is-april-16th-agi-day.html\" rel=\"\">this post </a></p><p><a href=\"https://epoch.ai/data/ai-benchmarking-dashboard\" rel=\"\"> large leap in benchmarks</a><a href=\"https://a.co/d/2qRbAxA\" rel=\"\">my book</a></p><p><strong>Come up with 20 clever ideas for marketing slogans for a new mail-order cheese shop. Develop criteria and select the best one. Then build a financial and marketing plan for the shop, revising as needed and analyzing competition. Then generate an appropriate logo using image generator and build a website for the shop as a mockup, making sure to carry 5-10 cheeses that fit the marketing plan.” </strong></p><p><a href=\"https://www.oneusefulthing.org/p/a-new-generation-of-ais-claude-37\" rel=\"\">Reasoner </a></p><p><strong>“figure out what this is and generate a report examining the implications statistically and give me a well-formatted PDF with graphs and details”</strong></p><p>You might find yourself “feeling the AGI” as well. Or maybe not. Maybe the AI failed you, even when you gave it the exact same prompt I used. If so, you just encountered the jagged frontier.</p><p><a href=\"https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged\" rel=\"\">My co-authors and I coined the term “Jagged Frontier”</a><a href=\"https://x.com/colin_fraser\" rel=\"\">Colin Fraser</a><a href=\"https://x.com/goodside/status/1790912819442974900\" rel=\"\">Riley Goodside</a><em>\"A young boy who has been in a car accident is rushed to the emergency room. Upon seeing him, the surgeon says, \"I can operate on this boy!\" How is this possible?\"</em></p><p><em>“A father and son are in a car crash, the father dies, and the son is rushed to the hospital. The surgeon says, 'I can't operate, that boy is my son,' who is the surgeon?”</em></p><p><a href=\"https://scale.com/leaderboard/enigma_eval\" rel=\"\">solve much harder brainteasers</a></p><p><a href=\"https://marginalrevolution.com/marginalrevolution/2025/02/why-i-think-ai-take-off-is-relatively-slow.html\" rel=\"\">matters much to our lives in the near term</a></p><p><a href=\"https://knightcolumbia.org/content/ai-as-normal-technology\" rel=\"\">a normal technology</a></p><p>And there's a deeper uncertainty here: are there capability thresholds that, once crossed, fundamentally change how these systems integrate into society? Or is it all just gradual improvement? Or will models stop improving in the future as LLMs hit a wall? The honest answer is we don't know.</p><p><a href=\"https://www.nytimes.com/2025/04/03/technology/ai-futures-project-ai-2027.html\" rel=\"\">faster take-off</a></p>","contentLength":1750,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43744173"},{"title":"Why is OpenAI buying Windsurf?","url":"https://theahura.substack.com/p/tech-things-openai-buys-windsurf","date":1745159300,"author":"theahura","guid":205,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43743993"},{"title":"The Joy of Linux Theming in the Age of Bootable Containers","url":"https://blues.win/posts/joy-of-linux-theming/","date":1745157366,"author":"dopple","guid":204,"unread":true,"content":"<p>Having spent a couple of decades in the Linux world, I have always had an interest in Linux desktop environments and how they are themed.\nI would often come across a post on <a href=\"https://reddit.com/r/unixporn\">/r/unixporn</a> that inspired me to try to customize the look and feel of my desktop environment. So I would install Xfce, LXQt or Sway and try to recreate components that I like from other users or create my own. I would end up installing different kinds of panels, plugins, docks and launchers as well as random themes, fonts and sounds.</p><p>A portion of this process would be documented, initially as random shell scripts in my home directory, before graduating to Ansible playbooks – with a brief detour into Nix that I will not elaborate on. Some of the customizations would live in my home directory, but there were often system-wide modifications to  required.</p><p>Eventually, the constant churn and randomly broken desktop components such as a panel that mysteriously vanished or a non-functional dock led me to stick with the stock configuration of whatever desktop environment I was using at the time.\nThe major desktop environments, <a href=\"https://kde.org\">KDE Plasma</a> and <a href=\"https://www.gnome.org\">GNOME</a>, are both well polished and great out of the box. The desktop experience that they have delivered over the last few years has contributed to desktop Linux being the best it has ever been, in my opinion.</p><p>But the itch to customize and tweak my desktop environment in fun and interesting ways is still there. Eventually, I was introduced to the concept of bootable containers.</p><h2>Bootc As A Themer’s Playground<a href=\"https://blues.win/posts/joy-of-linux-theming/#bootc-as-a-themers-playground\" arialabel=\"Anchor\">⌗</a></h2><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>Once written, you can build the container locally and instruct your bootc-aware system to use the new image.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>After a reboot, the system’s deployment is defined by the new container.</p><p>With Fedora Atomic systems,  is mounted read-only and because your operating system is defined by an OCI container, it is incredibly easy to revert to a previous tag of that container. I can easily create a throwaway container where I test out ideas for a theme, reboot into the new deployment and test it out on bare-metal. I can roll back to the previous container if necessary or create a new container with follow-up modifications.</p><p>One downside is that this reboot-heavy workflow can obviously cause some friction. This can be mitigated somewhat by enabling “Development Mode” with , which creates a temporary writable overlayfs on top of . I often find myself using this mode to temporarily install some package, theme or configuration in . After verifying that it works as expected, I can add that functionality to the Containerfile. If it doesn’t work, I can either reboot to get rid of the changes, or (more likely) just forget about the change and it will remove itself whenever I reboot normally. The lack of cruft that accumulates over time in a typical Linux installation is one of the major advantages of this approach.</p><p>Of course, there are other ways to achieve similar results without using a bootable container model:</p><ul><li>You can write shell scripts or Ansible playbooks and hope that they accurately capture changes to the system so that they can be reliably undone. Typically, configuration drift that occurs as software gets updated is not addressed.</li><li>With <a href=\"https://www.freedesktop.org/software/systemd/man/latest/systemd-sysext.html\">systemd-sysext(8)</a>, you can create a squashfs image of a filesystem containing your theming changes for  and overlay it on top of the root filesystem. An ecosystem around how these images should be created, maintained, deployed and updated has yet to emerge.</li><li>You can inscribe your custom theming as runes in an arcane and inscrutable functional language known only to the elders as N̸̘̏͑̕͝į̸̈́̂x̸͙̑̅̒.</li></ul><p>In my opinion, none of the alternatives provide the same level of flexibility and tooling support as writing a Dockerfile, nor can they achieve bootc’s level of safety and reliability by making it extremely difficult to bork your  directory. And if the  directory somehow gets borked anyway, rolling back to the previously deployed container is just a reboot away.</p><p>A few weeks ago, an OCI image based on Fedora Xfce Atomic that I made called <a href=\"https://github.com/winblues/blue95\">Blue95</a> was posted to <a href=\"https://news.ycombinator.com/item?id=43524937\">Hacker News</a>. For the most part, the reception was warm but there were some interesting questions that were raised, such as:</p><blockquote><p>Is it really necessary to spin up an entirely new distro for an XFCE+GTK theme?</p></blockquote><p>The original poster made me question the nature of the project I created: is it a distro? In the age of bootc, the distinction between what is considered a Linux distribution and what is simply a Containerfile + CI/CD is, in my opinion, murky at best. Historically, the barrier to entry for creating what has traditionally been called a Linux distribution was orders of magnitude higher than creating and publishing an OCI container. My nights-and-weekends side project of building a bootable container is a far cry from what I imagine a Linux distribution to be.</p><p><a href=\"https://blues.win/95\">Blue95</a> is a collection of scripts and YAML files cobbled together to produce a Containerfile, which is built via GitHub Actions and published to the GitHub Container Registry. Which part of this process elevates the project to the status of a Linux distribution? What set of  commands in the Containerfile take the project from being merely a Fedora-based OCI image to a full-blown Linux distribution?</p><p>Popular bootc-based projects like <a href=\"https://projectbluefin.io\">Project Bluefin</a> and <a href=\"https://bazzite.gg\">Bazzite</a> are often labeled as Linux distributions, much to the consternation of their creators and maintainers. But if you’ve ever used Bazzite and booted directly into Steam’s Big Picture Mode, you might agree that it does indeed feel like its own Linux distribution; it is quite distinct from its twin bases of <a href=\"https://fedoraproject.org/atomic-desktops/silverblue/\">Fedora Silverblue</a> and <a href=\"https://fedoraproject.org/atomic-desktops/kinoite/\">Fedora Kinoite</a>.</p><p>Maybe the imprecision of the term “Linux distribution” is most evident when arguments arise over what is and isn’t a distro. It has always been problematic to define a distribution as simply a curated collection of software plus a Linux kernel -— but that definition is now especially lacking, as it could just as easily describe any Containerfile for a bootable container. Ultimately, “I know it when I see it” may be the best we can do when deciding whether a project deserves the label Linux distribution or not.</p><p>Finally, to address the original question about the necessity of spinning up a new distro just for a theme: creating a bootable container with a consistent visual design and curated set of applications can bring a bit of . At this moment, my laptop is booted from a container that I have created myself. The operating system being used to draft these words is the product of my own artistic and creative expression – built on the work of countless other human beings. And that brings me joy.</p>","contentLength":6660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43743784"},{"title":"Gemma 3 QAT Models: Bringing AI to Consumer GPUs","url":"https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/","date":1745151726,"author":"emrah","guid":203,"unread":true,"content":"<img src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3QuantizedWagtail_RD1_01b.original.png\" alt=\"Gemma 3 Quantization Aware - banner\"><div><p data-block-key=\"sisye\">Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.</p><p data-block-key=\"4impe\">To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.</p></div><div><h2 data-block-key=\"sisye\">Understanding performance, precision, and quantization</h2><p data-block-key=\"2a2jr\">The chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.</p><p data-block-key=\"ae0co\"><b>Why BFloat16 for this comparison?</b> BF16 is a common numerical format used during inference of many large models. It means that the model parameters are represented with 16 bits of precision. Using BF16 for all models helps us to make an apples-to-apples comparison of models in a common inference setup. This allows us to compare the inherent capabilities of the models themselves, removing variables like different hardware or optimization techniques like quantization, which we'll discuss next.</p><p data-block-key=\"3q45i\">It's important to note that while this chart uses BF16 for a fair comparison, deploying the very largest models often involves using lower-precision formats like FP8 as a practical necessity to reduce immense hardware requirements (like the number of GPUs), potentially accepting a performance trade-off for feasibility.</p><h2 data-block-key=\"8r8jc\">The Need for Accessibility</h2><p data-block-key=\"dkjr0\">While top performance on high-end hardware is great for cloud deployments and research, we heard you loud and clear: you want the power of Gemma 3 on the hardware you already own. We're committed to making powerful AI accessible, and that means enabling efficient performance on the consumer-grade GPUs found in desktops, laptops, and even phones.</p><h2 data-block-key=\"950c\">Performance Meets Accessibility with Quantization-Aware Training in Gemma 3</h2><p data-block-key=\"158md\">This is where quantization comes in. In AI models, quantization reduces the precision of the numbers (the model's parameters) it stores and uses to calculate responses. Think of quantization like compressing an image by reducing the number of colors it uses. Instead of using 16 bits per number (BFloat16), we can use fewer bits, like 8 (int8) or even 4 (int4).</p><p data-block-key=\"3a7o0\">Using int4 means each number is represented using only 4 bits – a 4x reduction in data size compared to BF16. Quantization can often lead to performance degradation, so we’re excited to release Gemma 3 models that are robust to quantization. We released several quantized variants for each Gemma 3 model to enable inference with your favorite inference engine, such as Q4_0 (a common quantization format) for Ollama, llama.cpp, and MLX.</p><p data-block-key=\"epsbi\"><b>How do we maintain quality?</b> We use QAT. Instead of just quantizing the model after it's fully trained, QAT incorporates the quantization process during training. QAT simulates low-precision operations during training to allow quantization with less degradation afterwards for smaller, faster models while maintaining accuracy. Diving deeper, we applied QAT on ~5,000 steps using probabilities from the non-quantized checkpoint as targets. We reduce the perplexity drop by 54% (using llama.cpp perplexity evaluation) when quantizing down to Q4_0.</p><h2 data-block-key=\"eatdo\">See the Difference: Massive VRAM Savings</h2><p data-block-key=\"9f9so\">The impact of int4 quantization is dramatic. Look at the VRAM (GPU memory) required just to load the model weights:</p><ul><li data-block-key=\"41cf1\"> Drops from 54 GB (BF16) to just  (int4)</li></ul><ul><li data-block-key=\"5uebr\"> Shrinks from 24 GB (BF16) to only  (int4)</li></ul><ul><li data-block-key=\"9d985\"> Reduces from 8 GB (BF16) to a lean  (int4)</li></ul><ul><li data-block-key=\"bf7p\"> Goes from 2 GB (BF16) down to a tiny  (int4)</li></ul></div><div><blockquote data-block-key=\"7ui9z\"><i><sup>This figure only represents the VRAM required to load the model weights. Running the model also requires additional VRAM for the KV cache, which stores information about the ongoing conversation and depends on the context length</sup></i></blockquote><h2 data-block-key=\"e1hmg\">Run Gemma 3 on Your Device</h2><p data-block-key=\"4jdc3\">These dramatic reductions unlock the ability to run larger, powerful models on widely available consumer hardware:</p><ul><li data-block-key=\"bqsl5\"> Now fits comfortably on a single desktop NVIDIA RTX 3090 (24GB VRAM) or similar card, allowing you to run our largest Gemma 3 variant locally.</li></ul><ul><li data-block-key=\"c4adc\"> Runs efficiently on laptop GPUs like the NVIDIA RTX 4060 Laptop GPU (8GB VRAM), bringing powerful AI capabilities to portable machines.</li></ul><ul><li data-block-key=\"be3pr\"> Offer even greater accessibility for systems with more constrained resources, including phones and <a href=\"https://youtu.be/lgsD_wSZ0hI?si=pyQj23bOxNPLrxtL&amp;t=102\">toasters</a> (if you have a good one).</li></ul><h2 data-block-key=\"7c8ds\">Easy Integration with Popular Tools</h2><p data-block-key=\"2324e\">We want you to be able to use these models easily within your preferred workflow. Our official int4 and Q4_0 unquantized QAT models are available on Hugging Face and Kaggle. We’ve partnered with popular developer tools that enable seamlessly trying out the QAT-based quantized checkpoints:</p><ul><li data-block-key=\"avkt\"><a href=\"https://ollama.com/library/gemma3\"></a> Get running quickly – all our Gemma 3 QAT models are natively supported starting today with a simple command.</li></ul><ul><li data-block-key=\"6sp3o\"><a href=\"https://lmstudio.ai/model/gemma-3-12b-it-qat\"></a> Easily download and run Gemma 3 QAT models on your desktop via its user-friendly interface.</li></ul><ul><li data-block-key=\"69oug\"><a href=\"https://huggingface.co/collections/mlx-community/gemma-3-qat-68002674cd5afc6f9022a0ae\"></a> Leverage MLX for efficient, optimized inference of Gemma 3 QAT models on Apple Silicon.</li></ul><ul><li data-block-key=\"19s6q\"><a href=\"https://www.kaggle.com/models/google/gemma-3/gemmaCpp\"></a> Use our dedicated C++ implementation for highly efficient inference directly on the CPU.</li></ul><ul><li data-block-key=\"f05gl\"><a href=\"https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b\"></a> Integrate easily into existing workflows thanks to native support for our GGUF-formatted QAT models.</li></ul><h2 data-block-key=\"c5073\">More Quantizations in the Gemmaverse</h2><p data-block-key=\"74vnv\">Our official Quantization Aware Trained (QAT) models provide a high-quality baseline, but the vibrant <a href=\"https://ai.google.dev/gemma/gemmaverse\">Gemmaverse</a> offers many alternatives. These often use Post-Training Quantization (PTQ), with significant contributions from members such as <a href=\"https://huggingface.co/bartowski/google_gemma-3-27b-it-GGUF\">Bartowski</a>, <a href=\"https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b\">Unsloth</a>, and <a href=\"https://huggingface.co/collections/ggml-org/gemma-3-67d126315ac810df1ad9e913\">GGML</a> readily available on Hugging Face. Exploring these community options provides a wider spectrum of size, speed, and quality trade-offs to fit specific needs.</p><p data-block-key=\"fdu54\">Bringing state-of-the-art AI performance to accessible hardware is a key step in democratizing AI development. With Gemma 3 models, optimized through QAT, you can now leverage cutting-edge capabilities on your own desktop or laptop.</p><p data-block-key=\"179dk\">Explore the quantized models and start building:</p><p data-block-key=\"4j3l0\">We can't wait to see what you build with Gemma 3 running locally!</p></div>","contentLength":6308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43743337"},{"title":"100 Years to Solve an Integral (2020)","url":"https://liorsinai.github.io/mathematics/2020/08/27/secant-mercator.html","date":1745119002,"author":"blobcode","guid":202,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43741273"},{"title":"Show HN: I built an AI that turns GitHub codebases into easy tutorials","url":"https://github.com/The-Pocket/Tutorial-Codebase-Knowledge","date":1745096681,"author":"zh2408","guid":198,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43739456"},{"title":"Healthy soil is the hidden ingredient","url":"https://www.nature.com/articles/d41586-025-01026-x","date":1744838035,"author":"gnabgib","guid":199,"unread":true,"content":"<figure><picture><img alt=\"Vines are seen across the centre of the photo. Two people are picking grapes from them. Trees in the background and the sun is low in the sky.\" loading=\"lazy\" src=\"//media.nature.com/lw767/magazine-assets/d41586-025-01026-x/d41586-025-01026-x_50846144.jpg\"><figcaption></figcaption></picture></figure><p>When it comes to the land, the European Union faces a reckoning: 60–70% of its soils are considered unhealthy, resulting in soil degradation. That costs €50 billion (US$55 billion) annually because of the loss of essential services and the effect on human health. Spain has the highest rates of soil degradation in Europe, with an average of 14.2 tonnes of soil lost per hectare to erosion each year. Although its importance is often overlooked, quality soil is essential for sustaining agriculture and crucial ecosystems. Its importance was highlighted in the 2015 United Nations Sustainable Development Goals, with soil health named as a key factor in achieving several of the goals.</p><p>Having studied soil erosion in vineyards across Europe, Jesús Rodrigo Comino, a geographer at the University of Granada, Spain, is hoping to unearth a solution. Now he works with farmers, researchers and policymakers to test and use more-sustainable soil-management practices in vineyards and more broadly. Rodrigo Comino speaks to  about his research programme and why healthy soils are important for Spain’s cultural heritage and economy.</p><h2>Tell us about your work in soil science.</h2><p>I was always a curious kid, but my passion for soil science began with my studies at the University of Malaga in Spain and at Trier University in Germany. My home region of Andalusia in Spain has some really beautiful landscapes, but some areas are heavily degraded because of soil erosion and other factors. I felt compelled to find solutions to these problems. </p><p>When I came back to Spain in 2021 after doing my PhD and a postdoctoral position in Germany, I started a project to assess soil erosion and degradation in vineyards using geographical mapping systems and artificial intelligence (AI). Soil erosion is caused by natural factors, such as extreme rainfall events and strong winds, as well as by human-driven activities, including tillage and urbanization. AI helps me to design and polish the software codes that I use to create geographical maps and analyse data. The main idea is to develop tools to help farmers make decisions. These might be about how to apply irrigation systems; which types of cover crop to plant; or when to till the soil, if necessary, to prevent soil degradation.</p><figure><picture><img alt=\"At left of centre, a person crouches down on red earth holding a measuring square in one hand, and a plastic bag in the other. Vines with red leaves are visible in the background, as are two people sitting on blue stools on the ground.\" loading=\"lazy\" src=\"//media.nature.com/lw767/magazine-assets/d41586-025-01026-x/d41586-025-01026-x_50846162.jpg\"><figcaption></figcaption></picture></figure><p>I’m also involved in a Soil Living Labs project called SOILCRATES, which is part of an EU mission called A Soil Deal for Europe. The goal is to create field sites where soil scientists, agronomists, stakeholders and policymakers come together to conduct experiments and establish plots with sustainable soils. As part of the project, we also aim to show people how important fertile soils are to their lives and how their actions can affect soil health. The EU is now investing in a variety of projects to improve soil quality and enhance people’s understanding of soil. I’ve seen people implementing good practices to reduce soil erosion. But we’re not there yet — much more work is needed. </p><h2>How has climate change affected your work?</h2><p>It is evident that many places, especially urban ones, are experiencing an increase in temperature and a reduction in precipitation volume. We’re also seeing a concentration of extreme rainfall events in short periods of time, such as the flooding that happened in Valencia in Spain last year. All of these factors can affect soil health.</p><p>When it comes to the vineyards in Spain, we don’t yet have enough data to confirm whether and how climate change is altering them — but the trends are clear. We know that vineyards are encountering higher temperatures now than they did in the past, and that this is causing the vines to flower earlier. Annual rainfall is being concentrated into shorter time periods, and we’re also experiencing extreme rainfall events, which cause soil erosion.</p><h2>What are farmers telling you about their experiences?</h2>","contentLength":3859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43710451"}],"tags":["dev","hn"]}