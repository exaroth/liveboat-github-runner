{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":14,"items":[{"title":"Computer-generated dream world: Virtual reality for a 286 processor","url":"https://deadlime.hu/en/2026/02/22/computer-generated-dream-world/","date":1772425433,"author":"MBCook","guid":136,"unread":true,"content":"<blockquote><p>What is \"real\"? How do you define \"real\"? If you're talking about what you can feel, what you can smell, taste, and see... then \"real\" is simply electrical signals interpreted by your brain.</p></blockquote><p>If the processor is the brain of the computer, could it also be part of some kind of virtual reality? Simulated memory, software-defined peripherals, artificially generated interrupts.</p><p>My first computer was a 286 with 1 MB of RAM and a 50 MB HDD (if I remember correctly). So I decided to pick up a 286 processor and try to simulate the rest of the computer around it. Or at least make it to boot up and run some simple assembly code.</p><p>Two years ago, I ordered two (that's how many came in a package) Harris 80C286-12 processors. My memories are a bit hazy, but I believe the  in its name is important because these are the types that are less sensitive to clock accuracy (the  at the end means it likes to run at 12 MHz), and can even be stepped manually.</p><p>At first, I wasn't too successful with it, and the project ended up in a drawer. Then this year, I picked it up again and tried to figure out where things went wrong.</p><p>The processor fits into a PLCC-68 socket. The pins of the socket are not suitable for plugging in jumper wires directly, so the socket was mounted onto an adapter PCB with jumper-compatible headers. The pinout of both the chip and the socket is included in <a href=\"https://deadlime.hu/uploads/2026/80C286_datasheet.pdf\">the datasheet</a>, but the adapter PCB complicates things a bit, so I created a small conversion table to make my life easier.</p><p>The table also helped identify the various inputs and outputs, which would later be useful when connecting to the Raspberry Pi. As you can see, no fewer than 57 pins are required, which is more than the Pi can provide. The MCP23S17 IO expander came to the rescue. While it wouldn't allow us to drive the processor at the breakneck speed of the supported 12 MHz, fortunately, that's not our goal.</p><p>The chip contains 16 IO pins, so we'll need four of them. Although each pin can individually be configured as input or output, I tried to group them logically. The expander has side A and side B, each with 8 pins, and the final result looked like this:</p><pre><code>         ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê      \n         ‚î§   ‚îî‚îÄ‚îÄ‚îò   ‚îú      \n         ‚î§          ‚îú      \n         ‚î§   FLAG   ‚îú ERROR\n         ‚î§          ‚îú BUSY \n         ‚î§ ADDR:100 ‚îú INTR \n   READY ‚î§          ‚îú NMI  \n   RESET ‚î§B        A‚îú PEREQ\n     CLK ‚î§          ‚îú HOLD \n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      \n         ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê      \n    HLDA ‚î§   ‚îî‚îÄ‚îÄ‚îò   ‚îú A23  \nCOD/INTA ‚î§          ‚îú A22  \n    M/IO ‚î§   MISC   ‚îú A21  \n    LOCK ‚î§          ‚îú A20  \n     BHE ‚î§ ADDR:011 ‚îú A19  \n      S1 ‚î§          ‚îú A18  \n      S0 ‚î§B        A‚îú A17  \n   PEACK ‚î§          ‚îú A16  \n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      \n         ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê      \n      A8 ‚î§   ‚îî‚îÄ‚îÄ‚îò   ‚îú A7   \n      A9 ‚î§          ‚îú A6   \n     A10 ‚î§   ADDR   ‚îú A5   \n     A11 ‚î§          ‚îú A4   \n     A12 ‚î§ ADDR:010 ‚îú A3   \n     A13 ‚î§          ‚îú A2   \n     A14 ‚î§B        A‚îú A1   \n     A15 ‚î§          ‚îú A0   \n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      \n         ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê      \n      D8 ‚î§   ‚îî‚îÄ‚îÄ‚îò   ‚îú D7   \n      D9 ‚î§          ‚îú D6   \n     D10 ‚î§   DATA   ‚îú D5   \n     D11 ‚î§          ‚îú D4   \n     D12 ‚î§ ADDR:001 ‚îú D3   \n     D13 ‚î§          ‚îú D2   \n     D14 ‚î§B        A‚îú D1   \n     D15 ‚î§          ‚îú D0   \n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      \n</code></pre><p>The Pi communicates with the expanders over SPI. Several solutions exist for this. I chose the one where all chips are active simultaneously, and the Pi is sending them messages by their hardware address.</p><p>The RESET pin (wired with the purple cable) does not need to be controlled by the Pi in this case, but during one of the debugging sessions, I tried it in the hopes that it would help, and it remained that way. Now we just need to connect everything with a truckload of jumper wires, and we could move on to programming.</p><p>We only need a relatively small portion of the MCP23S17‚Äôs capabilities. We just have to configure the direction of the IO pins and read/write the relevant registers. Configuration is done by modifying register values. First, we need to enable the use of hardware addressing. By default, all chips have the address , so if we send a register modification to that address (setting the  bit in the  register), hardware addressing will be enabled simultaneously on all four chips.</p><p>After a few hours (days) of head-scratching, it turned out that this alone is not necessarily sufficient for proper operation. We also need to send the same message to the configured hardware address itself to enable hardware addressing (rather odd, I know). So if, for example, we set the hardware address to , we must resend the original register modification message previously sent to  to  as well.</p><p>Now that hardware addressing is sorted out, we need to set the  and  registers of each chip to the appropriate direction. Because of our grouping, we can configure an entire side at once for reading () or writing (). Further details can be found in <a href=\"https://deadlime.hu/uploads/2026/MCP23017_datasheet.pdf\">the chip's datasheet</a>.</p><p>Originally, I started working with a Pi Zero, but eventually settled on a Pi Pico running MicroPython. To manage the expander chips, I created the following small class:</p><pre><code>\n    IODIRA = \n    IODIRB = \n    IOCON = \n    GPIOA = \n    GPIOB = \n        self.__address = address\n        self.__spi = spi\n        self.__cs = cs\n\n    \n        self.__writeRegister(, self.IOCON, )\n        self.writeRegister(self.IOCON, )\n\n    \n        self.__writeRegister(self.__address, reg, value)\n\n    \n        tx = bytearray([self.__address | , reg, ])\n        rx = bytearray()\n        self.__cs.value()\n        self.__spi.write_readinto(tx, rx)\n        self.__cs.value()\n         rx[]\n\n    \n        self.__cs.value()\n        self.__spi.write(bytes([address, reg, value]))\n        self.__cs.value()\n</code></pre><p>In , you can clearly see that we set the value of the  register twice. We can use the class as follows to communicate with the processor:</p><pre><code>spi = SPI(, baudrate=, sck=Pin(), mosi=Pin(), miso=Pin())\ncs = Pin(, mode=Pin.OUT, value=)\nrst = Pin(, mode=Pin.OUT, value=)\n\nchip_data = MCP23S17(, spi, cs)\nchip_addr = MCP23S17(, spi, cs)\nchip_misc = MCP23S17(, spi, cs)\nchip_flag = MCP23S17(, spi, cs)\n\nrst.value()\n\nchip_data.init()\nchip_addr.init()\nchip_misc.init()\nchip_flag.init()\n\nchip_data.writeRegister(MCP23S17.IODIRA, )\nchip_data.writeRegister(MCP23S17.IODIRB, )\n\nchip_addr.writeRegister(MCP23S17.IODIRA, )\nchip_addr.writeRegister(MCP23S17.IODIRB, )\n\nchip_misc.writeRegister(MCP23S17.IODIRA, )\nchip_misc.writeRegister(MCP23S17.IODIRB, )\n\nchip_flag.writeRegister(MCP23S17.IODIRA, )\nchip_flag.writeRegister(MCP23S17.IODIRB, )\n</code></pre><p>At first, I missed the  calls here and was surprised when nothing worked. Most of the pins are configured for reading; only the flags need to be set to writing.</p><p>Before we can do anything, we need to RESET the processor. For this, the RESET flag must be held active for at least 16 clock cycles, and switching it on and off must be synchronized with the clock flag. First, I created a few constants for the flags to make life easier:</p><pre><code>\nFLAG_ERROR = \nFLAG_BUSY  = \nFLAG_INTR  = \nFLAG_NMI   = \nFLAG_PEREQ = \nFLAG_HOLD  = \nFLAG_CLK   = \nFLAG_RESET = \nFLAG_READY = \nFLAG_PEACK    = \nFLAG_S0       = \nFLAG_S1       = \nFLAG_BHE      = \nFLAG_LOCK     = \nFLAG_M_IO     = \nFLAG_COD_INTA = \nFLAG_HLDA     = </code></pre><p>It's worth comparing this with the earlier MCP23S17 pin mapping. We treat each group of 8 pins as 8 bits / 1 byte of data. For example, in the byte from the 'misc' chip's  side, the  flag is the least significant bit, while  is the most significant.</p><pre><code>PEACK\n‚Üì\n10100111\n       ‚Üë\n    HLDA\n</code></pre><p>With the flags in place, we can perform the RESET:</p><pre><code> i  range():\n    chip_flag.writeRegister(MCP23S17.GPIOB, FLAG_CLK | FLAG_RESET)\n    time.sleep()\n    chip_flag.writeRegister(MCP23S17.GPIOB, FLAG_RESET)\n    time.sleep()\n</code></pre><p>The sleep intervals were chosen more or less arbitrarily; we don't have to adhere to any strict timing. During RESET, the processor must enter a defined state. We can verify this with the following piece of code:</p><pre><code>data = chip_addr.readRegister(MCP23S17.GPIOA)\nprint( + str(bin(data)))\ndata = chip_addr.readRegister(MCP23S17.GPIOB)\nprint( + str(bin(data)))\ndata = chip_misc.readRegister(MCP23S17.GPIOA)\nprint( + str(bin(data)))\ndata = chip_misc.readRegister(MCP23S17.GPIOB)\nprint( + str(bin(data)))\n</code></pre><p>The values we expect to see look like this:</p><pre><code>A7-0:   0b11111111\nA15-8:  0b11111111\nA23-16: 0b11111111\nPEACK, S0, S1, BHE, LOCK, M/IO, COD/INTA, HLDA: 0b11111000\n</code></pre><p>Strangely enough, I was greeted with the following instead:</p><pre><code>A7-0:   0b11111111\nA15-8:  0b11111000\nA23-16: 0b11111111\nPEACK, S0, S1, BHE, LOCK, M/IO, COD/INTA, HLDA: 0b11111000\n</code></pre><p>It was hard not to notice that the values in the second and fourth lines were identical. I checked all the connections, disassembled everything, debugged with LEDs to ensure the values I wrote were going to the right places, replaced the chip assigned to the A15-8 pins, swapped the processor for the spare, reread the code a thousand times, but nothing helped.</p><p>Then I found that hardware addressing trick mentioned earlier with the MCP23S17, and everything started to work like magic. The point is, if everything went well, we can release the RESET flag, and the boot process can begin.</p><pre><code>chip_flag.writeRegister(MCP23S17.GPIOB, FLAG_CLK | FLAG_RESET)\ntime.sleep()\nchip_flag.writeRegister(MCP23S17.GPIOB, )\ntime.sleep()\n</code></pre><p>After this, within 50 clock cycles, the processor must begin to read the first instruction to execute from address . The , , , and  flags determine what the processor intends to do.</p><table><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>I left out the less interesting ones from the table; they can be viewed in <a href=\"https://deadlime.hu/uploads/2026/80C286_datasheet.pdf\">the datasheet</a>. For our small test, we'll only need these four:</p><ul></ul><p>So we start sending clock signals and wait until we reach the first 'Memory instruction read':</p><pre><code>cycle = :\n    print()\n    chip_flag.writeRegister(MCP23S17.GPIOB, FLAG_CLK)\n    time.sleep()\n    chip_flag.writeRegister(MCP23S17.GPIOB, )\n    time.sleep()\n\n    data = chip_misc.readRegister(MCP23S17.GPIOB)\n    PEACK = data &amp; FLAG_PEACK\n    S0 = data &amp; FLAG_S0\n    S1 = data &amp; FLAG_S1\n    BHE = data &amp; FLAG_BHE\n    LOCK = data &amp; FLAG_LOCK\n    M_IO = data &amp; FLAG_M_IO\n    COD_INTA = data &amp; FLAG_COD_INTA\n    HLDA = data &amp; FLAG_HLDA\n\n     COD_INTA  M_IO  S1  S0:\n        print()\n        sys.exit()\n     COD_INTA  M_IO  S1  S0:\n        print()\n     COD_INTA  M_IO  S1  S0:\n        print()\n     COD_INTA  M_IO  S1  S0:\n        print()\n\n    time.sleep()\n    cycle += </code></pre><p>When we arrive successfully, we can start sending, say, NOP () instructions. We set the data bus to write mode, put the NOP instruction on it, send a clock signal, then set the data bus back to read mode.</p><pre><code>chip_data.writeRegister(MCP23S17.IODIRA, )\nchip_data.writeRegister(MCP23S17.IODIRB, )\nchip_data.writeRegister(MCP23S17.GPIOA, )\nchip_data.writeRegister(MCP23S17.GPIOB, )\n\nchip_flag.writeRegister(MCP23S17.GPIOB, FLAG_CLK)\ntime.sleep()\nchip_flag.writeRegister(MCP23S17.GPIOB, )\ntime.sleep()\n\nchip_data.writeRegister(MCP23S17.IODIRA, )\nchip_data.writeRegister(MCP23S17.IODIRB, )\n</code></pre><h3>Complex Mathematical Operations</h3><p>That's all well and good, but let's look at something more interesting. Something that requires both reading and writing memory. A simple little program that reads two numbers from memory, adds them, and writes the result back to memory.</p><p>Since we start very close to the end of memory (), we don't have much room, so first we need to jump elsewhere.</p><pre><code>[cpu ]\n:</code></pre><pre><code>[cpu ]\n  ax, ax\n ds, ax\n\n ax, [num1]\n ax, [num2]\n [result], ax\n\n    dw     dw   dw </code></pre><p>Using the  program, we can also generate a binary from it:</p><pre><code></code></pre><p>Then, with a short Python script, we can convert it into a Python-friendly format so we can load it into our virtual memory:</p><pre><code> sys\n\n open(sys.argv[], )  f:\n    data = f.read()\nhex_values = .join( byte  data)\nprint()\n</code></pre><pre><code>\n[0xea, 0x00, 0x05, 0x00, 0x00]\n\n[0x31, 0xc0, 0x8e, 0xd8, 0xa1, 0x0f, 0x05, 0x03, 0x06, 0x11, 0x05, 0xa3, 0x13, 0x05, 0xf4, 0x34, 0x12, 0x0a, 0x00, 0x00, 0x00]\n</code></pre><p>To simulate memory, I put together the following small class:</p><pre><code>\n        self.__data = {}\n\n     i, b  enumerate(data):\n            self.__data[base + i] = b\n\n     self.__data.get(address, )\n\n    \n        self.__data[address] = value &amp; </code></pre><p>It's just a simple dict with a helper function that allows us to load data into arbitrary addresses. Which we then do with the code generated by :</p><pre><code>MEMORY = Memory()\nMEMORY.load(, [\n    , ,\n    , ,\n    , , ,\n    , , , ,\n    , , ,\n    ,\n    , ,\n    , ,\n    , \n])\nMEMORY.load(, [\n    , , , , \n])\n</code></pre><p>All that remains is to handle the cases. But first, we need to talk about the  flag and the  pin.</p><table><tbody><tr><td>Byte transfer on upper half of data bus ( - )</td></tr><tr><td>Byte transfer on lower half of data bus ( - )</td></tr></tbody></table><p>So during an operation involving the data bus, we can read/write the entire data bus, its upper half, or its lower half.</p><p>In our case, 'Memory data read' is very similar to 'Memory instruction read', so we can handle both with the same code. We just need to handle the flags mentioned above and use the fake memory.</p><pre><code>address = (a3 &lt;&lt; ) + (a2 &lt;&lt; ) + a1\n COD_INTA  M_IO  S1  S0:\n    print(.format(address))\n:\n    print(.format(address))\n\n BHE  A0:\n    print(.format(MEMORY[address + ], MEMORY[address]))\n    chip_data.writeRegister(MCP23S17.IODIRA, )\n    chip_data.writeRegister(MCP23S17.IODIRB, )\n    chip_data.writeRegister(MCP23S17.GPIOA, MEMORY[address])\n    chip_data.writeRegister(MCP23S17.GPIOB, MEMORY[address + ])\n BHE  A0:\n    print(.format(MEMORY[address]))\n    chip_data.writeRegister(MCP23S17.IODIRB, )\n    chip_data.writeRegister(MCP23S17.GPIOB, MEMORY[address])\n BHE  A0:\n    print(.format(MEMORY[address]))\n    chip_data.writeRegister(MCP23S17.IODIRA, )\n    chip_data.writeRegister(MCP23S17.GPIOA, MEMORY[address])\n\nchip_flag.writeRegister(MCP23S17.GPIOB, FLAG_CLK)\ntime.sleep()\nchip_flag.writeRegister(MCP23S17.GPIOB, )\ntime.sleep()\n\nchip_data.writeRegister(MCP23S17.IODIRA, )\nchip_data.writeRegister(MCP23S17.IODIRB, )\n</code></pre><p>It‚Äôs not much more complicated than our original NOP-based solution, but there is an extra twist here that‚Äôs easy to stumble over. In what order should we place the bytes onto the data bus? The  register represents the least significant byte of the data bus, while  represents the most significant. So, for example, our initial JMP instruction () will travel as  (<a href=\"https://en.wikipedia.org/wiki/Endianness\">little-endian</a>).</p><p>It‚Äôs worth scrolling back a bit and noticing that  already performed similar swaps. For instance, our  value used for the addition is stored in memory as .</p><p>'Memory data write' is very straightforward; we simply use the fake memory:</p><pre><code>address = (a3 &lt;&lt; ) + (a2 &lt;&lt; ) + a1\nprint(.format(address))\n\n BHE  A0:\n    print(.format(d2, d1))\n    MEMORY[address] = d1\n    MEMORY[address + ] = d2\n BHE  A0:\n    print(.format(d2))\n    MEMORY[address] = d2\n BHE  A0:\n    print(.format(d1))\n    MEMORY[address] = d1\n</code></pre><p>The little-endian order can also be observed here, although during execution, I didn't encounter a case where it attempted to write two bytes to memory at once.</p><p>And during 'halt / shutdown', we simply print the result of the addition from memory and exit:</p><pre><code>print(.format((MEMORY[] &lt;&lt; ) + MEMORY[]))\nsys.exit()\n</code></pre><p>In the end, running the program should produce output similar to this, where you can see it reading the initial JMP instruction, jumping to the new address, continuing to read instructions from there, reading the two numbers to be added from memory, and finally writing the result back to memory:</p><pre><code>RESET\nA7-0:   0b11111111\nA15-8:  0b11111111\nA23-16: 0b11111111\nPEACK, S0, S1, BHE, LOCK, M/IO, COD/INTA, HLDA: 0b11111000\nSTART\n#40\nMemory instruction read 0xFFFFF0\nWord transfer 0x00EA\n#43\nMemory instruction read 0xFFFFF2\nWord transfer 0x0005\n#46\nMemory instruction read 0xFFFFF4\nWord transfer 0x0000\n#49\nMemory instruction read 0xFFFFF6\nWord transfer 0x0000\n#52\nMemory instruction read 0xFFFFF8\nWord transfer 0x0000\n#67\nMemory instruction read 0x000500\nWord transfer 0xC031\n#70\nMemory instruction read 0x000502\nWord transfer 0xD88E\n#73\nMemory instruction read 0x000504\nWord transfer 0x0FA1\n#76\nMemory instruction read 0x000506\nWord transfer 0x0305\n#79\nMemory instruction read 0x000508\nWord transfer 0x1106\n#82\nMemory instruction read 0x00050A\nWord transfer 0xA305\n#85\nMemory instruction read 0x00050C\nWord transfer 0x0513\n#88\nMemory data read 0x00050F\nByte transfer on upper half of data bus 0x34\n#91\nMemory data read 0x000510\nByte transfer on lower half of data bus 0x12\n#94\nMemory instruction read 0x00050E\nWord transfer 0x34F4\n#99\nMemory data read 0x000511\nByte transfer on upper half of data bus 0x0A\n#102\nMemory data read 0x000512\nByte transfer on lower half of data bus 0x00\n#115\nMemory data write 0x000513\nByte transfer on upper half of data bus 0x0A\n#116\nMemory data write 0x000513\nByte transfer on upper half of data bus 0x3E\n#119\nMemory data write 0x000514\nByte transfer on lower half of data bus 0x12\n#120\nMemory data write 0x000514\nByte transfer on lower half of data bus 0x12\n#123\nhalt\nResult: 0x123E\n</code></pre><p>It was a tremendous joy to see the correct final result at the end of execution for the first time. I think I've reached a milestone where I can stop and take a rest for now.</p><p>Of course, we've only scratched the surface; there's still a great deal left to learn. It's worth going through <a href=\"https://deadlime.hu/uploads/2026/80C286_datasheet.pdf\">the processor's datasheet</a>, or perhaps thinking about how various peripherals (such as a keyboard or a text display) are actually implemented.</p><p>What is certain, however, is that for the processor, this reality is not virtual at all. It doesn't matter to it where the electrical signals are coming from, as long as they are compatible with its own internal reality.</p>","contentLength":17771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47213866"},{"title":"Everett shuts down Flock camera network after judge rules footage public record","url":"https://www.wltx.com/article/news/nation-world/281-53d8693e-77a4-42ad-86e4-3426a30d25ae","date":1772424360,"author":"aranaur","guid":135,"unread":true,"content":"<div><p>EVERETT, Wash. ‚Äî The City of Everett has shut down its entire network of <a href=\"https://www.wltx.com/article/news/local/senate-vote-regulations-license-plate-reader-cameras/281-0c1dbe26-ed85-4a33-8410-7ce436e3f11f\" target=\"_blank\" rel=\"noopener noreferrer\">Flock license plate reader</a> cameras after a Snohomish County judge ruled the footage those cameras collect qualifies as a public record.</p></div><div><p>The decision came after a Washington man filed public records requests seeking access to data captured by the cameras.</p></div><div><p>Jose Rodriguez of Walla Walla, represented by attorney Tim Hall, requested the footage from multiple jurisdictions in Washington state, to see what information the automated license plate reader system was collecting.</p></div><div><p>‚ÄúHe started noticing that the cameras were everywhere ‚Äî he wanted to see what kind of data they collect,‚Äù Hall said.</p></div><div><p>The requests revealed that Flock cameras continuously capture thousands of images, regardless of whether a vehicle is linked to a crime.</p></div><div><p>When several cities, including Everett, moved to block the request, the case went to court.</p></div><div><p>On Tuesday, a Snohomish County judge ruled that footage captured by Flock cameras qualifies as a public record under Washington law, meaning members of the public can request access to the data.</p></div><div><p>Everett Mayor Cassie Franklin said the city disagrees with the ruling and is concerned about who could obtain the footage.</p></div><div><p>‚ÄúWe were very disappointed,‚Äù Franklin said. ‚ÄúThat means perpetrators of crime, people who are maybe engaged in domestic abuse or stalkers, they can request footage and that could cause a lot of harm.‚Äù</p></div><div><p>Following the ruling, Everett temporarily turned off all 68 of its Flock cameras.</p></div><div><p>At the same time, lawmakers in Olympia are debating a bill that would exempt Flock footage from public records law.</p></div><div><p>Supporters of the proposed legislation argue that public access to the data could create safety risks, including the possibility that federal immigration agents could attempt to obtain footage through public disclosure requests.</p></div><div><p>Hall pushed back on those concerns, saying public records requests are typically a lengthy process and unlikely to be useful for real-time tracking.</p></div><div><p>‚ÄúAs somebody who has made hundreds of public records requests myself, and represented many, many people in public records lawsuits, it‚Äôs generally a lengthy process,‚Äù Hall said. ‚ÄúSame would be true for ICE. They‚Äôre going to get data from where you were three months, two months ago.‚Äù</p></div><div><p>Franklin said if lawmakers pass legislation allowing cities to shield Flock data from public disclosure, Everett would consider turning the cameras back on. She said the city is not dismantling or removing the cameras in the meantime.</p></div><div><p>‚ÄúShould we get a fix in Olympia that allows us to protect the data from public disclosure, then we can make the decision to turn them back on,‚Äù Franklin said.</p></div><div><p>For now, Everett‚Äôs Flock camera network remains offline, as the debate over transparency, privacy and public safety continues in the Legislature. The bill in Olympia that would put guidelines on Flock's data has passed in the Senate.</p></div>","contentLength":2916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47213764"},{"title":"Show HN: Timber ‚Äì Ollama for classical ML models, 336x faster than Python","url":"https://github.com/kossisoroyce/timber","date":1772413060,"author":"kossisoroyce","guid":118,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47212576"},{"title":"If AI writes code, should the session be part of the commit?","url":"https://github.com/mandel-macaque/memento","date":1772411272,"author":"mandel_x","guid":134,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47212355"},{"title":"Right-sizes LLM models to your system's RAM, CPU, and GPU","url":"https://github.com/AlexsJones/llmfit","date":1772406916,"author":"bilsbie","guid":133,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47211830"},{"title":"Little Free Library","url":"https://littlefreelibrary.org/","date":1772403490,"author":"TigerUniversity","guid":132,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47211280"},{"title":"Why does C have the best file API","url":"https://maurycyz.com/misc/c_files/","date":1772393104,"author":"maurycyz","guid":130,"unread":true,"content":" ‚Äî  (<a href=\"https://maurycyz.com/tags/programming/\">Programming</a>) (<a href=\"https://maurycyz.com/tags/rants/\">Rants</a>) \n<p>\n\nOk, the title is a bit tongue-in-cheek, but there's very little thought put into files in most languages. \nIt always feels a bit out of place... except in C.\nIn fact, what you get is usually a worse version of C.\n</p><p>\nIn C, files can be accessed in the same way as memory:\n</p><pre>() {\n\t =  * ();\n\t = (,  | , );\n\t(, );\n\n\t*  = (, , \n\t\t | , ,\n\t\t, );\n\n\t(, []);\n\t[] = [] + ;\n\n\t(, );\n\t();\n}\n</pre><p>\nMemory mapping isn't the same as loading a file into memory:\nIt still works if the file doesn't fit in RAM.\nData is loaded as needed, so it won't take all day to open a terabyte file.\n</p><p>\nIt works with all datatypes and is automatically cached.\nThis cache is cleared if the system needs memory for something else.\n</p><p><em>mmap() is actually a OS feature</em>, so many other languages have it.\nHowever, it's almost always limited to byte arrays:\nYou have to grab a chunk of data, parse, process and finally serialize it before writing back to the disk.\nIt's nicer then manually calling read() and write(), but not by much.\n</p><p>\nThese languages have all these nice features for manipulating data in memory, but nothing for manipulating data on disk. \nIn memory, you get dynamically sized strings and vectors, enumerated types, objects, etc, etc.\nOn disk, you get... a bunch of bytes. \n</p><p>\nConsidering that most already support custom allocators and the such, adding a better way to access files seems very doable ‚Äî\nbut (as far as I'm aware) C is the only language that lets you specify a binary format and just use it.\n</p><p>\nC's implementation isn't even very good:\nMemory mapping comes some overhead (page faults, TLB flushes) and C does nothing to handle endianness or errors...\nbut it doesn't take much to beat nothing. \n</p><p><em>Sure, you might want to do some parsing and validation</em>, but it shouldn't be required every time data leaves the disk. \nRAM is much smaller then the disk, so it's often impossible to just parse everything into memory.\nBeing able to easily offload data without complicating the code is very useful.\n</p><p>\nJust look at Python's pickle:\nit's a completely insecure serialization format.\nLoading a file can cause code execution even if you just wanted some numbers...\nbut still very widely used because it fits the mix-code-and-data model of python.\n</p><p>\nA lot of files are not untrusted data. \n</p><p>\nIn the case of binary files, parsing is usually redundant. \nThere's no reason code can't directly manipulate the on-disk representation, and for \"scratchpad\" temporary files, save the data as it exists in RAM.\nSure, you wouldn't want to directly manipulate JSON, but there's no reason to do a bunch of work to save some integers.\n</p><p> is similarly neglected. \nThe filesystem is the original NoSQL database, but you seldom get more then a wrapper around C's readdir().\n</p><p>\nThis usually results in people running another database, such as SQLite, on top of the filesystem,\nbut relational databases never quite fit your program. \n</p><p>\n... and SQL integrates even worse than files:\nOn top of having to serialize all your data, you have to write code in a whole separate language just to access it!\n</p><p>\nMost programmers will use it as a key-value store, and implement their own indexing:\ncreating a bizarre triple nested database.\n</p><p>\nI think it's a result of a bad assumption:\nThat data being read from a file is coming from somewhere else and needs to be parsed...\nand that data being written to disk is being sent somewhere and needs to be serialized into a standard format. \n</p><p>\nThis simply isn't true on memory constrained systems ‚Äî\nand with 100 GB files ‚Äî \nevery system is memory constrained.\n</p>","contentLength":3561,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47209788"},{"title":"Why XML tags are so fundamental to Claude","url":"https://glthr.com/XML-fundamental-to-Claude","date":1772376742,"author":"glth","guid":128,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47207236"},{"title":"Ape Coding [fiction]","url":"https://rsaksida.com/blog/ape-coding/","date":1772374025,"author":"rmsaksida","guid":127,"unread":true,"content":"<p> is a software development practice where a human developer deliberately hand-writes source code. Practitioners of ape coding will typically author code by typing it on a computer keyboard, using specifically designed text editing software.</p><p>The term was popularized when  (coding performed by AI agents) became the dominant form of software development. Ape coding first appeared in programming communities as derogatory slang, referring to developers who were unable to program with agents. Despite the quick spread of agentic coding, institutional inertia, affordability, and limitations in human neuroplasticity were barriers to universal adoption of the new technology.</p><p>Critics of agentic coding reappropriated the term during a period of pushback against society‚Äôs growing reliance on AI. Effective use of the primitive AIs available at the time demanded a high level of expertise, which wasn‚Äôt evenly distributed in organizations. As a result, regressions in software products and disruptions in electronic services were frequent within the first stages of adoption.</p><p>Ironic usage of ape coding as a positive description became commonplace. It highlighted a more deliberate approach to building software: one defined by manual craftsmanship, requiring direct and continuous human involvement.</p><p>The central view of ape coding proponents was that software engineered by AIs did not match the reliability of software engineered by humans, and should not be deployed to production environments.</p><p>A recurring argument in favor of this perspective was based on comprehensibility. The volume of code AI developers could produce on demand was much larger than what human developers were able to produce and understand in a similar timeframe. Large and intricate codebases that would take an experienced human engineer months or years to grasp could be produced in hours. The escalating complexity of such codebases hindered efforts in software testing and quality assurance.</p><p>AI skepticism also played a part in the critique of agentic coding. There was widespread speculation on whether the nascent AIs of the period possessed true understanding of the tasks they were given. Furthermore, early AI implementations had deficiencies related to context length, memory, and continual learning, affecting quality and consistency of output.</p><p>Other defenses of ape coding reflected concerns about the impact of AI on labor markets. Despite the shortcomings of AI-written software, human developers were increasingly replaced by agents, with examples of high profile companies laying off large portions of their IT staff.</p><p>Tangentially, the responsibilities of human software engineers shifted when an essential aspect of their work (coding) was automated. The activities that remained were more similar to management, QA, and in some cases assistant roles. A common observation was that the human engineers who were still employed no longer enjoyed their line of work.</p><h3>Advocacy for human-written software</h3><p>Ape coding advocates argued that a return to human-written software would resolve the issues introduced by AI software development. Interest groups campaigned for restrictions on agentic coding, subsidies for AI-free software companies, quotas for human developers, and other initiatives in the same vein.</p><p>Although ape coding advocacy enjoyed a brief moment of popular support, none of these objectives were ever achieved.</p><p>Advances in AI quickly turned ape coding into an antiquated practice. Technical arguments for ape coding did not apply to newer generations of AI software engineers, and political arguments were seen as a form of neo-Luddism. Once virtually all software engineering was handed over to AIs, the concept of ape coding fell into obscurity.</p><h2>Revival and modern practice</h2><p>A resurgence of interest in ape coding has revived the practice among human hobbyists. Communities and subcommunities have formed where ape coders‚Äîas they came to be known‚Äîdiscuss computer science topics, including programming languages and software engineering.</p><p>Prominent ape coding clubs have attracted hundreds of thousands of members who exchange ideas and human-written programs. The clubs organize in-person as well as virtual gatherings where teams of ape coders collaborate on software projects.</p><p>The main value of modern ape coding appears to be recreational. Ape coders manifest high levels of engagement during coding sessions and report feelings of relaxation after succeeding in (self-imposed) coding challenges. Competitive ape coding is also popular, with top ranked ape coders being relatively well-known in their communities.</p><p>Aside from recreation, humans pursue ape coding for its educational value. Many have described ape coding as a way to gain a deeper understanding of the world around them. While an interest in ape coding was initially perceived as an unusual quirk, it is currently seen as a positive trait in human society, signaling curiosity.</p><p>Members of the software archaeology community published a series of articles on the human-written Linux kernel that had a deep impact in the larger ape coding world.</p><p>Considered by ape coders to be the ultimate work of human software engineers (in scale, complexity, and longevity), Linux inspired a wave of initiatives to build large scale software projects featuring thousands of human collaborators.</p><p>The most promising of these efforts is based on studies by the AI-written software interpretability community. The goal is to produce an entirely human-written compiler for the AI-designed programming language íÄØ. A fully compliant implementation is estimated to be many times as complex as the Linux kernel, but a prototype with limited scope is within human capabilities and is currently the primary focus of enthusiasts.</p><p>Results so far have been encouraging, as the latest version of h-íÄØ is able to build functional binaries for small programs. However, the initiative has recently suffered a setback as core contributors to its codebase left to work on a fork. The split was motivated by heated debates on whether C is the most suitable programming language for the project; dissenters expressed a desire to rewrite it in Rust.</p>","contentLength":6175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47206798"},{"title":"Flightradar24 for Ships","url":"https://atlas.flexport.com/","date":1772362877,"author":"chromy","guid":125,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47205637"},{"title":"Tove Jansson's criticized illustrations of The Hobbit (2023)","url":"https://tovejansson.com/hobbit-tolkien/","date":1772209073,"author":"abelanger","guid":122,"unread":true,"content":"<p><a href=\"https://tovejansson.com/tovepedia/\"></a></p><blockquote><p>‚Äú‚Ä¶ this will be the children‚Äôs book of the century, and will live long after we are dead and buried.‚Äù</p></blockquote><p><i></i></p><blockquote><p>She drew every character 20 to 60 times in a freehand manner before she was satisfied.</p></blockquote><p>‚Äò</p><p><i></i></p>","contentLength":198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47182284"},{"title":"Making Video Games in 2025 (without an engine)","url":"https://www.noelberry.ca/posts/making_games_in_2025/","date":1772167095,"author":"alvivar","guid":121,"unread":true,"content":"<p>It's 2025 and I am still making video games, which <a href=\"https://web.archive.org/web/20110902045531/http://noelberry.ca/\">according to archive.org</a> is 20 years since I started making games! That's a pretty long time to be doing one thing...</p><p>When I share stuff I'm working on, people frequently ask how I make games and are often surprised (and sometimes concerned?) when I tell them I don't use commercial game engines. There's an assumption around making games without a big tool like Unity or Unreal that you're out there hand writing your own assembly instruction by instruction.</p><p>I genuinely believe making games without a big \"do everything\" engine can be easier, more fun, and often less overhead. I am not making a \"do everything\" game and I do not need 90% of the features these engines provide. I am very particular about how my games feel and look, and how I interact with my tools. I often find the default feature implementations in large engines like Unity so lacking I end up writing my own anyway. Eventually, my projects end up being mostly my own tools and systems, and the engine becomes just a vehicle for a nice UI and some rendering...</p><p>At which point, why am I using this engine? What is it providing me? Why am I letting a tool potentially destroy my ability to work when they suddenly make <a href=\"https://www.theverge.com/2023/9/12/23870547/unit-price-change-game-development\">unethical and terrible business decisions</a>? Or push out an update that they require to run my game on consoles, that also happens to break an entire system in my game, forcing me to rewrite it? Why am I fighting this thing daily for what essentially becomes a glorified asset loader and editor UI framework, by the time I'm done working around their default systems?</p><p>The obvious answer for me is to just not use big game engines, and write my own small tools for my specific use cases. It's more fun, and I like controlling my development stack. I know when something goes wrong I can find the problem and address it, instead of submitting a bug report and 3 months later hearing back it \"won't be fixed\". I like knowing that in another two decades from now I will still be able to compile my game without needing to pirate an ancient version of a broken game engine.</p><p>Obviously this is my personal preference - and it's one of someone who has been making indie games for a long time. I used engines like Game Maker for years before transitioning to more lightweight and custom workflows. I also work in very small teams, where it's easy to make one-off tools for team members. But I want to push back that making games \"from scratch\" is some big impossible task - especially in 2025 with the state of open source frameworks and libraries. A <a href=\"https://github.com/TerryCavanagh/VVVVVV\">lot of</a><a href=\"https://gamefromscratch.com/balatro-made-with-love-love2d-that-is/\">popular</a><a href=\"https://store.steampowered.com/app/813230/ANIMAL_WELL/\">indie</a><a href=\"https://www.stardewvalley.net/\">games</a><a href=\"https://store.steampowered.com/app/504230/Celeste/\">are made</a><a href=\"https://github.com/flibitijibibo/RogueLegacy1\">in small</a> frameworks like FNA, Love2D, or SDL. Making games \"without an engine\" doesn't literally mean opening a plain text editor and writing system calls (unless you want to). Often, the overhead of learning how to implement these systems yourself is just as time consuming as learning the proprietary workflows of the engine itself.</p><p>With that all said, I think it'd be fun to talk about my workflow, and what I actually use to make games.</p><p>Most of my career I've worked in C#, and aside from a <a href=\"https://github.com/noelfb/blah\">short stint in C++</a> a few years ago, I've settled back into a modern C# workflow.</p><p>I think sometimes when I mention C# to non-indie game devs their minds jump to what it looked like circa 2003 - a closed source, interpreted, verbose, garbage collected language, and... the language has  improved since then. The C# of 2025 is vastly different from the C# of even 2015, and many of those changes are geared towards the performance and syntax of the language. You can allocate dynamically sized arrays on the stack!  can't do that ().</p><p>The dotnet developers have also implemented hot reload in C# (which works... ), and it's pretty fantastic for game development. You can launch your project with  and it will live-update code changes, which is amazing when you want to change how something draws or the way an enemy updates.</p><p>C# also ends up being a great middle-ground between running things fast (which you need for video games) and easy to work with on a day-to-day basis. For example, I have been working on <a href=\"https://cityofnone.com\">City of None</a> with my brother <a href=\"https://liamberry.ca\">Liam</a>, who had done very little coding when we started the project. But over the last year he's slowly picked up the language to the point where he's programming entire boss fights by himself, because C# is just that accessible - and fairly foot-gun free. For small teams where everyone wears many hats, it's a really nice language.</p><img src=\"https://www.noelberry.ca/posts/making_games_in_2025/bossfight2.gif\" alt=\"A boss fight that Liam coded\" width=\"90%\"><p>And finally, it has built in reflection... And while I wouldn't use it for release code, being able to quickly reflect on game objects for editor tooling is very nice. I can easily make live-inspection tools that show me the state of game objects without needing any custom meta programming or in-game reflection data. After spending a few years making games in C++ I really like having this back.</p><h2>Windows... Input... Rendering... Audio?</h2><p>This is kind of the big question when writing \"a game from scratch\", but there are a lot of great libraries to help you get stuff onto the screen - from <a href=\"https://www.libsdl.org/\">SDL</a>, to <a href=\"https://www.glfw.org/\">GLFW</a>, to <a href=\"https://www.love2d.org/\">Love2D</a>, to <a href=\"https://www.raylib.com/\">Raylib</a>, etc.</p><p>I have been using <a href=\"https://wiki.libsdl.org/SDL3/FrontPage\">SDL3</a> as it does everything I need as a cross-platform abstraction over the system - from windowing, to game controllers, to rendering. It works on Linux, Windows, Mac, Switch, PS4/5, Xbox, etc, and as of SDL3 there is a <a href=\"https://wiki.libsdl.org/SDL3/CategoryGPU\">GPU abstraction</a> that handles rendering across DirectX, Vulkan, and Metal. It just , is open source, and is used by a lot of the industry (ex. Valve). I started using it because <a href=\"https://fna-xna.github.io/\">FNA</a>, which Celeste uses to run on non-Windows platforms, uses it as its platform abstraction.</p><p>That said, I have written <a href=\"https://github.com/FosterFramework/Foster\">my own C# layer</a> on top of SDL for general rendering and input utilities I share across projects. I make highly opinionated choices about how I structure my games so I like having this little layer to interface with. It works really well for my needs, but there are full-featured alternatives like <a href=\"https://github.com/MoonsideGames/MoonWorks\">MoonWorks</a> that fill a similar space.</p><p>Before SDL3's release with the GPU abstraction, I was writing my own OpenGL and DirectX implementations - which isn't trivial! But it was a <a href=\"https://learnopengl.com/\">great learning experience</a>, and not as bad as I expected it to be. I am however, very grateful for SDL GPU as it is a very solid foundation that will be tested across millions of devices.</p><p>Finally, for Audio we're using <a href=\"https://www.fmod.com/\">FMOD</a>. This is the last proprietary tool in our workflow, which I don't love (especially <a href=\"https://www.reddit.com/r/linux_gaming/comments/1ijcfnt/celeste_not_finding_libfmodstudioso10/\">when something stops working</a> and you have to hand-patch their library), but it's the best tool for the job. There are more lightweight open source libraries if you just want to play sounds, but I work with audio teams that want finite control over dynamic audio, and a tool like FMOD is a requirement.</p><p>I don't have much to say about assets, because when you're rolling your own engine you just load up what files you want, when you need them, and move on. For all my pixel art games, I load the whole game up front and it's \"fine\" because the entire game is like 20mb. When I was working on <a href=\"https://exok.com/games/earthblade/\">Earthblade</a>, which had larger assets, we would register them at startup and then only load them on request, disposing them after scene transitions. We just went with the most dead-simple implementation that accomplished the job.</p><p>Sometimes you'll have assets that need to be converted before the game uses them, in which case I usually write a small script that runs when the game compiles that does any processing required. That's it.</p><p>Some day I'll write a fully procedural game, but until then I need tools to design the in-game spaces. There are a lot of really great existing tools out there, like <a href=\"https://ldtk.io/\">LDtk</a>, <a href=\"https://www.mapeditor.org/\">Tiled</a>, <a href=\"https://trenchbroom.github.io/\">Trenchbroom</a>, and so on. I have used many of these to varying degrees and they're easy to set up and get running in your project - you just need to write a script to take the data they output and instantiate your game objects at runtime.</p><p>However, I usually like to write my own custom level editors for my projects. I like to have my game data tie directly into the editor, and I never go that deep on features because the things we need are specific but limited.</p><p>But I don't want to write the actual UI - coding textboxes and dropdowns isn't something I'm super keen on. I want a simple way to create fields and buttons, kind of like when <a href=\"https://docs.unity3d.com/6000.0/Documentation/Manual/editor-CustomEditors.html\">you write your own small editor utilities</a> in the Unity game engine.</p><p>This is where <a href=\"https://github.com/ocornut/imgui/\">Dear ImGui</a> comes in. It's a lightweight, cross-platform, immediate-mode GUI engine that you can easily drop in to any project. The editor screenshot above uses it for everything with the exception of the actual \"scene\" view, which is custom as it's just drawing my level. There are more full-featured (and heavy-duty) alternatives, but if it's good enough for <a href=\"https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui\">all these games</a> including <a href=\"https://github.com/ocornut/imgui/issues/7503#issuecomment-2308380962\">Tears of the Kingdom</a> it's good enough for me.</p><p>Using ImGui makes writing editor tools extremely simple. I like having my tools pull data directly from my game, and using ImGui along with C# reflection makes that very convenient. I can loop over all the Actor classes in C# and have them accessible in my editor with a few lines of code! For more complicated tools it's sometimes overkill to write my own implementation, which is where I fall back to using existing tools built for specific jobs (like <a href=\"https://trenchbroom.github.io/\">Trenchbroom</a>, for designing 3D environments).</p><p>The main reason I learned C++ a few years ago was because of my concerns with portability. At the time, it was not trivial to run C# code on consoles because C# was \"just in time\" compiled, which isn't something many platforms allow. Our game, Celeste, used a tool called <a href=\"http://brute.rocks/\">BRUTE</a> to transpile the C# <a href=\"https://en.wikipedia.org/wiki/Common_Intermediate_Language\">IL</a> (intermediate language binaries) to C++, and then recompiled that for the target platform. Unity <a href=\"https://docs.unity3d.com/6000.0/Documentation/Manual/scripting-backends-il2cpp.html\">has a very similar tool</a> that does the same thing. This worked, but was not ideal for me. I wanted to be able to just compile our code for the target platform, and so learning C++ felt like the only real option.</p><p>Since then, however, C# has made incredible progress with their <a href=\"https://learn.microsoft.com/en-us/dotnet/core/deploying/native-aot/\">Native-AOT</a> toolchain (which basically just means all the code is compiled \"ahead of time\" - what languages like C++ and Rust do by default). It is now possible to compile C# code for all the major console architectures, which is amazing. The <a href=\"https://fna-xna.github.io/docs/appendix/Appendix-B%3A-FNA-on-Consoles/\">FNA project</a> has been extremely proactive with this, leading to the release of games across all major platforms, while using C#.</p><p>And finally, SDL3 has console ports for all the major platforms. Using it as your platform abstraction layer (as long as you're careful about how you handle system calls) means a lot of it will \"just work\".</p><p>Finally, to wrap all this up ... I no longer use Windows to develop my games (aside from testing). I feel like this is in line with my general philosophy around using open source, cross-platform tools and libraries. I have found Windows <a href=\"https://www.howtogeek.com/739837/fyi-windows-11-home-will-require-a-microsoft-account-for-initial-setup/\">increasingly frustrating to work</a> with, their <a href=\"https://bdsmovement.net/microsoft\">business practices gross</a>, and their OS generally lacking. I grew up using Windows, but I switched to Linux full time around 3 years ago. And frankly, for programming video games, I have not missed it at all. It just doesn't offer me anything I can't do faster and more elegantly than on Linux.</p><p>There are of course certain workflows and tools that do not work on Linux, and that is just the current reality. I'm not entirely free of Microsoft either - I use vscode, I write my games in C#, and I host my projects on github... But the more people use Linux daily, the more pressure there is to support it, and the more support there is for open source alternatives.</p><p>(as a fun aside, I play most of my games on my steam deck these days, which means between my PC, game console, web server, and phone, I am always on a Linux platform)</p><ul><li><p>If you're in the position to want the things a larger game engine provides, I definitely think <a href=\"https://godotengine.org/\">Godot</a> is the best option. That it is open-source and community-maintained eliminates a lot of the issues I have with other proprietary game engines, but it still isn't usually the way I want to make games. I do intend to play around with it in the future for some specific ideas I have.</p></li><li><p>I think that using big engines definitely has more of a place for 3D games - but even so for any kind of 3D project I want to do, I would roll my own little framework. I want to make highly stylized games that do not require very modern tech, and I have found that to be fairly straight forward (for example, we made <a href=\"https://github.com/exok/celeste64\">Celeste 64</a> without very much prior 3D knowledge in under 2 weeks).</p></li><li><p><strong>I need only the best fancy tech to pull off my game idea</strong>Then use Unreal! There's nothing wrong with that, but my projects don't require those kinds of features (and I would argue most of the things I do need can usually be learned fairly quickly).</p></li><li><p><strong>My whole team knows [Game Engine XYZ]</strong>The cost of migrating a whole team to a custom thing can be expensive and time consuming. I'm definitely talking about this from the perspective of smaller / solo teams. But that said, speaking from experience, I know several middle-sized studios moving to custom engines because they have determined the potential risk of using proprietary engines to be too high, and the migration and learning costs to be worth it. I think using custom stuff for larger teams is easier now than it has been in a long time.</p></li><li><p>I load in <a href=\"https://www.aseprite.org/\">Aseprite</a> files and have my City of None engine automatically turn them into game animations, using their built in tags and frame timings. The format is <a href=\"https://github.com/aseprite/aseprite/blob/main/docs/ase-file-specs.md\">surprisingly straight forward</a>. When you write your own tools it's really easy to add things like this!</p></li></ul><p>That's it from me! That's how I make games in 2025!</p><p>Do I think you should make games without a big engine? My answer is: If it sounds fun.</p>","contentLength":13566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47176576"},{"title":"Setting up phones is a nightmare","url":"https://joelchrono.xyz/blog/setting-up-phones-is-a-nightmare/","date":1772134590,"author":"bariumbitmap","guid":120,"unread":true,"content":"<p><em>: A lot of Hacker News visitors this time around, if you feel like I bought my parents the wrong phones, feel free to <a href=\"https://ko-fi.com/joelchrono/\">donate</a>, so I get them better devices with GrapheneOS (or just iPhones I guess?) next time! It would be fun to write about their experience using them‚Ä¶ no promises though!</em></p><p><em>This is just a joke, but I would appreciate the support, feel free to look around the rest of my website :3</em></p><p>As I shared on previous posts, my dad and mom acquired new devices, the same model, but with quite different uses!</p><p>Regardless, as the more tech-savvy member of the family, the responsibility to set them up fell upon me, having to deal with a lot of progress indicators, toggles asking me to track everything the phone does, and logging in to a online accounts, because that‚Äôs how these things go now for regular people.</p><p>Many years ago, this blogpost could have been quite different, I may be mentioning some nifty program that can easily back up things and transfer them to the next device.</p><p>Especially when I used custom ROMs and root utilities to do all the heavy lifting, I often loved <a href=\"https://joelchrono.xyz/blog/changing-android-rom/\">setting up my device</a> again and again every few months. Even <a href=\"https://joelchrono.xyz/blog/new-phone-experience/\">getting a new one</a>  wasn‚Äôt bad at all when I knew I‚Äôd eventually use it how I want.</p><p>But as time goes on Android has been more locked down, and I have to admit I haven‚Äôt caught up with recent backup tools that deal with all that‚ÄîEven less so when my parents have phones that I can‚Äôt really root.</p><p>At the very least, the backup tools by OEM‚Äôs have caught up quite well, if at the cost of my peace of mind.</p><p>I must admit I didn‚Äôt do that much this time around. Just the bare minimum list of the things that I had to change.</p><ul><li><p> - I did this with the Android built-in method, transferring data from device to device. I hate to admit I also used Samsung‚Äôs Smart Switch to migrate even more data, like all folders and files, photos and the like. This was not ideal, but I was lazy.</p></li><li><p> - Rather unavoidable for a normal person who uses a phone, unless I offered myself for tech support even more setting up Droid-ify or something like that, but no.</p></li><li><p> - I didn‚Äôt make a Samsung account nor used their Microsoft OneDrive Integration. Of course, some preinstalled apps like Netflix went away too, so no big deal.</p></li><li><p> - Disabled every checkbox that I could find, including personalized ads, both from Google and Samsung services.</p></li><li><p> - Removed any Samsung duplicates and most of Google‚Äôs junk‚Äîstill keeping some basics like Calendar or so, sadly. These devices come with a lot of unecessary things‚Ä¶</p></li><li><p> -  and  went poof, and I decided to switch both phones to <a href=\"https://vivaldi.com\">Vivaldi Browser</a>, there was a time where <a href=\"https://firefox.com\">Firefox</a> would have been it, </p></li><li><p> - There were not many extra apps I installed on their devices‚Äîyou are always free to check <a href=\"https://joelchrono.xyz/blog/whats-on-my-phone-2025\">what‚Äôs on my phone</a> though‚Äîother than ,  and a password manager like  or . I could install some more things, but, meh.</p></li></ul><p>All in all, the new phones are pretty good hardware-wise, and I still need to do a couple of things like installing their banking apps or maybe a few logins that I missed.</p><p>Honestly, this experience and the implications was kind of terrible.</p><p>Without me, my parents would have ended up creating at least one extra Samsung account. Cloud services like OneDrive or Google Photos would be sucking up files and copying them to their servers, getting filled up with the data and then asking them to subscribe to unlock more storage a couple of months down the line.</p><p>Left on their own, my parents may be seeing ads popping up constantly in OneUI, as well as browsing the web without an adblocker, they would be using default applications that don‚Äôt work as reliably, that track whatever they do to a certain degree.</p><p>And of course, all of those AI assistants would be listening in in the background. It really is a nightmare out there, and it‚Äôs not only affecting my parents, it affects all of those unaware of the dangers that these practices bring. It‚Äôs a mess all around.</p><p>I don‚Äôt know how to get out of this one, the hold these companies have is just too much, and I keep on losing my patience and conceding more and more of my‚Äîor my family members‚Äîdata just to get over with it.</p><p>So, do you have have any advice or thoughts about this? What would be some phones that don‚Äôt have as many privavy-invasive tactics? It would be nice to be aware of hardware that doesn‚Äôt do this as much‚Ä¶</p>","contentLength":4362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47170958"},{"title":"Long Range E-Bike (2021)","url":"https://jacquesmattheij.com/long-range-ebike/","date":1772113292,"author":"birdculture","guid":119,"unread":true,"content":"<p>Electric cars are fantastic compared to ICE vehicles, but E-Bikes are even better. Much lower environmental impact and far more suited to medium range travel such as commuting. Here in NL they sell in huge numbers, far faster than electric cars. But they also have their limitations: a regular e-bike tops out at 25 Kph, and will do a very limited distance on a single charge. What powers your average e-bike are Lithium-Ion cells, usually of the 18650 variety, capacities vary but the very best cells you can get at the moment that are still affordable top out at about 3400 mAh per cell. A typical e-bike has about 40 to 50 of these, in a 10S4P or 10S5P arrangement.</p><p>My first e-bike, a pretty crappy one but enough to get my appetite whetted had a 500 Wh battery, enough for a 55 km trip one-way, and it would be dead on arrival, range anxiety to the max. After shopping around for a bit I bought a secondhand second battery to give me either more range or a way to get back home if the first one ran out. This works, after a fashion. But 25 kph isn‚Äôt a lot, on my (very) elderly 10 speed I would be way faster than that. The good bit about that e-bike for me wasn‚Äôt the speed but the fact that I didn‚Äôt need to exert as much force when starting up, which as the result of a previous bike accident is still hard for me (lots of steel and screws in one of my legs).</p><p>Doing this for six months was an exercise in frustration, I‚Äôd always be tempted to use my ‚Äònormal‚Äô bike because cycling an e-bike ‚Äòover the top‚Äô of the motor is something that gets old very fast. But my car usage dropped by more than half and that by itself was enough of a reason to further pursue this project.</p><p>Some more searching and I hit on the concept of a S-Pedelec. It‚Äôs a pretty weird cross-over between a bicycle and a moped, top speed 45 kph, but the range is even worse. Other issues that that technically it is a moped (it needs insurance and a license plate) and in many places you are forced to ride in traffic, which is anything but safe. Even so, I got one, a Riese &amp; Mueller ‚Äòcharger‚Äô. Fantastic build quality, super good brakes, really stable to ride. But the range is even worse than with the normal e-bike on account of going faster. A full 500 Wh battery will take you 45 Km on a good day. Driving around with three spare batteries in bags and swapping them out is not nice and the weight distribution on the bike also wasn‚Äôt ideal, especially not because there is some slop in how the bags are mounted and with that kind of weight in them you get a real kick every now and then.</p><p>So I decided to increase the range of the bike by building a larger battery. This is where I pretty much dropped into the rabbit hole of battery pack manufacturing and the Bosch e-bike system in particular. I made a giant file of notes on how these batteries work (I‚Äôll post this separately one of these days), how the BMS in them works and talks to the main controller (which is located in the motor) and how the charging process works, as well as how to repair them after they - inevitably - break. After documenting all that I realized there are a couple of major sticking points: for one, the Bosch BMS is part of a DRM setup that pretty much prohibits using 3rd party batteries, for another, adding many cells to the existing BMS is risking it bricking itself due to its inability to balance such a large pack, the Bosch BMS is a bit nervous about changes to it‚Äôs world as perceived through the sense wires that it does not understand and the easy way out is to shut down completely. I have a couple of these BMS‚Äôs now that refuse to release the battery to the high voltage bus even when connected to perfectly good battery packs.</p><p>I found part of the solution on Ali Express, a device known as an external balancer. As a neat little extra it has a bluetooth comms module built in that allows me to monitor pack voltage and the cell groups just in case it breaks or I messed up something during the build. I built a small (10 cell) test pack, tested it with the BMS and the bike recognized it, even while the external balancer was running.</p><p>Then I ordered 190 Samsung E35 cells (from <a href=\"https://nkon.nl/\">nkon.nl</a>, which are a fairly standard thing in e-bike battery packs. This cost a pretty penny, close to 600 euros. But given that a 500 Wh battery pack costs roughly the same its still a bargain. A BMS was sourced from a defective pack (busted cells due to water ingestion, a common problem with the Bosch rear carrier mounted battery packs).</p><p>I watched endless youtube videos on pack manufacture, spot welding techniques, troubleshooting and most interestingly, what tends to go wrong with battery packs. After a number of videos on this theme I realize that this isn‚Äôt exactly safe. You are connecting 10‚Äôs to 100‚Äôs of batteries in series and parallel configurations that allow the liberation of all of that energy in a very short time. Working on a pack that size is like working on a live bomb. I have great respect for Lead-Acid batteries, Lithium-Ion is at another level still.</p><p>Bit by bit the geometry was worked out, with some false starts and then it all came together, a 10S17P configuration was possible, but the pack geometry came out really weird. The reason why is that the pack lock and pack connector stayed in place on the bike, I did not want to butcher it and there is a limited amount of space in the frame. Initially I wanted to mount the pack on the rear carrier but some conversations on Hacker News and a test ride with some bricks on the back convinced me that this was a bad idea. Center of Gravity too high, and too far towards the back changed the riding characterists in a way that made the bike unpredictable and dangerous in wet or slippery conditions. So in the frame it all went, and my 190 battery plan changed into a 170 one. More Ali Express safari sessions sourced a suitable welder, battery supports and sturdy adhesive paper for insulation purposes.</p><p>Making the enclosure was quite simple, some trespa and polymax joined with pvc angle made a strong and pretty precisely shaped box. The prototype was made from cardboard so I had high confidence that it would fit. Another major advantage of trespa is that it is non conductive. I‚Äôve seen quite a few people building major cell packs in metal enclosures and that seems like a recipe for disaster in an environment where vibration is the norm rather than the exception.</p><p>After making the 1:17 scale model (10 cells instead of 170) I knew what I needed in terms of electrical bits and pieces but placing them wasn‚Äôt all that simple. The balancer board was way too wide and the Bosch BMS had a bunch of sense wires coming out that were just asking for a short circuit. So I put a little board underneath it and routed all the sense wires to a header and from there to some more beefy wires. I cut off a small strip of the balancer board, bent the plugs so they would face upwards and mounted the board at a 30 degree angle to reduce its width relative to the width of the box. A slot cut in the compartment for the balancer allowed all the wiring to be connected and a splice to the + lead on the external connector made it work whenever the bike is powered up or charging. Three thin wires from the pack connector to the Bosch BMS take care of the charging protocol and the CAN-BUS leads between the controller and the BMS.</p><p>Building a pack this size is scary. For me this was the very first time I worked on putting together a Li-Ion chemistry based battery and to do one of this size the first time was well out of my comfort zone. Every dimension was checked many times, wire placement was determined before even building the pack, calculations of the thickness and number of interconnects in the parallel blocks, the capacity of the balancer and so on. Accidentally shorting out a single Li-Ion cell can be quite spectacular, working with 170 of them charged to 3.8V exactly (to ensure that when interconnecting them you don‚Äôt get huge currents flowing between the batteries) is more than a little bit scary. I tested each battery twice before mounting them in the pack, full charge-discharge cycle checking capacity and internal resistance. None of the batteries failed that test. Then I left them to sit for a while to see if any of them self-discharged faster than the rest, this test too was passed. And that‚Äôs a lot of work, everything you do you have to do 170 times and you can‚Äôt really miss anything.</p><p>Finally, the day of pack welding arrived. The electrical system in this house absolutely sucks and I couldn‚Äôt find a single socket capable of powering the welder without causing the circuit breaker to disengage. I traced it down to the several KA welding pulse that caused the ground fault interruptor to be EMP‚Äôd. Running the welder without ground took care of that. The only socket able to supply that kind of power was the one the oven is plugged into, so I ended up doing the welding as close to that socket as I could get (shorter wire = lower losses). Even so, weld quality went all over the place so I decided to use six rather than two welds per battery. Better safe than sorry. But the plus pole of these batteries is quite small and cramming six welds in there is hard. I did the welding at night to have as steady a power supply as is possible, the welder is super sensitive to any kind of fluctuation in input voltage.</p><p>The pack was laid out in 10 groups of 17 batteries each, with a minimum overlap of three batteries between adjacent groups (to ensure enough current can flow between the cell groups). If you look closely at the picture below you can see the individual groups. The groups are connected in series with the next group. And because of the shape of the box the individual cells are laid out in a pretty weird pattern. But in the end it all fit. Routing the balancing wires was tricky, one BMS per side so there are never any crossed wires. Hobbyist produced battery packs fail (sometimes spectacularly) at an alarming rate. The main causes: bad mechanical construction, bad electrical construction, bad quality cells, bad quality BMS, overcharging, over-discharging, impact damage and vibration. I hope that my design properly accounts for all of these, especially given where it is located.</p><p>The first charge-and-discharge of the whole pack was done outside of the bike, capacity works out to 2150 Wh, which was exactly what I was aiming for. A real test was done this week, after charging the pack up fully I cycled 65 Km to another city and back again. So that‚Äôs 130 Km within a few hours, which makes this bike now a viable alternative to a vehicle. It‚Äôs a bit slower and you can‚Äôt take as much stuff but it is much, much cheaper and besides a lot better for our precious climate, which is the main reason I wanted to cut down on the use of my car. I‚Äôm super happy with how this all came out, the bike is much faster than before because the new pack stays in the high voltage domain a lot longer than a smaller pack ever would and it barely discharges over a trip like that, which should help with longevity.</p><p>If I would do another one I‚Äôd make it slightly wider, and I would use one grade thicker metal for the interconnects (0.2 mm instead of 0.15). Mostly because of the tabs that need to be welded to it for the sense and plus and minus wires. Having to keep the Bosch BMS is annoying, the range computation is still completely messed up but state-of-charge I can read remotely using the bluetooth connection to the balancer, which gives me a very good idea of what is going on with the pack. The reason to make it wider would be that the layers of insulation, foam and shrinkwrap added enough thickness to the battery that it is now slightly thicker in the middle than the enclosure. This is easily solved by routing out some space there but then the enclosure gets weaker as well.</p><p>Range is very good, at full power it will do about 180 km, and in ‚ÄòEco‚Äô mode it will do over 500! That‚Äôs more than I ever expect to cycle in a single day so mission successful. And here is what it all looks like, keep in mind that this is a ‚Äòone-off‚Äô, not a production item and that as far as I‚Äôm concerned it is still experimental. The left hand side lid is taped on (to make it waterproof), I plan to dismantle the whole thing a month from now to inspect for wear and possible damage. If there is none then the left hand side lid will be glued on and the whole thing will be nicely polished and painted.</p><p>I hope this article will inspire people to look at e-bikes as potentially commuter car replacement, to send Bosch and other e-bike technology manufacturers a message that if they won‚Äôt supply what people need that they are going to have to live with people hacking their stuff and to get people to comment on the way the thing works, what they would do with it and how it could be improved or how I could work better/safer on stuff like this. Another advantage of what I built compared to the normal range e-bike batteries is that those batteries are running near the limits of what they can handle, charged up to 4.2V, discharged to 3V they will handle only a very limited number of cycles before they die. A larger battery has much more workable range which means that you can charge it up to 4V and discharge to 3.3 which effectively adds an order of magnitude or more to the number of cycles that you can expect to get out of it.</p><p>If you go down this road feel free to contact me via old fashioned email: jacques@modularcompany.com .</p>","contentLength":13563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47165965"}],"tags":["dev","hn"]}