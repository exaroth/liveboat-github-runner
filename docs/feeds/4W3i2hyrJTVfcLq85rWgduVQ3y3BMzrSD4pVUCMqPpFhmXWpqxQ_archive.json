{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":17,"items":[{"title":"If OpenSSL were a GUI (2022)","url":"https://smallstep.com/blog/if-openssl-were-a-gui/","date":1737952480,"author":"tambourine_man","guid":218,"unread":true,"content":"<blockquote><p>\"When something exceeds your ability to understand how it works,\nit sort of becomes magical.\" - Jony Ive</p></blockquote><p>*This is incomplete. It covers about 80% of one corner of OpenSSL's functionality. The certificate policy options have a lot more knobs that I didn't include.</p><div><div><p>Carl Tashian (<a href=\"https://tashian.com\">Website</a>, <a href=\"https://www.linkedin.com/in/tashian/\">LinkedIn</a>) is an engineer, writer, exec coach, and startup all-rounder. He's currently an Offroad Engineer at Smallstep. He co-founded and built the engineering team at Trove, and he wrote the code that opens your Zipcar. He lives in San Francisco with his wife Siobhan and he loves to play the modular synthesizer üéõÔ∏èüéöÔ∏è</p></div></div>","contentLength":613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42837462"},{"title":"Purelymail: Cheap, no-nonsense email","url":"https://purelymail.com/","date":1737945505,"author":"LorenDB","guid":217,"unread":true,"content":"<p>Let's get straight to the point:</p><ul><li>We host your email address.</li><li>We're IMAP and POP3 compatible, so we work with most mail apps.</li><ul><li>Or you can use our webmail, powered by <a href=\"https://roundcube.net/\" target=\"_blank\" rel=\"noopener\">Roundcube</a>.</li></ul><li>No arbitrary limits. Have as many users and store as much mail as you want.</li><li>Bring as many of your own domains as you want, or use one of ours. No extra charges.</li><li>It's cheap. Really, really cheap.</li></ul><p>Let's say you only need one email address, and have 3 GB of data to store. For one year of use, this will cost:</p><p>For most use cases, Purelymail is the cheapest option available.  if you need more than one user. All of the plans above charge per user, but we don't. Check out our <a href=\"https://purelymail.com/pricing\">pricing page</a> for more information. (For an in-depth explanation of this particular comparison, <a href=\"https://purelymail.com/pricingcomparisonindepth\" target=\"_blank\" rel=\"noopener\">see here.</a>)</p><p>What you get here is   mail. </p><p>We're not trying to bamboozle you with glossy images, or sell you a lofty ideal. If we have a gimmick, it's that we offer a known service at a low price. What you do with it is up to you.</p><h3>An honest list of drawbacks</h3><p>Based on our experience, here are our weak points:</p><ul><li>Occasionally, obscure email servers will block emails sent through us. Usually this can be resolved within a day or two.</li><li>We don't have a 24/7 support staff, although we do try to get back to any inquiries within a day.</li><li>Some features other providers have, such as calendar syncing, are not yet implemented.</li><li>We're not for sending any type of unsolicited or marketing emails (you should use a dedicated marketing platform anyway).</li><li>Our UI can be a little unpolished and unglamorous, if that bothers you.</li></ul><p>That means there might be a few hiccups along the way. If you run across any problems you can always <a href=\"https://purelymail.com/support\">let us know,</a> and we'll do our best to fix them.</p><p>If you're interested in Purelymail but want to wait until we're out of beta, check out <a href=\"https://purelymail.com/betamailinglist\">our mailing list.</a></p><b>What about security and reliability?</b><p>As for reliability, we're in beta, but our architecture has proven itself robust so far, our infrastructure runs on the highly reliable AWS cloud, and even if we do have an outage the email protocols are pretty forgiving. (See our <a href=\"https://news.purelymail.com/status.html\">status page</a> for our historical issues- there aren't that many.)</p><b>Do you sell user data or ads?</b><b>What features do you support?</b><h2>Still can't decide if we're right for you?</h2>","contentLength":2215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42836818"},{"title":"Marginalia ‚Äì A search engine that prioritizes non-commercial content","url":"https://marginalia-search.com/","date":1737941945,"author":"herbertl","guid":216,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42836405"},{"title":"Openhaystack: Build 'AirTags' ‚Äì track Bluetooth devices via Apple's network","url":"https://github.com/seemoo-lab/openhaystack","date":1737936687,"author":"thunderbong","guid":215,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42835772"},{"title":"Lessons in creating family photos that people want to keep (2018)","url":"https://estherschindler.medium.com/the-old-family-photos-project-lessons-in-creating-family-photos-that-people-want-to-keep-ea3909129943","date":1737933181,"author":"mooreds","guid":214,"unread":true,"content":"<p><em>As a consequence of scanning thousands of slides, I learned quite a bit about taking photos that capture a family‚Äôs life. Here‚Äôs a personal memoir, with a few lessons in taking memorable snapshots.</em></p><p>My father was an avid amateur photographer. He loved to take pictures, he invested in expensive cameras, and I‚Äôve plenty of vacation memories where he had one of those cameras in hand.</p><p>But organizing the slides afterwards? Labeling them? No way. Pop threw the boxes of slides in big piles and said, ‚ÄúI‚Äôll sort them after I retire.‚Äù And, in preparation for his retirement, he put all those slides into five huge boxes ‚Äî the kind you‚Äôd use to ship vinyl records.</p><p>Whereupon, three days after my father formally retired in 1988, he died in his sleep.</p><p>The slides stayed with my mother. When she moved into assisted living, the boxes went into my sister‚Äôs garage. After mom died, three years ago, they came to me. The result was a huge project of scanning family slides ‚Äî between 8,000 and 10,000 of them.</p><p>The primary goal was to save the photos before the media deteriorated beyond hope. It was too late in some cases. I remember Pop telling me how much cheaper Ektachrome was (compared to Kodachrome), but many of those images were as ghost-like as a half-remembered dream. Memories fade even faster. Is that a photo of my second cousin Charlotte? I‚Äôve no idea.</p><p>For those who want practical lessons, herein you will find two categories:</p><ul><li>How to go about a family photo archive project (or at least how I did it) and</li><li>Practical suggestions for taking photos that your family will treasure long after you‚Äôre gone.</li></ul><p>This was an oddly spiritual process. We take pictures of the moments we think are valuable or important. So, in the photos he took, I saw my father‚Äôs dreams, the things he thought were beautiful, his moments of pride. And in so doing, I gained more understanding of who my parents were. ‚Ä¶but I‚Äôll leave that essay to another time.</p><p>I have spoken with several people who have similar family photo archives, so let me begin by describing I went about the project.</p><p>Before I began, I had an inexpensive <a href=\"http://amzn.to/2BzhIkw\" rel=\"noopener ugc nofollow\" target=\"_blank\">Wolverine slide scanner</a> but I knew a manual unit would not cut it. I bought a heavy-duty slide scanner to help me process the images. It‚Äôs a <a href=\"http://amzn.to/2u4f8Ee\" rel=\"noopener ugc nofollow\" target=\"_blank\">Canon CanoScan 9000F</a>. I like it, in case you‚Äôre shopping for an affordable unit; in particular, I do not loathe the built-in software, which sets it apart from other scanners I‚Äôve used.</p><p>The project, which took me about a year, became a background process. I could scan a box of slides while I was reading my daily morning e-mail, then clean up and share the images during moments of down-time (such as waiting for poky websites to load site statistics). Over a weekend I usually could get through five or six boxes of slides.</p><p>Scanning a box of slides had several steps, each of which became a kind of emotional triage:</p><ul><li>I held up a slide (in front of a desk lamp) to identify it generally and decide if it was worth scanning. In other words: Do I care about this at all? Something out-of-focus easily could be thrown away. A picture of people I didn‚Äôt care about (e.g. someone my folks met on a bus tour and never spoke with again) could be dumped, too. It soon became obvious that I didn‚Äôt need to scan tourist photos; there are just-so-many pictures you need to see of the Tower of London (which looks the same today as it did in 1972 when my parents visited) or random sunsets over random mountains.</li><li>If the slide looked interesting, I did a fast preview scan. For instance, if my father took three pictures ‚Äújust to be sure‚Äù I could choose the best image; I could throw out the ones where my brother had his eyes closed. And I could eliminate the pictures that were inherently uninteresting, by which I mean it brought me no sense of nostalgia.</li><li>By the time I scanned an image, I was pretty committed to keeping it and sharing it with my siblings. Sometimes, if an image was entertaining or meaningful, I‚Äôd share it among my friends on Facebook.</li></ul><p>From a box of 24‚Äì36 slides, I usually shared about 8 with my brother and sisters. By the end of the project, I‚Äôd shared 2,800 images with my siblings, and a few hundred on Facebook.</p><p>I used iPhoto to clean up the images and sort them into a dedicated folder. While tools like PhotoShop certainly could do a better job (and were trotted out for a few special images), 98% were treated with iPhoto‚Äôs crop, straighten, and the ‚ÄúEnhance‚Äù button. I also added dates and locations to the images‚Äô metadata.</p><p>To share the images with my family, I uploaded photos to Flickr. Other photo sharing sites have far better user interfaces, but Flickr has two advantages: I can limit sharing to a set of people marked as friends-and-family, and viewers can comment on the images. Plus you can search images, if you‚Äôre smart enough to add tags as you go. (Do.)</p><p>Towards the end I also created private Facebook groups, which let me share with cousins as well as siblings, though its search capabilities are poor. It‚Äôs been useful for sharing those videos, though, and for encouraging conversations among my relatives.</p><p>For general sharing online, I created an Old Family Photos album on Facebook. iPhoto makes it easy to share to an album (though, alas, not to a private group). I‚Äôve been astonished by how many of my family‚Äôs history touches a chord. Don‚Äôt be shy; but do keep your family‚Äôs privacy sensitivities in mind when you share.</p><p>The earliest roll of slides was from my parents‚Äô engagement party circa 1941, followed by their honeymoon snapshots in 1942. Thousands of slides record their lives all the way through the 1980s, with a Family Circle gathering held only two months before my father‚Äôs death.</p><p>Inside the big boxes were two shoeboxes with a hundred 8mm video movies, which went back to the 1920s but mainly record 1950s camping trips. (<a href=\"https://www.imemories.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\">iMemories</a> did a very good job at digitizing those.)</p><p>Most images are from family vacations and special occasions, rather than ‚Äúdaily life.‚Äù Earlier vacations (1950s and 1960s) are mostly camping trips; later pictures are from trips to Europe, particularly when money eased up after ‚Äúthe kids left home.‚Äù</p><p>But more is visible than the campsites and Boy Scout trips. I saw a young couple‚Äôs struggles to cope with three young children (I was an afterthought); I watched their idealism diminish and exhaustion set in.</p><p>I threw away many thousands of pictures. Some of them undoubtedly had meaning to my parents, but nobody alive cares about those photos. Yet I also came across special moments ‚Äî and none of us need to have ‚Äúbeen there‚Äù to appreciate them.</p><p>In reviewing thousands of slides, I learned quite a bit about taking photos that capture a family‚Äôs life. Perhaps these lessons can help you, too, in considering which images to snap ‚Äî on vacation or in daily life.</p><p><strong>Those ‚Äútitle slides‚Äù are meaningful after all</strong>. I remember rolling my eyes whenever my father would station me in front of a road sign or National Park entrance. Such pictures seemed really lame.</p><p>As I reviewed the pictures, though, the title slides were priceless. In all those years, my parents went to dozens of beaches, gardens, and campsites in random mountain ranges. Other than the date on the slide (‚ÄúSep 83‚Äù) I have no way to identify which one it is. (Occasionally, there‚Äôs a scribbled note, like, ‚ÄúExplorer Trip‚Äù or ‚ÄúLondon.‚Äù Um, thanks, Pop.)</p><p>So I was always glad when I found a photo of us kids standing next to a ‚ÄúMystic Seaport‚Äù sign or ‚ÄúUnderground tours‚Äù (always looking put-upon and sullen, because we were told to ‚ÄúStand up straight! And smile ‚Äî it might turn out good!‚Äù).</p><p>Labels matter. Even a few words helped me know when-and-where something happened: ‚Äú1955 Nova Scotia‚Äù or my grandfather‚Äôs name. One of the saddest experiences was looking at a family-gathering photo from the 50s with several people in it, and having no idea who‚Äôs in it. (Is that my great-aunt? Maybe my sister remembers? ‚Ä¶and too often she didn‚Äôt.) If you inherit the photos, take the time to identify the people in them. Even if it‚Äôs obvious to you that the picture is of cousin Janet who died in 1943, you can‚Äôt assume that the next viewer will know.</p><p>: Do take pictures that give the viewer a clue of where you are, and with whom.</p><p><strong>Kodak picture spots aren‚Äôt memorable</strong>. Destination pictures surely remind the travelers of their experience. I‚Äôm sure that that picture of the beach in Portugal would have encouraged my father to say, ‚ÄúThelma, remember that night?‚Äù That‚Äôs fine, for the people who participated. I‚Äôve taken thousands of such photos myself.</p><p>But if I wasn‚Äôt there, the image brings me no nostalgia.</p><p>The worst of these pictures are the touristy photos. My father took plenty of pictures of the Eiffel Tower on their trip in the 70s. But the tower doesn‚Äôt look any different today, so I didn‚Äôt bother to scan those photos. In fact, I dumped boxes without even looking at the contents, because there‚Äôs nothing in that experience that speaks to anyone but the participants.</p><p>: It‚Äôs fine to take pictures that capture a moment for those who were present. But if  could have taken that photo, don‚Äôt expect anyone to care.</p><p><strong>People pictures matter the most</strong>. Especially the non-staged ones. The formal pictures of special occasions, where we kids are lined up like we‚Äôre in front of a firing squad, are not the ones that bind us.</p><p>The best family photos are the ones where we‚Äôre clowning around and laughing, or where we‚Äôre doing something together, or a moment captured without the subject realizing it. The most precious are those where the family is putting up a pup tent, or using the water pump, or packing the car for a trip.</p><p>In general, try to capture your family when they are actively doing something, ideally an entire process. Let it be a photo essay: ‚ÄúMom making Thanksgiving dinner‚Äù or ‚ÄúDaddy taking the kids to the petting zoo.‚Äù Don‚Äôt choose only the ‚Äúreveal‚Äù moments such as Mom presenting the turkey to the table; include a picture of her hurriedly putting on lipstick before Grandpa arrives, or the kids conked out, asleep in the back seat, on the car trip home.</p><p>A few exceptions: Nobody looks attractive or interesting while he‚Äôs swimming. Few people look great sitting on a towel on the beach, wearing a bathing cap. Also don‚Äôt take pictures of people eating dinner, even at a fancy dinner. And while it‚Äôs no longer relevant, it was never a good idea to photograph exhausted travelers arriving at an airport gate.</p><p>Include the photographer. I have few pictures of my father, because he was always the guy behind the camera. When he did ask someone to take a picture it was always posed, such as ‚ÄúMom and Pop standing in front of the Grand Canyon.‚Äù</p><p>: Photos that capture you ‚Äúbeing there‚Äù ‚Äî which means most selfies ‚Äî rarely have meaning.  matters far more.</p><p><strong>Take photos of daily life</strong>. I‚Äôm stunned by the pictures my father  take. There isn‚Äôt a single photo that represents what my parents did for a living. They weren‚Äôt the type to attend company picnics, fine. But I found nothing indicating ‚Äútake your daughter to work‚Äù or ‚ÄúMom typing up a report‚Äù or ‚Äúthe building I worked in‚Äù or ‚Äúthe woman Mom commuted to work with for 10 years.‚Äù That would be more understandable if my parents disliked their jobs, but both of them were passionate about their careers.</p><p>Take photos of people at rest. Even though I spent much of my childhood writing letters, there is only one photo of me with a pen in my hand ‚Äî and that was taken by a friend at summer camp. Yet my friends and family all recall me with a book or pen within reach. My father never captured that essential part of who I was.</p><p>Some of the absences may reflect their superstitions. There are zero photos of any woman who is visibly pregnant. Maybe that was considered bad luck; I don‚Äôt know.</p><p>: Don‚Äôt limit photo-taking to special occasions.</p><p><strong>Take at least a short class in photography basics</strong>. Or read a basic book on the topic. As much as my father loved photography, he never got any kind of formal training. I spend a lot of editing time cropping images to take advantage of the simplest rule-of-thirds, for instance.</p><p>Even if you aren‚Äôt devoted to photography that much: Crop photos closely. My father took a lot of photos of ‚ÄúMom in front of a pretty vista‚Äù but in the long run I care more about Mom‚Äôs expression than the expanse of mountains in the background. Thanks to iPhoto I can zoom in, but a lot of detail is lost.</p><p>: Take the best quality photos you can. Your grandchildren will appreciate it.</p>","contentLength":12639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42835282"},{"title":"Kansas tuberculosis outbreak is America's largest recorded since the 1950s","url":"https://www.cjonline.com/story/news/politics/government/2025/01/24/kansas-tuberculosis-outbreak-is-largest-in-recorded-history-in-u-s/77881467007/","date":1737932590,"author":"toastedwedge","guid":213,"unread":true,"content":"<p>An ongoing tuberculosis outbreak in Kansas has become the largest in recorded history in the United States.</p><p>\"Currently, Kansas has the largest outbreak that they've ever had in history,\" Ashley Goss, a deputy secretary at the Kansas Department of Health and Environment, told the Senate Public Health and Welfare Committee on Tuesday.</p><p>As of Jan. 17, <a href=\"https://www.kdhe.ks.gov/2242/Tuberculosis-Outbreaks\" data-type=\"link\" data-id=\"https://www.kdhe.ks.gov/2242/Tuberculosis-Outbreaks\" target=\"_blank\" rel=\"noreferrer noopener\" data-t-l=\":b|z|k|${u}\">public health officials reported</a> that they had documented 66 active cases and 79 latent infections in the Kansas City, Kansas, metro area since 2024. Most of the cases have been in Wyandotte County, with a handful in Johnson County.</p><p>Jill Bronaugh, a KDHE spokesperson, confirmed Goss's statement afterward.</p><p>\"The current KCK Metro TB outbreak is the largest documented outbreak in U.S. history, presently,\" Bronaugh said in a statement to The Capital-Journal. \"This is mainly due to the rapid number of cases in the short amount of time. This outbreak is still ongoing, which means that there could be more cases. There are a few other states that currently have large outbreaks that are also ongoing.\"</p><p>She noted that the Centers for Disease Control and Prevention started monitoring and reporting tuberculosis cases in the U.S. in the 1950s.</p><p>Tuberculosis is caused by a bacterium that typically affects the lungs, <a href=\"https://www.kdhe.ks.gov/2242/Tuberculosis-Outbreaks\" data-type=\"link\" data-id=\"https://www.kdhe.ks.gov/2242/Tuberculosis-Outbreaks\" target=\"_blank\" rel=\"noreferrer noopener\" data-t-l=\":b|z|k|${u}\">according to KDHE</a>. People with an active infection feel sick and can spread it to others, while people with a latent infection don't feel sick and can't spread it. Tuberculosis is spread person-to-person through the air when a person with an active infection coughs, speaks or sings. It is treatable with antibiotics.</p><p>State public health officials say there is \"very low risk to the general public.\"</p><p>\"Some of you are aware, we have and still have mobilized staff and resources addressing an unprecedented tuberculosis outbreak in one of our counties,\" Goss told lawmakers. \"We are working collaboratively with CDC on that. CDC remains on the ground with us to support. That's not a negative. This is normal when there's something unprecedented or a large outbreak of any kind, they will come and lend resources to us to help get a stop to that. We are trending in the right direction right now.\"</p><p>Goss said that when KDHE got involved with the Kansas City outbreak last summer, there were 65 active cases and roughly the same number of latent cases. She said the number is now down to about 32 active cases.</p><p>For active patients, after 10 days of taking medications and having three sputum tests, they will generally no longer be able to transmit tuberculosis.</p><p>\"They're no longer contagious,\" Goss said. \"They can go about their lives, they don't have to stay away from people, and they can go back to work, do the things, as long as they continue to take their meds.\"</p><p>The course of treatment is several months long for active and latent cases.</p><p>\"We still have a couple of fairly large employers that are involved that we're working with on this,\" Goss said. \"So we do expect to find more, but we're hoping the more that we find is latent TB not active, so that their lives are not disrupted and having to stay home from work. Because it is highly contagious.\"</p><p><em>(This story was updated because an earlier version included an inaccuracy.)</em></p><p><em>Jason Alatidd is a Statehouse reporter for The Topeka Capital-Journal. He can be reached by email at jalatidd@gannett.com. Follow him on X&nbsp;</em><a href=\"https://twitter.com/Jason_Alatidd\" target=\"_blank\" rel=\"noreferrer noopener\" data-t-l=\":b|z|k|${u}\"></a>.</p>","contentLength":3324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42835183"},{"title":"Show HN: DeepSeek My User Agent","url":"https://www.jasonthorsness.com/20","date":1737929019,"author":"jasonthorsness","guid":183,"unread":true,"content":"<p><a href=\"https://github.com/deepseek-ai/DeepSeek-R1\">DeepSeek R1</a> is a new model and service that exposes\nchain-of-thought to the user. You can use it live for free at\n<a href=\"https://chat.deepseek.com/\">chat.deepseek.com</a>, or via an API at\n<a href=\"https://platform.deepseek.com/\">platform.deepseek.com</a> that is currently significantly less\nexpensive than OpenAI o1. OR, simply click  to see what the model thinks about your user\nagent, browser capabilities, and IP location headers. If you dare.</p>","contentLength":364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42834648"},{"title":"Astronomers delete asteroid because it turned out to be Tesla Roadster","url":"https://www.astronomy.com/science/astronomers-just-deleted-an-asteroid-because-it-turned-out-to-be-elon-musks-tesla-roadster/","date":1737925156,"author":"geox","guid":212,"unread":true,"content":"<p>On Jan. 2, the Minor Planet Center at the Harvard-Smithsonian Center for Astrophysics in Cambridge, Massachusetts, announced the <a href=\"https://minorplanetcenter.net/mpec/K25/K25A38.html\">discovery of an unusual asteroid</a>, designated 2018 CN41. First identified and submitted by a citizen scientist, the object‚Äôs orbit was notable: It came less than 150,000 miles (240,000 km) from Earth, closer than the orbit of the Moon. That qualified it as a near-Earth object (NEO) ‚Äî one worth monitoring for its potential to someday slam into Earth.</p><p>But less than 17 hours later, the Minor Planet Center (MPC) issued an <a href=\"https://minorplanetcenter.net/mpec/K25/K25A49.html\">editorial notice</a>: It was deleting 2018 CN41 from its records because, it turned out, the object was not an asteroid.</p><p>To be precise, it was <a href=\"https://en.wikipedia.org/wiki/Elon_Musk%27s_Tesla_Roadster\">Elon Musk‚Äôs Tesla Roadster</a> mounted to a Falcon Heavy upper stage, which boosted into orbit around the Sun on Feb. 6, 2018. The car ‚Äî which had been owned and driven by Musk ‚Äî was a test payload for the <a href=\"https://www.nasa.gov/history/5-years-ago-first-flight-of-the-falcon-heavy-rocket/\">Falcon Heavy‚Äôs first flight</a>. At the time, it received a great deal of notoriety as the first production car to be flung into space, complete with a suited-up mannequin in the driver‚Äôs seat named Starman.</p><p>The case of mistaken identity was resolved swiftly in a collaboration between professional and amateur astronomers. But some astronomers say it is also emblematic of a growing issue: the lack of transparency from nations and companies operating craft in deep space, beyond the orbits used by most satellites. While objects in lower Earth orbits are tracked by the U.S. Space Force, deeper space remains an unregulated frontier.</p><p>If left unchecked, astronomers say the growing number of untracked objects could hinder efforts to protect Earth from potentially hazardous asteroids. They could lead to wasted observing effort and ‚Äî if sufficiently numerous ‚Äî even throw off statistical analyses of the threat posted by near-Earth asteroids, said Center for Astrophysics (CfA) astrophysicist Jonathan McDowell in an email to . ‚ÄúWorst case, you spend a billion launching a space probe to study an asteroid and only realize it‚Äôs not an asteroid when you get there,‚Äù he said.</p><p>And it is a problem that is set to worsen as more nations and companies venture to the Moon and beyond.</p><p>The <a href=\"https://minorplanetcenter.net/\">Minor Planet Center</a> ‚Äî which operates under the auspices of the International Astronomical Union ‚Äî is the globally accepted authority on handling observations and reports of new asteroids, comets, and other small bodies in the solar system. Its responsibilities include identifying, designating, and computing their orbits.</p><p>It is also no stranger to spacecraft and discarded rocket stages masquerading as asteroids. In the 2000s, NASA‚Äôs Wilkinson Microwave Anisotropy Probe (WMAP), stationed in deep space around a million miles (1.5 million kilometers) from Earth, made it multiple times onto the MPC‚Äôs Near-Earth Object Confirmation Page (NEOCP), <a href=\"https://www.minorplanetcenter.net/iau/NEO_dev/toconfirm_tabular.html\">a list of NEOs pending confirmation</a>. And in 2007, the MPC had to retire the asteroid designation 2007 VN84 when the object was discovered to be the Rosetta spacecraft ‚Äî a high-profile European mission then performing a flyby of Earth en route to make the first ever landing on a comet.</p><p>‚ÄúThis incident, along with previous NEOCP postings of the WMAP spacecraft, highlights the deplorable state of availability of positional information on distant artificial objects,‚Äù the MPC fumed when it <a href=\"https://www.minorplanetcenter.net/mpec/K07/K07V70.html\">retracted 2007 VN84</a>. ‚ÄúA single source for information on all distant artificial objects would be very desirable.‚Äù</p><p>That central repository has yet to manifest itself. And the rise in space launches coupled with advances in telescope surveys means the MPC is seeing an uptick in reports of artificial objects, said the center‚Äôs director, Matthew Payne, in an email.</p><p>These include defunct craft and rocket boosters as well as operational space missions. Spacecraft that are swinging by Earth for a gravity assist (like Rosetta) to more distant locales are particularly prone to being misidentified as near-Earth asteroids. So are spacecraft stationed at the L2 Lagrange point of gravitational stability beyond the Moon, like WMAP.</p><p>Over the course of 2020 through 2022, at least four spacecraft were added to the MPC‚Äôs asteroid record books ‚Äî and quickly deleted. They include the European-Japanese <a href=\"https://minorplanetcenter.net/mpec/K20/K20G97.html\">BepiColombo mission</a> (in transit to Mercury), NASA‚Äôs <a href=\"https://minorplanetcenter.net/mpec/K20/K20G97.html\">Lucy mission</a> (headed to the Trojan asteroids in Jupiter‚Äôs orbit), the <a href=\"https://www.minorplanetcenter.net/mpec/K20/K20X56.html\">Spektr-RG</a> X-ray observatory at L2, and what is thought to be the <a href=\"https://minorplanetcenter.net/mpec/K21/K21D62.html\">Centaur upper rocket stage</a> for the 1966 Surveyor 2 lunar probe.</p><p>Closer to Earth, spacecraft are monitored and tracked with much more more scrutiny. Satellites in Earth orbit are regulated by national and international agencies, like the U.S. Federal Communications Commission. Companies also routinely publish orbit information for their own satellites, traditionally in a format known as two-line elements (TLEs). These data are collated by the U.S. Space Force, which also performs its own radar tracking observations and issues alerts to operators when two satellites are at risk of colliding so that they can take avoiding actions. Sharing positions and trajectories is generally in companies‚Äô best interest as it protects their own assets from collisions and helps prevents destructive clouds of debris that could, <a href=\"https://www.astronomy.com/science/study-satellite-traffic-jam-calls-for-urgent-changes/\">in a worst-case scenario</a>, render near-Earth space unusable.</p><p>But the situation is different in deep space, which is filled with a growing fleet of spacecraft at the Moon, in orbit around the Sun, and at associated Lagrange points of gravitational stability. Because of the Tesla Roadster‚Äôs fame, it happens to be included in a database maintained by NASA‚Äôs Jet Propulsion Lab called <a href=\"https://ssd.jpl.nasa.gov/horizons/\">Horizons</a>, which computes orbits for natural bodies in the solar system. But disclosing artificial bodies‚Äô trajectories in deep space is not a standard industry practice.</p><p>Deep space is ‚Äúlargely unregulated,‚Äù McDowell told a special-session audience Jan. 14 at the American Astronomical Society‚Äôs (AAS) winter meeting in National Harbor, Maryland. ‚ÄúThere‚Äôs no requirement to file some kind of public flight plan, no equivalent of the TLEs or the corporate data that we get for low-orbit satellites.‚Äù</p><p>McDowell has also been critical of the asteroid mining startup <a href=\"https://www.astroforge.com/\">AstroForge</a>, which plans to launch two probes this year, ridesharing on the Intuitive Machines IM-2 and IM-3 missions. The craft will visit a target asteroid, prospecting for valuable platinum group metals that the company hopes to one day mine. But in order to avoid giving competitors a chance to get there first, the company does not intend to disclose which asteroid it is going to. ‚ÄúThat‚Äôs kind of not OK,‚Äù said McDowell dryly at the AAS meeting.</p><p>Last September, the AAS raised the issue of deep-space transparency <a href=\"https://compasse.aas.org/aas-releases-a-compasse-led-statement-on-transparency-in-cislunar-and-interplanetary-spaceflight-activities/\">in a statement</a> led by its Committee for the Protection of Astronomy and the Space Environment (of which McDowell is a member). It called on U.S. space operators ‚Äî government agencies and non-governmental alike ‚Äî to publicly report and update trajectories of deep-space objects. It also urged operators to place those data in a public repository like JPL‚Äôs Horizons, echoing the call from the MPC 17 years earlier.</p><p>AstroForge says it will be transparent about aspects of its target asteroid ‚Äî other than its identity ‚Äî including releasing images of it. The company‚Äôs co-founder and CEO Matt Gialich told  that Astroforge has not yet settled on a target asteroid because ‚Äúas a ride share customer, we don‚Äôt control our launch date.‚Äù He added, ‚ÄúJonathan McDowell is someone I respect, and I love the pushback. It‚Äôs what science is built on. I hope that images and information we deliver outweigh the perceived negatives in this case.‚Äù</p><p>At the time of publication, SpaceX had not responded to a query from .</p><h2>‚ÄòA rare confluence of factors‚Äô</h2><p>The Tesla Roadster mix-up came as something of a disappointment to the Turkish amateur astronomer, who asked to be identified as ‚ÄúG.‚Äù He hoped he had discovered a near-Earth asteroid, not a used car from 2010 with a few billion miles on it.</p><p>He identified (the object briefly known as) 2018 CN41 with software he wrote in his spare time to parse through the MPC‚Äôs public archive of observations of objects, which anyone can peruse in search of asteroids and other small solar system bodies. His code identified several candidate objects that could be traced through multiple observations from various telescopes around the world. 2018 CN41 was one of them. It had shown up in images taken by the Catalina Sky Survey at Steward Observatory near Tucson, Arizona, and the Pan-STARRS and ATLAS surveys in Hawaii, among others.</p><p>After G. calculated an orbit to fit the observations, he saw that the object had a very small minimum orbital intersection distance (MOID) from Earth. In other words, its orbit came very close to Earth‚Äôs, making it a potential near-Earth object. ‚ÄúI was ecstatic and submitted the identification‚Äù to the MPC, he told  in an email. The MPC accepted the submission and notified the astronomical community in what it calls an ‚Äú<a href=\"https://minorplanetcenter.net/mpec/K25/K25A38.html\">electronic circular</a>,‚Äù a term of art that reflects the long legacy of observational tradition.</p><p>But after seeing the object‚Äôs trajectory plotted in 3D on the MPC‚Äôs website, he began to harbor doubts about its origins. He realized the orbit resembled that of a spacecraft traveling to Mars, using a <a href=\"https://en.wikipedia.org/wiki/Hohmann_transfer_orbit\">Hohmanm transfer orbit</a>, with the exception that it slightly overshoots Mars‚Äô orbit. (He credits, only half-jokingly, his time playing the spaceflight simulation video game Kerbal Space Program.)</p><blockquote><p>I first went to JPL‚Äôs Small Body Database to quickly take a look at the Earth close approach dates and potential Mars close approach dates, to see if I could correlate those to a known interplanetary mission. I failed ‚Äî the Falcon launch had never crossed my mind. I almost concluded it was an actual NEO and stopped looking, but I asked around on the Minor Planet Mailing List just to erase my final doubts. To my surprise, Jonathan McDowell quickly figured out it was the Falcon upper stage. Being slightly embarrassed that I might have caused unnecessary excitement (it WAS quite a low MOID), I quickly went to MPC‚Äôs help desk and let them know the NEO I just submitted was a rocket stage.</p></blockquote><p>The MPC has multiple checks to flag artificial objects, said Payne, the center director, all of which broke down on the Tesla Roadster. ‚ÄúThis case highlights a rare confluence of factors,‚Äù he said.</p><p>First, the MPC uses a routine called <a href=\"https://github.com/Bill-Gray/sat_code/blob/master/sat_id.cpp\">sat_id</a>, written by Bill Gray and commonly used by the minor-planet community, to see if an observation of an object matches the position of a known satellite on the sky. The database of satellites it checks against is maintained by the research community of both professional and amateur astronomers.</p><p>Payne noted that when the Tesla Roadster was originally launched in 2018, the community caught it and flagged it as an artificial object, and the MPC ‚Äúcorrectly labeled it as such without assigning a minor planet designation.‚Äù</p><p>But when subsequent observations were archived by the MPC and later identified by G., sat_id failed to locate the Roadster, said Payne. And the object was not caught upon further review because unlike most satellites, it orbits the Sun and not Earth. In addition, it is an unusual Sun-centric orbit for a spacecraft. Because it was a test flight for the Falcon Heavy, there was no destination in particular; that is why its trajectory originates near Earth but overshoots Mars‚Äô orbit, as G. noted.</p><p>Payne agreed that a central repository, ‚Äúregularly updated by national and private space agencies, would significantly enhance the identification process.‚Äù Currently, he said, the MPC is collaborating with JPL on a system to better detect artificial objects that aren‚Äôt in Earth orbit and filter them out of the MPC‚Äôs observational database.</p><h2>Citizen science remains key</h2><p>In one sense, this case shows the scientific process at work. Mistakes are inevitable, but quick corrections mean science is working as it should.</p><p>It also highlights the crucial role that amateur astronomers play in making discoveries ‚Äî a role they have played for centuries, well before the term ‚Äúcitizen scientists‚Äù came into vogue. ‚ÄúTheir involvement significantly improves the overall efficiency of object identification and contributes to the broader mission of the MPC,‚Äù said Payne.</p><p>G. is able to see the bright side of what he calls ‚Äúthe Tesla incident.‚Äù</p><p>‚ÄúI‚Äôm still sort of disappointed it wasn‚Äôt a NEO, but it was an interesting experience to say the least,‚Äù he said. ‚ÄúAt the very least we managed to filter out some non-minor-planet observations from [the] MPC database.‚Äù</p><p>G. continues to hunt for small bodies in the solar system on his own and in citizen science projects like <a href=\"https://web-coias.u-aizu.ac.jp/\">Come On! Impacting ASteroids</a> (COIAS). Developed by a team of Japanese astronomers, COIAS allows anyone to scour observations taken by the Subaru Telescope on Maunakea in Hawaii for asteroids, comets, and trans-Neptunian objects and report their measurements to the MPC.</p><p>Through COIAS, G. has been a co-discoverer of two named asteroids: 697402 Ao and 718492 Quro. The asteroids are named for one of the main characters and the author, respectively, of a slice-of-life manga named  (also adapted as an anime), about two high school friends who join their school‚Äôs Earth sciences club and dream of discovering an asteroid. G. said that while he didn‚Äôt know much about it before, he ‚Äúloved people who were fans of the manga get crazy about it on social media.‚Äù</p><p>Recently on COIAS, G. came across a small, ‚Äúbarely noticeable‚Äù speck of light moving slowly across the sky. According to his measurements, it appears to be a small body in the outer solar system that crosses Neptune‚Äôs orbit. He identified the measurements and submitted them to the MPC. On Jan. 18, <a href=\"https://x.com/Astro_Guler/status/1880713913656119689\">he posted about it on X</a>, the social media platform now owned by Musk, noting that the object‚Äôs orbit takes it within half an astronomical unit ‚Äî the average Earth-Sun distance ‚Äî from Neptune. If confirmed, the object would be a member of a dynamically intriguing subset of trans-Neptunian objects, one that has recently been studied for clues to the <a href=\"https://doi.org/10.3847/2041-8213/ad3cd2\">whereabouts of the theorized Planet Nine</a>.</p><p>Of course, G. has his sights set on even rarer observational feats. In an email, he wrote: ‚ÄúI‚Äôm thinking the holy grail could be a beautiful comet, an interstellar visitor, or an alien spacecraft like in [Arthur C.] Clarke‚Äôs book , heh üôÇ None of that might happen, but that won‚Äôt stop me from dreaming about it.</p><p>‚ÄúRealistically, at this point in time I will settle for anything that‚Äôs not a car.‚Äù</p><p><strong>Editor‚Äôs note (Jan. 25, 2025):</strong> At the request of the amateur astronomer who identified the object, the story has been updated to remove his first initials and last name.</p>","contentLength":14844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42834043"},{"title":"OpenJazz is a free, open-source version of the classic Jazz Jackrabbit games","url":"https://alister.eu/jazz/oj/about.php","date":1737913211,"author":"doener","guid":211,"unread":true,"content":"OpenJazz is a free, open-source version of the classic Jazz Jackrabbit‚Ñ¢ games.\n     OpenJazz can be compiled on a wide range of operating systems, including Windows\n     98/Me/XP and Linux.\n     To play, you will need the files from one of the original games.\n     <p>With the demise of DOS-based operating systems, it became necessary\n     to use emulators to play old DOS games.\n     Jazz Jackrabbit‚Ñ¢ deserves more - and would benefit greatly from new features.\n     </p><p>OpenJazz was started on the 23rd of August, 2005, by </p><a href=\"https://alister.eu/\">AJ Thomson</a>.\n     Academic pressures put the project on hold until late December 2005. The\n     source code was released on the 25th, and the first version with a degree of playability\n     was released on the 15th of January. Since then, a variety of ports have been released\n     by other people.\n     <p>In 2009, a multiplayer version was released.</p>","contentLength":870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42831927"},{"title":"Qwen2.5-1M: Deploy your own Qwen with context length up to 1M tokens","url":"https://qwenlm.github.io/blog/qwen2.5-1m/","date":1737912255,"author":"meetpateltech","guid":210,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42831769"},{"title":"No one is disrupting banks ‚Äì at least not the big ones","url":"https://www.popularfintech.com/p/no-one-is-disrupting-banks","date":1737900315,"author":"kazanins","guid":208,"unread":true,"content":"<p><strong>despite almost 30 years of trying, Fintech companies have not disrupted banks</strong></p><p><strong>they hardly challenged the key banking model - taking deposits and issuing loans</strong></p><p><strong>banks and Fintech companies will need to learn how to co-exist</strong></p><p><em><a href=\"https://x.com/jevgenijs\" rel=\"\">on X/Twitter</a></em></p><p><strong>The money center banks survived the ‚Äúdeposit flight‚Äù</strong></p><blockquote><p><em><strong>we expect to see a more visible growth trend</strong><strong>we can already see that trend in consumer checking deposits</strong></em></p><p><em>Jeremy Barnum, JPMorgan Q4 2024 earnings call</em></p></blockquote><p><strong>a perfect time for Fintech companies to get deposits</strong></p><p><strong>high-yield savings accounts to make people move their money from the incumbents</strong></p><p><strong>These customers want to bank with us now</strong></p><p><strong>for how long will Wells Fargo keep paying 4% to Cash App customers?</strong></p><blockquote><p><em><strong>these are the most valuable deposits in the franchise</strong></em></p><p><em>Alastair Borthwick, Bank of America Q4 2024 earnings call</em></p></blockquote><p><strong>At the end of Q3 2024, LendingClub had $9.5 billion in deposits, and SoFi had $24.4 billion</strong></p><p><strong>Fintechs haven‚Äôt managed to challenge banks in lending. </strong></p><p><strong>Affirm quickly got to 1.4 million Affirm Card cardholders</strong></p><p><strong>Robinhood claims that 2 million people are on the waitlist</strong></p><blockquote><p><em><strong>Robinhood Gold credit card crossed 2 million on the waitlist and is adding roughly 200,000 waitlist sign-ups per month</strong></em></p><p><em>Vlad Tenev, Robinhood 2024 Investor Day</em></p></blockquote><p><strong>in 2024, Amex opened 13 million new card accounts, Chase opened 10 million</strong></p><p><strong>JPMorgan finished 2024 with $233 billion in credit card loans</strong></p><blockquote><p><em><strong>We expect healthy card loan growth again this year, but below the 12% pace we saw in 2024</strong></em></p><p><em>Jeremy Barnum, JPMorgan Q4 2024 earnings call</em></p></blockquote><p><strong>last few years were ideal for building a credit card business as loan balances exploded</strong></p><blockquote><p><em><strong>a Fintech perspective on the consumer side, we really have not seen anything. Not that we don't look at it, not that we're not aware of it.</strong></em></p><p><em>Stephen Squeri, American Express Q4 2024 earnings call</em></p></blockquote><p><strong>Banks might be losing (or have already lost?) payment acceptance business to Fintech companies. </strong></p><blockquote><p><em><strong>have you considered about whether you should get rid of this business and deploy the capital to other areas where you're in a much stronger position</strong></em></p><p><em>Analyst‚Äôs question on U.S. Bank Q4 2024 earnings call</em></p></blockquote><p><strong>be losing the cross-border payments business</strong></p><p><strong>Ramp and Brex are certainly becoming a force in commercial cards</strong></p><blockquote><p><em><strong>we keep our eye on Ramp, Brex</strong><strong>they have good products, and they're making some inroads</strong></em></p><p><em>Stephen Squeri, American Express, Q4 2024 earnings call</em></p></blockquote><p><strong>‚Ä¶but (and that‚Äôs an important but!) big banks have figured out mobile too. </strong></p><p><strong>JPMorgan Chase reported 58 million active mobile users</strong></p><p><strong>threw billions on catching up with Fintech companies</strong><strong>still delivering crazy high profitability. </strong></p><blockquote><p><em><strong>we had probably reached peak modernization spend</strong><strong>to focus on features and new product development</strong></em></p><p><em>Jeremy Barnum, JPMorgan Q4 2024 earnings call</em></p></blockquote><p><strong>Fintech companies might be disrupting community banks, but was that the ambition? </strong></p><p><strong>a dozen or so largest banks (with $250+ in assets) generate 60% of the industry‚Äôs profit</strong></p><p><strong>banks and Fintech companies will need to learn how to co-exist</strong></p><div data-attrs=\"{&quot;url&quot;:&quot;https://www.popularfintech.com/p/no-one-is-disrupting-banks?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>Thanks for reading Popular Fintech! This post is public so feel free to share it.</p></div></div><p><em>Disclaimer: Information contained in this newsletter is intended for educational and informational purposes only and should not be considered financial advice. You should do your own research or seek professional advice before making any investment decisions.</em></p>","contentLength":3243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42830155"},{"title":"Show HN: Bagels ‚Äì TUI expense tracker","url":"https://github.com/EnhancedJax/Bagels","date":1737881871,"author":"EnhancedJax","guid":181,"unread":true,"content":"<p>Hi! I'm Jax and I've been building this cool little terminal app for myself to track my expenses and budgets!</p><p>Other than challenging myself to learn Python, I built this mainly around the habit of budget tracking at the end of the day. (I tried tracking on-the-go, but the balance was always out of sync.) All data is stored in a single sqlite file, so you can export and process them all you want!</p><p>The app is built using the textual API for Python! Awesome framework which feels like I'm doing webdev haha.</p>","contentLength":505,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42828833"},{"title":"The impact of competition and DeepSeek on Nvidia","url":"https://youtubetranscriptoptimizer.com/blog/05_the_short_case_for_nvda","date":1737819025,"author":"eigenvalue","guid":207,"unread":true,"content":"<p>As someone who spent ~10 years working as a generalist investment analyst at various long/short hedge funds (including stints at Millennium and Balyasny), while also being something of a math and computer nerd who has been studying deep learning since 2010 (back when Geoff Hinton was still talking about <a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">Restricted Boltzmann Machines</a> and everything was still programmed using <a href=\"https://en.wikipedia.org/wiki/MATLAB\">MATLAB</a>, and researchers were still trying to show that they could get better results at classifying handwritten digits than by using <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\">Support Vector Machines</a>), I'd like to think that I have a fairly unusual perspective on how AI technology is developing and how this relates to equity valuations in the stock market.</p><p>For the past few years, I have been working more as a developer, and have several popular open-source projects for working with various forms of AI models/services (e.g., see <a href=\"https://github.com/Dicklesworthstone/llm_aided_ocr\">LLM Aided OCR</a>, <a href=\"https://github.com/Dicklesworthstone/swiss_army_llama\">Swiss Army Llama</a>, <a href=\"https://github.com/Dicklesworthstone/fast_vector_similarity\">Fast Vector Similarity</a>, <a href=\"https://github.com/Dicklesworthstone/your-source-to-prompt.html\">Source to Prompt</a>, and <a href=\"https://github.com/pastelnetwork/python_inference_layer_server\">Pastel Inference Layer</a> for a few recent examples). Basically, I am using these frontier models all day, every day, in about as intense a way as possible. I have 3 Claude accounts so I don't run out of requests, and signed up for ChatGPT Pro within minutes of it being available.</p><p>I also try to keep on top of the latest research advances, and carefully read all the major technical report papers that come out from the major AI labs. So I think I have a pretty good read on the space and how things are developing. At the same time, I've shorted a ton of stocks in my life and have won the best idea prize on the Value Investors Club twice (for <a href=\"https://valueinvestorsclub.com/idea/TMS_INTERNATIONAL_CORP/5881178219\">TMS long</a> and <a href=\"https://valueinvestorsclub.com/idea/PETROLOGISTICS_LP/5967783148\">PDH short</a> if you're keeping track at home).</p><p>I say this not to brag, but rather to help establish my bona fides as someone who could opine on the subject without coming across as hopelessly naive to either technologists or professional investors. And while there are surely many people who know the math/science better, and people who are better at long/short investing in the stock market than me, I doubt there are very many who are in the middle of the Venn diagram to the extent I can claim to be.</p><p>With all that said, whenever I meet with and chat with my friends and ex colleagues from the hedge fund world, the conversation quickly turns to Nvidia. It's not every day that a company goes from relative obscurity to being worth more than the combined stock markets of England, France, or Germany! And naturally, these friends want to know my thoughts on the subject. Because I am such a dyed-in-the-wool believer in the long term transformative impact of this technology‚Äî I truly believe it's going to radically change nearly every aspect of our economy and society in the next 5-10 years, with basically no historical precedent‚Äî it has been hard for me to make the argument that Nvidia's momentum is going to slow down or stop anytime soon.</p><p>But even though I've thought the valuation was just too rich for my blood for the past year or so, a confluence of recent developments has caused me to flip a bit to my usual instinct, which is to be a bit more contrarian in outlook and to question the consensus when it seems to be more than priced in. The saying \"what the wise man believes in the beginning, the fool believes in the end\" became famous for a good reason.</p><p>Before we get into the developments that give me pause, let's pause to briefly review the bull case for NVDA shares, which is basically now known by everyone and his brother. Deep learning and AI are the most transformative technologies since the internet, and poised to change basically everything in our society. Nvidia has somehow ended up with something close to a monopoly in terms of the share of aggregate industry capex that is spent on training and inference infrastructure.</p><p>Some of the largest and most profitable companies in the world, like Microsoft, Apple, Amazon, Meta, Google, Oracle, etc., have all decided that they must do and spend whatever it takes to stay competitive in this space because they simply cannot afford to be left behind. The amount of capex dollars, gigawatts of electricity used, square footage of new-build data centers, and, of course, the number of GPUs, has absolutely exploded and seems to show no sign of slowing down. And Nvidia is able to earn insanely high 90%+ gross margins on the most high-end, datacenter oriented products.</p><p>We've just scratched the surface here of the bull case. There are many additional aspects to it now, which have made even people who were already very bullish to become incrementally more bullish. Besides things like the rise of humanoid robots, which I suspect is going to take most people by surprise when they are rapidly able to perform a huge number of tasks that currently require an unskilled (or even skilled) human worker (e.g., doing laundry, cleaning, organizing, and cooking; doing construction work like renovating a bathroom or building a house in a team of workers; running a warehouse and driving forklifts, etc.), there are other factors which most people haven't even considered.</p><p>One major thing that you hear the smart crowd talking about is the rise of \"a new scaling law,\" which has created a new paradigm thinking about how compute needs will increase over time. The original scaling law, which is what has been driving progress in AI since <a href=\"https://en.wikipedia.org/wiki/AlexNet\">AlexNet</a> appeared in 2012 and the Transformer architecture was invented in 2017, is the pre-training scaling law: that the more billions (and now trillions) worth of tokens we can use as training data, and the larger the parameter count of the models we are training, and the more FLOPS of compute that we expend on training those models on those tokens, the better the performance of the resulting models on a large variety of highly useful downstream tasks.</p><p>Not only that, but this improvement is somewhat knowable, to the point where the leading AI labs like OpenAI and Anthropic have a pretty good idea of just how good their latest models would be even before they started the actual training runs‚Äî in some cases, predicting the benchmarks of the final models to within a couple percentage points. This \"original scaling law\" has been vitally important, but always caused some doubts in the minds of people projecting the future with it.</p><p>For one thing, we seem to have already exhausted the world's accumulated set of high quality training data. Of course, that's not literally true‚Äî there are still so many old books and periodicals that haven't yet been properly digitized, and even if they have, are not properly licensed for use as training data. The problem is that, even if you give credit for all that stuff‚Äî say the sum total of \"professionally\" produced English language written content from the year 1500 to, say, the year 2000, it's not such a tremendous amount in percentage terms when you're talking about a training corpus of nearly 15 trillion tokens, which is the scale of current frontier models.</p><p>For a quick reality check of those numbers: Google Books has digitized around 40mm books so far; if a typical book has 50k to 100k words, or 65k to 130k tokens, then that's between 2.6T and 5.2T tokens just from books, though surely a large chunk of that is already included in the training corpora used by the big labs, whether it's strictly legal or not. And there are lots of academic papers, with the arXiv website alone having over 2mm papers. And the Library of Congress has over 3 billion digitized newspaper pages. Taken together, that could be as much as 7T tokens in total, but since much of this is in fact included in training corpora, the remaining \"incremental\" training data probably isn't all that significant in the grand scheme of things.</p><p>Of course, there are other ways to gather more training data. You could automatically transcribe every single YouTube video for example, and use that text. And while that might be helpful on the margin, it's certainly of much lower quality than, say, a highly respected textbook on Organic Chemistry as a source of useful knowledge about the world. So we've always had a looming \"data wall\" when it comes to the original scaling law; although we know we can keep shoveling more and more capex into GPUs and building more and more data centers, it's a lot harder to mass produce useful new human knowledge which is correct and incremental to what is already out there. Now, one intriguing response to this has been the rise of \"synthetic data,\" which is text that is itself the output of an LLM. And while this seems almost nonsensical that it would work to \"get high on your own supply\" as a way of improving model quality, it actually seems to work very well in practice, at least in the domain of math, logic, and computer programming.</p><p>The reason, of course, is that these are areas where we can mechanically check and prove the correctness of things. So we can sample from the vast universe of possible math theorems or possible Python scripts, and then actually check if they are correct, and only include them in our corpus if they are. And in this way, we can very dramatically expand our collection of high quality training data, at least in these kinds of areas.</p><p>And then there are all the other kinds of data we could be training AI on besides text. For example, what if we take the entire whole genome sequencing (around 200 GB to 300 GB uncompressed for a single human being) for 100 million people? That's a  of data obviously, although the vast majority of it would be nearly identical between any two people. Of course, this could be misleading to compare to textual data from books and the internet for various reasons:</p><ul><li>Raw genome size isn't directly comparable to token counts</li><li>The information content of genomic data is very different from text</li><li>The training value of highly redundant data isn't clear</li><li>The computational requirements for processing genomic data are different</li></ul><p>But it's still another large source of diverse information that we could train huge models on in the future, which is why I included it.</p><p>So while there is some hope in terms of being able to capture more and more additional training data, if you look at the rate at which training corpora have grown in recent years, it quickly becomes obvious that we are close to hitting a wall in terms of data availability for \"generally useful\" knowledge that can get us closer to the ultimate goal of getting artificial super-intelligence which is 10x smarter than John von Neumann and is an absolute world-class expert on every specialty known to man.</p><p>Besides the limited amount of available data, there have always been a couple other things that have lurked in the back of the mind of proponents of the pre-training scaling law. A big one of these is, after you've finished training the model, what are you supposed to do with all that compute infrastructure? Train the next model? Sure, you can do that, but given the rapid improvement in GPU speed and capacity, and the importance of electricity and other opex in the economic calculations, does it even really make sense to use your 2 year old cluster to train your new model? Surely you'd rather use the brand new data center you just built that costs 10x the old data center and is 20x more powerful because of better technology. The problem is, at some point you do need to amortize the up-front cost of these investments and recoup it with a stream of (hopefully positive) operating profit, right?</p><p>The market is so excited about AI that it has thankfully ignored this, allowing companies like OpenAI to post breathtaking from-inception, cumulative operating losses while garnering increasingly eye-popping valuations in follow-up investment rounds (although, to their credit, they have also been able to demonstrate very fast growing revenues). But eventually, for this situation to be sustainable over a full market cycle, these data center costs do need to eventually be recouped, hopefully with a profit, which over time is competitive with other investment opportunities on a risk-adjusted basis.</p><p>OK, so that was the pre-training scaling law. What's this \"new\" scaling law? Well, that's something that people really just started focusing on in the past year: inference time compute scaling. Before, the vast majority of all the compute you'd expend in the process was the up-front training compute to create the model in the first place. Once you had the trained model, performing inference on that model‚Äî i.e., asking a question or having the LLM perform some kind of task for you‚Äî used a certain, limited amount of compute.</p><p>Critically, the total amount of inference compute (measured in various ways, such as FLOPS, in GPU memory footprint, etc.) was much, much less than what was required for the pre-training phase. Of course, the amount of inference compute does flex up when you increase the context window size of the models and the amount of output that you generate from them in one go (although researchers have made breathtaking algorithmic improvements on this front relative to the initial quadratic scaling people originally expected in scaling this up). But essentially, until recently, inference compute was generally a lot less intensive than training compute, and scaled basically linearly with the number of requests you are handling‚Äî the more demand for text completions from ChatGPT, for instance, the more inference compute you used up.</p><p>With the advent of the revolutionary Chain-of-Thought (\"COT\") models introduced in the past year, most noticeably in OpenAI's flagship O1 model (but very recently in DeepSeek's new R1 model, which we will talk about later in much more detail), all that changed. Instead of the amount of inference compute being directly proportional to the length of the output text generated by the model (scaling up for larger context windows, model size, etc.), these new COT models also generate intermediate \"logic tokens\"; think of this as a sort of scratchpad or \"internal monologue\" of the model while it's trying to solve your problem or complete its assigned task.</p><p>This represents a true sea change in how inference compute works: now, the more tokens you use for this internal chain of thought process, the better the quality of the final output you can provide the user. In effect, it's like giving a human worker more time and resources to accomplish a task, so they can double and triple check their work, do the same basic task in multiple different ways and verify that they come out the same way; take the result they came up with and \"plug it in\" to the formula to check that it actually does solve the equation, etc.</p><p>It turns out that this approach works almost amazingly well; it is essentially leveraging the long anticipated power of what is called \"reinforcement learning\" with the power of the Transformer architecture. It directly addresses the single biggest weakness of the otherwise phenomenally successful Transformer model, which is its propensity to \"hallucinate\".</p><p>Basically, the way Transformers work in terms of predicting the next token at each step is that, if they start out on a bad \"path\" in their initial response, they become almost like a prevaricating child who tries to spin a yarn about why they are actually correct, even if they should have realized mid-stream using common sense that what they are saying couldn't possibly be correct.</p><p>Because the models are always seeking to be internally consistent and to have each successive generated token flow naturally from the preceding tokens and context, it's very hard for them to course-correct and backtrack. By breaking the inference process into what is effectively many intermediate stages, they can try lots of different things and see what's working and keep trying to course-correct and try other approaches until they can reach a fairly high threshold of confidence that they aren't talking nonsense.</p><p>Perhaps the most extraordinary thing about this approach, beyond the fact that it works at all, is that the more logic/COT tokens you use, the better it works. Suddenly, you now have an additional dial you can turn so that, as you increase the amount of COT reasoning tokens (which uses a lot more inference compute, both in terms of FLOPS and memory), the higher the probability is that you will give a correct response‚Äî code that runs the first time without errors, or a solution to a logic problem without an obviously wrong deductive step.</p><p>I can tell you from a lot of firsthand experience that, as good as Anthropic's Claude3.5 Sonnet model is at Python programming‚Äî and it is indeed VERY good‚Äî whenever you need to generate anything long and complicated, it invariably ends up making one or more stupid mistakes. Now, these mistakes are usually pretty easy to fix, and in fact you can normally fix them by simply feeding the errors generated by the Python interpreter, without any further explanation, as a follow-up inference prompt (or, more usefully, paste in the complete set of detected \"problems\" found in the code by your code editor, using what something called a Linter), it was still an annoying additional step. And when the code becomes very long or very complicated, it can sometimes take a lot longer to fix, and might even require some manual debugging by hand.</p><p>The first time I tried the O1 model from OpenAI was like a revelation: I was amazed how often the code would be perfect the very first time. And that's because the COT process automatically finds and fixes problems before they ever make it to a final response token in the answer the model gives you.</p><p>In fact, the O1 model used in OpenAI's ChatGPT Plus subscription for $20/month is basically the same model as the one used in the O1-Pro model featured in their new ChatGPT Pro subscription for 10x the price ($200/month, which raised plenty of eyebrows in the developer community); the main difference is that O1-Pro thinks for a lot longer before responding, generating vastly more COT logic tokens, and consuming a far larger amount of inference compute for every response.</p><p>This is quite striking in that, even a very long and complex prompt for Claude3.5 Sonnet or GPT4o, with ~400kb+ of context given, generally takes less than 10 seconds to begin responding, and often less than 5 seconds. Whereas that same prompt to O1-Pro could easily take 5+ MINUTES before you get a response (although OpenAI does show you some of the \"reasoning steps\" that are generated during the process while you wait; critically, OpenAI has decided, presumably for trade secret related reasons,to hide from you the exact reasoning tokens it generates, showing you instead a highly abbreviated summary of these).</p><p>As you can probably imagine, there are tons of contexts where accuracy is paramount‚Äî where you'd rather give up and tell the user you can't do it at all rather than give an answer that could be trivially proven wrong or which involves hallucinated facts or otherwise specious reasoning. Anything involving money/transactions, medical stuff, legal stuff, just to name a few.</p><p>Basically, wherever the cost of inference is trivial relative to the hourly all-in compensation of the human knowledge worker who is interacting with the AI system, that's a case where it become a complete no-brainer to dial up the COT compute (the major drawback is that it increases the latency of responses by a lot, so there are still some contexts where you might prefer to iterate faster by getting lower latency responses that are less accurate or correct).</p><p>Some of the most exciting news in the AI world came out just a few weeks ago and concerned OpenAI's new unreleased O3 model, which was able to solve a large variety of tasks that were previously deemed to be out of reach of current AI approaches in the near term. And the way it was able to do these hardest problems (which include exceptionally tough \"foundational\" math problems that would be very hard for even highly skilled professional mathematicians to solve), is that OpenAI threw insane amount of compute resources at the problems‚Äî in some cases, spending $3k+ worth of compute power to solve a single task (compare this to traditional inference costs for a single task, which would be unlikely to exceed a couple dollars using regular Transformer models without chain-of-thought).</p><p>It doesn't take an AI genius to realize that this development creates a new scaling law that is totally independent of the original pre-training scaling law. Now, you still want to train the best model you can by cleverly leveraging as much compute as you can and as many trillion tokens of high quality training data as possible, but that's just the beginning of the story in this new world; now, you could easily use incredibly huge amounts of compute just to do inference from these models at a very high level of confidence or when trying to solve extremely tough problems that require \"genius level\" reasoning to avoid all the potential pitfalls that would lead a regular LLM astray.</p><h2>But Why Should Nvidia Get to Capture All The Upside?</h2><p>Even if you believe, as I do, that the future prospects for AI are almost unimaginably bright, the question still remains, \"Why should one company extract the majority of the profit pool from this technology?\" There are certainly many historical cases where a very important new technology changed the world, but the main winners were not the companies that seemed the most promising during the initial stages of the process. The Wright Brothers' airplane company in all its current incarnations across many different firms today isn't worth more than $10b despite them inventing and perfecting the technology well ahead of everyone else. And while Ford has a respectable market cap of $40b today, it's just 1.1% of Nvidia's current market cap.</p><p>To understand this, it's important to really understand why Nvidia is currently capturing so much of the pie today. After all, they aren't the only company that even makes GPUs. AMD makes respectable GPUs that, on paper, have comparable numbers of transistors, which are made using similar process nodes, etc. Sure, they aren't as fast or as advanced as Nvidia's GPUs, but it's not like the Nvidia GPUs are 10x faster or anything like that. In fact, in terms of naive/raw dollars per FLOP, AMD GPUs are something like half the price of Nvidia GPUs.</p><p>Looking at other semiconductor markets such as the DRAM market, despite the fact that it is also very highly consolidated with only 3 meaningful global players (Samsung, Micron, SK-Hynix), gross margins in the DRAM market range from negative at the bottom of the cycle to ~60% at the very top of the cycle, with an average in the 20% range. Compare that to Nvidia's overall gross margin in recent quarters of ~75%, which is dragged down by the lower-margin and more commoditized consumer 3D graphics category.</p><p>So how is this possible? Well, the main reasons have to do with software‚Äî better drivers that \"just work\" on Linux and which are highly battle-tested and reliable (unlike AMD, which is notorious for the low quality and instability of their Linux drivers), and highly optimized open-source code in popular libraries such as <a href=\"https://en.wikipedia.org/wiki/PyTorch\">PyTorch</a> that has been tuned to work really well on Nvidia GPUs.</p><p>It goes beyond that though‚Äî the very programming framework that coders use to write low-level code that is optimized for GPUs, CUDA, is totally proprietary to Nvidia, and it has become a de facto standard. If you want to hire a bunch of extremely talented programmers who know how to make things go really fast on GPUs, and pay them $650k/year or whatever the going rate is for people with that particular expertise, chances are that they are going to \"think\" and work in CUDA.</p><p>Besides software superiority, the other major thing that Nvidia has going for it is what is known as interconnect‚Äî essentially, the bandwidth that connects together thousands of GPUs together efficiently so they can be jointly harnessed to train today's leading-edge foundational models. In short, the key to efficient training is to keep all the GPUs as fully utilized as possible all the time‚Äî not waiting around idling until they receive the next chunk of data they need to compute the next step of the training process.</p><p>The bandwidth requirements are extremely high‚Äî much, much higher than the typical bandwidth that is needed in traditional data center use cases. You can't really use traditional networking gear or fiber optics for this kind of interconnect, since it would introduce too much latency and wouldn't give you the pure terabytes per second of bandwidth that is needed to keep all the GPUs constantly busy.</p><p>Nvidia made an incredibly smart decision to purchase the Israeli company Mellanox back in 2019 for a mere $6.9b, and this acquisition is what provided them with their industry leading interconnect technology. Note that interconnect speed is a lot more relevant to the training process, where you have to harness together the output of thousands of GPUs at the same time, than the inference process (including COT inference), which can use just a handful of GPUs‚Äî all you need is enough VRAM to store the quantized (compressed) model weights of the already-trained model.</p><p>So those are arguably the major components of Nvidia's \"moat\" and how it has been able to maintain such high margins for so long (there is also a \"flywheel\" aspect to things, where they aggressively invest their super-normal profits into tons of R&amp;D, which in turn helps them improve their tech at a faster rate than the competition, so they are always in the lead in terms of raw performance).</p><p>But as was pointed out earlier, what customers really tend to care about, all other things being equal, is performance per dollar (both in up-front capex cost of equipment and in energy usage, so performance per watt), and even though Nvidia's GPUs are certainly the fastest, they are not the best price/performance when measured naively in terms of FLOPS.</p><p>But the thing is, all other things are NOT equal, and the fact that AMD's drivers suck, that popular AI software libraries don't run as well on AMD GPUs, that you can't find really good GPU experts who specialize in AMD GPUs outside of the gaming world (why would they bother when there is more demand in the market for CUDA experts?), that you can't wire thousands of them together as effectively because of lousy interconnect technology for AMD‚Äî all this means that AMD is basically not competitive in the high-end data center world, and doesn't seem to have very good prospects for getting there in the near term.</p><p>Well, that all sounds very bullish for Nvidia, right? Now you can see why the stock is trading at such a huge valuation! But what are the other clouds on the horizon? Well, there are few that I think merit significant attention. Some have been lurking in the background for the last few years, but too small to make a dent considering how quickly the pie has been growing, but where they are getting ready to potentially inflect upwards. Others are very recent developments (as in, the last 2 weeks) that might dramatically change the near-term trajectory of incremental GPU demand.</p><p>At a very high level, you can think of things like this: Nvidia operated in a pretty niche area for a very long time; they had very limited competition, and the competition wasn't particular profitable or growing fast enough to ever pose a real threat, since they didn't have the capital needed to really apply pressure to a market leader like Nvidia. The gaming market was large and growing, but didn't feature earth shattering margins or particularly fabulous year over year growth rates.</p><p>A few big tech companies started ramping up hiring and spending on machine learning and AI efforts around 2016-2017, but it was never a truly significant line item for any of them on an aggregate basis‚Äî more of a \"moonshot\" R&amp;D expenditure. But once the big AI race started in earnest with the release of ChatGPT in 2022‚Äî only a bit over 2 years ago, although it seems like a lifetime ago in terms of developments‚Äî that situation changed very dramatically.</p><p>Suddenly, big companies were ready to spend many, many billions of dollars incredibly quickly. The number of researchers showing up at the big research conferences like <a href=\"https://papers.nips.cc/\">Neurips</a> and <a href=\"https://icml.cc/\">ICML</a> went up very, very dramatically. All the smart students who might have previously studied financial derivatives were instead studying Transformers, and $1mm+ compensation packages for non-executive engineering roles (i.e., for independent contributors not managing a team) became the norm at the leading AI labs.</p><p>It takes a while to change the direction of a massive cruise ship; and even if you move really quickly and spend billions, it takes a year or more to build greenfield data centers and order all the equipment (with ballooning lead times) and get it all set up and working. It takes a long time to hire and onboard even smart coders before they can really hit their stride and familiarize themselves with the existing codebases and infrastructure.</p><p>But now, you can imagine that absolutely biblical amounts of capital, brainpower, and effort are being expended in this area. And Nvidia has the biggest target of any player on their back, because they are the ones who are making the lion's share of the profits TODAY, not in some hypothetical future where the AI runs our whole lives.</p><p>So the very high level takeaway is basically that \"markets find a way\"; they find alternative, radically innovative new approaches to building hardware that leverage completely new ideas to sidestep barriers that help prop up Nvidia's moat.</p><h2>The Hardware Level Threat</h2><p>For example, so-called \"wafer scale\" AI training chips from Cerebras, which dedicate an entire 300mm silicon wafer to an absolutely gargantuan chip that contains orders of magnitude more transistors and cores on a single chip (see this recent <a href=\"https://cerebras.ai/blog/100x-defect-tolerance-how-cerebras-solved-the-yield-problem\">blog post</a> from them explaining how they were able to solve the \"yield problem\" that had been preventing this approach from being economically practical in the past).</p><p>To put this into perspective, if you compare Cerebras' newest WSE-3 chip to Nvidia's flagship data-center GPU, the H100, the Cerebras chip has a total die area of 46,225 square millimeters compared to just 814 for the H100 (and the H100 is itself considered an enormous chip by industry standards); that's a multiple of ~57x! And instead of having 132 \"streaming multiprocessor\" cores enabled on the chip like the H100 has, the Cerebras chip has ~900,000 cores (granted, each of these cores is smaller and does a lot less, but it's still an almost unfathomably large number in comparison). In more concrete apples-to-apples terms, the Cerebras chip can do around ~32x the FLOPS in AI contexts as a single H100 chip. Since an H100 sells for close to $40k a pop, you can imagine that the WSE-3 chip isn't cheap.</p><p>So why does this all matter? Well, instead of trying to battle Nvidia head-on by using a similar approach and trying to match the Mellanox interconnect technology, Cerebras has used a radically innovative approach to do an end-run around the interconnect problem: inter-processor bandwidth becomes much less of an issue when everything is running on the same super-sized chip. You don't even need to have the same level of interconnect because one mega chip replaces tons of H100s.</p><p>And the Cerebras chips also work extremely well for AI inference tasks. In fact, you can try it today for free <a href=\"https://cloud.cerebras.ai/\">here</a> and use Meta's very respectable Llama-3.3-70B model. It responds basically instantaneously, at ~1,500 tokens per second. To put that into perspective, anything above 30 tokens per second feels relatively snappy to users based on comparisons to ChatGPT and Claude, and even 10 tokens per second is fast enough that you can basically read the response while it's being generated.</p><p>Cerebras is also not alone; there are other companies, like Groq (not to be confused with the <a href=\"https://x.ai/\">Grok</a> model family trained by Elon Musk's X AI). Groq has taken yet another innovative approach to solving the same fundamental problem. Instead of trying to compete with Nvidia's CUDA software stack directly, they've developed what they call a \"tensor processing unit\" (TPU) that is specifically designed for the exact mathematical operations that deep learning models need to perform. Their chips are designed around a concept called \"deterministic compute,\" which means that, unlike traditional GPUs where the exact timing of operations can vary, their chips execute operations in a completely predictable way every single time.</p><p>This might sound like a minor technical detail, but it actually makes a massive difference for both chip design and software development. Because the timing is completely deterministic, Groq can optimize their chips in ways that would be impossible with traditional GPU architectures. As a result, they've been demonstrating for the past 6+ months inference speeds of over 500 tokens per second with the Llama series of models and other open source models, far exceeding what's possible with traditional GPU setups. Like Cerebras, this is available today and you can try it for free <a href=\"https://console.groq.com/playground\">here</a>.</p><p>Using a comparable Llama3 model with \"speculative decoding,\" Groq is able to generate 1,320 tokens per second, on par with Cerebras and far in excess of what is possible using regular GPUs. Now, you might ask what the point is of achieving 1,000+ tokens per second when users seem pretty satisfied with ChatGPT, which is operating at less than 10% of that speed. And the thing is, it does matter. It makes it a lot faster to iterate and not lose focus as a human knowledge worker when you get instant feedback. And if you're using the model programmatically via the API, which is increasingly where much of the demand is coming from, then it can enable whole new classes of applications that require multi-stage inference (where the output of previous stages is used as input in successive stages of prompting/inference) or which require low-latency responses, such as content moderation, fraud detection, dynamic pricing, etc.</p><p>But even more fundamentally, the faster you can serve requests, the faster you can cycle things, and the busier you can keep the hardware. Although Groq's hardware is extremely expensive, clocking in at $2mm to $3mm for a single server, it ends up costing far less per request fulfilled if you have enough demand to keep the hardware busy all the time.</p><p>And like Nvidia with CUDA, a huge part of Groq's advantage comes from their own proprietary software stack. They are able to take the same open source models that other companies like Meta, DeepSeek, and Mistral develop and release for free, and decompose them in special ways that allow them to run dramatically faster on their specific hardware.</p><p>Like Cerebras, they have taken different technical decisions to optimize certain particular aspects of the process, which allows them to do things in a fundamentally different way. In Groq's case, it's because they are entirely focused on inference level compute, not on training: all their special sauce hardware and software only give these huge speed and efficiency advantages when doing inference on an already trained model.</p><p>But if the next big scaling law that people are excited about is for inference level compute‚Äî and if the biggest drawback of COT models is the high latency introduced by having to generate all those intermediate logic tokens before they can respond‚Äî then even a company that only does inference compute, but which does it dramatically faster and more efficiently than Nvidia can‚Äî can introduce a serious competitive threat in the coming years. At the very least, Cerebras and Groq can chip away at the lofty expectations for Nvidia's revenue growth over the next 2-3 years that are embedded in the current equity valuation.</p><p>Besides these particularly innovative, if relatively unknown, startup competitors, there is some serious competition coming from some of Nvidia's biggest customers themselves who have been making custom silicon that specifically targets AI training and inference workloads. Perhaps the best known of these is Google, which has been developing its own proprietary TPUs since 2016. Interestingly, although it briefly sold TPUs to external customers, Google has been using all its TPUs internally for the past several years, and it is already on its <a href=\"https://cloud.google.com/blog/products/compute/introducing-trillium-6th-gen-tpus\">6th generation</a> of TPU hardware.</p><p>Amazon has also been developing its own custom chips called <a href=\"https://press.aboutamazon.com/2023/11/aws-unveils-next-generation-aws-designed-chips\">Trainium2</a> and <a href=\"https://aws.amazon.com/ai/machine-learning/inferentia/\">Inferentia2</a>. And while Amazon is building out data centers featuring billions of dollars of Nvidia GPUs, they are also at the same time investing many billions in other data centers that use these internal chips. They have one cluster that they are bringing online for Anthropic that features over 400k chips.</p><p>Amazon gets a lot of flak for totally bungling their internal AI model development, squandering massive amounts of internal compute resources on models that ultimately are not competitive, but the custom silicon is another matter. Again, they don't necessarily need their chips to be better and faster than Nvidia's. What they need is for their chips to be good enough, but build them at a breakeven gross margin instead of the ~90%+ gross margin that Nvidia earns on its H100 business.</p><p>OpenAI has also announced their <a href=\"https://www.theregister.com/2024/09/04/openai_ai_chips_tsmc/\">plans</a> to build custom chips, and they (together with Microsoft) are obviously the single largest user of Nvidia's data center hardware. As if that weren't enough, Microsoft have themselves announced their <a href=\"https://azure.microsoft.com/en-us/blog/azure-maia-for-the-era-of-ai-from-silicon-to-software-to-systems/\">own</a> custom chips!</p><p>And Apple, the most valuable technology company in the world, has been blowing away expectations for years now with their highly innovative and disruptive custom silicon operation, which now completely trounces the CPUs from both Intel and AMD in terms of performance per watt, which is the most important factor in mobile (phone/tablet/laptop) applications. And they have been making their own internally designed GPUs and \"Neural Processors\" for years, even though they have yet to really demonstrate the utility of such chips outside of their own custom applications, like the advanced software based image processing used in the iPhone's camera.</p><p>While Apple's focus seems somewhat orthogonal to these other players in terms of its mobile-first, consumer oriented, \"edge compute\" focus, if it ends up spending enough money on its new contract with OpenAI to provide AI services to iPhone users, you have to imagine that they have teams looking into making their own custom silicon for inference/training (although given their secrecy, you might never even know about it directly!).</p><p>Now, it's no secret that there is a strong power law distribution of Nvidia's hyper-scaler customer base, with the top handful of customers representing the lion's share of high-margin revenue. How should one think about the future of this business when literally every single one of these VIP customers is building their own custom chips specifically for AI training and inference?</p><p>When thinking about all this, you should keep one incredibly important thing in mind: Nvidia is largely an IP based company. They don't make their own chips. The true special sauce for making these incredible devices arguably comes more from TSMC, the actual fab, and ASML, which makes the special EUV lithography machines used by TSMC to make these leading-edge process node chips. And that's critically important, because TSMC will sell their most advanced chips to anyone who comes to them with enough up-front investment and is willing to guarantee a certain amount of volume. They don't care if it's for Bitcoin mining ASICs, GPUs, TPUs, mobile phone SoCs, etc.</p><p>As much as senior chip designers at Nvidia earn per year, surely some of the best of them could be lured away by these other tech behemoths for enough cash and stock. And once they have a team and resources, they can design innovative chips (again, perhaps not even 50% as advanced as an H100, but with that Nvidia gross margin, there is plenty of room to work with) in 2 to 3 years, and thanks for TSMC, they can turn those into actual silicon using the exact same process node technology as Nvidia.</p><p>As if these looming hardware threats weren't bad enough, there are a few developments in the software world in the last couple years that, while they started out slowly, are now picking up real steam and could pose a serious threat to the software dominance of Nvidia's CUDA. The first of these is the horrible Linux drivers for AMD GPUs. Remember we talked about how AMD has inexplicably allowed these drivers to suck for years despite leaving massive amounts of money on the table?</p><p>Well, amusingly enough, the infamous hacker George Hotz (famous for jailbreaking the original iphone as a teenager, and currently the CEO of self-driving startup Comma.ai and AI computer company Tiny Corp, which also makes the open-source tinygrad AI software framework), recently announced that he was sick and tired of dealing with AMD's bad drivers, and desperately wanted to be able to to leverage the lower cost AMD GPUs in their TinyBox AI computers (which come in multiple flavors, some of which use Nvidia GPUs, and some of which use AMD GPUS).</p><p>Well, he is making his own custom drivers and software stack for AMD GPUs without any help from AMD themselves; on Jan. 15th of 2025, he <a href=\"https://x.com/__tinygrad__/status/1879615316378198516\">tweeted</a> via his company's X account that <em>\"We are one piece away from a completely sovereign stack on AMD, the RDNA3 assembler. We have our own driver, runtime, libraries, and emulator. (all in ~12,000 lines!)\"</em> Given his track record and skills, it is likely that they will have this all working in the next couple months, and this would allow for a lot of exciting possibilities of using AMD GPUs for all sorts of applications where companies currently feel compelled to pay up for Nvidia GPUs.</p><p>OK, well that's just a driver for AMD, and it's not even done yet. What else is there? Well, there are a few other areas on the software side that are a lot more impactful. For one, there is now a massive concerted effort across many large tech companies and the open source software community at large to make more generic AI software frameworks that have CUDA as just one of many \"compilation targets\".</p><p>That is, you write your software using higher-level abstractions, and the system itself can automatically turn those high-level constructs into super well-tuned low-level code that works extremely well on CUDA. But because it's done at this higher level of abstraction, it can just as easily get compiled into low-level code that works extremely well on lots of other GPUs and TPUs from a variety of providers, such as the massive number of custom chips in the pipeline from every big tech company.</p><p>The most famous examples of these frameworks are MLX (sponsored primarily by Apple), Triton (sponsored primarily by OpenAI), and JAX (developed by Google). MLX is particularly interesting because it provides a PyTorch-like API that can run efficiently on Apple Silicon, showing how these abstraction layers can enable AI workloads to run on completely different architectures. Triton, meanwhile, has become increasingly popular as it allows developers to write high-performance code that can be compiled to run on various hardware targets without having to understand the low-level details of each platform.</p><p>These frameworks allow developers to write their code once using high powered abstractions and then target tons of platforms automatically‚Äî doesn't that sound like a better way to do things, which would give you a lot more flexibility in terms of how you actually run the code?</p><p>In the 1980s, all the most popular, best selling software was written in hand-tuned assembly language. The PKZIP compression utility for example was hand crafted to maximize speed, to the point where a competently coded version written in the standard C programming language and compiled using the best available optimizing compilers at the time, would run at probably half the speed of the hand-tuned assembly code. The same is true for other popular software packages like WordStar, VisiCalc, and so on.</p><p>Over time, compilers kept getting better and better, and every time the CPU architectures changed (say, from Intel releasing the 486, then the Pentium, and so on), that hand-rolled assembler would often have to be thrown out and rewritten, something that only the smartest coders were capable of (sort of like how CUDA experts are on a different level in the job market versus a \"regular\" software developer). Eventually, things converged so that the speed benefits of hand-rolled assembly were outweighed dramatically by the flexibility of being able to write code in a high-level language like C or C++, where you rely on the compiler to make things run really optimally on the given CPU.</p><p>Nowadays, very little new code is written in assembly. I believe a similar transformation will end up happening for AI training and inference code, for similar reasons: computers are good at optimization, and flexibility and speed of development is increasingly the more important factor‚Äî especially if it also allows you to save dramatically on your hardware bill because you don't need to keep paying the \"CUDA tax\" that gives Nvidia 90%+ margins.</p><p>Yet another area where you might see things change dramatically is that CUDA might very well end up being more of a high level abstraction itself‚Äî a \"specification language\" similar to <a href=\"https://en.wikipedia.org/wiki/Verilog\">Verilog</a> (used as the industry standard to describe chip layouts) that skilled developers can use to describe high-level algorithms that involve massive parallelism (since they are already familiar with it, it's very well constructed, it's the lingua franca, etc.), but then instead of having that code compiled for use on Nvidia GPUs like you would normally do, it can instead be fed as source code into an LLM which can port it into whatever low-level code is understood by the new Cerebras chip, or the new Amazon Trainium2, or the new Google TPUv6, etc. This isn't as far off as you might think; it's probably already well within reach using OpenAI's latest O3 model, and surely will be possible generally within a year or two.</p><p>Perhaps the most shocking development which was alluded to earlier happened in the last couple of weeks. And that is the news that has totally rocked the AI world, and which has been dominating the discourse among knowledgeable people on Twitter despite its complete absence from any of the mainstream media outlets: that a small Chinese startup called DeepSeek released two new models that have basically world-competitive performance levels on par with the best models from OpenAI and Anthropic (blowing past the Meta Llama3 models and other smaller open source model players such as Mistral). These models are called <a href=\"https://api-docs.deepseek.com/news/news1226\">DeepSeek-V3</a> (basically their answer to GPT-4o and Claude3.5 Sonnet) and <a href=\"https://api-docs.deepseek.com/news/news250120\">DeepSeek-R1</a> (basically their answer to OpenAI's O1 model).</p><p>Why is this all so shocking? Well, first of all, DeepSeek is a tiny Chinese company that reportedly has under 200 employees. The story goes that they started out as a quant trading hedge fund similar to TwoSigma or RenTec, but after Xi Jinping cracked down on that space, they used their math and engineering chops to pivot into AI research. Who knows if any of that is really true or if they are merely some kind of front for the CCP or the Chinese military. But the fact remains that they have released two incredibly detailed technical reports, for <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\">DeepSeek-V3</a> and <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\">DeepSeekR1</a>.</p><p>These are heavy technical reports, and if you don't know a lot of linear algebra, you probably won't understand much. But what you should really try is to download the free DeepSeek app on the AppStore <a href=\"https://apps.apple.com/us/app/deepseek-ai-assistant/id6737597349\">here</a> and install it using a Google account to log in and give it a try (you can also install it on Android <a href=\"https://play.google.com/store/apps/details?id=com.deepseek.chat&amp;hl=en_US&amp;pli=1\">here</a>), or simply try it out on your desktop computer in the browser <a href=\"https://chat.deepseek.com/\">here</a>. Make sure to select the \"DeepThink\" option to enable chain-of-thought (the R1 model) and ask it to explain parts of the technical reports in simple terms.</p><p>This will simultaneously show you a few important things:</p><ul><li><p>One, this model is absolutely legit. There is a lot of BS that goes on with AI benchmarks, which are routinely gamed so that models appear to perform great on the benchmarks but then suck in real world tests. Google is certainly the worst offender in this regard, constantly crowing about how amazing their LLMs are, when they are so awful in any real world test that they can't even reliably accomplish the simplest possible tasks, let alone challenging coding tasks. These DeepSeek models are not like that‚Äî the responses are coherent, compelling, and absolutely on the same level as those from OpenAI and Anthropic.</p></li><li><p>Two, that DeepSeek has made profound advancements not just in model quality, but more importantly in model training and inference efficiency. By being extremely close to the hardware and by layering together a handful of distinct, very clever optimizations, DeepSeek was able to train these incredible models using GPUs in a dramatically more efficient way. By some measurements, over ~45x more efficiently than other leading-edge models. DeepSeek claims that the complete cost to train DeepSeek-V3 was just over $5mm. That is absolutely nothing by the standards of OpenAI, Anthropic, etc., which were well into the $100mm+ level for training costs for a single model as early as 2024.</p></li></ul><p>How in the world could this be possible? How could this little Chinese company completely upstage all the smartest minds at our leading AI labs, which have 100 times more resources, headcount, payroll, capital, GPUs, etc? Wasn't China supposed to be crippled by Biden's restriction on GPU exports? Well, the details are fairly technical, but we can at least describe them at a high level. It might have just turned out that the relative GPU processing poverty of DeepSeek was the critical ingredient to make them more creative and clever, necessity being the mother of invention and all.</p><p>A major innovation is their sophisticated mixed-precision training framework that lets them use 8-bit floating point numbers (FP8) throughout the entire training process. Most Western AI labs train using \"full precision\" 32-bit numbers (this basically specifies the number of gradations possible in describing the output of an artificial neuron; 8 bits in FP8 lets you store a much wider range of numbers than you might expect‚Äî it's not just limited to 256 different equal-sized magnitudes like you'd get with regular integers, but instead uses clever math tricks to store both very small and very large numbers‚Äî though naturally with less precision than you'd get with 32 bits.) The main tradeoff is that while FP32 can store numbers with incredible precision across an enormous range, FP8 sacrifices some of that precision to save memory and boost performance, while still maintaining enough accuracy for many AI workloads.</p><p>DeepSeek cracked this problem by developing a clever system that breaks numbers into small tiles for activations and blocks for weights, and strategically uses high-precision calculations at key points in the network. Unlike other labs that train in high precision and then compress later (losing some quality in the process), DeepSeek's native FP8 approach means they get the massive memory savings without compromising performance. When you're training across thousands of GPUs, this dramatic reduction in memory requirements per GPU translates into needing far fewer GPUs overall.</p><p>Another major breakthrough is their multi-token prediction system. Most Transformer based LLM models do inference by predicting the next token‚Äî one token at a time. DeepSeek figured out how to predict multiple tokens while maintaining the quality you'd get from single-token prediction. Their approach achieves about 85-90% accuracy on these additional token predictions, which effectively doubles inference speed without sacrificing much quality. The clever part is they maintain the complete causal chain of predictions, so the model isn't just guessing‚Äî it's making structured, contextual predictions.</p><p>One of their most innovative developments is what they call Multi-head Latent Attention (MLA). This is a breakthrough in how they handle what are called the Key-Value indices, which are basically how individual tokens are represented in the attention mechanism within the Transformer architecture. Although this is getting a bit too advanced in technical terms, suffice it to say that these KV indices are some of the major uses of VRAM during the training and inference process, and part of the reason why you need to use thousands of GPUs at the same time to train these models‚Äî each GPU has a maximum of 96 gb of VRAM, and these indices eat that memory up for breakfast.</p><p>Their MLA system finds a way to store a compressed version of these indices that captures the essential information while using far less memory. The brilliant part is this compression is built directly into how the model learns‚Äî it's not some separate step they need to do, it's built directly into the end-to-end training pipeline. This means that the entire mechanism is \"differentiable\" and able to be trained directly using the standard optimizers. All this stuff works because these models are ultimately finding much lower-dimensional representations of the underlying data than the so-called \"ambient dimensions\". So it's wasteful to store the full KV indices, even though that is basically what everyone else does.</p><p>Not only do you end up wasting tons of space by storing way more numbers than you need, which gives a massive boost to the training memory footprint and efficiency (again, slashing the number of GPUs you need to train a world class model), but it can actually end up improving model quality because it can act like a \"regularizer,\" forcing the model to pay attention to the truly important stuff instead of using the wasted capacity to fit to noise in the training data. So not only do you save a ton of memory, but the model might even perform better. At the very least, you don't get a massive hit to performance in exchange for the huge memory savings, which is generally the kind of tradeoff you are faced with in AI training.</p><p>They also made major advances in GPU communication efficiency through their DualPipe algorithm and custom communication kernels. This system intelligently overlaps computation and communication, carefully balancing GPU resources between these tasks. They only need about 20 of their GPUs' streaming multiprocessors (SMs) for communication, leaving the rest free for computation. The result is much higher GPU utilization than typical training setups achieve.</p><p>Another very smart thing they did is to use what is known as a Mixture-of-Experts (MOE) Transformer architecture, but with key innovations around load balancing. As you might know, the size or capacity of an AI model is often measured in terms of the number of parameters the model contains. A parameter is just a number that stores some attribute of the model; either the \"weight\" or importance a particular artificial neuron has relative to another one, or the importance of a particular token depending on its context (in the \"attention mechanism\"), etc.</p><p>Meta's latest Llama3 models come in a few sizes, for example: a 1 billion parameter version (the smallest), a 70B parameter model (the most commonly deployed one), and even a massive 405B parameter model. This largest model is of limited utility for most users because you would need to have tens of thousands of dollars worth of GPUs in your computer just to run at tolerable speeds for inference, at least if you deployed it in the naive full-precision version. Therefore most of the real-world usage and excitement surrounding these open source models is at the 8B parameter or highly quantized 70B parameter level, since that's what can fit in a consumer-grade Nvidia 4090 GPU, which you can buy now for under $1,000.</p><p>So why does any of this matter? Well, in a sense, the parameter count and precision tells you something about how much raw information or data the model has stored internally. Note that I'm not talking about reasoning ability, or the model's \"IQ\" if you will: it turns out that models with even surprisingly modest parameter counts can show remarkable cognitive performance when it comes to solving complex logic problems, proving theorems in plane geometry, SAT math problems, etc.</p><p>But those small models aren't going to be able to necessarily tell you every aspect of every plot twist in every single novel by Stendhal, whereas the really big models can potentially do that. The \"cost\" of that extreme level of knowledge is that the models become very unwieldy both to train and to do inference on, because you always need to store every single one of those 405B parameters (or whatever the parameter count is) in the GPU's VRAM at the same time in order to do any inference with the model.</p><p>The beauty of the MOE model approach is that you can decompose the big model into a collection of smaller models that each know different, non-overlapping (at least fully) pieces of knowledge. DeepSeek's innovation here was developing what they call an \"auxiliary-loss-free\" load balancing strategy that maintains efficient expert utilization without the usual performance degradation that comes from load balancing. Then, depending on the nature of the inference request, you can intelligently route the inference to the \"expert\" models within that collection of smaller models that are most able to answer that question or solve that task.</p><p>You can loosely think of it as being a committee of experts who have their own specialized knowledge domains: one might be a legal expert, the other a computer science expert, the other a business strategy expert. So if a question comes in about linear algebra, you don't give it to the legal expert. This is of course a very loose analogy and it doesn't actually work like this in practice.</p><p>The real advantage of this approach is that it allows the model to contain a huge amount of knowledge without being very unwieldy, because even though the aggregate number of parameters is high across all the experts, only a small subset of these parameters is \"active\" at any given time, which means that you only need to store this small subset of weights in VRAM in order to do inference. In the case of DeepSeek-V3, they have an absolutely massive MOE model with <a href=\"https://x.com/eliebakouch/status/1872304368462004608\">671B parameters</a>, so it's much bigger than even the largest Llama3 model, but only 37B of these parameters are active at any given time‚Äî enough to fit in the VRAM of two consumer-grade Nvidia 4090 GPUs (under $2,000 total cost), rather than requiring one or more H100 GPUs which cost something like $40k each.</p><p>It's rumored that both ChatGPT and Claude use an MoE architecture, with some leaks suggesting that GPT-4 had a total of 1.8 trillion parameters split across 8 models containing 220 billion parameters each. Despite that being a lot more doable than trying to fit all 1.8 trillion parameters in VRAM, it still requires multiple H100-grade GPUs just to run the model because of the massive amount of memory used.</p><p>Beyond what has already been described, the technical papers mention several other key optimizations. These include their extremely memory-efficient training framework that avoids tensor parallelism, recomputes certain operations during backpropagation instead of storing them, and shares parameters between the main model and auxiliary prediction modules. The sum total of all these innovations, when layered together, has led to the ~45x efficiency improvement numbers that have been tossed around online, and I am perfectly willing to believe these are in the right ballpark.</p><p>One very strong indicator that it's true is the cost of DeepSeek's API: despite this nearly best-in-class model performance, DeepSeek charges something like <a href=\"https://x.com/ai_for_success/status/1881371370120216618\">95% less money</a> for inference requests via its API than comparable models from OpenAI and Anthropic. In a sense, it's sort of like comparing Nvidia's GPUs to the new custom chips from competitors: even if they aren't quite as good, the value for money is so much better that it can still be a no-brainer depending on the application, as long as you can qualify the performance level and prove that it's good enough for your requirements and the API availability and latency is good enough (thus far, people have been <a href=\"https://x.com/VictorTaelin/status/1873876097794007545\">amazed</a> at how <a href=\"https://x.com/artificialguybr/status/1882979975692984373\">well</a> DeepSeek's infrastructure has held up despite the truly incredible surge of demand owing to the performance of these new models).</p><p>But unlike the case of Nvidia, where the cost differential is the result of them earning monopoly gross margins of 90%+ on their data-center products, the cost differential of the DeepSeek API relative to the OpenAI and Anthropic API could be simply that they are nearly 50x more compute efficient (it might even be significantly more than that on the inference side‚Äî the ~45x efficiency was on the training side). Indeed, it's not even clear that OpenAI and Anthropic are making great margins on their API services‚Äî they might be more interested in revenue growth and gathering more data from analyzing all the API requests they receive.</p><p>Before moving on, I'd be remiss if I didn't mention that many people are speculating that DeepSeek is simply lying about the number of GPUs and GPU hours spent training these models because they actually possess far more H100s than they are supposed to have given the export restrictions on these cards, and they don't want to cause trouble for themselves or hurt their chances of acquiring more of these cards. While it's certainly possible, I think it's more likely that they are telling the truth, and that they have simply been able to achieve these incredible results by being extremely clever and creative in their approach to training and inference. They explain how they are doing things, and I suspect that it's only a matter of time before their results are widely replicated and confirmed by other researchers at various other labs.</p><h2>A Model That Can Really Think</h2><p>The newer R1 model and technical report might even be even more mind blowing, since they were able to beat Anthropic to Chain-of-thought and now are basically the only ones besides OpenAI who have made this technology work at scale. But note that the O1 preview model was only released by OpenAI in mid-September of 2024. That's only ~4 months ago! Something you absolutely must keep in mind is that, unlike OpenAI, which is incredibly secretive about how these models really work at a low level, and won't release the actual model weights to anyone besides partners like Microsoft and other who sign heavy-duty NDAs, these DeepSeek models are both completely open-source and permissively licensed. They have released extremely detailed technical reports explaining how they work, as well as the code that anyone can look at and try to copy.</p><p>With R1, DeepSeek essentially cracked one of the holy grails of AI: getting models to reason step-by-step without relying on massive supervised datasets. Their DeepSeek-R1-Zero experiment showed something remarkable: using pure reinforcement learning with carefully crafted reward functions, they managed to get models to develop sophisticated reasoning capabilities completely autonomously. This wasn't just about solving problems‚Äî the model organically learned to generate long chains of thought, self-verify its work, and allocate more computation time to harder problems.</p><p>The technical breakthrough here was their novel approach to reward modeling. Rather than using complex neural reward models that can lead to \"reward hacking\" (where the model finds bogus ways to boost their rewards that don't actually lead to better real-world model performance), they developed a clever rule-based system that combines accuracy rewards (verifying final answers) with format rewards (encouraging structured thinking). This simpler approach turned out to be more robust and scalable than the process-based reward models that others have tried.</p><p>What's particularly fascinating is that during training, they observed what they called an \"aha moment,\" a phase where the model spontaneously learned to revise its thinking process mid-stream when encountering uncertainty. This emergent behavior wasn't explicitly programmed; it arose naturally from the interaction between the model and the reinforcement learning environment. The model would literally stop itself, flag potential issues in its reasoning, and restart with a different approach, all without being explicitly trained to do this.</p><p>The full R1 model built on these insights by introducing what they call \"cold-start\" data‚Äî a small set of high-quality examples‚Äî before applying their RL techniques. They also solved one of the major challenges in reasoning models: language consistency. Previous attempts at chain-of-thought reasoning often resulted in models mixing languages or producing incoherent outputs. DeepSeek solved this through a clever language consistency reward during RL training, trading off a small performance hit for much more readable and consistent outputs.</p><p>The results are mind-boggling: on AIME 2024, one of the most challenging high school math competitions, R1 achieved 79.8% accuracy, matching OpenAI's O1 model. On MATH-500, it hit 97.3%, and it achieved the 96.3 percentile on Codeforces programming competitions. But perhaps most impressively, they managed to distill these capabilities down to much smaller models: their 14B parameter version outperforms many models several times its size, suggesting that reasoning ability isn't just about raw parameter count but about how you train the model to process information.</p><p>The recent <a href=\"https://x.com/orikron/status/1882503121320214777\">scuttlebutt</a> on Twitter and Blind (a corporate rumor website) is that these models caught Meta completely off guard and that they perform better than the new Llama4 models which are still being trained. Apparently, the Llama project within Meta has attracted a lot of attention internally from high-ranking technical executives, and as a result they have something like 13 individuals working on the Llama stuff who each individually earn more per year in total compensation than the combined training cost for the DeepSeek-V3 models which outperform it. How do you explain that to Zuck with a straight face? How does Zuck keep smiling while shoveling multiple billions of dollars to Nvidia to buy 100k H100s when a better model was trained using just 2k H100s for a bit over $5mm?</p><p>But you better believe that Meta and every other big AI lab is taking these DeepSeek models apart, studying every word in those technical reports and every line of the open source code they released, trying desperately to integrate these same tricks and optimizations into their own training and inference pipelines. So what's the impact of all that? Well, naively it sort of seems like the aggregate demand for training and inference compute should be divided by some big number. Maybe not by 45, but maybe by 25 or even 30? Because whatever you thought you needed before these model releases, it's now a lot less.</p><p>Now, an optimist might say \"You are talking about a mere constant of proportionality, a single multiple. When you're dealing with an exponential growth curve, that stuff gets washed out so quickly that it doesn't end up matter all that much.\" And there is some truth to that: if AI really is as transformational as I expect, if the real-world utility of this tech is measured in the trillions, if inference-time compute is the new scaling law of the land, if we are going to have armies of humanoid robots running around doing massive amounts of inference constantly, then maybe the growth curve is still so steep and extreme, and Nvidia has a big enough lead, that it will still work out.</p><p>But Nvidia is pricing in a LOT of good news in the coming years for that valuation to make sense, and when you start layering all these things together into a total mosaic, it starts to make me at least feel extremely uneasy about spending ~20x the 2025 estimated sales for their shares. What happens if you even see a slight moderation in sales growth? What if it turns out to be 85% instead of over 100%? What if gross margins come in a bit from 75% to 70%‚Äî still ridiculously high for a semiconductor company?</p><p>At a high level, NVIDIA faces an unprecedented convergence of competitive threats that make its premium valuation increasingly difficult to justify at 20x forward sales and 75% gross margins. The company's supposed moats in hardware, software, and efficiency are all showing concerning cracks. The whole world‚Äî thousands of the smartest people on the planet, backed by untold billions of dollars of capital resources‚Äî are trying to assail them from every angle.</p><p>On the hardware front, innovative architectures from Cerebras and Groq demonstrate that NVIDIA's interconnect advantage‚Äî a cornerstone of its data center dominance‚Äî can be circumvented through radical redesigns. Cerebras' wafer-scale chips and Groq's deterministic compute approach deliver compelling performance without needing NVIDIA's complex interconnect solutions. More traditionally, every major NVIDIA customer (Google, Amazon, Microsoft, Meta, Apple) is developing custom silicon that could chip away at high-margin data center revenue. These aren't experimental projects anymore‚Äî Amazon alone is building out massive infrastructure with over 400,000 custom chips for Anthropic.</p><p>The software moat appears equally vulnerable. New high-level frameworks like MLX, Triton, and JAX are abstracting away CUDA's importance, while efforts to improve AMD drivers could unlock much cheaper hardware alternatives. The trend toward higher-level abstractions mirrors how assembly language gave way to C/C++, suggesting CUDA's dominance may be more temporary than assumed. Most importantly, we're seeing the emergence of LLM-powered code translation that could automatically port CUDA code to run on any hardware target, potentially eliminating one of NVIDIA's strongest lock-in effects.</p><p>Perhaps most devastating is DeepSeek's recent efficiency breakthrough, achieving comparable model performance at approximately 1/45th the compute cost. This suggests the entire industry has been massively over-provisioning compute resources. Combined with the emergence of more efficient inference architectures through chain-of-thought models, the aggregate demand for compute could be significantly lower than current projections assume. The economics here are compelling: when DeepSeek can match GPT-4 level performance while charging 95% less for API calls, it suggests either NVIDIA's customers are burning cash unnecessarily or margins must come down dramatically.</p><p>The fact that TSMC will manufacture competitive chips for any well-funded customer puts a natural ceiling on NVIDIA's architectural advantages. But more fundamentally, history shows that markets eventually find a way around artificial bottlenecks that generate super-normal profits. When layered together, these threats suggest NVIDIA faces a much rockier path to maintaining its current growth trajectory and margins than its valuation implies. With five distinct vectors of attack‚Äî architectural innovation, customer vertical integration, software abstraction, efficiency breakthroughs, and manufacturing democratization‚Äî the probability that at least one succeeds in meaningfully impacting NVIDIA's margins or growth rate seems high. At current valuations, the market isn't pricing in any of these risks.</p><p>I hope you enjoyed reading this article. If you work at a hedge fund and are interested in consulting with me on NVDA or other AI-related stocks or investing themes, I'm already signed up as an expert on <a href=\"https://glginsights.com/\">GLG</a> and <a href=\"https://www.colemanrg.com/\">Coleman Research</a>.</p>","contentLength":71791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42822162"},{"title":"Using the most unhinged AVX-512 instruction to make fastest phrase search algo","url":"https://gab-menezes.github.io/2025/01/13/using-the-most-unhinged-avx-512-instruction-to-make-the-fastest-phrase-search-algo.html","date":1737668307,"author":"cmcollier","guid":206,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42808355"},{"title":"Show HN: 3D printing giant things with a Python jigsaw generator","url":"https://calbryant.uk/blog/3d-printing-giant-things-with-jigsaw-generator/","date":1737639304,"author":"naggie","guid":174,"unread":true,"content":"<p>I really like the idea of a fully automated pipeline when I build anything ‚Äì it‚Äôs highly satisfying to see a machine do all the work for you. Combine this with parametric design, and it makes iteration and customisation a breeze. My flagship example is my recent <a href=\"https://calbryant.uk/blog/speakers/\">speaker project.</a></p><p>With the previous speaker project I was literally at the margins for the largest speaker I could comfortably make with my 3D printer ‚Äì without significantly more work splitting up the design. I was looking to see what large format printers were out there ‚Äì there are a handful but they‚Äôre expensive and possibly not as capable as what I have now.</p><p>Then I happened to watch a video where Richard from <a href=\"https://www.retrocollective.co.uk/\" target=\"_blank\" rel=\"noopener\">RMC</a> 3D prints an <a href=\"https://www.youtube.com/watch?v=SONO6LTHuR8\" target=\"_blank\" rel=\"noopener\">entire arcade machine</a> using a farm of relatively small printers. After creating the model,  he split it into macro layers, each of which where split into smaller parts that had a dovetail slot for assembly. It apparently worked really well.</p><p><strong>What if I could automate this process?</strong></p><p>In the last article about speaker design I mention the possibility of floor-standing speakers, glossing over the fact that I‚Äôd have to segment the print. If I constrained the system to work on panel-type designs such as this speaker system, it would be quite straightforward to implement.</p><p>I already wrote a system to place the parts nested on individual beds; I decided to adapt that code to also be able to split up panels into a what is effectively a jigsaw. I could use dovetail joins to do this, allowing something that can be easily glued and requiring no extra parts like dowels.</p><p>If I got it right, it would divide the parts through complex geometry without a significant impact on the final finish.</p><p>Dovetail joins are a traditional way to join wood together. They‚Äôre generally used for strength, but also aesthetics. They can be manually cut, or made using a handheld router (using a jig) or CNC router.</p><p>The strength comes from the tightening/wedging effect when pulling the join apart. If the dovetail is tapered, the join can also tighten when it aligns too ‚Äì this is highly desirable for gluing, as it means the glue will not be scraped away.</p><p>I took a look at some dovetail implementations in OpenSCAD, but none had all the features I needed.</p><p>I desired an approach that would subtract plastic once such that we end up with a fully mated join straight away. In theory this would simplify the design such that the two parts don‚Äôt need 2 separate, complicated and overlapping negatives to subtract.</p><p>This calls for a thin ‚Äúshell‚Äù type structure, like a zig-zagging ribbon; even better it should be tapered!</p><p>OpenSCAD, being <a href=\"https://en.wikipedia.org/wiki/Constructive_solid_geometry\" target=\"_blank\" rel=\"noopener\">CSG</a> based is not well suited to creating thin shells, so this can be a bit awkward to do.</p><p>Here is the deconstruction of the dovetail profile, after a few iterations to remove artefacts and optimise.</p><p>Interestingly, step 8 was originally done as an intersection, but I found that it absolutely destroyed OpenSCAD performance. You‚Äôre talking one frame every few minutes instead of dozens per second!</p><p>I didn‚Äôt remove the interfering edges at first ‚Äì see the hanging artefacts screenshot later on. I was confused and thought it was a bug at first, until I examined the 3D tooth again.</p><p>Here‚Äôs the code that does exactly what you see above, complete with annotations to show what step corresponds to what. Only 68 lines!</p><div><div><div><div><table><tbody><tr><td><pre tabindex=\"0\"><code></code></pre></td><td><pre tabindex=\"0\"><code data-lang=\".scad\"></code></pre></td></tr></tbody></table></div></div><a href=\"https://calbryant.uk/blog/3d-printing-giant-things-with-jigsaw-generator/dovetail.scad\" target=\"_blank\">Download</a><a href=\"javascript:;\">Copy</a></div></div><p>I made several test prints to find what felt like the right set of parameters for the ratios described above. I settled on quite a small tooth, as it would reduce the size of any artefacts produced. Plus, the fit felt tighter.</p><p>In case you‚Äôre wondering, I found the 0.2mm interference fit that Richard used in the video worked best, leaving a bit of play and somewhere for the glue to go.</p><p>Performance was kept in check by limiting the number of faces () in the tooth code. This was a good compromise, smooth enough and reduces stress concentration sufficiently.</p><p>I am a little concerned about the <a href=\"https://en.wikipedia.org/wiki/Z-fighting\" target=\"_blank\" rel=\"noopener\">Z-fighting</a> hat occurs when the parts are joined. Usually I make sure the parts intersect a little to avoid this. It looks fine, I‚Äôll just hope I don‚Äôt have any manifolding issues later on for now.</p><p>Sometimes the top part of a tapered tooth can be suspended in mid-air ‚Äì this occurs when a boundary occurs at a tooth edge. This is a problem as it will cause spaghetti when printing, not to mention missing chunks in the final design.</p><p>I realised I could at least detect and remove these artefacts in the (post-processing) code by looking for independent tiny fragments that aren‚Äôt touching the bed. I can at least then prevent the spaghetti; there will still be a hole in the design but I presume most of the time that can be addressed in the finishing step.</p><h2>Automatically splitting up STLs</h2><p>Now that I had geometry to create the joins in-place, I needed to automate the process so I can cut up an <a href=\"https://www.xkcd.com/974/\" target=\"_blank\" rel=\"noopener\">arbitrary</a> design to fit on a given printer.</p><p>As I‚Äôve mentioned, this only has to work on panels so I have the luxury of only having to operate in 2D and assume the parts are mainly flat and rectangular.</p><p>To automate it, I figured it would be far easier to do this (mostly) outside of OpenSCAD and operate on <a href=\"https://en.wikipedia.org/wiki/STL_(file_format)\" target=\"_blank\" rel=\"noopener\">STLs</a> directly. That way this system will work on 3D printed models from any CAD software.</p><p>I have already developed some part nesting software using <a href=\"https://github.com/secnot/rectpack\" target=\"_blank\" rel=\"noopener\">rectpack</a> and <a href=\"https://pypi.org/project/numpy-stl/\" target=\"_blank\" rel=\"noopener\">numpy-stl</a>, so I decided to use that as a base.</p><p>The resulting code is straightforward. Here‚Äôs how it works:</p><ol><li>Load the STL file and find the bounding box of the model</li><li>Rotate it so the aspect ratio matches the bed of the printer (and optionally, so the design is at its longest along the same axis as the printer bed)</li><li>Calculate how m any sub-divisions are necessary on each axis to produce parts that will fit on the bed (margin of tooth size required)</li><li>Execute an OpenSCAD template that will subtract the dovetail teeth from the model, translating the STL so each cut is in the right place</li><li>Split those STLs into separate files with  cli</li><li>Remove any edge artefacts by looking for small objects</li></ol><p>I refactored by nesting code to allow adding the dovetail splitting code without causing a mess. After a lot of debugging I ran the code, only to be disappointed! It took 4 hours to run per operation, only to fail with these errors:</p><ul><li><code>ERROR: CGAL error in CGALUtils::applyBinaryOperator difference: CGAL ERROR: assertion violation!</code></li><li><code>ERROR: The given mesh is not closed! Unable to convert to CGAL_Nef_Polyhedron</code></li></ul><p>OpenSCAD uses the <a href=\"https://www.cgal.org/\" target=\"_blank\" rel=\"noopener\">CGAL</a> library which is notoriously slow, and it can produce non-manifold meshes. The speaker design I made produces <a href=\"https://blender.stackexchange.com/questions/7910/what-is-non-manifold-geometry\" target=\"_blank\" rel=\"noopener\">non-manifold</a> STLs ‚Äì I think they have holes in due to some OpenSCAD implementation issues, or something with my code.</p><p>Those errors above are likely to be caused by these non-manifold edges. What could I do? I didn‚Äôt want to get this far only to abandon the project. Luckily, recently OpenSCAD has integrated a new geometry library called <a href=\"https://github.com/elalish/manifold\" target=\"_blank\" rel=\"noopener\">manifold</a> which is apparently several orders of magnitude faster and more robust.</p><p>I tried this using an unstable version of OpenSCAD with the  flag, and it worked! Not only did it work, but it computed the design in  This is 64500x improvement (and it works).</p><p>I should try manifold with the rest of the design. Manifold seems to use more memory and is multi-threaded, so I‚Äôd have to modify my build scripts which currently build 16 parts at once ‚Äì last time I tried it overwhelmed the system.</p><p>Anyway, after an evening of more hacking I got it properly integrated and behaving as expected. Here‚Äôs <a href=\"https://calbryant.uk/blog/speakers/\">the previous speaker design</a> with a bed size reduced to 200x200 ‚Äì this means it could be printed on a Prusa i3!</p><p>It seems to have worked exceptionally well. I like how the nesting algorithm has placed the split parts together as well to reduce required beds. This will decrease total printing time and effort. In one case (bed 17) the part did not fit in one bed, but was split so it could be.</p><p>Looking at the corners, it seems quite common for the above to occur. As we know where the intersections of dovetail joins are, we could skip teeth to avoid the issue. That‚Äôs for another day though.</p><p>I‚Äôve linked the code above. It expects , ,  and , as well as the dovetail.scad to be in . I hacked it from 2 files, so it might need a few fixes to work.</p><p>The chosen profile is self-aligning, and increases the surface area for glue to bond. It‚Äôs easy to align against a straight edge if a silicone mat is used to prevent the glue from sticking to things it shouldn‚Äôt.</p><p>This has successfully unlocked a floor-standing speaker design using this method in the near future, after validating the design and experimenting with different adhesives and finishes. Or, perhaps I will repackage my small MDF subwoofer I built a few years ago.</p><p>I could have implemented this as part of the speaker design with about 12 special cases. This way I could also avoid edge artefacts and more carefully place the joins to avoid detailed geometry. However, this is less transferable to other designs and less elegant in my opinion.</p><p>With some improvements, I could have the best of both worlds:</p><ul><li>Give it the ability to vary the part division location to avoid complex geometry for the best part finish. This would involve evaluating potential cross-sections of the model for complexity; a simple heuristic of the number of faces in a given cross section would be a good start.</li><li>Do the same to try to avoid big gaps (for the brace, for instance)</li><li>Skip teeth where joins intersect as mentioned above</li><li>Skip teeth near voids/edges</li></ul><p>To do this, and possibly to replace the use of numpy-stl and/or OpenSCAD, I could look into <a href=\"https://trimsh.org\" target=\"_blank\" rel=\"noopener\">trimesh</a> which can do a lot more.</p><p>I‚Äôm confident the assembled parts will allow for a flawless finish if filled and sanded, given my experience with the last project.</p><p>I have made the code available linked to this post for the time being ‚Äì I will happily package this up properly if there‚Äôs significant interest. I‚Äôm curious as to what other people could do with the idea or variant thereof.</p><div role=\"doc-endnotes\"><ol><li><p>‚Ä¶because I‚Äôm a coder at heart&nbsp;</p></li><li><p>derived from an imperfect scanned reference. Impressive.&nbsp;</p></li><li><p>The video doesn‚Äôt do justice to the monumental amount of work this represents.&nbsp;</p></li><li><p>OK so technically it‚Äôs not arbitrary, but it‚Äôs a good word! I had to get that XKCD reference in there somewhere.&nbsp;</p></li><li><p>Of course you could go the other way and print something giant on my 256x256 X1C bed, but then I‚Äôd be doing the title justice!&nbsp;</p></li><li><p>I had assumed I‚Äôd want to mill the next speaker, but my last project was such as success I will happily take the advantages!&nbsp;</p></li></ol></div><p>Please <a href=\"https://calbryant.uk/cdn-cgi/l/email-protection#f192909d9d909fdf938388909f85b1969c90989ddf929e9c\">email me</a> with any corrections or feedback.</p>","contentLength":10535,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42803822"},{"title":"Using generative AI as part of historical research: three case studies","url":"https://resobscura.substack.com/p/the-leading-ai-models-are-now-very","date":1737588561,"author":"benbreen","guid":205,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42798649"},{"title":"So you wanna write Kubernetes controllers?","url":"https://ahmet.im/blog/controller-pitfalls/","date":1737585200,"author":"gokhan","guid":204,"unread":true,"content":"<p>Any company using Kubernetes eventually starts looking into developing their\ncustom controllers. After all, what‚Äôs not to like about being able to provision\nresources with declarative configuration: <a href=\"https://youtu.be/zCXiXKMqnuE?t=128\">Control loops</a> are fun,\nand <a href=\"https://kubebuilder.io/\">Kubebuilder</a> makes it extremely easy to get started with writing Kubernetes\ncontrollers. Next thing you know, customers in production are relying on the\nbuggy controller you developed without understanding how to design idiomatic\nAPIs and building reliable controllers.</p><p>Low barrier to entry combined with good intentions and the ‚Äúillusion of\n implementation‚Äù is not a recipe for\nsuccess while developing production-grade controllers. I‚Äôve seen the real-world\nconsequences of controllers developed without adequate understanding of\nKubernetes and the controller machinery at multiple large companies. We went\nback to the drawing board and rewritten nascent controller implementations a few\ntimes to observe which mistakes people new to controller development\nmake.</p><h2>Design CRDs like Kubernetes APIs</h2><p>It takes less than 5 minutes to write a Go struct and generate a Kubernetes\nCustomResourceDefinition (CRD) from it thanks to controller-gen. Then it takes\nseveral months to <a href=\"https://www.linkedin.com/blog/engineering/infrastructure/how-linkedin-moved-its-kubernetes-apis-to-a-different-api-group\">migrate</a> from this poorly designed API to a better v2 design\nwhile the old API is being used in productionDon‚Äôt do yourself that.</p><p>If you‚Äôre serious about developing long-lasting production grade controllers,\nyou have to deeply understand the <a href=\"https://github.com/kubernetes/community/blob/8a99192b3780b656f9dd53c0c37d9372a1c975f9/contributors/devel/sig-architecture/api-conventions.md\">API Conventions</a> that\nKubernetes uses to design its builtin APIs. Then, you need to study the builtin\nAPIs, and think about things like ‚Äúwhy is this field here‚Äù, ‚Äúwhy is this field\nnot a boolean‚Äù, ‚Äúwhy is this a list of objects and not a string array‚Äù. Only\nwhen you‚Äôre able to reason about the builtin Kubernetes APIs and their design\nprinciples, you‚Äôll be able to design a long-lasting custom resource API.</p><ol><li><p>They don‚Äôt understand the difference between  and  and who\nshould be updating each field (more about this later ).</p></li><li><p>They don‚Äôt understand how to embed a child object within a parent object\n(e.g. how  becomes a ) so they end up\nre-creating child object properties in the parent object, usually with a\nworse organized structure.</p></li><li><p>They don‚Äôt understand field semantics well (e.g. zero values, defaulting,\nvalidation) and end up with fields that are not set, or set to wrong values\naccepted into the API.\nI covered this topic in my\n<a href=\"https://ahmet.im/blog/crd-generation-pitfalls/\">CRD generation pitfalls article</a>. If the behavior of the\nAPI is not clear when a field is not set, you‚Äôve already failed.\n<a href=\"https://github.com/kubernetes/community/blob/8a99192b3780b656f9dd53c0c37d9372a1c975f9/contributors/devel/sig-architecture/api-conventions.md\">API conventions</a> guide covers this topic fairly well.</p></li></ol><p>If you study the builtin Kubernetes APIs extensively, you‚Äôll find out things like\n field is not a ‚Äúmust have‚Äù, and not all APIs offer a  field.\nI would go as far as to say that you should also study custom APIs of projects\nlike Knative, Istio and other popular controllers to develop a better\nunderstanding of organizing fields, and how to reuse some core types Kubernetes\nalready offers (like , ).</p><h2>Single-responsibility controllers</h2><p>Time and time again we find engineers adding new unrelated responsibilities to\nexisting controllers because it seems like a good place their  can be\nshoved into. Kubernetes core controllers don‚Äôt have this problem for a reason.</p><p>One of the main Kubernetes <a href=\"https://github.com/kubernetes/design-proposals-archive/blob/acc25e14ca83dfda4f66d8cb1f1b491f26e78ffe/architecture/principles.md#design-principles\">design principles</a> is that controllers have clear\ninputs and outputs ‚Äîand they do a well-defined job. For example, the Job\ncontroller watches  objects and creates s, which is a clear mental\nmodel to reason about. Similarly, each API is designed to offer a well defined\nfunctionality. A controller‚Äôs output can be an input to another controller.\nThis is all what the <a href=\"https://en.wikipedia.org/wiki/Unix_philosophy#Origin\">UNIX\nphilosophy</a> suggests for\na well reasoned system.</p><p>I recommend studying the common <a href=\"https://youtu.be/zCXiXKMqnuE?t=539\">controller shapes</a> (great talk by Daniel Smith,\none of the architects of kube-apiserver) and the core Kubernetes controllers.\nYou‚Äôll notice that each core controller in Kubernetes core has a very clear job\nand inputs/outputs that can be explained in a small diagram. If your controller\nisn‚Äôt like this, you‚Äôre probably misarchitecting either your controller, or your\nCRDs.</p><p>If you architect your APIs and controllers correctly, your controllers will run\nin harmony as if they‚Äôre integrating with Kubernetes core APIs or an\noff-the-shelf operator.</p><p>When you controller design doesn‚Äôt quite feel right, or has too many\ninputs/outputs, does too much, or in general doesn‚Äôt , you‚Äôre\nprobably doing it unidiomatically. I struggled with this a lot myself,\nespecially while developing controllers that manage external resources that have\na non-declarative configuration paradigm.</p><p>Assuming you use kubebuilder (which uses <a href=\"https://github.com/kubernetes-sigs/controller-runtime\">controller-runtime</a> like\nalmost everyone else to develop a controller, and you implement the\n method that controller-runtime invokes every time one of your\ninputs change. This is where your controller does its magic, and since it‚Äôs\npossible to implement this method in any way, most beginners dump their\nspaghetti here.</p><p>Therefore, large projects like Knative define their own <a href=\"https://github.com/knative/pkg/tree/accfe36491888e45ce8bd923ff8996283c055ae1/reconciler\">common controller\nshapes</a>\nwhere every controller runs the same set steps in the same order.  By developing\na common controller shape/framework, you create a ‚Äúguardrail‚Äù so that other\nengineers don‚Äôt deviate and introduce bugs in the reconciliation flow easily.</p><p>Sadly, controller-runtime is not opinionated about this topic. Your best bet is\nto read other controllers (like <a href=\"https://github.com/kubernetes-sigs/cluster-api\">Cluster\nAPI</a> to learn the idioms and\nmaster the reconciliation flow.</p><p>There are also new projects like <a href=\"https://github.com/reddit/achilles-sdk\">Apollo SDK by\nReddit</a> that claims to offer\nfinite-state machines for controller-runtime reconcilers.</p><p>Over time, we found that almost all our controllers have a similar\nreconciliation flow. Here‚Äôs a pseudo-code of how our controllers look like:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Most notably, you‚Äôll see here that we always initialize conditions and always\nupdate status inside  even if the reconciliation fails.</p><p>I recommend enforcing a similar common shape for controllers developed at your\ncompany (you can use custom kubebuilder plugins during scaffolding, but you\ncan‚Äôt really  that either).</p><h2>Report  and </h2><p>I practically never seen a beginner engineer create a CRD that has properly\ndesigned  fields (if one exists, at all). Kubernetes <a href=\"https://github.com/kubernetes/community/blob/8a99192b3780b656f9dd53c0c37d9372a1c975f9/contributors/devel/sig-architecture/api-conventions.md\">API\nconventions</a> discuss this at length, so I‚Äôll keep it\nbrief. If an API object is reconciled by a controller, the resource should\nexpose its status in  fields. For example, there‚Äôs no ConfigMap\ncontroller so ConfigMap doesn‚Äôt have a  field.</p><p>At LinkedIn, our custom API objects have a  field, similar to\nthe Kubernetes core or <a href=\"https://github.com/knative/pkg/blob/accfe36491888e45ce8bd923ff8996283c055ae1/apis/condition_types.go#L58-L85\">Knative\nconditions</a>,\nand we use something similar to <a href=\"https://github.com/knative/pkg/blob/accfe36491888e45ce8bd923ff8996283c055ae1/apis/condition_set.go\">Knative condition set manager</a> that provides\nhigh-level accessor methods to set the conditions, and sort them etc.</p><p>This helps us define and report conditions for API objects in a high-level way\nin the reconciler code:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Every time we mark a condition, the condition manager recalculates the top-level\n condition, which all our objects have as Kubernetes API conventions\nsuggest. Other controllers and humans consume this top-level condition to\nunderstand how the objects are doing (plus you get to use <a href=\"https://github.com/ahmetb/kubectl-cond/\"></a>\non your objects).</p><h2>Learn to use </h2><p>Something notable is that all our  have an \nfield. You‚Äôll even see some popular community CRDs (like ArgoCD\n<a href=\"https://doc.crds.dev/github.com/argoproj/argo-cd/argoproj.io/Application/v1alpha1@v2.13.3\">Application</a>)\ndo not offer this field.</p><p>Essentially, this field tells us whether the condition is calculated based on\nthe last configuration of the object ‚Äîor whether we‚Äôre looking at\na scale status information because the controller hasn‚Äôt gotten to reconciling\nthe object after the update.</p><p>For example, observing a  condition set to  alone means nothing\n(other than at some point in the past it was true). The condition offers a\nmeaningful status info if and only if the <code>cond.observedGeneration == metadata.generation</code>.</p><p> A controller we had in production didn‚Äôt have the notion\nof , so its callers would update the object‚Äôs  and\nimmediately check its  condition. This condition would almost always be\nstale, as the controller hadn‚Äôt reconciled the object yet. So the callers\ninterpreted an app rollout as , even though it hadn‚Äôt even started\nyet (and sometimes actually failed, but that failure was never noticed).</p><h2>Understand the cached clients</h2><p>controller-runtime, by default, gives you a client to the Kubernetes API which\nserves the reads from an in-memory cache (as it uses <a href=\"https://leftasexercise.com/2019/07/15/understanding-kubernetes-controllers-part-iii-informers/\">shared informers</a> from\n under the covers). This is mostly fine, as controllers are designed\nto operate on stale data ‚Äîbut it is detrimental if you didn‚Äôt know this\nwas the case since you might be writing buggy controllers due to this (more on\nthis later in ‚Äúexpectations‚Äù section).</p><p>When you perform a write (which directly hits the API server), its results may\nnot be immediately visible in the cached client. For example, when you delete an\nobject, it may still show up in the list result in a subsequent reconciliation\nto your surprise.</p><p>The lesser-known behavior of controller-runtime most beginners don‚Äôt realize is\nthat controller-runtime establishes new informers on-the-fly. Normally, when you\nspecify explicit event sources while building your controller (e.g. in\n<code>builder.{For,Owns,Watches}</code>), the informers are started and caches are started\nduring startup.</p><p>However, if you try to make queries with  on resources that\nyou haven‚Äôt declared upfront in your controller setup, controller-runtime will\ninitialize an informer on-the-fly and block on warming up its cache. This leads\nto issues like:</p><ul><li>Controller-runtime starting a watch for a resource type and start caching all\nits objects in memory (even if you were trying to query only one resource),\npotentially leading to the process running out of memory.</li><li>Unpredictable reconciliation times while the informer cache is syncing, during\nwhich your worker goroutine will be blocked from reconciling other resources.</li></ul><p>That‚Äôs why I recommend setting <code>ReaderFailOnMissingInformer: true</code> and disabling\nthis behavior so you‚Äôre fully aware of what kinds your controller is maintaining\nwatches/caches on. Otherwise, controller-runtime doesn‚Äôt provide any\nobservability on what informers it‚Äôs maintaining in the process.</p><p>controller-runtime offers <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/cache#Options\">a lot of other cache knobs</a>, such as entirely\ndisabling the cache on certain types, dropping some fields from the in-memory\ncache, or limit the cache to certain namespaces. I recommend studying them to\nbetter understand how you can customize the cache behavior.</p><h2>Fast and offline reconciliation</h2><p>Reconciling an object that is alredy up-to-date (i.e. goal state == current\nstate) should be really fast and offline ‚Äîmeaning it should not make any\nAPI calls (to external APIs or writes to Kubernetes API). That‚Äôs why controllers\nuse <a href=\"https://ahmet.im/blog/controller-pitfalls/#understand-the-cached-clients\">cached clients</a> to serve reads from the\ncache to determine the state of the world.</p><p>I‚Äôve seen many real-world controllers making API calls to external systems, or\nmake status updates to Kubernetes API (even when nothing has changed) every time\n got invoked. This is an anti-pattern, and a really bad idea for\nwriting scalable and reliable controllers:</p><ol><li><p>They bombarded the external APIs with unnecessary calls during controller\nstartup (or full resyncs, or when they had bugs causing infinite requeue\nloops)</p></li><li><p>When the external API was down, reconciliation would fail even though\nnothing has changed in the object. Depending on the implementation, this\ncan block the next steps in the reconciliation flow even though those steps\ndon‚Äôt depend on this external API call.</p></li><li><p>Logic that takes long to execute in a reconciliation loop will hog the worker\ngoroutine, and cause workqueue depth to increase, and reduce the\nthroughput/responsiveness of the controller as the worker goroutine is\noccupied with the slow task.</p></li></ol><p>Let‚Äôs go through a concrete example: Assume you have an S3Bucket controller that\ncreates and manages S3 buckets using AWS S3 API. If you make a query to S3 API\non every reconciliation, you‚Äôre doing it wrong. Instead, you should store the\nresult of the S3 API calls you made, in a field like\n<code>status.observedGeneration</code>, to reflect what‚Äôs the last generation of the object\nthat was successfully conveyed to S3 API. If this field has 0 value, the\ncontroller knows it needs to make a ‚ÄúCreate Bucket‚Äù call to S3 API. When a\nclient updates the S3Bucket custom resource, its  will no\nlonger match its stored <code>status.observedGeneration</code>, so the controller knows it\nneeds to make a ‚ÄúUpdate Bucket‚Äù call to S3 API, and only upon success it will\nupdate the <code>status.observedGeneration</code> field. This way, you avoid making calls\nto the external S3 API when the object is already up-to-date.</p><p>Your  function signature returns\n<a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/reconcile#Result\"></a>\n+ values. Usually beginners don‚Äôt have a solid grasp on what values to\nreturn from .</p><p>You should know that your  function is invoked every time your\nevent sources declared in <code>builder.{For,Owns,Watches}</code> changes. If you know\nthat, my general advice while returning reconciliation values:</p><ol><li><p>If you have s during reconciliation, return the error; not . Controller-runtime will requeue for you.</p></li><li><p>Use  only when there‚Äôs no error but something you started is\nstill in progress, and you want to check its status with the <a href=\"https://github.com/kubernetes/client-go/blob/9897373fe6348db656b1f4039033d509b8a4f241/util/workqueue/default_rate_limiters.go#L51-L53\">default backoff\nlogic</a>.</p></li><li><p>Use  only when you want to reconcile the object after\na certain time has passed. This is useful for implementing a wall-clock based\nperiodic reconciliation (e.g. a CronJob controller, or you want to retry\nreconciliation at a custom poll interval).</p></li></ol><p>It‚Äôs a <a href=\"https://groups.google.com/g/operator-framework/c/K7zwQiCJVYg/m/03NT2HYeCAAJ\">matter of\npreference</a>\nwhether your  function should make as much progress as possible in\na single run; or, return early every time you change something and requeue\nitself again. You‚Äôll see the earlier approach is more unit-test friendly and\nwhat you‚Äôll find more frequently in the open-source controllers because if your\nevent triggers are set up correctly, the object will get requeued anyway.</p><h2>Workqueue/resync mechanics</h2><p>OpenKruise has an <a href=\"https://openkruise.io/blog/learning-concurrent-reconciling/\">article about workqueue mechanics</a>, go read\nthat. I frequently see beginners not relying on assumptions like an object\nis guaranteed to be reconciled at the same time in different workers, so they\nend up implementing unnecessary locking mechanisms in their controllers.</p><p>Similarly, beginners frequently don‚Äôt understand  and  an\nobject gets reconciled. For example, when your controller updates the object\nit‚Äôs working on, it‚Äôll be requeued for a reconciliation immediately again\n(because the update you made triggers watch event).</p><p>Even when no objects were updated, all watched resources will be requeued\nperiodically  to get reconciled again (called ‚Äúresync‚Äù configured via\n option). This is the default behavior since the controllers may\nmiss watch events (very rare), or skip processing some events during leadership\nchange. But this behavior causes you to do a full reconciliation of all objects\ncached. So by default, your controller should assume it‚Äôll reconcile the\nentire world periodically.</p><p> We had a controller that managed several thousand objects\nand it did a full resync every 20 minutes. Every object took several seconds to\nreconcile. So any time a client created or updated an object, it would not get\nreconciled until many minutes later, as it goes to the back of the workqueue\namong. If this happened during full resync or controller startup, it took\nmany minutes until any work was done on this object.</p><p>Starting with controller-runtime v0.20 has\n<a href=\"https://github.com/kubernetes-sigs/controller-runtime/releases/tag/v0.20.0\">introduced</a>\na <a href=\"https://github.com/alvaroaleman/controller-runtime/blob/bd6eede09a6d6fd3eebb374ee62c9338886a7e13/designs/priorityqueue.md\">priority\nqueue</a>\nimplementation for the workqueue. This would deprioritize reconciliation of\nobjects that were not edge-triggered (i.e. due to an create/update etc.) and\nmake the controller more responsive during full resyncs and controller startups.</p><p>That‚Äôs why understanding the workqueue semantics, worker count\n() and monitoring your controller‚Äôs reconciliation\nlatency, workqueue depth and active workers count is super important to know if\nyour controller scales or not.</p><p>We discussed above that controller-runtime client serves the  from an\ninformer cache, and doesn‚Äôt query the API server except during the\nstartup/resyncs.</p><p>This cache is kept up-to-date based on the received ‚Äúwatch‚Äù events from the API\nserver. Therefore, your controller will almost certainly read stale data at some\npoint, since the watch events arrive asynchronously after the writes you make.\nCached clients don‚Äôt offer <a href=\"https://arpitbhayani.me/blogs/read-your-write-consistency/\">read-your-writes\nconsistency</a>.</p><p>This means you need to program your  method with this assumption at\nall times. This is not at all intuitive, but a reality when you work with\na cached client. I‚Äôll give several real-world examples:</p><p> You‚Äôre implementing the  controller. Controller sees\n*a\nReplicaSet with  , so it lists the pods with  (which\nis served from the cache), and you get 3 Pods. It turns out the informer cache\nwasn‚Äôt up-to-date, but the API actually had 5 pods.  Your controller creates 2\nmore pods, now you have 7 Pods.  Definitely not what you wanted.</p><p> Now you‚Äôre scaling down a ReplicaSet from 5 to 3. You list the\nPods, you see 5 Pods, you delete 2 Pods, and next time you list the Pods again,\nyou still see, you delete another 2 Pods. If your deletion logic is not\ndeterministic (e.g. sorting Pods by name), you scaled from 5 to 1\n‚Äîdefinitely not what you wanted.</p><p> For every object kind=, you create an object kind=. When\n gets updated, you update . The update succeeds, but next time you reconcile\n again, you don‚Äôt see an updated version of , so you update  to the\ngoal state again, and you get a  error because you‚Äôre updating the old\nversion of the object. But you already updated it, why update again?</p><p>If you don‚Äôt know how to solve these problems in your controller, it‚Äôs likely\nbecause you haven‚Äôt seen the ‚Äúexpectations‚Äù pattern before.</p><p>In this case, controllers need to do in memory bookkeeping of their expectations\nthat resulted from the successful writes they made.\nOnce an expectation is recorded, the controller knows it needs to wait for the\ncache to catch up (which will trigger another reconciliation), and not do its\njob based on the stale result it sees from the cache.</p><p>You can see many core controllers <a href=\"https://github.com/kubernetes/kubernetes/blob/a882a2bf50e630a9ffccbd02b8f759ea51de1c8f/pkg/controller/controller_utils.go#L119-L132\">use this\npattern</a>,\nand Elastic operator also has a <a href=\"https://github.com/elastic/cloud-on-k8s/blob/6c1bf954555e5a65a18a17450ccddab46ed7e5a5/pkg/controller/common/expectations/expectations.go#L16-L78\">great\nexplanation</a>\nalongside their implementation. We implemented a couple of variants of these\nat LinkedIn ourselves.</p><p>Usually when you have controller development questions, join the Kubernetes\nslack and ask in the #controller-runtime channel. The maintainers are very\nhelpful!  If you‚Äôre looking for a good controller implementation, I recommend\nstudying the <a href=\"https://github.com/kubernetes-sigs/cluster-api/\">Cluster API</a>\ncodebase.  Also, Operator SDK has a <a href=\"https://sdk.operatorframework.io/docs/best-practices/best-practices/\">best practices\nguide</a> you\nshould check out.</p><p>I‚Äôm not the most experienced person to write a detailed guide on this, but I‚Äôll\nbe writing more about beginner pitfalls and controller development anti-patterns.</p><p>At LinkedIn we use a controller development exercise <a href=\"https://twitter.com/diptanu\">a former\ncolleague</a> came up with to onboard new engineers to\nget them to understand the controller machinery. This exercise touches many\naspects of controller development and gets people familiar with core Kubernetes\nAPIs:</p><blockquote><p>\nImplement a SequentialJob API and controller. The API should\nallow users to specify a series of run-to-completion (batch job) container\nimages to run sequentially.</p><ul><li><em>How do users specify the list of containers? (Do you use the core types?)</em></li><li><em>Do you report status? How is status calculated? How do you surface job failures?</em></li><li><em>Where do you validate user inputs? Where do you report reconciliation failures?</em></li><li><em>What happens if the SequentialJob changes while the jobs are running?</em></li><li><em>How are the child resources you created are cleaned up?</em></li></ul></blockquote><p>I hope this article helps you be a better controller developer. If you feel like\nthis sort of work resonates with you, we‚Äôre usually hiring nowadays <a href=\"https://www.linkedin.com/jobs/view/4118956306/\">[1]</a>\n[<a href=\"https://www.linkedin.com/jobs/view/4033362822/\">2</a>] so reach out to me for a referral!</p><p><em>Thanks to Mike Helmick for reading drafts of this article and giving feedback.</em></p>","contentLength":19657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42798230"}],"tags":["dev"]}