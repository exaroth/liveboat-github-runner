{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":16,"items":[{"title":"Reverse Engineering Bambu Connect","url":"https://wiki.rossmanngroup.com/wiki/Reverse_Engineering_Bambu_Connect","date":1737342529,"author":"pabs3","guid":171,"unread":true,"content":"<div lang=\"en\" dir=\"ltr\"><p>Bambu Connect is an Electron App with Security through Obscurity principles, hence it is inherently insecure.\n</p><p>To read the main.js for further analysis or extracting the private key stored by Bambu in the app:\n</p><ol><li>Use the MacOs .dmg file, not the exe. Finding the needed decryption code is easier in the .dmg</li><li>Extract <i>bambu-connect-beta-darwin-arm64-v1.0.4_4bb9cf0.dmg</i>, in there you can find the files of the underlying Electron app in <i>Bambu Connect (Beta).app/Contents/Resources</i> folder</li><li>The app uses asarmor to prevent easy reading, the key is stored in ./app.asar.unpacked/.vite/build/main.node and can be extracted. Unpacking app.asar without fixing it first will result in an encrypted main.js file and 100 GB of decoy files generated, don't try it.</li><li>Load main.node in Ghidra and Auto-Analyze it. Then search for the GetKey function, or press G and go to 0000b67e</li><li>Write down the hex key, for this build it's B0AE6995063C191D2B404637FBC193AE10DAB86A6BC1B1DE67B5AEE6E03018A2</li><li>Install the npm package asarfix and use it to fix the archive: <code>npx asarfix app.asar -k B0AE6995063C191D2B404637FBC193AE10DAB86A6BC1B1DE67B5AEE6E03018A2 -o fixed.asar</code></li><li>Now you can extract it in cleartext with  <code>npx asar extract fixed.asar src</code></li><li>./src/.vite/build/main.js is minified, use any JavaScript beautifier to make it better readable. Interesting user code including the private key is at the end of the file.</li></ol><p>The private key and certs are further obfuscated, to get cleartext you need to do: Encrypted string from cy() -&gt; ure(string, key) -&gt; RC4 decryption -&gt;  decodeURIComponent() -&gt; final string.\n</p><p>Example Python reimplementation to extract the secrets, easy to run. Copy the content of t from function cy() in main.js and paste it here. After running, you have a private key from Bambu Lab.\n</p><pre>import urllib.parse\n\ndef cy():\n    t = [\n\t\t# copy from main.js\n\t]\n    return t\n\ndef ure(t, e):\n    # RC4 implementation\n    r = list(range(256))\n    n = 0\n    s = \"\"\n    \n    # Key-scheduling algorithm (KSA)\n    for o in range(256):\n        n = (n + r[o] + ord(e[o&nbsp;% len(e)]))&nbsp;% 256\n        r[o], r[n] = r[n], r[o]\n    \n    # Pseudo-random generation algorithm (PRGA)\n    o = n = 0\n    for byte in t:\n        o = (o + 1)&nbsp;% 256\n        n = (n + r[o])&nbsp;% 256\n        r[o], r[n] = r[n], r[o]\n        k = r[(r[o] + r[n])&nbsp;% 256]\n        s += chr(byte ^ k)\n    \n    return s\n\ndef lt(t, e):\n    r = cy()\n    n = t - 106\n    s = r[n]\n    s = ure(s, e)\n    return urllib.parse.unquote(s)\n\ndef extract_certs_and_key():\n    try:\n        result = {}\n        result[\"Are\"] = lt(106, \"1o9B\")\n        result[\"fre\"] = lt(107, \"FT2A\")\n        result[\"private_key\"] = lt(108, \"Tlj0\")\n        result[\"cert\"] = lt(109, \"NPub\")\n        result[\"crl\"] = lt(110, \"x077\")\n    except Exception as e:\n        print(f\"Error extracting certs/key: {e}\")\n\n    for key, value in result.items():\n        print(f\"{key}:\\n{value}\\n\")\n\nif __name__ == \"__main__\":\n    extract_certs_and_key()\n</pre></div>","contentLength":2916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42764602"},{"title":"Ask HN: Is anyone making money selling traditional downloadable software?","url":"https://news.ycombinator.com/item?id=42764185","date":1737338171,"author":"101008","guid":170,"unread":true,"content":"<div>Curious if any HNers are running successful businesses selling desktop/downloadable software with a one-time payment model - not SaaS, not subscriptions. Something like the old days. How's the market for that? What's your experience with support and updates?</div>","contentLength":258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42764185"},{"title":"UK's hardware talent is being wasted","url":"https://josef.cn/blog/uk-talent","date":1737330733,"author":"sebg","guid":169,"unread":true,"content":"<p>Imperial, Oxford, and Cambridge produce world-class engineers. Yet post-graduation, their trajectory is an economic tragedy - and a hidden arbitrage opportunity.</p><ul><li data-preset-tag=\"p\"><p>Top London hardware engineer graduates: £30,000-£50,000</p></li><li data-preset-tag=\"p\"><p>Silicon Valley equivalent: $150,000+</p></li></ul><p>The reality for most graduates is even grimmer:</p><ul><li data-preset-tag=\"p\"><p>£25,000 starting salaries at traditional engineering firms</p></li><li data-preset-tag=\"p\"><p>Exodus to consulting or finance just because it's compensated better</p></li></ul><p>Meanwhile computer science graduates land lucrative jobs in big tech or quant trading, often starting at £100,000+</p><p>Examples of wasted potential:</p><ul><li data-preset-tag=\"p\"><p>Sarah: Built a fusion reactor at 16. Now? Debugging fintech payment systems.</p></li><li data-preset-tag=\"p\"><p>James: 3D-printed prosthetic limbs for A-levels. Today? Writing credit risk reports.</p></li><li data-preset-tag=\"p\"><p>Alex: Developed AI drone swarms for disaster relief at 18. Graduated with top honours from Imperial. His job? Tweaking a single button's ergonomics on home appliances.</p></li></ul><p>These aren't outliers. They're a generation of engineering prodigies whose talents are being squandered.</p><p>This isn't just wage disparity. <strong>It's misallocation of human capital on a national scale.</strong></p><p>As a hardware founder in London, I've witnessed this firsthand. We have the talent for groundbreaking innovation, but lack the means to realise it.</p><ol><li data-preset-tag=\"p\"><p>: Unlike lucrative software jobs, hardware engineering demands physical presence.</p></li><li data-preset-tag=\"p\"><p>European VCs, mostly bullish on fintech and SaaS, remain wary of hardware. Result? A feedback loop of underinvestment and missed opportunities.</p></li><li data-preset-tag=\"p\"><p> Traditional engineering firms fail to innovate in talent strategies and match compensation, accelerating brain drain.</p></li></ol><ol><li data-preset-tag=\"p\"><p> We're not just losing salary differences; we're missing out on the next ARM or Tesla.</p></li><li data-preset-tag=\"p\"><p> One successful hardware company can spawn dozens of ancillary businesses. We're losing these compounding effects.</p></li><li data-preset-tag=\"p\"><p><strong>National Security Implications: </strong>In an era where technological edge equals geopolitical power, can we afford to let our best hardware talent languish?</p></li><li data-preset-tag=\"p\"><p><strong>Brain Drain Acceleration: </strong>We risk losing our top talent permanently to overseas markets.</p></li></ol><blockquote><p>\"London's lower living costs justify lower salaries.\"</p></blockquote><p>False. London is around the same as NYC and more expensive than most parts of California and definitely Texas. This also ignores:</p><ul><li data-preset-tag=\"p\"><p>Wealth Creation and Ecosystem Acceleration: High salaries and successful exits compound dramatically over time, that's why the US has so much more VC and angel capital.</p></li><li data-preset-tag=\"p\"><p>Talent attraction: Top jobs draw global talent. Example: Google's entry into London with competitive salaries reshaped the entire tech ecosystem.</p></li></ul><blockquote><p>\"UK's small market limits growth.\"</p></blockquote><p>Outdated thinking. Consider:</p><ul><li data-preset-tag=\"p\"><p>Dyson: From a Wiltshire barn to a global technology powerhouse, now innovating in Singapore and Malaysia.</p></li><li data-preset-tag=\"p\"><p>Ocado: Online grocer turned global automation technology provider, with robotics solutions deployed across Europe and North America.</p></li><li data-preset-tag=\"p\"><p>ARM: Powering 95% of smartphones globally.</p></li></ul><blockquote><p>\"Hardware is riskier than software.\"</p></blockquote><ul><li data-preset-tag=\"p\"><p>Development speed: 3D prints and PCB prototypes now available in 24 hours, rivalling software iteration speeds.</p></li><li data-preset-tag=\"p\"><p>Moat strength: Apple's hardware-software ecosystem is far more defensible than most pure software plays.</p></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>ARM: Sold to SoftBank for $32B in 2016, now worth $140B+.</p></li><li data-preset-tag=\"p\"><p>CSR (Cambridge Silicon Radio): Acquired by Qualcomm for $2.5B in 2015.</p></li><li data-preset-tag=\"p\"><p>Dyson: While not an exit, it's valued at over £20B as of 2023.</p></li></ul></li></ul><p>It isn't about costs. It's about ambition.</p><p>While software talent flows freely globally, ambitious UK hardware startups can exclusively tap into a world-class, locally-bound talent pool.</p><ol><li data-preset-tag=\"p\"><p><strong>The Software Brain Drain:</strong></p><ul><li data-preset-tag=\"p\"><p>US tech giants easily poach UK software talent</p></li><li data-preset-tag=\"p\"><p>Remote work erases geographical boundaries</p></li><li data-preset-tag=\"p\"><p>Result: Constant outflow of top software engineers</p></li></ul></li><li data-preset-tag=\"p\"><p><strong>The Hardware Opportunity:</strong></p><ul><li data-preset-tag=\"p\"><p>Physical presence matters - can't build rockets remotely</p></li><li data-preset-tag=\"p\"><p>UK hardware talent largely untapped by global competition</p></li><li data-preset-tag=\"p\"><p>Build something ambitious, attract local engineering superstars</p></li></ul></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>Brilliant minds wasting away in soul-crushing corporate jobs</p></li><li data-preset-tag=\"p\"><p>Your future \"10x engineer\" is someone else's bored employee</p></li></ul></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>Forget software. Hardware is the new frontier.</p></li><li data-preset-tag=\"p\"><p>Build the next ARM or Dyson, not another fintech app</p></li><li data-preset-tag=\"p\"><p>Leverage UK's world-class research institutions</p></li></ul></li><li data-preset-tag=\"p\"><ul><li data-preset-tag=\"p\"><p>Incumbents are unambitious, startups are few (for now)</p></li><li data-preset-tag=\"p\"><p>Top-tier VCs awakening to UK hardware potential</p></li><li data-preset-tag=\"p\"><p>First movers will have pick of the talent pool</p></li></ul></li></ol><p>This arbitrage won't last forever. As you read this, others are waking up to the opportunity. The first movers will reap the rewards. The followers will wonder why they didn't see it sooner.</p><p><strong>The Hardware Revolution Starts Now</strong></p><p>Wake up, UK. Our engineering talent is our nuclear fusion. Ignite it or lose the future.</p><p>Your next unicorn isn't code. It's cobalt and circuits. Back the tangible.</p><p>Stop fleeing to the US. London can be the hardware capital of the world. We have the talent. We have the creativity. What we need is your audacity.</p><p>Your brain's worth billions. Build empires, not apps.</p><p>While the world obsesses over the next GPT wrapper, we'll forge the next industrial revolution.</p><p>This isn't a pipe dream. It's an imperative.</p>","contentLength":4978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42763386"},{"title":"FrontierMath was funded by OpenAI","url":"https://www.lesswrong.com/posts/cu2E8wgmbdZbqeWqb/meemi-s-shortform","date":1737329279,"author":"wujerry2000","guid":168,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42763231"},{"title":"It's time to make computing personal again","url":"https://www.vintagecomputing.com/index.php/archives/3292/the-pc-is-dead-its-time-to-make-computing-personal-again","date":1737328282,"author":"mariuz","guid":167,"unread":true,"content":"<p><em>How surveillance capitalism and DRM turned home tech from friend to foe.</em></p><p>For a while—in the ’80s, ’90s, and early 2000s—it <a href=\"https://www2.itif.org/2017-why-so-sad.pdf\">felt like</a> nerds were making the world a better place. Now, it feels like the most successful tech companies are making it worse.</p><p>Internet surveillance, the <a href=\"https://www.npr.org/2022/09/09/1121295499/facebook-twitter-youtube-instagram-tiktok-social-media\">algorithmic polarization</a> of social media, <a href=\"http://<a href=\" https:=\"\" techcrunch.com=\"\" 2018=\"\" 10=\"\" 15=\"\" sneaky-subscriptions-are-plaguing-the-app-store=\"\" \"=\"\">predatory app stores</a>, and extractive business models have eroded the freedoms the personal computer once promised, effectively ending the PC era for most tech consumers.</p><p>The “personal computer” was <a href=\"https://en.wikipedia.org/wiki/Computer_Lib/Dream_Machines\">once a radical idea</a>—a computer an individual could own and control completely. The concept emerged in the early 1970s when microprocessors made it economical and practical for a person to own their very own computer, in contrast to the rise of data processing mainframes in the 1950s and 60s.</p><p>At its core, the PC movement was about a kind of tech liberty—–which I’ll define as the freedom to explore new ideas, control your own creative works, and make mistakes without punishment.</p><p>The personal computer era bloomed in the late 1970s and continued into the 1980s and 90s. But over the past decade in particular, the Internet and <a href=\"https://en.wikipedia.org/wiki/Digital_rights_management\">digital rights management</a> (DRM) have been steadily pulling that control away from us and putting it into the hands of huge corporations. We need to take back control of our digital lives and make computing personal again.</p><p>Don’t get me wrong: I’m not calling the tech industry evil. I’m a huge fan of technology. The industry is full of great people, and this is not a personal attack on anyone. I just think runaway market forces and a handful of <a href=\"https://www.theatlantic.com/technology/archive/2013/03/the-copyright-rule-we-need-to-repeal-if-we-want-to-preserve-our-cultural-heritage/274049/\">poorly-crafted US laws</a> like section 1201 of the DMCA have put all of us onto the wrong track (more on that below).</p><p>To some extent, tech companies were always predatory. To some extent, all companies are predatory. It’s a matter of degrees. But I believe there’s a fundamental truth that we’ve charted a deeply unhealthy path ahead with consumer technology at the moment.</p><p>Tech critic Ed Zitron calls this phenomenon “<a href=\"https://www.wheresyoured.at/the-rot-economy/\">The Rot Economy</a>,” where companies are more obsessed with continuous growth than with providing useful products. “Our economy isn’t one that produces things to be used, but things that increase usage,” Zitron <a href=\"https://www.wheresyoured.at/the-anti-economy/\">wrote</a> in another piece, bringing focus to ideas I’ve been mulling for the past half-decade.</p><p>This post started as a 2022 <a href=\"https://x.com/benjedwards/status/1541029918783311873\">Twitter thread</a>, and I’ve offered to write editorials about my frustrations with increasingly predatory tech business practices since 2020 for my last two employers, but both declined to publish them. I understand why. These are uncomfortable truths to face. But if you love technology like I do, we have to accept what we’re doing wrong if we are going to make it better.</p><p>While consumer and computer tech today is more powerful than ever before—and in some ways far more convenient—some of the structural ways we used to interface with technology companies were arguably healthier in the past.</p><p>Further, what percentage of your income had to go towards annual <a href=\"https://variety.com/2023/digital/news/apple-one-billion-paid-subscriptions-services-earnings-june-2023-1235686807/\">software subscriptions</a> on a 20th century Windows PC (like this Sony VAIO)? You bought an application and you owned an indefinite license to use it. If there was an upgrade, you bought that too. And if you liked an older version of the software, you could keep using it without having it vanish in an automatic update.</p><p>How many Nintendo Entertainment System games sustained themselves with <a href=\"https://appleinsider.com/articles/20/12/13/kid-spends-16k-on-in-app-purchases-for-ipad-game-sonic-forces\">in-app purchases</a> and <a href=\"https://www.reddit.com/r/Games/comments/znpaau/i_worked_in_the_mobile_industry_for_10_years_loot/\">microtransactions</a>? What more did the console ask of you after you bought a cartridge? Maybe to buy another one later if it was fun?</p><p>Which part of this Motorola StarTAC cellular phone kept track of your every move and <a href=\"https://www.zdnet.com/article/us-cell-carriers-selling-access-to-real-time-location-data/\">sold the information</a>, behind your back, to private data brokers? And which part included <a href=\"https://www.consumerreports.org/consumer-rights/people-want-to-get-phones-appliances-fixed-but-often-cant-a1117945195/\">sealed-in batteries</a> that would ruin the entire phone if they went bad?</p><p>Which part of Google in the 1990s and early 2000s blanketed its results with <a href=\"https://www.reddit.com/r/google/comments/yz3e85/google_shows_more_ads_than_results_now/\">deceptive ads</a> or made you <a href=\"https://boingboing.net/2022/01/03/tip-add-reddit-to-search-queries-on-to-get-authentic-human-results-untainted-by-seo.html\">add “Reddit”</a> to every search to get good results that weren’t overwhelmed by SEO-seeking filler content?</p><p>Which part of this VHS tape disappeared or became unplayable if the publisher suddenly <a href=\"https://www.theguardian.com/tv-and-radio/2023/jun/28/why-are-movies-and-tv-shows-disappearing-from-streaming-services\">decided</a> it didn’t like it anymore—or didn’t want to <a href=\"https://www.npr.org/2023/03/17/1164146728/why-are-dozens-of-tv-shows-disappearing-from-streaming-platforms-like-hbo-max\">pay the writers and actors</a> residual fees?</p><p>Americans have allowed runaway business models, empowered by tech, to subvert privacy and individual liberty on the road to making money. Our default tech business model has become extractive, like part of a strip-mining operation. Consumers—and now <a href=\"https://arstechnica.com/information-technology/2022/09/have-ai-image-generators-assimilated-your-art-new-tool-lets-you-check/\">creative works</a> (used for training AI)—are treated as a natural resource to be milked and exploited.</p><p>The extractive model may end up being self-destructive for the tech industry itself. In the physical world, resource extraction needs limits and regulations to be sustainable. It can be wildly profitable until a resource becomes over-harvested, or the harvesting process corrupts the environment that lets the industry exist in the first place.</p><p>There’s also the drive to lock consumers into an ecosystem, powered by DRM. You should buy tech products and get direct value fairly, not unleash a secret vampire to track you, manipulate you, and attempt to extract money from you forever. Just because companies have unlocked this “everything as a service” endless money hack does not mean they should do it.</p><p>And there’s another problem. Very soon, we might be <a href=\"https://www.fastcompany.com/90549441/how-to-prevent-deepfakes\">threatening the continuity of history itself</a> with technologies that pollute the historical record with AI-generated noise. It sounds dramatic, but that could eventually undermine the shared cultural bonds that hold cultural groups together.</p><p>That’s important because history is what makes this type of criticism possible. History is how we know if we’re being abused because we can rely on written records of people who came before (even 15 minutes ago) and make comparisons. If technology takes history away from us, there may be no hope of recovery.</p><h2>How We Can Reclaim Control</h2><p>Every generation <a href=\"https://twitter.com/paulisci/status/1669113362058444800?s=20\">looks back</a> and says, “Things used to be better,” whether they are accurate or not.</p><p>But I’m not suggesting we live in the past. It is possible to learn from history and integrate the best of today’s technology with fair business practices that are more sustainable and healthy for everyone in the long run.</p><p>In the short term, we can do things like support open projects like Linux, support non-predatory and open source software, and run apps and store data locally as much as possible. But some bigger structural changes are necessary if we really want to launch the era of Personal Computer 2.0.</p><p>I’ve shown this editorial to friends, and some people felt that I did not emphasize the benefits of current technology enough. But I argue that my criticism is less about the actual technology and more about how we use it—and how companies make money from it.</p><p>Since I originally wrote my thoughts in a <a href=\"https://x.com/benjedwards/status/1541029918783311873?s=20\">viral Twitter thread</a> in June 2022, others have expanded on these ideas with far more eloquence. Five months after my thread, Cory Doctorow wrote <a href=\"https://doctorow.medium.com/social-quitting-1ce85b67b456\">his first</a> post on “enshittification,” a hallmark piece identifying a tendency for online platforms to decay over time.</p><p>A common thread between many of the issues hinted at above and in both Doctorow and Zitron’s work has been the rise of the ubiquitous Internet, which has allowed content owners and device makers to keep an eye on (and influence) consumer habits remotely, pulling our strings like puppeteers and putting a drip feed on our wallets.</p><p>Additionally, <a href=\"https://en.wikipedia.org/wiki/Anti-circumvention\">section 1201</a> of the <a href=\"https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act\">DMCA</a> made it illegal to circumvent DRM, allowing manufacturers to lock down platforms in a way that challenges the traditional concept of ownership, enables predatory app stores, and <a href=\"https://www.theatlantic.com/technology/archive/2013/03/the-copyright-rule-we-need-to-repeal-if-we-want-to-preserve-our-cultural-heritage/274049/\">threatens our cultural history</a>.</p><p>Tech monopolies must be held to account, the outsized influence of some tech billionaires must be held in check, and competition must be allowed to thrive. We may also need to consider the protection of both consumers themselves and human-created works (including our history) as part of a conservation effort before extractive models permanently pollute our shared cultural resources.</p><p>The way the political winds are blowing right now in the US, significant legal reform seems unlikely for now. Things may need to get much worse before they get better. But the kind of extractive lock-in we’re seeing with technology is fundamentally incompatible with freedom in my opinion, so something needs to change if we still value the kind of personal liberty the PC once promised—that freedom to explore, create, and make mistakes without surveillance or punishment.</p><p>Sure, things will never be perfect in the United States. Profits will always be chased, and there will be collateral damage. And yes, some parts of technology today are better than ever (computing power, screen resolutions, and bandwidth to name a few).</p><p>The Internet has brought amazing things, including Wikipedia, The Internet Archive, multiplayer online gaming, , and work-from-home jobs. But the urge to exploit users to the maximum extent through digital locks and surveillance should be held in check so that we can earnestly and honestly make the tech industry a beacon of optimism once more.</p><p>The stakes are higher now than they were in the 1970s. There is no “logging off,” nearly everyone has a smartphone in their pocket, and the digital world increasingly overlaps with every aspect of our lives. That means digital freedom is now equivalent to actual legal and personal freedom, and we must be allowed to control our own destinies.</p><p>Whether through purposeful reform or the eventual collapse of digital strip mining, I believe the personal computer will eventually rise again—–along with our chance to reclaim control of our digital lives.</p>","contentLength":9705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42763095"},{"title":"Why is Git Autocorrect too fast for Formula One drivers?","url":"https://blog.gitbutler.com/why-is-git-autocorrect-too-fast-for-formula-one-drivers/","date":1737314423,"author":"birdculture","guid":166,"unread":true,"content":"<p>A while ago, I happened to see <a href=\"https://x.com/dhh/status/1853955671647260806?ref=blog.gitbutler.com\" rel=\"noreferrer\">a tweet</a> from <a href=\"https://x.com/dhh?ref=blog.gitbutler.com\" rel=\"noreferrer\">@dhh</a> where he mistyped a Git command as  and was surprised to notice that Git figured out that he probably meant  and then gave him  to verify if that's what he wanted to run before it ran it anyways.</p><p>As David is a semi-professional <a href=\"https://x.com/dhhracing?ref=blog.gitbutler.com\" rel=\"noreferrer\">race car driver</a> in addition to being a fellow Ruby programming nerd, he naturally noticed that the amount of time that Git afforded him to react was impossible for even Formula One drivers.</p><p>Of course this seems like a ludicrous bit of Git functionality, but I figured if this was surprising to David, you too might wonder why Git gave him (and possibly gives you) about the length of time that it takes a human eye to blink in order to:</p><ul><li>determine if it's correct</li><li>attempt to cancel the command</li></ul><p>What could possibly be the reason to wait ? So little time is essentially equivalent to simply running the command.</p><p>Well, it's a combination of a misunderstanding, a misconfiguration, and the suggestion, 17 years ago, of a somewhat questionable unit of time by the Git maintainer himself.</p><h2>How was this designed to work?</h2><p>It's important to note that this is  the default functionality of Git.</p><p>The  response to typing a command that doesn't exist is to simply not run anything, figure out which commands you might have meant by string similarity and then just exit. </p><p>If most of you type , you'll probably get this instead:</p><pre><code>❯ git pushy\ngit: 'pushy' is not a git command. See 'git --help'.\n\nThe most similar command is\n        push</code></pre><p>Originally, if you typed an unknown command, it would just say \"this is not a git command\". Then in 2008, Johannes Schindelin (somewhat jokingly) introduced a <a href=\"https://public-inbox.org/git/alpine.DEB.1.00.0807222100150.8986@racer/?ref=blog.gitbutler.com\" rel=\"noreferrer\">small patch</a> to go through all the known commands, show you what is most similar to what you typed and if there is only one closely matching, simply run it.</p><p>Then Alex Riesen introduced <a href=\"https://public-inbox.org/git/20080722210354.GD5113@blimp.local/?ref=blog.gitbutler.com\" rel=\"noreferrer\">a patch</a> to make it configurable via the  setting. In this initial patch, this setting was simply a boolean. </p><p>Since Git config settings that expect a boolean will interpret a  value as , you could originally set  to  to have it automatically run the corrected command rather than just tell you what is similar.</p><p>As part of the conversation around this patch, Junio Hamano, to this day the Git maintainer, <a href=\"https://public-inbox.org/git/7vsku1jz4u.fsf@gitster.siamese.dyndns.org/?ref=blog.gitbutler.com\">suggested</a>:</p><pre><code>Please make autocorrect not a binary but optionally the number of\ndeciseconds before it continues, so that I have a chance to hit ^C ;-)</code></pre><p>Which was what the setting value was changed to in the patch that was eventually accepted. This means that setting  to  logically means \"wait 100ms (1 decisecond) before continuing\".</p><p>Now, why Junio thought  was a reasonable unit of time measurement for this is never discussed, so I don't really know why that is. Perhaps 1 full second felt too long so he wanted to be able to set it to half a second? We may never know. All we truly know is that this has never made sense to anyone ever since.</p><p>, the reason why it waits 100ms for David is that at some point he presumably learned about this setting, quite reasonably assumed that it was a boolean and set it to what Git config also generally considers to be a 'true' value in order to enable it:</p><pre><code>❯ git config --global help.autocorrect 1</code></pre><p>Not understanding that in this context, this means \"wait 1 decisecond, then do whatever you think is best\" rather than \"please turn this feature on\".</p><p>So, clearly you can set it to  for a full second or whatever. However, over the years, this setting has gathered a few other options that it will recognize. </p><p>According to the <a href=\"https://git-scm.com/docs/git-config?ref=blog.gitbutler.com#Documentation/git-config.txt-helpautoCorrect\" rel=\"noreferrer\">documentation</a>, here are the values it can be set to:</p><ul><li>0 (default): show the suggested command.</li><li>positive number: run the suggested command after specified deciseconds (0.1 sec).</li><li>\"immediate\": run the suggested command immediately.</li><li>\"prompt\": show the suggestion and prompt for confirmation to run the command.</li><li>\"never\": don’t run or show any suggested command.</li></ul><p>Honestly, \"prompt\" is probably what most people would find the most reasonable, rather than a specific amount of time to wait for you to cancel the command.</p><p>If you  want to have it prompt you, you can run this:</p><pre><code>❯ git config --global help.autocorrect prompt\n\n❯ git pushy\nWARNING: You called a Git command named 'pushy', which does not exist.\nRun 'push' instead [y/N]?</code></pre><p>To keep picking on David, he followed up after doing some quick testing to see what the logic could be and it turns out that Git won't just take wild guesses. </p><p>There is a point where it will simply assume you're way off and not guess anything:</p><p>However, it's interesting to play around with this a bit:</p><pre><code>❯ git bass\nWARNING: You called a Git command named 'bass', which does not exist.\nRun 'rebase' instead [y/N]? n\n\n❯ git bassa\ngit: 'bassa' is not a git command. See 'git --help'.\n\n❯ git dm\ngit: 'dm' is not a git command. See 'git --help'.\n\nThe most similar commands are\n        am\n        rm\n\n❯ git dma\nWARNING: You called a Git command named 'dma', which does not exist.\nRun 'am' instead [y/N]?</code></pre><p>So,  is close enough to  for it to guess that this could be what you mean. But  is not close enough for it to  think you maybe meant .</p><p>Also,  could mean  or , but interestingly it matches on the end of the string and not necessarily from the beginning. Also,  confidently matches .</p><p>As some of you may have guessed, it's based on a fairly simple, modified Levenshtein distance algorithm - which is basically a way to figure out how expensive it is to change one string into a second string given single character edits, with some operations being more expensive than others. </p><p>It has a hard coded cutoff, so once it's too expensive for any of the known commands, it just assumes you really messed up, which is why some of these don't match anything and others, even though quite different, match several options.</p><p>In going through a bunch of the related autocorrect Git code in order to research this little blog post, I realized that there could be a relatively simple and largely backwards compatible fix. </p><p>Since a  value is so fast, it's in all human terms functionally equivalent to \"immediately\", I wrote up a <a href=\"https://lore.kernel.org/git/09e516e7-37a5-4489-a30b-f26dd2462fc3@revi.email/T/?ref=blog.gitbutler.com#t\" rel=\"noreferrer\">small patch</a> to interpret a  as \"immediately\" rather than \"wait 100ms\".</p><p>Junio came back to request that instead of special casing the \"1\" string, we should properly interpret any boolean string value (so \"yes\", \"no\", \"true\", \"off\", etc), so version two of my patch is currently in flight to additionally do that. </p><p>If I can get this landed, maybe future versions of Git will no longer test the mettle of Formula One drivers.</p><p>Anyhow, hope you enjoyed that little trip down this old alley of seemingly strange Git functionality. As is often the case with Git, there is some hidden method to the apparent madness, and like any open source project, there is a path to make it slightly better!</p>","contentLength":6695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42760620"},{"title":"Using your Apple device as an access card in unsupported systems","url":"https://github.com/kormax/apple-device-as-access-card","date":1737309702,"author":"ValentineC","guid":165,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42759557"},{"title":"TikTok says it is restoring service for U.S. users","url":"https://www.nbcnews.com/tech/tech-news/tiktok-says-restoring-service-us-users-rcna188320","date":1737308564,"author":"Leary","guid":164,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42759336"},{"title":"The surprising struggle to get a Unix Epoch time from a UTC string in C or C++","url":"https://berthub.eu/articles/posts/how-to-get-a-unix-epoch-from-a-utc-date-time-string/","date":1737303011,"author":"PascalW","guid":163,"unread":true,"content":"<p>So how hard could it be. As input we have something like <code>Fri, 17 Jan 2025 06:07:07</code> in UTC, and we’d like to turn this into 1737094027, the notional (but not actual) number of seconds that have passed since 1970-01-01 00:00:00 UTC.</p><p>Trying to figure this out led me to discover many ‘surprise features’ and otherwise unexpected behaviour of POSIX time handling functions as implemented in various C libraries &amp; the languages that build on them. There are many good things in the world of C and UNIX, but time handling is not one of them.</p><p>There is a narrow path of useful behaviour however. But first some context.</p><blockquote><p>The tl;dr: <strong>as long as you never call</strong>, you can use  to parse a UTC time string. Do not use %z or %Z. Pass the  calculated by  to the pre-standard function  ( on Windows) to get the correct UNIX epoch timestamp for your UTC time string. Do read on for solutions for if you do use locales. C++ has better support, which could also help you from C.</p></blockquote><p>Time is difficult enough by itself, even if we ignore leap seconds and <a href=\"https://en.wikipedia.org/wiki/Barycentric_Dynamical_Time\">general relativity</a>. When we add human behaviour and politics, it all becomes exceptionally challenging. Timestamps as used by human beings range from impossible to imprecise.</p><p>For example in Amsterdam, “the 30th of March 2025, 02:20” does not exist as a time:</p><pre tabindex=\"0\"><code>$ TZ=Europe/Amsterdam date -d '20250330 01:59:59'\nSun Mar 30 01:59:59 AM CET 2025\n$ TZ=Europe/Amsterdam date -d '20250330 02:30:00'\ndate: invalid date ‘20250330 02:30:00’\n</code></pre><p>This at least is clear. Because of daylight saving time we go straight from 01:59:59 to 03:00:00. Our tooling rightfully refuses to parse “02:30:00” since that timestamp never exists in Amsterdam on that day.</p><p>But “the 27th of October 2024, 02:30” is harder to interpret. The second after 02:59:59, it is 02:00:00 again. This means we have  points in time locals would call ‘02:00’. And here already we can see our tooling starting to make arbitrary choices:</p><pre tabindex=\"0\"><code>$ TZ=Europe/Amsterdam date -d '20241027 01:59:59' +\"%Y-%m-%d %H:%M:%S %s %z\"\n2024-10-27 01:59:59 1729987199 +0200\n$ TZ=Europe/Amsterdam date -d '20241027 02:00:00' +\"%Y-%m-%d %H:%M:%S %s %z\"\n2024-10-27 02:00:00 1729990800 +0100\n</code></pre><p>Apparently, when asked to interpret 02:00:00, my copy of GNU date picks the  time this happens. As far as I can tell, this is because I’m running the command in January. In April it would likely have picked the first 02:00:00 instance. Wild eh?</p><p>The only useful way anyone should ever be specifying a point of time is of course as a number of seconds after or before a known ’epoch’. For POSIX/Unix this is 1970-01-01 00:00:00 UTC, for GPS this is 1980-01-06 00:00:00 UTC, for Galileo (‘EU GPS’) 21st of August 1999, 23:59:47 UTC, for BeiDou 2006-01-01 00:00:00 UTC. GPS, Galileo and BeiDou wisely ignore leap seconds, leaving these as things for human beings to worry about.</p><p>But, our preference for the POSIX/Unix “time_t” is well founded. There is never any ambiguity, except during leap seconds, which might never happen again.</p><p>However, human beings have a hard time parsing 1737214750, so we do need to convert to and from timestamps that include messy things like ‘months’. To this end, UNIX offered us , holding the ‘broken-down time’:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>The standards say that  contains  these fields. There could be more. Now, this struct is of course wildly overdetermined. Day of the week and day of the year follow from the rest, for example. The meanings of ,  and  are badly specified and also badly understood, and vary based on how the struct is used.</p><blockquote><p>Interestingly, the Soviet GLONASS satellite navigation system is not based on an epoch timestamp.  They took the  approach based on ‘Moscow wall clock time’, including leap seconds. This reportedly causes lots of problems. <a href=\"https://en.wikipedia.org/wiki/Slava_Ukraini\">And they deserve them</a>.</p></blockquote><p>One major role of  is as input to , which as  of its work turns a ‘broken-down time <strong>according to your local TZ</strong>’ into a UNIX epoch timestamp. However, it also does many other things!</p><blockquote><p>Note that at least the Linux glibc manpage for  is pretty vague. The <a href=\"https://pubs.opengroup.org/onlinepubs/9799919799/functions/mktime.html\">IEEE Std 1003.1-2024</a> specification offers a lot more (discouraging) words.</p></blockquote><p>It is important to understand that  does absolutely nothing with  or . Its inputs are defined to exclusively be: , , , , , , and .  can be negative, which means that  should figure out if daylight saving time is active at the specified time.</p><p>As noted, time is difficult. If you for example want to adjust a date by a week, you could add 604800 seconds to a  timestamp. However, if that adjustment crosses a daylight savings time boundary, your 2PM appointment might suddenly turn into a 1PM or 3PM appointment next week. Humans do not expect this.</p><p> not only returns a , it also  the  you passed it. And, at least as of 2024 <a href=\"https://pubs.opengroup.org/onlinepubs/9799919799/functions/mktime.html\">there are rules for how it should do so</a>. This means that to get the date that a human being would identify as “the same time next week” you can take the current time, add 7 to , and call  again. Although you just created a date like ‘March 35’,  will fix that up for you.</p><p>Now, if we do this, we find that it doesn’t work:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>Here in the Europe/Amsterdam timezone this prints:</p><pre tabindex=\"0\"><code>original:        Fri Mar 28 14:00:00 2025\nmktime adjusted: Fri Apr  4 15:00:00 2025\n</code></pre><p>What happened, why did our appointment shift by an hour?  is what happened.  may not be perfect, but it does force you to make up your mind. Is the time it is looking at daylight savings time or not? Or, at your choice, do you want  to take a stab at figuring that out for you? The latter option is what we initially chose by setting  to -1.</p><p>When we then ran , it discovered we were initially not in daylight saving time, so it set  to 0. When we ran  for the second time, this DST setting remained in place, even though the new intended time  happen in DST. The fix is to reset  to -1 before the second call.</p><p>Now,  will interpret whatever you pass it as “local time”. This in means you should set your timezone to UTC before calling  to process a UTC time. Changing the timezone for your whole application however might have side effects if you have other threads running. But, you could do that if you have no other threads.</p><p>There is a non-standard/pre-standard function that is very widely available that makes the UTC situation a lot better. From IEEE Std 1003.1-2024: “A future version of this standard is expected to add a  function that is similar to , except that the tm structure pointed to by timeptr contains a broken-down time in Coordinated Universal Time”.</p><ol><li>When using  on local times, set  to -1, which is almost always what ‘human beings’ expect. There is a chance you randomly get back one of two instances of ‘02:30’ (or equivalent) during DST fall back.</li><li>Make sure to zero the rest of  before filling it out, just to be sure.</li><li>Be aware that  does surgery on your , and that this may have side effects. At least reset  before reuse.</li><li>No matter what you do with  or ,  will use your current timezone. If you want it to interpret your  as UTC, you actually need to set the TZ environment variable to UTC. This will however mess with any other threads doing time operations.</li><li>Just use  or  instead</li></ol><p>But, how do we get our time string into a ?</p><p>Now, it would be lovely if we could feed <code>Fri, 17 Jan 2025 06:07:07 GMT</code> into  and get a sensible  in return. The <a href=\"https://man7.org/linux/man-pages/man3/strptime.3.html\">Linux glibc  manpage</a> has some airy words about how the %z and %Z timezone format specifiers might or likely might not do something. You just can’t tell:</p><blockquote><p>“For reasons of symmetry, glibc tries to support for strptime() the same format characters as for strftime(3).  (In , the corresponding fields are parsed, but no field in tm is changed.)</p></blockquote><p>Now, our goal is to convert a UTC time string into a UNIX epoch timestamp. Many people justifiably harbor some expectations that  with %Z (to parse ‘GMT’) followed by  could make that happen.</p><p>Above we learned however that  does not look at the  or  fields at all, so there really is nothing that  can achieve there, even if it did the right thing. Oh, and it also does not do the right thing.</p><p>Because of this  has no more specified behaviour for ‘%z’, which was one day supposed to do something with “+0200” style offset identifiers, which would have been lovely. But you can’t count on ‘%z’ doing anything useful. <a href=\"https://en.wiktionary.org/wiki/nasal_demon\">Nasal demons</a> might ensue.</p><p>There is however some wording on , but it is very limited. If your locale is known to have a DST timezone identifier (‘CEST’) that is different from the regular time zone (‘CET’), and if ‘%Z’ sees any of these two, it will set  to the right value for you. <a href=\"https://www.redhat.com/en/blog/brief-history-mktime\">Except possibly if you live in Ireland</a>. In general, it is also not useful to try to parse a string like ‘EST’, as it has no well defined meaning anyhow, except perhaps locally.</p><p>Luckily, because we discovered  above, we can just ignore %z and %Z as we don’t need them anyhow.</p><p>Now, what I didn’t know is that unless they specifically ask for it, C and C++ programs will stick to the “C” locale, which effectively is American English. This means that out of the box all LC_TIME etc environment variables are ignored. For our purposes of parsing time strings found in data, this is usually great, since these are almost exclusively in English.</p><p>However, if you are in a C or C++ program that  call , thus asking for a possibly non-C locale, suddenly your program might only work for Dutch time strings. And these are quite rare.</p><p>Now, you might ponder changing the locale to “C” before calling  and then changing it back, but sadly  is not safe to call in multithreaded programs (except before launching threads). Also you might confuse the output of other threads even if this was safe.</p><p>So in general, if you need to parse specific time strings and you want to use , do make sure your program is on the locale you expect it to be. And although  exists, in which you can specify the locale to use for formatting time, the equivalent  is not officially available.</p><p>Alternatively, it is not that hard to parse a string like  and fill out a  and then let  do the actual hard work of calculating a UNIX epoch timestamp.</p><p>C++ iostreams are not much loved, but they did have a better think about locales than C/POSIX did. In C++ you can set the locale per iostream. <a href=\"https://github.com/berthubert/utchelper\">Here is a C++ helper that you can call from C</a> if you want to parse arbitrary UTC time strings in programs that do set the locale:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>This incidentally also shows how to do error handling for , which will helpfully return -1, , if you ask it to look at 31st of December 1969 23:59. The trick is to employ  as a sentinel to see if anything was processed or not. This tells you there was no error.</p><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><p>C++20 and beyond contain a luxurious timezone database. This is not yet available on all compilers, but luckily the <a href=\"https://howardhinnant.github.io/date/tz.html\">pre-standardized version</a> is available for standalone use. Sometimes we get lucky because someone sacrifices a few years of their life to give us some truly excellent code that we really don’t deserve, and Howard Hinnant clearly delivered.</p><p>Here is a glorious example:</p><div><pre tabindex=\"0\"><code data-lang=\"C++\"></code></pre></div><p>This picks “the first Monday of May 2016, 9AM local time in New York”, and then seamlessly converts this to two other timezones:</p><pre tabindex=\"0\"><code>The New York meeting is 2016-05-02 09:00:00 EDT\nThe London   meeting is 2016-05-02 14:00:00 BST\nThe Sydney   meeting is 2016-05-02 23:00:00 AEST\n</code></pre><p>Being truly luxurious, the tz library supports using not just your operating system’s time zone databases, which might lack crucial leap second detail, but can also source the IANA tzdb directly. This allows you to faithfully calculate the actual duration of a plane flight in 1978 that not only passed through a DST change, but also an actual leap second. I don’t swoon easily, <a href=\"https://howardhinnant.github.io/date/tz.html#Examples\">but I’m swooning</a>.</p>","contentLength":11638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42758257"},{"title":"Build a tiny CA for your homelab with a Raspberry Pi","url":"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/","date":1737301851,"author":"timkq","guid":162,"unread":true,"content":"<time datetime=\"{updatedAt}\">Updated on: January 19, 2025</time><p> In this tutorial, we're going to build a tiny, standalone, online Certificate Authority (CA) that will mint TLS certificates and is secured with a YubiKey. It will be an internal ACME server on our local network (ACME is the same protocol used by <a href=\"https://letsencrypt.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Let's Encrypt</a>). The YubiKey will securely store the CA private keys and sign certificates, acting as a cheap alternative to a Hardware Security Module (HSM). We'll also use an open-source True Random Number Generator, called <a href=\"https://www.crowdsupply.com/13-37/infinite-noise-trng/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Infinite Noise TRNG</a>, to spice up the Linux entropy pool.</p><h3>Why would I want a Certificate Authority in my homelab?!<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#why-would-i-want-a-certificate-authority-in-my-homelab\"></a></h3><ul><li>Because end-to-end TLS is great and you should easily be able to run TLS wherever you need it. Especially in your homelab. Internal networks are no longer perceived as a safe zone where unencrypted traffic is okay. But you need certificates.</li><li>Because the ACME protocol (used by Let's Encrypt) can easily be deployed internally, so you can automate renewal and never have to think about your certificates.</li><li>Because maybe you've done the 'self-signed certificate' rigmarole with OpenSSL a dozen times already. Might as well formalize things and get your devices to trust a CA that you can use wherever you need it.</li><li>Because setting up a simple CA is a great learning experience.</li></ul><h3>Basic OS &amp; Networking Setup<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#basic-os--networking-setup\"></a></h3><ul><li>Fire up the Raspberry Pi, plug it into your network, and find its initial IP address.\nYou can run <code>arp -na | grep -e \"b8:27:eb\" -e \"dc:a6:32\" -e \"e4:5f:01\"</code> to discover Raspberry Pi devices on the local network.</li><li>Login via SSH (username and password will be ), and change the password.</li><li>Set the hostname via <code>hostnamectl set-hostname tinyca</code></li><li>Set the timezone using <code>timedatectl set-timezone America/Los_Angeles</code> (or whatever your timezone is; <code>timedatectl list-timezones</code> will list them all)</li><li>Be sure NTP is working. Check status with — make sure \"NTP Service\" is \"active\". If not, you can add some NTP servers to <code>/etc/systemd/timesyncd.conf</code> and run <code>systemctl restart systemd-timesyncd</code>.</li><li>You'll need the machine to have a DNS name (for me it's ) and/or a static IP on your network.</li></ul><p>Now that you have good time synchronization and a stable hostname, we can proceed.</p><h3>Install prerequisite: <a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#install-prerequisite-ykman\"></a></h3><p>Now, insert your YubiKey. Let's install the  (and dependency ) and make sure you can connect to the YubiKey:</p><pre><section><code><pre>$ sudo apt update\n$ sudo apt install -y yubikey-manager\n$ ykman info\nDevice type: YubiKey 5 NFC\nSerial number: 13910388\nFirmware version: 5.2.7\nForm factor: Keychain (USB-A)\nEnabled USB interfaces: OTP+FIDO+CCID\nNFC interface is enabled.\n</pre></code></section></pre><p>You'll need Go in order to build the  server.</p><pre><section><code><pre>$ cd\n$ curl -LO https://go.dev/dl/go1.20.1.linux-arm64.tar.gz\n$ sudo tar -C /usr/local -xzf go1.20.1.linux-arm64.tar.gz\n$ echo \"export PATH=\\$PATH:/usr/local/go/bin\" &gt;&gt; .profile\n$ source .profile\n$ go version\ngo version go1.20.1 linux/arm64\n</pre></code></section></pre><h3>Build and install  and <a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#build-and-install-step-ca-and-step\"></a></h3><p>You'll need to install both  (the CA server software) and  (the command used to configure and control ).</p><pre><section><code><pre>$ curl -LO https://github.com/smallstep/certificates/releases/download/v0.23.2/step-ca_0.23.2.tar.gz\n$ mkdir step-ca\n$ tar -xvzf step-ca_0.23.2.tar.gz -C step-ca\n$ cd step-ca\n</pre></code></section></pre><p>Now build . This will take some time on a Raspberry Pi, so be patient:</p><pre><section><code><pre>$ sudo apt-get install -y libpcsclite-dev gcc make pkg-config\n$ make bootstrap\n$ make build GOFLAGS=\"\"\n....\nBuild Complete!\n$ sudo cp bin/step-ca /usr/local/bin\n$ sudo setcap CAP_NET_BIND_SERVICE=+eip /usr/local/bin/step-ca\n$ step-ca version\nSmallstep CA/0.23.2 (linux/arm64)\nRelease Date: 2023-02-16 22:25 UTC\n</pre></code></section></pre><pre><section><code><pre>$ cd\n$ curl -LO https://github.com/smallstep/cli/releases/download/v0.23.2/step_linux_0.23.2_arm64.tar.gz\n$ tar xvzf step_linux_0.23.2_arm64.tar.gz\n$ sudo cp step_0.23.2/bin/step /usr/local/bin\n$ step version\nSmallstep CLI/0.23.2 (linux/arm64)\nRelease Date: 2023-02-07T00:53:54Z\n</pre></code></section></pre><h3>Optional, but 🔥: Set up the outboard random number generator<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#optional-but--set-up-the-outboard-random-number-generator\"></a></h3><p><a href=\"https://www.crowdsupply.com/13-37/infinite-noise-trng\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Infinite Noise TRNG</a> is an open-source USB True Random Number Generator. It uses a \"modular entropy multiplier\" architecture to generate a  of random data quickly. For this setup, a daemon will continuously feed entropy into Linux's system entropy pool by writing to .</p><blockquote><p><strong>But will this lovely new entropy generator actually be used by the CA?</strong> I needed to answer two questions here:</p><ol><li>How does the CA generate random numbers? I had to dig around a little to confirm this.  uses Go's  for all of its key generation, and  uses  as its random data source on Linux systems.</li><li>Does the entropy created via writing to  actually affects what is read from ? It does—because Linux has only one entropy pool, shared by  and .</li></ol><p>We also need to confirm that the outboard TRNG is actually generating high quality noise. We'll do that in a minute.\nYou'll need to <a href=\"https://github.com/13-37-org/infnoise/releases\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">compile the driver from source</a>, because there's no pre-built  package available.</p></blockquote><pre><section><code><pre>$ curl -LO https://github.com/leetronics/infnoise/archive/refs/tags/0.3.3.tar.gz\n$ tar xvzf 0.3.3.tar.gz\n$ cd infnoise-0.3.3/software\n$ sudo apt-get install -y libftdi-dev libusb-dev\n$ make -f Makefile.linux\n$ sudo make -f Makefile.linux install\ninstall -d /usr/local/sbin\ninstall -m 0755 infnoise /usr/local/sbin/\ninstall -d /usr/local/lib/udev/rules.d/\ninstall -m 0644 init_scripts/75-infnoise.rules /usr/local/lib/udev/rules.d/\ninstall -d /usr/local/lib/systemd/system\ninstall -m 0644 init_scripts/infnoise.service /usr/local/lib/systemd/system\n$ infnoise --version\nGIT VERSION -\nGIT COMMIT  -\nGIT DATE    -\n</pre></code></section></pre><p>Now, plug in the TRNG and restart your system.</p><p>After a restart, you should see that the driver has started up. It will start and stop based on whether the TRNG is present.</p><pre><section><code><pre>$ systemctl status infnoise\n● infnoise.service - Wayward Geek InfNoise TRNG driver\n     Loaded: loaded (/usr/local/lib/systemd/system/infnoise.service; disabled; preset: enabled)\n     Active: active (running) since Thu 2023-02-16 14:43:02 PST; 1min 44s ago\n    Process: 655 ExecStart=/usr/local/sbin/infnoise --dev-random --daemon --pidfile /var/run/infnoise.pid (code=e&gt;\n   Main PID: 661 (infnoise)\n      Tasks: 1 (limit: 2082)\n     Memory: 700.0K\n        CPU: 162ms\n     CGroup: /system.slice/infnoise.service\n             └─661 /usr/local/sbin/infnoise --dev-random --daemon --pidfile /var/run/infnoise.pid\n\nFeb 16 14:43:02 tinyca systemd[1]: Starting Wayward Geek InfNoise TRNG driver...\nFeb 16 14:43:02 tinyca systemd[1]: Started Wayward Geek InfNoise TRNG driver.\n</pre></code></section></pre><p>Finally, let's run a health check to make sure the TRNG is ready for use:</p><pre><section><code><pre>$ infnoise --debug --no-output\nGenerated 1048576 bits.  OK to use data.  Estimated entropy per bit: 0.878415, estimated K: 1.838354\nnum1s:50.466260%, even misfires:0.119403%, odd misfires:0.156459%\n^C\n</pre></code></section></pre><p>Entropy is written to  by  every second. You're all set on randomness! Now that you have more than enough entropy, you're ready to generate your CA keys.</p><h2>Part 2: Creating Your PKI<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#part-2-creating-your-pki\"></a></h2><p>Now you'll create your root and intermediate CA certificates and keys, and store them securely on the YubiKey.</p><p>Ideally, your Raspberry Pi should be kept offline for this section. Disconnect the Ethernet cable, and connect directly to the device via HDMI and a keyboard.</p><h3>Prepare a USB thumb drive for storing the private keys<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#prepare-a-usb-thumb-drive-for-storing-the-private-keys\"></a></h3><p>You can't just have your CA private keys live  on the YubiKey. You'll want at least one backup of them, in case the YubiKey breaks!</p><p>Insert a USB thumb drive. You'll generate the keys directly on this drive, so that they never touch the Pi's microSD card. First, find the device name of your USB drive:</p><pre><section><code><pre>$ sudo fdisk -l\n...\nDisk /dev/sda: 14.91 GiB, 16005464064 bytes, 31260672 sectors\nDisk model: Cruzer Fit\nUnits: sectors of 1 * 512 = 512 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\n...\n</pre></code></section></pre><p>In this case, the drive is . Let's initialize it with a single  partition:</p><pre><section><code><pre>$ sudo fdisk /dev/sda\nWelcome to fdisk (util-linux 2.36).\nChanges will remain in memory only, until you decide to write them.\nBe careful before using the write command.\nCommand (m for help): n\nPartition type\n   p   primary (0 primary, 0 extended, 4 free)\n   e   extended (container for logical partitions)\nSelect (default p): p\nPartition number (1-4, default 1):\nFirst sector (2048-31260671, default 2048):\nLast sector, +/-sectors or +/-size{K,M,G,T,P} (2048-31260671, default 31260671):\nCreated a new partition 1 of type 'Linux' and of size 14.9 GiB.\nCommand (m for help): w\nThe partition table has been altered.\nCalling ioctl() to re-read partition table.\nSyncing disks.\n$ sudo mkfs.ext4 /dev/sda1 -v\nmke2fs 1.45.6 (20-Mar-2020)\nfs_types for mke2fs.conf resolution: 'ext4'\nFilesystem label=\nOS type: Linux\n...\nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done\n</pre></code></section></pre><h3>Generate your PKI on the thumb drive<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#generate-your-pki-on-the-thumb-drive\"></a></h3><p>Great, now you're ready to create your Public Key Infrastructure (PKI). Specifically, you'll be creating CA keys and certificates.</p><blockquote><ul><li>Tiny CA has a root CA key and certificate, and an intermediate CA key and certificate.</li><li>The root CA key signs the Intermediate CA certificate.</li><li>The root CA certificate is self-signed (signed with the root CA key)</li><li>The intermediate CA key will sign all of your TLS certificates.</li><li>By default,  issues certificates with a 24-hour lifetime. I hope this default will compel you to <a href=\"https://smallstep.com/docs/step-ca/renewal\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">set up automated renewal</a> on your clients. And you can always increase the TLS certificate duration in the CA configuration, if you want something a bit more relaxed.</li><li>If a device is configured to trust your root CA, it will trust certificates you create with .</li><li>You can throw away the root CA key if you never need another intermediate.</li></ul></blockquote><pre><section><code><pre>$ sudo mount /dev/sda1 /mnt\n$ cd /mnt\n$ sudo mkdir ca\n$ sudo chown ubuntu:ubuntu ca\n$ export STEPPATH=/mnt/ca\n$ step ca init --pki --name=\"Tiny\" --deployment-type standalone\n✔ What do you want your password to be? [leave empty and we'll generate one]: ...\nGenerating root certificate...\nall done!\nGenerating intermediate certificate...\nall done!\n✔ Root certificate: /mnt/ca/certs/root_ca.crt\n✔ Root private key: /mnt/ca/secrets/root_ca_key\n✔ Root fingerprint: d6b3b9ef79a42aeeabcd5580b2b516458ddb25d1af4ea7ff0845e624ec1bb609\n✔ Intermediate certificate: /mnt/ca/certs/intermediate_ca.crt\n✔ Intermediate private key: /mnt/ca/secrets/intermediate_ca_key\nFEEDBACK 😍 🍻\n      The step utility is not instrumented for usage statistics. It does not\n      phone home. But your feedback is extremely valuable. Any information you\n      can provide regarding how you’re using `step` helps. Please send us a\n      sentence or two, good or bad: feedback@smallstep.com or join\n      https://github.com/smallstep/certificates/discussions.\n</pre></code></section></pre><p>Don't forget to give your CA a cute name! It will appear on all of your certificates. Hold onto your root fingerprint, too; you'll need it to bootstrap your clients later.</p><h3>Import the CA into the YubiKey<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#import-the-ca-into-the-yubikey\"></a></h3><p>Now, let's import our PKI to the YubiKey.</p><pre><section><code><pre>$ sudo systemctl enable pcscd\n$ sudo systemctl start pcscd\n$ ykman piv certificates import 9a /mnt/ca/certs/root_ca.crt\nSuccessfully imported a new certificate.\n$ ykman piv keys import 9a /mnt/ca/secrets/root_ca_key\nEnter PEM pass phrase: ...\nSuccessfully imported a new private key.\n$ ykman piv certificates import 9c /mnt/ca/certs/intermediate_ca.crt\nSuccessfully imported a new certificate.\n$ ykman piv keys import 9c /mnt/ca/secrets/intermediate_ca_key\nEnter PEM pass phrase: ...\nSuccessfully imported a new private key.\n$ ykman piv info\nPIV version: 5.2.7\nPIN tries remaining: 3\nCHUID:\t3019d4e739da739ced39ce739d836858210842108421c84210c3eb34104610300df33f7fd273e44f17361ce7c4350832303330303130313e00fe00\nCCC: \tNo data available.\nSlot 9a:\n\tAlgorithm:\tECCP256\n\tSubject DN:\tCN=Tiny CA Root CA\n\tIssuer DN:\tCN=Tiny CA Root CA\n\tSerial:\t\t280998571002718115143415195266043025218\n\tFingerprint:\td6b3b9ef79a42aeeabcd5580b2b516458ddb25d1af4ea7ff0845e624ec1bb609\n\tNot before:\t2020-12-08 20:12:15\n\tNot after:\t2030-12-08 20:12:15\nSlot 9c:\n\tAlgorithm:\tECCP256\n\tSubject DN:\tCN=Tiny CA Intermediate CA\n\tIssuer DN:\tCN=Tiny CA Root CA\n\tSerial:\t\t38398140468675846143165983044297636289\n\tFingerprint:\tfa21279c114ef44be899cb41e830b920faa6ce2c0ec5bc4f1c9310194e5837d2\n\tNot before:\t2020-12-08 20:12:15\n\tNot after:\t2030-12-08 20:12:15\n</pre></code></section></pre><p>OK! Now you'll copy out the CA certificate files, leave the private keys on the USB stick, and continue creating your CA.</p><pre><section><code><pre>$ sudo cp /mnt/ca/certs/intermediate_ca.crt /mnt/ca/certs/root_ca.crt /root\n$ cd\n$ sudo umount /mnt\n</pre></code></section></pre><p>Finally, reconnect your CA to your local network to continue the setup.</p><h2>Part 3: Configuring Your CA<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#part-3-configuring-your-ca\"></a></h2><p>You're going to re-run  now, but <em>you're not going to use the certificates or keys that it generates</em>.\nYou're just doing this to create the configuration files.\nThe password you choose when prompted will be your <em>admin provisioner password</em>.\nAnyone with this password will be able to administer your CA and get any certificate from it,\nusing the  subcommand.</p><p>Don't use your root CA password for your provisioner,\nbut pick something strong and store it somewhere safe.</p><pre><section><code><pre>$ sudo useradd step\n$ sudo passwd -l step\n$ sudo mkdir /etc/step-ca\n$ export STEPPATH=/etc/step-ca\n$ sudo --preserve-env step ca init --name=\"Tiny CA\" \\\n    --dns=\"tinyca.internal,10.20.30.42\" --address=\":443\" \\\n    --provisioner=\"you@example.com\" \\\n    --deployment-type standalone \\\n    --remote-management\nChoose a password for your CA keys and first provisioner.\n✔ [leave empty and we'll generate one]:\n\nGenerating root certificate... done!\nGenerating intermediate certificate... done!\n\n✔ Root certificate: /etc/step-ca/certs/root_ca.crt\n✔ Root private key: /etc/step-ca/secrets/root_ca_key\n✔ Root fingerprint: 60440dc6ef5b923810b22f85a907f307badb58314c5fdc2231a3c1a892d6c275\n✔ Intermediate certificate: /etc/step-ca/certs/intermediate_ca.crt\n✔ Intermediate private key: /etc/step-ca/secrets/intermediate_ca_key\n✔ Database folder: /etc/step-ca/db\n✔ Default configuration: /etc/step-ca/config/defaults.json\n✔ Certificate Authority configuration: /etc/step-ca/config/ca.json\n✔ Admin provisioner: you@example.com (JWK)\n✔ Super admin subject: step\n\nYour PKI is ready to go. To generate certificates for individual services see 'step help ca'.\n</pre></code></section></pre><p>Next, let's get your certificates in place.</p><pre><section><code><pre>$ sudo mv /root/root_ca.crt /root/intermediate_ca.crt /etc/step-ca/certs\n$ sudo rm -rf /etc/step-ca/secrets\n</pre></code></section></pre><p>Next, you'll need to configure  to use your YubiKey to sign certificates, using the intermediate key on the YubiKey. Notice that the default YubiKey PIN () is shown here, too.</p><blockquote><p>You should change your YubiKey PIN, PUK, and management key if you haven't already! <a href=\"https://developers.yubico.com/PIV/Guides/Device_setup.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Learn how in this guide.</a>\nNow edit the file <code>/etc/step-ca/config/ca.json</code>. You'll want the top of the file to look like this:</p></blockquote><pre><section><code><pre>\n...\n</pre></code></section></pre><p>Now you'll start up the CA and make sure it's running properly:</p><pre><section><code><pre>$ sudo chown -R step:step /etc/step-ca\n$ sudo -u step step-ca /etc/step-ca/config/ca.json\n2020/12/08 14:17:06 Serving HTTPS on :443 ...\n</pre></code></section></pre><p>In another window, you'll generate a test certificate for localhost.\nThis is where you'll need the CA fingerprint, which is displayed when you start up the CA. Run:</p><pre><section><code><pre>$ step ca bootstrap --ca-url \"https://tinyca.internal\" --fingerprint d6b3b9ef79a42aeeabcd5580b2b516458ddb25d1af4ea7ff0845e624ec1bb609\nThe root certificate has been saved in /home/ubuntu/.step/certs/root_ca.crt.\nYour configuration has been saved in /home/ubuntu/.step/config/defaults.json.\n$ step ca certificate \"localhost\" localhost.crt localhost.key\n✔ Provisioner: you@example.com (JWK) [kid: izgi9tn1YWbVnY_rmIUKzE-Dn-XIuKz-_J1dnnKeDRA]\n✔ Please enter the password to decrypt the provisioner key:\n✔ CA: https://tinyca.internal:443\n✔ Certificate: localhost.crt\n✔ Private Key: localhost.key\n$ step certificate inspect localhost.crt --short\nX.509v3 TLS Certificate (ECDSA P-256) [Serial: 2903...3061]\n  Subject:     localhost\n  Issuer:      Tiny Intermediate CA\n  Provisioner: you@example.com [ID: izgi...eDRA]\n  Valid from:  2023-02-16T23:03:52Z\n          to:  2023-02-17T23:04:52Z\n</pre></code></section></pre><p>Great! You just signed your first X.509 TLS leaf certificate using the YubiKey and .</p><p>When you ask the CA to issue a leaf certificate for a TLS endpoint, you'll get a certificate file and a (locally-generated) private key file.  The certificate file will contain both the intermediate CA certificate and the leaf certificate you requested. This way, a device which trusts your root CA can verify the chain of trust from the root to the intermediate, and from the intermediate to the leaf.</p><p>Finally, you'll add an ACME provisioner, which will turn your Tiny CA into a tiny Let's Encrypt!</p><pre><section><code><pre>$ step ca provisioner add acme --type acme --admin-name step\nNo admin credentials found. You must login to execute admin commands.\n✔ Provisioner: you@example.com (JWK) [kid: izgi9tn1YWbVnY_rmIUKzE-Dn-XIuKz-_J1dnnKeDRA]\nPlease enter the password to decrypt the provisioner key:\n</pre></code></section></pre><p>Sign in with your admin password, and your new new ACME provisioner will be created.</p><p>You can now shut down the  process you started in the other terminal window.</p><h3>Configure  to start the CA<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#configure-systemd-to-start-the-ca\"></a></h3><p>In this section you'll set up a systemd service for  so it starts when the system starts up.\nYou'll also configure systemd to stop the CA when the YubiKey is removed, and restart it when the YubiKey is reinserted.\nFirst, you need to tell <a href=\"https://wiki.archlinux.org/index.php/Udev\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">udev</a> about your YubiKey by adding some udev rules, which will help make the YubiKey visible to systemd as a device.</p><pre><section><code><pre>$ sudo tee /etc/udev/rules.d/75-yubikey.rules &gt; /dev/null &lt;&lt; EOF\nACTION==\"add\", SUBSYSTEM==\"usb\", ENV{PRODUCT}==\"1050/407/*\", TAG+=\"systemd\", SYMLINK+=\"yubikey\"\nACTION==\"remove\", SUBSYSTEM==\"usb\", ENV{PRODUCT}==\"1050/407/*\", TAG+=\"systemd\"\nEOF\n$ sudo udevadm control --reload-rules\n</pre></code></section></pre><p>Here, the format of the  value is . Yubico's vendor ID is , and  is the product ID for the YubiKey 5 NFC. If you're using a different YubiKey, <a href=\"https://devicehunt.com/view/type/usb/vendor/1050\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">you can find your model number here</a>.</p><ul><li>run on system startup, when the YubiKey is inserted</li><li>stop when the YubiKey is removed</li><li>start again when the YubiKey is reinserted</li></ul><pre><section><code><pre>$ sudo tee /etc/systemd/system/step-ca.service &gt; /dev/null &lt;&lt; EOF\n[Unit]\nDescription=step-ca\nBindsTo=dev-yubikey.device\nAfter=dev-yubikey.device\n[Service]\nUser=step\nGroup=step\nExecStart=/bin/sh -c '/usr/local/bin/step-ca /etc/step-ca/config/ca.json'\nType=simple\nRestart=on-failure\nRestartSec=10\n[Install]\nWantedBy=multi-user.target\nEOF\n$ sudo mkdir /etc/systemd/system/dev-yubikey.device.wants\n$ sudo ln -s /etc/systemd/system/step-ca.service /etc/systemd/system/dev-yubikey.device.wants/\n$ sudo systemctl daemon-reload\n$ sudo systemctl enable step-ca\n</pre></code></section></pre><p>Now insert the YubiKey and the service should start:</p><pre><section><code><pre>$ sudo systemctl status step-ca\n● step-ca.service - step-ca\n     Loaded: loaded (/etc/systemd/system/step-ca.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2020-12-08 14:27:02 PST; 3s ago\n   Main PID: 3269 (sh)\n      Tasks: 9 (limit: 2099)\n     CGroup: /system.slice/step-ca.service\n             ├─3269 /bin/sh -c /usr/local/bin/step-ca /etc/step-ca/config/ca.json\n             └─3270 /usr/local/bin/step-ca /etc/step-ca/config/ca.json\nDec 08 14:27:02 tinyca systemd[1]: Started step-ca.\nDec 08 14:27:02 tinyca sh[3270]: 2020/12/08 14:27:02 Serving HTTPS on :443 ...\n</pre></code></section></pre><p>Now restart your system and ensure that the CA starts up automatically.</p><p>Test out removing the YubiKey, and you should see that the CA stops.</p><p>Reinsert it, and the CA should start up again.</p><h3>Finally, turn on the firewall and disable SSH access<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#finally-turn-on-the-firewall-and-disable-ssh-access\"></a></h3><p>Your tiny CA will be most secure without any SSH access at all. The only open port will be 443, for the CA. For maintenance, you'll need to plug in a keyboard and a display.</p><pre><section><code><pre>$ sudo tee /etc/ufw/applications.d/step-ca-server &gt; /dev/null &lt;&lt; EOF\n[step-ca]\ntitle=Smallstep CA\ndescription=step-ca is an online X.509 and SSH Certificate Authority\nports=443/tcp\nEOF\n$ sudo ufw allow step-ca\n$ sudo ufw enable\nCommand may disrupt existing ssh connections. Proceed with operation (y|n)? y\nFirewall is active and enabled on system startup\n</pre></code></section></pre><p>You did it! Your CA is up and running.</p><h4>Bootstrapping a new device into your PKI<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#bootstrapping-a-new-device-into-your-pki\"></a></h4><p>When you run <a href=\"https://smallstep.com/docs/step-cli/reference/ca/bootstrap\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"></a> (as above) on a new device,\nthe root certificate  is downloaded from the CA.\nIf you run <code>step ca bootstrap --install --ca-url=https://your.ca --fingerprint=your-ca-fingerprint</code>,\nit will install the root certificate into your device's trust store.</p><p>You can also use the  command for easy installation of your root CA certificate (<a href=\"https://smallstep.com/docs/step-cli/reference/certificate/install\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"></a>),\nfor ACME enrollment (<code>step ca certificate example.com example.crt example.key --provisioner acme</code>)\nand for renewal of any certificate that hasn't yet expired (<code>step ca renew example.crt example.key</code>).</p><p>For mobile devices, you can usually install a certificate by sending it to yourself via Bluetooth or AirDrop, or as an email attachment. Make sure the certificate isn't just installed, but actually trusted by the device. This usually involves a couple of confirmation steps on the device.</p><h4>Automating certificate renewal<a href=\"https://smallstep.com/blog/build-a-tiny-ca-with-raspberry-pi-yubikey/#automating-certificate-renewal\"></a></h4><p>Because certificates from your CA have a 24-hour lifetime, you'll want to renew them every 16ish hours.\nOur <a href=\"https://smallstep.com/docs/step-ca/renewal\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">renewal documentation</a> has a few options\nfor setting up renewal on your clients.</p><p>Now that you have an internal CA, here's a few useful resources:</p><ul><li><a href=\"https://smallstep.com/hello-mtls\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Hello mTLS</a> shows you how to get mutual TLS authentication configured for several common services and programming languages, using the  command.</li><li>There's also a lot to learn about the different provisioners you can add to your CA to suit your workflows.\nSee <a href=\"https://smallstep.com/docs/step-ca/configuration\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Configuring </a>.</li><li>Bonus: Want to use SSH certificates?\nYou can turn your tiny CA into an SSH CA, and use certificates and single sign-on for your SSH hosts.\nWe have a <a href=\"https://smallstep.com/blog/diy-single-sign-on-for-ssh/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">blog post</a> and <a href=\"https://www.youtube.com/watch?v=ZhxLRlcNUM4\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">video walk-through</a> that describes how to set it up.</li></ul><div><div><p>Carl Tashian (<a href=\"https://tashian.com\">Website</a>, <a href=\"https://www.linkedin.com/in/tashian/\">LinkedIn</a>) is an engineer, writer, exec coach, and startup all-rounder. He's currently an Offroad Engineer at Smallstep. He co-founded and built the engineering team at Trove, and he wrote the code that opens your Zipcar. He lives in San Francisco with his wife Siobhan and he loves to play the modular synthesizer 🎛️🎚️</p></div></div>","contentLength":21763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42758070"},{"title":"The Fuzzing Book","url":"https://www.fuzzingbook.org/","date":1737287838,"author":"chautumn","guid":161,"unread":true,"content":"<p>This book is written by <em>Andreas Zeller, Rahul Gopinath, Marcel Böhme, Gordon Fraser, and Christian Holler</em>.  All of us are long-standing experts in software testing and test generation; and we have written or contributed to some of the most important test generators and fuzzers on the planet.  As an example, if you are reading this in a Firefox, Chrome, or Edge Web browser, you can do so safely partly because of us, as <em>the very techniques listed in this book have found more than 2,600 bugs in their JavaScript interpreters so far.</em>  We are happy to share our expertise and making it accessible to the public.</p>","contentLength":612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42756286"},{"title":"Haskell: A Great Procedural Language","url":"https://entropicthoughts.com/haskell-procedural-programming","date":1737265818,"author":"kqr","guid":160,"unread":true,"content":"<p>\nEffectful computations in Haskell are first class values. This means we can\nstore them in variables or data structures for later use. There is a Haskell\nfunction\n</p><div><pre> (, ) </pre></div><p>\nwhich, when given two integers as arguments, picks a random integer between\nthem. We can put calls to this function into a list, like so:\n</p><div><pre> [ randomRIO(1, 6), randomRIO(1, 6) ]\n</pre></div><p>\nThis is a list of two calls to . What surprises non-Haskellers is\nthat when this list is created, no random numbers are generated. Coming from\nother programming languages, we are used to side effects (such as random\ngeneration) being executed directly when the side effectful function is\ncalled.</p><p>\nWe can add more random generation to the list:\n</p><div><pre> some_dice  [ randomRIO(1, 6) ]\n</pre></div><p>\nand still no random numbers will be generated. We can go ahead and manipulate\nthis list in all sorts of ways, and  no random numbers would be\ngenerated.\n</p><p>\nTo be clear, the  function could well be called, and when\nit is called it returns a value of type . It’s just that this value . If anything, we can think of it as a set of instructions for\neventually, somehow, getting an integer. It’s not an actual integer. It’s an\nobject encapsulating a side effect. When this side effect object executes, it\nwill produce a random integer, but the object itself just describes the\ncomputation, it is not an integer.\n</p><p>\nIn other words, in Haskell, it is not enough to call a side effectful function\nto execute its side effects. When we call the side effectful function, it\nproduces an object encapsulating the side effect, and this object can be\nexecuted in the future to produce the result of the side effect.</p><p>\nThe common way we teach beginners to do execute side effect objects is by\ncalling them from a  block, using the special  assignment operator to\nextract their result. As a first approximation, we can think of the following\ncode as the way to force side effects to execute.\n</p><div><pre>\n  side  randomRIO(1, 6)\n  printf  side\n</pre></div><p>\nWe can imagine that the  arrow executes the side effect object returned by\n and captures the value it produces. Similarly, the side effect\nobject returned by  gets executed, but we don’t capture the result; we\ndon’t care about the value produced by it, we only care about the side effect\nitself.\n</p><p>\nThe lie-to-children here is that we pretend the  block is magical and that\nwhen it executes, it also executes side effects of functions called in it. This\nmental model will take the beginner a long way, but at some point, one will want\nto break free of it. That is when Haskell starts to really shine as a procedural\nlanguage.\n</p><p>\nThis article features another lie-to-children: it will have type signatures\nspecialised to  and . All the functions I mention are more generic\nthan I’m letting on.\n</p><ul><li>Anywhere this article says  it will work with any type of side effect\n(like , , , etc.)</li><li>Anywhere this article says  it probably also works with other\ncollection/container types (like , , , , etc.)</li></ul><p>\nThe reason this article uses more direct type signatures is to hopefully be\nreadable also to someone who does not use Haskell.</p>","contentLength":3056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42754098"},{"title":"Robotics and ROS 2 Essentials","url":"https://henkirobotics.com/robotics-and-ros-2-essentials-course-announcement/","date":1737154337,"author":"dtquad","guid":159,"unread":true,"content":"<p>Have you been looking for a good place to start your robotics and ROS 2 learning journey? Look no further!</p><p>We are proud to announce that together with the <strong>University of Eastern Finland</strong>, we have open-sourced the robotics part from our “Robotics &amp; XR” master’s-level course. You can find the course materials and all the hands-on exercises on GitHub: <a href=\"https://github.com/henki-robotics/robotics_essentials_ros2\" target=\"_blank\" rel=\"noreferrer noopener\">Robotics &amp; ROS 2 Essentials</a></p><h2>What is the course about?</h2><p>Last semester we were offered the opportunity to help develop the latest iteration of the “Robotics &amp; XR” course for the University of Eastern Finland (UEF). Combining UEF’s broad theoretical robotics expertise, and the industry knowledge from Henki Robotics, we wanted to upgrade the existing Robotics course to prepare students with the most up-to-date skills required for modern robotics development. The course goes through the essential robotics skills and the basic concepts related to ROS 2, which is nowadays the most commonly used robotics development framework.</p><h2>Why did we open-source the course?</h2><p>People and students often ask where and how they should start learning robotics and ROS 2. Robotics is a rapidly growing field, so more and more experts are needed in the area. We believe in open source and knowledge sharing, so we wanted to make the course material available for everyone, for free!</p><h2>What Will I Learn <strong>from following this course</strong>?</h2><p>The main goal of the course is to be a beginner-friendly introduction to robotics and ROS 2, and it has a focus on learning by doing and experimenting through exercises. The practical exercises that accompany the theoretical content are based on a Gazebo simulation with the simulated <a href=\"https://github.com/Ekumen-OS/andino_gz/tree/humble\" target=\"_blank\" rel=\"noreferrer noopener\">Andino robot from Ekumen</a>. The course exercises cover the following topics:</p><ol start=\"0\"><li><a href=\"https://github.com/henki-robotics/robotics_essentials_ros2/tree/main/0-setup\">Setup</a><ul><li>Run the Gazebo simulation</li></ul></li><li><a href=\"https://github.com/henki-robotics/robotics_essentials_ros2/tree/main/1-ros_2_introduction\" target=\"_blank\">ROS 2 Introduction</a><ul><li>ROS 2 topics; publish and subscribe</li><li>Transformations and tf-frames</li></ul></li><li><a href=\"https://github.com/henki-robotics/robotics_essentials_ros2/tree/main/4-robot_odometry\" target=\"_blank\">Robot Odometry</a><ul><li>Calculate and publish your robot’s odometry using wheel velocities</li><li>Robot odometry and how to calculate it</li><li>Publish and subscribe to topics from Python code</li></ul></li><li><a href=\"https://github.com/henki-robotics/robotics_essentials_ros2/tree/main/5-path_planning\" target=\"_blank\">Path Planning</a><ul><li>Basic navigation concepts</li><li>Custom path planning using Nav2</li></ul></li></ol><h2>Do I need ROS 2 installed?</h2><p>No preliminary ROS 2 installation is required. Instead, the course uses Docker to provide all the course exercises in a containerized environment, requiring no manual installation of ROS 2, simulation, packages, and dependencies. All you need is an Ubuntu Operating System and Docker installed, and you are all set! Learning to use Docker is also a valuable skill, as it is a widely used tool in the software development industry, not just in robotics.</p><p>We are always looking to improve our work and provide the best learning experience for people looking to learn the basics of ROS. Feel free to send us your feedback, and if you encounter any issues while following the course, or have any suggestions for improvements, do not hesitate to reach out to us!</p>","contentLength":2851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42744106"},{"title":"How hard would it be to display the contents of an image file on the screen?","url":"https://wolf.nereid.pl/posts/image-viewer/","date":1737059578,"author":"ingve","guid":158,"unread":true,"content":"<p>How hard would it be to display the contents of an image file on the screen? You just load the image pixels somehow, perhaps using a readily available library, and then display those pixels on the screen. Easy, right? Well, not quite, as it turns out.</p><p>I may have some experience with this, because I made <a href=\"https://wolf.nereid.pl/projects/vv/\">an image viewer</a> that displays images in the terminal emulator. But why do such a thing, there are countless image viewers already available, including those that work with terminal emulators, why write yet another one? That’s an excellent question! As always, the answer is because no other viewer was good enough for me.</p><p>For example, <a href=\"https://github.com/posva/catimg\" target=\"_blank\" rel=\"noopener\">catimg</a> uses <a href=\"https://github.com/nothings/stb/blob/master/stb_image.h\" target=\"_blank\" rel=\"noopener\">stb_image</a> to load images. While stb_image is an outstanding library that can be integrated very quickly, it doesn’t really excel in the number of image formats it supports. There’s the baseline of JPEG, PNG, GIF, plus a few other more or less obscure formats.</p><p>Another example is <a href=\"https://github.com/atanunq/viu\" target=\"_blank\" rel=\"noopener\">viu</a>, which again is limited to the well-known baseline of three “web” formats, with the modern addition of WebP. Following the dependency graph of the program shows that the image loading library it uses should support more formats, but ultimately I’m interested in what the executable I have on my system can do, not what some readme says.</p><p>The overall situation is that there is widespread expectation and support for viewing PNG files (1996), JPEG files (1992) and GIF files (1987). So… what happened? Did image compression research fizzle out in the XXI century? Of course not. There’s JPEG XL (2022), AVIF (2019), HEIC (2017), WebP (2010). The question now is, why is there no wide support for these image codecs in software? Because nobody uses them. And why is nobody using them? Because there’s no software support.</p><p>So maybe these new formats just aren’t worth it, maybe they don’t add enough value to be supported? Fortunately, that’s easy to answer with the following image. Which of the quality + size combinations do you prefer?</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/avif.avif\" alt=\"Image codec comparison\"></figure><p>But that’s not all. There is a variety of image formats that are arguably intended for more specialized use. And these formats are old, too. Ericsson Texture Compression (ETC) was developed in 2005, while Block Compression (BC) and OpenEXR date back to 1999. BC is supported by all desktop GPUs, and virtually all games use it. ETC is supported by all mobile GPUs. So why is it nearly impossible to find an image viewer for them?</p><p>And speaking of texture compression, I also have <a href=\"https://wolf.nereid.pl/projects/etcpak/\">an ETC/BC codec</a> which is limited in speed by how fast the PNG files can be decoded. There are some interesting observations if you look into it. For example, PNG has  different checksums to calculate, one at the zlib data stream level, and the second at the PNG data level. Another one is that zlib is slooow. The best you can do is replace zlib with zlib-ng, which provides some much-needed speed improvements. Yet how much better would it be to replace the zlib (deflate) compression in PNG files with a more modern compression algorithm, such as Zstd or LZ4? The PNG format even supports this directly with a “compression type” field in the header, but there’s only one value it can be set to. And it’s not going to change, because then you’d have to update every single program that can load a PNG file to support it. Which is hopeless.</p><p>Some time ago, I submitted a bunch of patches for KDE that were largely ignored, which, let’s say, annoyed me a bit. I went off to write my own Wayland compositor. In the meantime, KDE got better enough to not bother me as much, and the exciting task of figuring out how to draw two textured triangles with Vulkan, potentially using two different GPU drivers at the same time, became rather tedious, so the project was shelved.</p><p>But one of the things I had to do was write an image loader. To show the desktop background or the mouse cursor instead of just colored rectangles. Little things like that really do make a difference.</p><p>Loading mouse cursors is not yet available in vv because it requires some special handling with animations, hot spots, and so on. Extending this very specialised implementation to the general image loading functionality requires some thought, and at the moment it’s hard to test if it would even work as intended, so there you have it.</p><p>It’s still an interesting enough topic to talk about.</p><p>To load Xcursor images, as used in the X Window System, you can use the Xcursor library, or one of its forks (?), like the wayland-cursor library. The downside of this approach is that the file format remains a bit of a mystery. I did not want it to be a mystery.</p><p>It turned out that the Xcursor files are simple enough to parse on your own in about 20 lines of code. You need to read the file header, then the table of contents structure, and then individual images (more than one forming an animation), where the image data is RGBA 32-bit pixels.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>KDE recently started using <a href=\"https://blog.vladzahorodnii.com/2024/10/06/svg-cursors-everything-that-you-need-to-know-about-them/\" target=\"_blank\" rel=\"noopener\">SVG mouse cursors</a>, which is a step in the right direction, but it’s also generally not related to the previous Xcursor file format.</p><p>Looking at the wlroots implementation of cursor loading, I noticed some problems with Xcursor path handling and theme inheritance. It turns out that it’s trivial to reliably crash KDE by placing a cursor theme which inherits itself, in a known location. It was also possible to segfault any wlroots-based compositor by doing the same thing, but in a completely undocumented directory, because the path handling was kind of bad. The wlroots path problem seems to be now fixed, but only by accident, as the commit message describes it as a “cosmetic change”.</p><p>The type of cursor determines what shape it should have. It can be the usual arrow pointer, or the hourglass-like busy indicator, the I-beam to indicate text entry, the directional arrows to indicate something is resizable. You get the idea. Hilariously enough, the currently agreed-upon set of cursor types (in Wayland, for example) is what was selected for the needs of <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/cursor\" target=\"_blank\" rel=\"noopener\">web development</a>.</p><p>Before modern standardization, it was all a hodgepodge of mostly wrong ideas and bad implementations. Here’s what I think is the complete list of original X cursors, taken from <a href=\"https://www.oreilly.com/library/view/x-window-system/9780937175149/ChapterD.html\" target=\"_blank\" rel=\"noopener\">some old book</a>:</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/xcursor.avif\" alt=\"I don’t even\"></figure><p>This list may be a bit too symbolic here and there, so let’s take a look at more recent (well, 2003 recent) renderitions of some of the most absurd entries:</p><p>Good? Okay, so how would you implement an X cursor theme on a modern system? Easy, just draw a bunch of cursors, then add a semi-random collection of symlinks on top of them, because nothing is standardized or documented. To show a “question mark” cursor, GNOME will refer to , but KDE might want . That’s just a simple example, but the list is much longer, and even includes some hashes, because why not.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/xtheme.avif\" alt=\"Probably not even a complete list\"><figcaption><p>Probably not even a complete list</p></figcaption></figure><p>Here’s how ridiculous all this is, summed up in one bug report:</p><p>The Windows  cursor file is basically the  icon file, with some minor changes. It contains a header, an image directory (for animated cursors), and then the image data. The image data is a Windows Bitmap payload, so you can just use the existing BMP file loader you have.</p><p>The BMP height is double the icon height, because the BMP payload contains both the image color data and the alpha mask. To make things worse, the color and alpha halves very likely will use different bit depths. Alpha is always one bit per pixel, and the bit depth just changes in the middle of a data stream.</p><p>Then all the extra quirks come out that you have to take into account. For example, you really don’t want to touch the alpha channel when the color payload is 32 bpp. And the image data may be in the raw format, whatever that means, I surely don’t know. Or it could be a PNG file. Also, make sure that the RGB order is correct and that the image is not flipped vertically. And that you calculate the 4-byte aligned 1-bit stride correctly for cursors that are not power-of-two in size (for some rare 48 px cursors). And of course, there may be several versions of the bitmap data in a cursor, for different numbers of bits per pixel, or sizes, so choose the appropriate one! Clear as mud, right?</p><p>Now you have the cursor image loaded, and you can view it at 1:1 scale, as on the left in the image below. But what happens when you scale the image up, as in the image on the right?</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/wincursor.avif\" alt=\"The default Windows 95 cursor\"><figcaption><p>The default Windows 95 cursor</p></figcaption></figure><p>Where are those annoying glowing pixels coming from? It turns out that this mouse pointer is actually repurposed from another type of cursor, with some pixels masked out. It does not matter when you draw the cursor 1:1, but with the filtering applied when the cursor is scaled, the colors bleed out, so you need to take care of that too.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/win95pointer.png\" alt=\"What lies beneath\"></figure><p>For what it’s worth, animated Windows cursors don’t need to store repeated frames more than once, unlike X cursors, which is nice. It is implemented by using an optional animation index table, which just references the actual image data.</p><p>Windows cursors are stored in RIFF containers. RIFF is basically a collection of chunks, each consisting of a FourCC, a 32-bit data size, and the data payload. The byte order is the difference between the original Amiga’s IFF and Microsoft’s RIFF.</p><p>This format was invented in 1985, and at the time it must have seemed like a fantastic thing. It was general enough to store any kind of data, it could be extended in the future, it was easy to parse and load. What’s there not to like?</p><p>In practice, this is all a misfeature that should never happen.</p><ol><li>Having a generic container for everything doesn’t make sense. It’s a cool concept on paper, but when you have to write file loaders, it turns out that you have very specific needs, and none of those needs play well with “anything can happen lol”.</li><li>Extending the data format may have been a nice idea in the 1980s, but we now know that file formats are eternal and you can’t just add or change something because all previously compatible loaders will break.</li><li>Nothing is easy to load or parse when you technically have to create a table of contents of all the chunk types in the container and then load them all in the right order. I suspect that all files out there follow the unwritten agreement that the chunks should be present in parsing order, since doing anything more complicated would be way over the head for the memory-constrained machines of the 1980s. But that’s a strong , and there may be valid files that would require parsing chunks in a non-linear fashion.</li></ol><p>When I hooked up the loaded mouse cursors into my compositor, I noticed something was not right with how they looked. Then I started seeing it everywhere, including the Desktop Mode on the Steam Deck.</p><p>After some debugging, the problem was narrowed down to only be happening on AMD GPUs, and Tom Forsyth provided a reasonable explanation.</p><p>With the mouse cursors done, I moved on to implementing image loading functionality for the purpose of having a desktop background in my compositor. I wanted support for modern formats like AVIF or JPEG XL, and it all went fairly quickly and relatively smoothly. Include a library, follow the documentation, get an image, rinse and repeat.</p><p>Back in June 2023, I did a little review of the various image format loading libraries available. (It turned out to be completely wrong, but let’s not spoil the surprise.)</p><ol><li><p>Call a function. You’re done.</p></li><li><p>A bunch of functions you need to call in sequence, all presented as a neat and short program in the README.</p></li><li><p> libpng, libjpeg</p><p>You have to go through a narrated guide on what to call. You can get the work done, but you have to wade through super important stuff like low-quality decoding in case you want to run on Amiga.</p></li><li><p>You get doxygen docs, so you don’t know where to start. The example program is your best bet, but it is convoluted and outputs each color channel as a float.</p><p>The documentation is sometimes vague. To check if a file might be JPEG XL, you have to provide “the beginning of the file”. The function may fail and ask for more data. The documentation never specifies how much is needed. Reading the source code shows that it’s 12 bytes.</p><p>The decoding process requires repeated calls to JxlDecoderProcessInput. The documentation lists several return codes you must handle. You start to build up a mental map of how all of this is supposed to work, and then you realize that some of it does not make any sense at all. Then you read the source code and find out that the decoding function can actually return a much wider variety of return codes, but the documentation does not tell you that.</p></li></ol><p>At this point, Aras chimed in with an interesting tidbit about libtiff.</p><p>On April Fool’s Day 2024, I was looking for a good image viewer on Linux, and it turns out they are all bad in one way or another. For context, I’m running the Wayland desktop with fractional scaling, and that’s a minefield of sorts. Here’s an example.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/gwenview.avif\" alt=\"The default KDE image viewer\" width=\"70%\"><figcaption><p>The default KDE image viewer</p></figcaption></figure><p>The image I’m looking at here is completely transparent, with only the four corners marked to indicate where it begins and ends. The checkerboard pattern is drawn in the background by the viewer to indicate, by convention, where the transparent area is. So why does it only cover part of the image? Because fractional scaling, or maybe just DPI scaling in general, I have not checked how it works with 200% scale.</p><p>That may be a silly issue, but there’s also one that is much more impacting, as shown below.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/gwenview2.avif\" alt=\"Close-up comparison of two images. Left: How gwenview displays the image. Right: How the image really looks like\" width=\"60%\"><figcaption><p>Close-up comparison of two images. Left: How gwenview displays the image. Right: How the image really looks like</p></figcaption></figure><p>This is… what?! Is my image viewer lying to me? Is it not able to display an image correctly, the only functionality that needs to be 100% reliable in an image viewer?</p><p>It can be hard to see exactly what is happening with an image that has random content, so I created a pixel checkerboard test pattern image and opened it in gwenview. This is what it displayed instead of a smooth surface:</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/checkerboard.png\" alt=\"What are these lines?\"></figure><p>As for the other image viewers, it’s all a roll of the dice. One will depend on Qt being built with the right options, otherwise it won’t support JPEG XL. Another will not be able to load a 16k×16k PNG file, because who uses such absurdly large images? Yet another will pan the image at 5 FPS because somehow everything is software rendered.</p><p>In other cases, support for various image formats will be lost over time due to <a href=\"https://code.qt.io/cgit/qt/qtimageformats.git/commit/src/plugins/imageformats?id=06ee5a2abc560a1041d2c9f80eaa42f5de80a4f9\" target=\"_blank\" rel=\"noopener\">maintenance overhead</a> and because no one seems to care.</p><p>I set out to write my own image viewer. I had already done the image loading, so only the output side needs to be handled. It would talk directly to the Wayland compositor, and use the appropriate protocol to handle fractional scaling in the right way to get a 1:1 pixel representation of images on the screen. I had previously done something similar for the <a href=\"https://wolf.nereid.pl/projects/tracy-profiler/\">Tracy Profiler</a>, so it should be easy.</p><p>Long story short, I can now view the images properly, but it uses Vulkan in not really the right way, and it requires a lot of polishing to get it where it should be, so I never released it.</p><p>By the way, going with what seemed like a “smart” name that played on already existing abbreviations only resulted in me being confused as to whether the image codec was called AVIF or AFIV. Not recommended.</p><p>Fast forward half a year, and I finally got annoyed enough with viu’s lack of support for modern image codecs to decide to do something of my own. How hard can it be? I have an image loader waiting to be used, and I just have to figure out how to display the pixels in the terminal.</p><p>There is a Unicode block that contains block elements (yay for semantic overloading!). Here are some examples of these elements: █ ▇ ▆ ▅ ▄ ▃ ▂ ▁. Or maybe something like that: ░ ▒ ▓. You get the point, these are meant to make crude graphics possible.</p><p>The most interesting block element is the one that’s half filled and half empty: ▄, or perhaps its negative counterpart: ▀. Typically, the font used in a terminal is more or less twice as high as it is wide, so by setting the foreground and background colors appropriately and using these half-block characters, you can get two very large squarish pixels.</p><p>The pipeline is thus as follows: load the image, query the terminal size, resize the image to fit the terminal, print out the pixels by emitting ANSI color codes to set the two halves of the half-block characters, and this results in the following output:</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/vv1.png\" alt=\"Unicode block elements image\" width=\"70%\"><figcaption><p>Unicode block elements image</p></figcaption></figure><p>It may be worth noting that <a href=\"https://hpjansson.org/chafa/\" target=\"_blank\" rel=\"noopener\">Chafa</a> uses more of the block symbols, but the printouts it makes look ugly to me, like a JPEG image compressed with very low quality.</p><p>The concept of setting terminal attributes dates back to the 1970s, and it really feels like a very old technology now. The nitty-gritty details can be read in the document <a href=\"https://invisible-island.net/xterm/ctlseqs/ctlseqs.pdf\" target=\"_blank\" rel=\"noopener\">XTerm Control Sequences</a>, but the general gist is that you print a special sequence of bytes, and that changes something about how the text is displayed on the terminal.</p><p>You’ve probably seen or even used things like “print  (or , or ) to set the color to red”, but how exactly does that work? Well, I don’t really know, because the Control Sequences document is way too complicated, and at the same time it avoids defining some stuff that was probably assumed to be common knowledge, but here’s my understanding of things.</p><p>The  element is “escape code 27” (straight out of the ASCII table) and indicates the start of a control sequence you want to send. You have to literally print out 27 as a byte, which can be done either by writing it as , or , and probably some other way too.</p><p>The  sequence is the Control Sequence Introducer, or . Supposedly it’s also 0x9b as a byte, but nobody uses it that way?</p><p>The  Pₘ  sequence sets Character Attributes. Pₘ is any number of single parameters (Pₛ), separated by the  character. Example single parameters for character attributes are  to make the text bold,  to enable underline,  to set the foreground color to red, and so on.</p><p>Thus, the above  ANSI sequence can be decoded as , which sets the foreground color to red. Similarly,  contains two different parameters, and it sets the foreground color to red and also enables text underlining.</p><p>I think it’s safe to say that most people would assume that there are only 16 colors available on the terminal, more or less based on the 1981 CGA text mode color palette. The eight parameters  to  set the basic color to be used, and then you can also use the  bold parameter to make the selected color brighter.</p><p>It is possible to use the more advanced 256-color mode in the terminal, where you choose the color from the palette of predefined colors. It still uses the Character Attribute ANSI sequence, but with the parameter set to  Pₛ, where Pₛ is the color index.</p><p>Finally, there’s the true-color mode, where the parameter is  Pr  Pg  Pb. The parameter values in this command are, of course, the RGB color values.</p><p>In vv I just assume that the terminal you have is capable of true-color display. This was standardized in 1994, so if the terminal of your choice doesn’t support it, I don’t really care.</p><p>Sixel is another ancient technology, originally introduced in DECwriter IV in 1982. It was largely forgotten, only to be rediscovered recently. It is now more or less widely supported, but is lacking in some key areas. One of these is the limited number of available colors, which requires the use of dithering.</p><p>To get the control sequences used to output sixel images, you can use the <a href=\"https://github.com/saitoha/libsixel\" target=\"_blank\" rel=\"noopener\">libsixel</a> library. I use it as a fallback, but it’s not in really good shape, because I don’t really have the means to test it properly, and the library itself is largely undocumented.</p><p>This is the main driver for outputting graphics in vv. It is <a href=\"https://sw.kovidgoyal.net/kitty/graphics-protocol/\" target=\"_blank\" rel=\"noopener\">relatively well documented</a>, has support in the terminals I use, and the image data is both true-color and supports an alpha channel.</p><p>The Kitty protocol is also very over-engineered, with a lot of seemingly unnecessary features. But if you only want to write an application that uses certain parts of the protocol, you don’t really need to worry about all the extra stuff you won’t be using.</p><p>To send the RGBA pixel data to the terminal, you start with a header message that specifies the image size and some other image data details. Then you send the data, first compressing it with deflate, then encoding it with base64. The payload must be split into 4KB chunks.</p><p>This results in a full color image being displayed in the terminal.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/vv2.avif\" alt=\"Kitty graphics protocol\" width=\"70%\"></figure><p>As it turns out, the terminal control sequences are not a one-way road. The terminal may want to respond to your query. For example, if you send the  Send Device Attributes sequence, the terminal may respond with  to indicate that it is “VT100 with Advanced Video Option” (whatever that means).</p><p>As a side note, you can use this query mechanism to determine if your terminal supports sixel images, or the kitty graphics protocol. I was very surprised to discover that the viu viewer also supports the kitty protocol, as it only printed sixel images on my Konsole terminal. It turns out that viu doesn’t do proper detection, and only enables the functionality if the  environment variable is set to . Which it isn’t on Konsole, even though Konsole supports the protocol. That’s not what the variable is for!</p><p>Getting back to the topic, in order to be able to read the terminal’s response, you have to do some magic first. This code is just following what I found somewhere, and it seems to work as intended, so I didn’t dig into it too much.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>This will give you a terminal file descriptor to write to and read from. Since you don’t know if the terminal will respond, you should poll for data being available before reading (or you will deadlock). You should also use a sufficiently long timeout period, as you may be running on a slow ssh connection, and the data may appear after quite a delay. Yeah, this is really an ancient technology, not really suited for modern use cases.</p><p>This is another true-color graphics display protocol that is available across many terminals now. It feels more like a  platform-specific hack, as it specifies the image data to be transferred as “any image format that macOS supports”.</p><p>On the topic of “which protocol is the best”, I think I can recommend the typical <a href=\"https://gitlab.freedesktop.org/terminal-wg/specifications/-/issues/12\" target=\"_blank\" rel=\"noopener\">FOSS SNAFU discussion</a>. The only upside, compared to the Wayland situation, is that there’s already a good protocol implementation existing that I can just use, instead of waiting for these endless debates to finally figure out the most simple and obvious things.</p><p>No, seriously, the idea that every terminal should implement support for every image format on its own, and do it in the right way (which we have yet to cover), is just something that will never happen.</p><p>There are certain formats, such as OpenEXR, that contain image data that cannot be displayed on a SDR display. Consider the following sample EXR image, where the RGB color channels are displayed directly as they are stored in the image file (this is also how gwenview displays it):</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/hdr1.avif\" alt=\"HDR image data viewed directly\"><figcaption><p>HDR image data viewed directly</p></figcaption></figure><p>Overall, the image is too dark. There are also large areas where the color is clipped, creating ugly oversaturated solid blobs of the same color. That’s the thing about HDR images. They can contain a lot of fine detail in dark areas. They can also contain very bright lights. None of this fits with what an SDR display can show. To view the image properly, the dynamic range must be compressed to what the monitor can display.</p><p>The process of doing this is called tone mapping. There are many different algorithms for doing this, and choosing a particular one is a matter of taste. For vv, I went with the <a href=\"https://www.khronos.org/news/press/khronos-pbr-neutral-tone-mapper-released-for-true-to-life-color-rendering-of-3d-products\" target=\"_blank\" rel=\"noopener\">PBR Neutral</a> operator, following Aras’ recommendation. The main reason for choosing it was that implementing it required only a little bit of math, instead of using a rather large lookup table. The other suggested tone mapping operators were <a href=\"https://github.com/h3r2tic/tony-mc-mapface\" target=\"_blank\" rel=\"noopener\">Tony McMapface</a> and <a href=\"https://github.com/FairplexVR/AgX-Tonemapping-Unity\" target=\"_blank\" rel=\"noopener\">AgX</a>.</p><p>It is important to note the way image data is specified in EXR files. The color values are in linear space, corresponding to measurements of the amount of photons from the given source in the given time. Double the number of photons, double the brightness, double the linear value.</p><p>But this is not how humans perceive light (or sound). The response of our visual system is more like an exponential curve, and that’s another big topic of gamma correction that I’m not going to get into. Long story short, the linear color values (tone mapping works with linear values) have to be converted to the “exponential” sRGB color space in order to be displayed correctly by the monitor.</p><p>With the tone mapping and the sRGB conversion, the image has a lot more detail, it is brighter in the dark areas, and the bright areas do not have their colors crushed.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/hdr2.avif\" alt=\"Tone mapped HDR image\"></figure><p>How important is it to handle the HDR images correctly? Why bother with HDR when you have the SDR monitor anyway? Isn’t it more of a movie thing anyway?</p><p>Well, it is what you make it. These HDR movies have to be made somehow, and to properly view the HDR movie data or HDR still images, you need an HDR monitor and a proper HDR pipeline in your operating system. Or what if you want to watch an HDR video on youtube that works in the confines of your web browser? Or maybe the HDR content is embedded directly into the web page you are viewing, like below?</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/hdr3.avif\" alt=\"HDR image as processed by your browser\"><figcaption><p>HDR image as processed by your browser</p></figcaption></figure><p>As far as games are concerned, the 2005 release of Half-Life 2: Lost Coast was commonly known as the showcase of HDR rendering, and HDR has only become more widespread since then. You simply must render with high dynamic range to make things look realistic. The game’s rendered output is typically tone-mapped for display on an SDR monitor, but recent game releases allow you to bypass that step and deliver the HDR output directly to HDR monitors.</p><p>The conversion from linear space to sRGB is commonly approximated with a $1/2.2$ power function. This is incorrect, the actual conversion is as follows:</p>$$\nL' = \\begin{cases}\n12.92 * L &amp; \\text{if $L &lt; 0.0031308$,} \\\\\n1.055 * L^{1/2.4} - 0.055 &amp; \\text{if $L &gt;= 0.0031308$.}\n\\end{cases}\n$$<p>Have a look at the images below for the difference.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/srgb-bad.avif\" alt=\"sRGB 2.2 power function approximation\"><figcaption><p>sRGB 2.2 power function approximation</p></figcaption></figure><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/srgb-good.avif\" alt=\"Proper sRGB transfer function\"><figcaption><p>Proper sRGB transfer function</p></figcaption></figure><p>While the trees at the bottom of the image are easier to see in the approximated version, the camera noise and color banding are much more visible. The linear part of the transform was explicitly designed to minimize this. Also note how the correct transformation results in more saturated colors.</p><p>I added support for vector images (SVG, PDF) in the second major release of vv. This required a different approach, because unlike raster images, which are loaded at their native size and then rescaled to fit the available space on the terminal, vector images have no innate size and should be rendered  the terminal size.</p><p>The libraries for both formats (librsvg, poppler) are quite easy to use, and both use the cairo library to do the actual drawing of the vector data.</p><p>The poppler library is licensed under the GPL. I don’t want my code to be GPLed.</p><p>As everyone on the internet will tell you, if you use a GPL library, your program must be GPL as well. If you don’t want that, there is the LGPL license. This is common knowledge.</p><p>Well, I have actually read the text of the GPL, and gave it some though. And I call bullshit.</p><blockquote><ol start=\"5\"><li>Conveying Modified Source Versions.</li></ol><p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p><p>c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. (…)</p></blockquote><p>This is the text that gives GPL the “viral” property. Makes sense, right? If you do a work (your program) based on the Program (in this case, some GPL library), you must also license the entire work (your program) under the GPL. When you link your program to the GPL library, you are surely basing your program on that library, right?</p><p>That’s not how it works. Let’s go back to the license text.</p><blockquote><p>To “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.</p><p>To “convey” a work means any kind of propagation that enables other parties to make or receive copies. (…)</p></blockquote><p>What is the title of section 5 again? “Conveying Modified Source Versions”. Do you copy or modify anything when you link? Yes, you do! You need to get the function prototypes from the headers, and you need to know the name of the library you are linking against!</p><p>The problem is that these are statements of fact, and thus do not fall under copyright protection, which excludes them from being modifications. This is what the license text says. And it would be true even if the license did not explicitly say so.</p><p>It’s interesting to think about what is actually copyrightable. <a href=\"https://www.eff.org/deeplinks/2023/01/beware-gifts-dragons-how-dds-open-gaming-license-may-have-become-trap-creators\" target=\"_blank\" rel=\"noopener\">This text</a> provides some commentary on the subject. Or think about phone books. You cannot copyright a list of names and phone numbers, because those are just simple facts. However, if you were to assign some kind of rating to each person (e.g., on a nice-rude axis), then that rating would be copyrightable. Notably, the library headers will contain comments, and these do fall under the copyright. Whether this is significant, if all you do is machine processing which ignores the comments, is a matter to discuss for the lawyers.</p><blockquote><p>Google’s copying of the Java SE API, which included only those lines of code that were needed to allow programmers to put their accrued talents to work in a new and transformative program, was a fair use of that material as a matter of law</p></blockquote><p>Anyway, here is what vv does to link to the GPLed poppler library. This was done using only the documentation.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>This is probably a bit exaggerated in how evasive it is, but it works. Go and argue that vv is a poppler derivative when you don’t even need to have poppler installed to build the program. Explain how someone could be bound by a license to something they do not use.</p><p>Finally, remember that the only legal text you are bound by (if at all) is the actual text of the GPL license. It does not matter what Stallman says he intended with the GPL, or what the GPL FAQ says should happen in some fantasy land.</p><p>The common knowledge is that if you want to have an animated image, you use the GIF format. While this may no longer be true, as most websites now deliver animated images as MP4 videos, those same websites will lie to you and tell you that it’s GIF this or GIF that, because that’s what people understand as an animated image.</p><p>It turns out that there are many other ways to deliver animated images. One example of this would be the APNG format, but I have never seen it out there in the wild. Another example is the WebP format. Unlike animated GIF or PNG images, which you can just load and get the first frame, animated WebP images will not load with the basic loader library. You have to use a different library, with a different API, all of which is not well documented, in order to even display anything in case of some WebP images. Considering how disjointed all of this is, and that there’s no overall design in sight, the animation feature was probably done as a cool and quick hack, and there was no one around with enough awareness to prevent it from happening.</p><p>WebP is one of the I-frame image codecs. In a proper video codec, I-frames are used to represent a complete video frame without references to other frames. This allows you to seek in the video, or make sure the video quality is high enough when there’s a quick scene change. Video codecs will also use P and B frames to encode only the changes between the frames in the video, at a much lower bitrate cost. The downside of this delta compression is that these frames are based on the content of previous (or next) frames, so you can’t just quickly seek to any one of them, you have to decode all the previous frames, starting with an I-frame.</p><p>In essence, someone thought “hey, we have all these advanced image codecs for keyframes in videos, why don’t we use them to encode images?” and WebP was created, based on the VP8 I-frame format. HEIC images use HEVC/H.265 I-frame encoding. And AVIF images use AV1 I-frame encoding. There’s also the VVC image format based on H.266, but adoption seems to be very limited as of now.</p><p>To summarize, the WebP image format is based on parts of the VP8 video codec. The animation support in WebP is  done by adding parts that were removed in the process, removed from the video codec, from the very thing it is now trying to replicate. It is done by concatenating multiple WebP images together in some sort of container format. Try to see the logic in that.</p><p>Kitty provides proper support for animating images in the terminal. You would probably think that the animation would have to be done by repeatedly sending the individual image frames that would replace what you had on display before. This isn’t the case with the Kitty protocol. Instead, you define an animation by providing all of its frames, and then start the animation loop. The animation will continue to play even if your program is stopped, as in the video below.</p><p>One big practical drawback is that support for this type of animation is currently very limited. For example, the KDE Konsole or Ghostty don’t support it at all.</p><p>At this point I thought that there are many ways to store the HDR image data, and the only format where I have complete control over the pipeline is OpenEXR. It would be nice to use the same algorithms for all formats instead of having a random selection of output processing.</p><p>I started working on this with the RGBE or Radiance HDR format. The stb_image loader only does the linear to sRGB conversion if you want the typical 8 bits per color channel data, but it also provides a way to get the raw HDR floating point color values that you can put into the tone mapping pipeline. The results are shown below.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/radiance.avif\" alt=\"Left: Linear to sRGB conversion by stb_image. Right: PBR Neutral tone mapping. Note the increased color saturation and detail in very bright areas.\" width=\"80%\"><figcaption><p>Left: Linear to sRGB conversion by stb_image. Right: PBR Neutral tone mapping. Note the increased color saturation and detail in very bright areas.</p></figcaption></figure><p>Then I looked for some HDR AVIF files, and this is what I got. The images were bland, desaturated, lacking contrast. They looked like this not only in my image viewer, but also in KDE Dolphin’s thumbnails, in Gwenview image viewer, in Okular PDF viewer, in GIMP. This is obviously wrong! Yet none of these fairly popular applications were able to get it right?</p><p>At this point <a href=\"https://piaille.fr/@rfnix\" target=\"_blank\" rel=\"noopener\">@rfnix@piaille.fr</a> chimed in with talk about “PQ-encoded”, “ICC profile” or maybe “CICP/nclx” not being handled properly. And what is that anyway, why should I know, I just want to load the RGB pixels from an image and have them look good on the screen. Yeah, it doesn’t work that way, as it turns out. The simple API the libraries give you is a trap that will not output the correct data in the more advanced cases.</p><p>Let’s ignore HDR completely for the time being. The color management problem applies just as well to standard SDR images, such as JPEGs. Take a look at the images below.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/colorspace.avif\" alt=\"Three color spaces. Left: Rec. 709, center: Rec. 2020, right: DCI-P3.\" width=\"80%\"><figcaption><p>Three color spaces. Left: Rec. 709, center: Rec. 2020, right: DCI-P3.</p></figcaption></figure><p>If I were to show you just one of these pictures, any one of these pictures, you would say that it is a good picture. But each picture has different colors! Which one is right? How would you know? Maybe it doesn’t matter that much, the difference is quite small and only noticeable if you have another picture to compare it with…</p><p>The Rec. 709 one is what it should look like. It defines the color primaries and white point used in sRGB that your browser and display expect. The conversion would be an identity in this case.</p><p>And it really does matter a great deal, as you can see from the <a href=\"https://github.com/AcademySoftwareFoundation/openexr-images/tree/main/Chromaticities\" target=\"_blank\" rel=\"noopener\">images</a> below.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/xyz.avif\" alt=\"Different color spaces. Left: Rec. 709, right: CIE XYZ.\" width=\"80%\"><figcaption><p>Different color spaces. Left: Rec. 709, right: CIE XYZ.</p></figcaption></figure><p>Not quite the same, right? But they are the same if you do your color management right!</p><p>Okay, but wait, why does this thing even exist? Why would anyone save a photo in a different “color space”? What is a color space anyway? Aren’t you just going to show a picture on the screen anyway? To understand this, let’s take a look at the <a href=\"https://commons.wikimedia.org/wiki/File:CIE1931xy_gamut_comparison.svg\" target=\"_blank\" rel=\"noopener\">chromaticity diagram</a>.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/chromaticity.svg\" alt=\"Chromaticity diagram\" width=\"50%\"></figure><p>The colored area represents all the colors that a human can see. The triangular shapes on the graph correspond to the color ranges (gamut) that can be represented by different color spaces. As you can see, the sRGB gamut is quite limited in what it can show. Have you ever wondered why photos of a vibrant sunset sky always look a bit bland compared to the real thing? This is why. The monitor simply cannot reproduce the right colors.</p><p>Working with a non-sRGB gamut gives you more room to maneuver in image processing, even if the resulting image will be displayed on a sRGB display. Or maybe you have a fancy screen that can display all those extra colors? For example, if you are proofing images for printing, in which case you will need to target the CMYK color space, which has a significant area outside the sRGB gamut (but within the Adobe RGB gamut).</p><p>As it turns out, the normalized 0 to 1 ranges of the red, green, and blue channels don’t mean anything by themselves. They are all defined in the context of a color space, and to get the right values to display on the screen, you need to convert these color values from the image color space to the display color space.</p><p>The reason why “nobody” seemingly cares about this is the same as for <a href=\"http://www.ericbrasseur.org/gamma.html\" target=\"_blank\" rel=\"noopener\">gamma correction</a>. This is computer graphics, and it’s all highly subjective (e.g., which digital camera manufacturer has the best color reproduction? Fight!), it’s not obviously wrong in most cases, and it requires some in-depth knowledge to even realize things are not correct.</p><p>Here’s a much more dramatic example of what happens when you don’t care about color management. This is a <a href=\"https://github.com/link-u/avif-sample-images/tree/master\" target=\"_blank\" rel=\"noopener\">test image</a> that  things to look wrong, if handled incorrectly.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/circle.avif\" alt=\"Left: wrong, right: correct\" width=\"50%\"><figcaption><p>Left: wrong, right: correct</p></figcaption></figure><p>Still not enough? Here’s a <a href=\"https://www.color.org/version4html.xalter\" target=\"_blank\" rel=\"noopener\">nice example</a> of what can happen even with the venerable JPEGs.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/Supports_None.avif\" alt=\"JPEG with various color profile handling fails\" width=\"25%\"><figcaption><p>JPEG with various color profile handling fails</p></figcaption></figure><p>And it gets even better. Here’s a <a href=\"https://www.hackerfactor.com/blog/index.php?/archives/608-A-Bit-Off-Color.html\" target=\"_blank\" rel=\"noopener\">specially crafted</a> single PNG image that can be displayed in 10 different ways, depending on the correctness and quirks of the image processing software.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/egg.avif\" alt=\"Which one is displayed properly?\" width=\"80%\"><figcaption><p>Which one is displayed properly?</p></figcaption></figure><p>The color space definition is stored in images in the form of ICC color profiles. For the purpose of handling them, it is enough to know that they are just some binary objects of some size that you need to pass from the image to the color management library via some function calls.</p><p>To implement color management in vv I decided to use the Little CMS library. It’s quite easy to use, and already a dependency for a lot of software. First you need to create input and output color profiles, for example by using  to load an ICC color profile embedded in the image and  to create a sRGB profile for display on the screen. Then you use both profiles to create a transform with , which can be applied to pixel data with . The transform seems to be thread-safe, and the input and output buffers can be the same (if the data types match). At least I haven’t had any problems using it in such a configuration.</p><p>Okay, so those not-quite-right-looking HDR images I showed you earlier are probably in some kind of HDR color space that needs to be handled properly, and that’s it, right?</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/libheif1.avif\" alt=\"HDR image looks bad\" width=\"80%\"></figure><p>Well, no, not really. That would be a reasonable assumption, but instead we have to write what feels like half of a video codec processing pipeline. </p><p>The image above was retrieved by calling the following libheif function, where we request the image to be decoded as RGB because that’s what we want to display on the screen in the end. Another reasonable assumption would be that the library will return an image that looks correct in such a case.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>This gives us data that is 8 bits per channel, which is not what we want for an HDR image. It is possible to get the 16-bit RGB data with the <code>heif_chroma_interleaved_RRGGBBAA_LE</code> enum value, but it is all horrible and wrong and I can’t even remember the details of what I did to try to get it to work. Let’s not talk about it.</p><p>The other available option is to request . Let’s see what it is.</p><p>We need to start with some definitions that can often be quite confusing. As mentioned earlier, we can measure the amount of photons hitting, say, the light-sensitive “pixels” of a camera. This linear measure is called  and is expressed in nits, or candelas per square meter. However, this measure is not really useful when we need to encode things as an electrical signal, which is just a voltage, such as between 0V and 5V. So for practical applications, the  is used, denoted as . Its value is calculated by dividing the luminance value by the maximum expected value of luminance, and it results in a 0 to 1 range that can be easily encoded.</p><p>If we have a color image, we can calculate the relative luminance of its pixels (i.e., convert it to grayscale) by assigning certain weights to the color channels, corresponding to how humans perceive color, and summing them. For correct results, the color channel values should be linear, not gamma compressed. Here are the weights as defined in Rec. 709 (other standards may use different weights):</p>$$\nY = 0.2126 * R + 0.7152 * G + 0.0722 * B\n$$<p>For the sake of practicality, and because getting things exactly right isn’t always the first priority, there is also , denoted as . This is the weighted sum (the coefficients are the same as for relative luminance) of the gamma-compressed color channels. In reality, the terms luma and luminance are often mixed up, even in the specialist literature, and Y’ is often written simply as Y.</p><p>The television signal was originally transmitted as a series of black and white images. Expanding the signal to include color, while maintaining compatibility with existing TV sets, required some smart thinking.</p><p>To make a long story short, the already existing black and white luma signal was supplemented with two color difference channels, calculated as follows:</p>$$\nU = B' - Y' \\\\\nV = R' - Y'\n$$<p>As you probably figured out, the ’ is an indication of gamma compression. The exact formula is a little bit different, and there are a couple of coefficients in there that I have left out, but none of that is important here.</p><p>This encoding is known as Y’UV, or simply YUV, and you can see what it looks like in the image below.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/yuv.avif\" alt=\"YUV channel decomposition. Left to right: original picture, Y channel, U channel, V channel.\"><figcaption><p>YUV channel decomposition. Left to right: original picture, Y channel, U channel, V channel.</p></figcaption></figure><p>The human visual system is much more sensitive to brightness changes than to color changes, so it was possible to reduce the resolution of the color difference channels to reduce bandwidth requirements. This is known as chroma subsampling.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/subsampling.svg\" alt=\"Chroma subsampling\" width=\"60%\"></figure><p>Chroma subsampling can be used in many different places. Image or video compression is one such place. Displaying on a TV or monitor is another. If your display subsamples the color signal, you probably will not see the degradation in photos or movies, because they may already have been subsampled, and otherwise the color distribution would be smoothed over many pixels. But if you were to look at a typical computer screen with a lot of text or fine iconography in such a display configuration, you would immediately notice that everything looks wrong. There are some examples of this on the <a href=\"https://www.rtings.com/tv/learn/chroma-subsampling\" target=\"_blank\" rel=\"noopener\">RTINGS web site</a>.</p><p>The YUV encoded image can be converted back to RGB using an inverse transform.</p><p>This is another color image encoding, but it is very similar to YUV. Technically it should be called Y’CbCr, but nobody cares, and you might as well encounter it as YCC, or simply YUV.  Just like YUV, YCbCr consists of the luma channel and two color channels that can be subsampled.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/ycbcr.avif\" alt=\"YCbCr channel decomposition. Left to right: original picture, Y channel, Cb channel, Cr channel.\"><figcaption><p>YCbCr channel decomposition. Left to right: original picture, Y channel, Cb channel, Cr channel.</p></figcaption></figure><p>As mentioned earlier, libheif gives us the option to load the image with the  option, and this is the color space we get. Unlike the RGB output, where the color channels are interleaved together in the RGBA pattern, YUV/YCbCr is a planar format, where each channel is stored disjointly in memory. This sort of makes sense, because chroma may be subsampled, and there would be no way to interleave such color planes. Well, at least we can request libheif to always return the planes as 4:4:4, even if they are subsampled in the image.</p><p>The video image codecs store the plane values as 8, 10, or 12 bit integer values which can be normalized to 0 to 1 range floating point values for further processing. And while we’re at it, there’s another quirk to take care of.</p><p>While in digital signal processing the full signal range, e.g. 0 to 255 in the case of 8 bits, is not a problem, things are different in the analog world. Using the full range of values would cause the signal to overshoot or undershoot the allowable signal levels (see <a href=\"https://en.wikipedia.org/wiki/Gibbs_phenomenon\" target=\"_blank\" rel=\"noopener\">Gibbs phenomenon</a>), so the range is limited by scaling and offsetting the Y to the 16 to 235 range and the UV to the 16 to 240 range. In order to perform the normalization correctly, it is necessary to check whether the image uses the full range or the limited range, and adjust accordingly if necessary.</p><p>We discussed color profiles earlier, and libheif gives you the ability to query for an ICC color profile. But it also has a function to query for an nclx color profile. So which one should you use? Are the two related in any way? Can you use one instead of the other? Little CMS has no support for nclx, so how do you use it? The documentation is very light on this.</p><p>As it turns out, an image can have none of these, one of these, or both at the same time. And you have to deal with them accordingly, in different ways, even when they overlap.</p><p>We already covered the ICC profile, but what is an nclx profile? Let’s take a look at the corresponding libheif struct (with some fields omitted):</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>The  indicates whether the YUV values are limited or full range. Notably, if the flag is not present (i.e. if there is no nclx profile), the value of the flag is assumed to be .</p><p>The  field rings a bell, but why is it an enum? Let’s see what it looks like.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>Oh my… What do you even do with this? The answer can be found in the H.273 specification document (commonly known as “coding-independent code points”, or CICP).</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/cicp.avif\" alt=\"ITU-T H.273 (V4) (07/2024), 8.1 Colour primaries\" width=\"50%\"><figcaption><p>ITU-T H.273 (V4) (07/2024), 8.1 Colour primaries</p></figcaption></figure><p>Ah, so these are the color primaries coordinates for the red, green, and blue channels, along with the white point location. These are given in xy coordinates, just like in the chromaticity diagram shown earlier. It looks simple now! You can just drop this into the color management library and get a color profile from it.</p><p>Okay, but the color transformation is in RGB, yet we still have the YCbCr values, don’t we? How do we convert these, is there some sort of function to handle this? To know how to perform the YCbCr to RGB transformation, we need to take a look at the  field, which can have the following values:</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>Again, the correct way to handle all these options is explained in the H.273 specification. It is a very dense, math-heavy section that spans six pages, which I will spare you from looking at. And most of it isn’t even needed at all, because only a few of these values are used in practice, see the table below.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/cicp2.avif\" alt=\"ITU-T Series H Supplement 19\" width=\"70%\"><figcaption><p>ITU-T Series H Supplement 19</p></figcaption></figure><p>Knowing this secret sauce makes things simple again. The 0 value indicates that the YCbCr values are actually GBR, and only require a channel swap to get RGB. The 1, 5, 6, and 9 values use the same equation, differing only in the coefficients used for multiplication. The 14 value does not operate in the YCbCr color space and is therefore irrelevant for libheif.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>The last thing to cover in the nclx profile are the .</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>In practice, the options to cover are value 13, which is just the linear to sRGB transfer function we covered earlier, and the Perceptual Quantize (16) and Hybrid Log-Gamma (18) transfer functions used for HDR images.</p><p>The reason for these HDR transfer functions to exist is the amount of bits available. While the OpenEXR format can simply store the linear color values as 32-bit or 16-bit (IEEE 754-2008 half-precision) floating point numbers, the video codec formats only have 10 or 12 integer bits to represent the dynamic range, and as we already know, the human visual system’s response to light stimulus is exponential. This does not play well with the evenly spaced integer values.</p><p>The PQ transfer function, as the name implies, is based on the characteristics of human perception. It distributes the available integer values over a wide dynamic range, scaling from 0.0001 nits to 10000 nits. The same function is used to transfer each of the color channels.</p><div><pre tabindex=\"0\"><code data-lang=\"c++\"></code></pre></div><p>The HLG transfer function is designed to be backwards compatible with SDR displays by combining an SDR gamma curve with an HDR logarithmic curve for color values above 1.0. The HDR images I found were all using the PQ characteristic, so the HLG support in vv is probably not implemented the right way. But what can you do if you can’t test it properly?</p><p>Going through my test images, I noticed that one of them was a bit odd. It did not have the nclx profile, and according to the <a href=\"https://github.com/AOMediaCodec/libavif/wiki/CICP\" target=\"_blank\" rel=\"noopener\">AVIF CICP spec</a>, the default matrix coefficient should be 6, or Rec. 601. However, when inspecting the image with  or , the reported coefficient was 1, or Rec. 709. How can this be?</p><p>When you load the image with libheif, you get the image handle, on which you can call the <code>heif_image_handle_get_nclx_color_profile()</code> function to get the nclx profile. However, in some cases the profile may only be available after decoding the image, with the <code>heif_image_get_nclx_color_profile()</code> function. So you have to take this into account.</p><p>To recap, the HDR processing pipeline with libheif is as follows:</p><ol><li>Convert the integer plane values to floating point, possibly taking into account the limited range adjustment,</li><li>Perform the conversion from YCbCr to RGB, following the matrix coefficients from the nclx profile,</li><li>Do the color management, either by loading the ICC profile or by using the values from the nclx profile,</li><li>Linearize the color values by applying the nclx transfer function,</li><li>Convert from linear color space to sRGB.</li></ol><p>That’s certainly something, isn’t it? But how does it look now? Check out the picture below to see how much better it is.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/libheif2.avif\" alt=\"HDR image looks good\" width=\"80%\"></figure><p>More examples can be seen in the following post.</p><p>Having things render correctly is very nice, but it also made everything a bit slow. One of my test images a resolution of 9504×6336 and it took over 8 seconds to load. This was unacceptable.</p><p>The most obvious thing to do was to parallelize the image processing. All the calculations I do are local to a single pixel and don’t depend on the neighbors, so it should be embarrassingly parallelizable.</p><p>To manage the jobs, I used my  class that I developed while working on etcpak. It is very simple and had better CPU usage than the other job dispatchers I compared it to a decade ago.</p><p>I started by parallelizing each of the processing steps separately to follow the serial way of doing things. Then, after a bit of profiling, I realized that this was quite inefficient. The test image I used was 9504 * 6336 * 4 channels * 4 bytes = 918 MB. So the parallelized color management function had to load that 918 MB, process it, and store the 918 MB back into memory. Then the parallelized transfer function had to load the 918 MB again, do the necessary math, and store the 918 MB again. And so on. No wonder the profiler showed that half of the execution time was in the memory instructions, since everything had to go through RAM all the time.</p><p>In order to make this better, I have completely changed the way the loader works. The parallelization is now done at the top level, and each job starts by loading a chunk of YCbCr data, then does all the necessary processing in a small temporary buffer, step by step, and finally writes the finished image section to the output bitmap. The chunks are small enough to fit into the cache, and the memory instructions no longer flare up when profiling.</p><p>Going wide with calculations is another obvious thing to do. My compiler (clang) was already nice enough to vectorize the integer to float YCbCr conversion, along with the YCbCr to RGB routine. Using gcc may not give you the same results, but I don’t really want to bother doing something that has already been done for me.</p><p>Since, for , we are still largely tied to the 2003 CPU architecture that only supports SSE2, vv is built with the  compiler option to be able to use SIMD at all. The other choice is to implement dynamic dispatch, so that a generically built binary can use features from more advanced architecture levels, depending on what the CPU supports. Bur I  don’t want to make the code more complicated just because the whole industry is afraid to stop supporting CPUs released before 2013, when Haswell with AVX2 and FMA became available.</p><p>The PQ function requires an implementation of the  function to be available. Implementing it for wide processing requires some complex math that I did not want to deal with. That’s something that should be readily available in some sort of a SIMD math library, right?</p><p>Replacing the series of mul + add instructions with fma (fused multiply-add) tipped the scales in favor of SIMD, albeit slightly. Extending the implementation to AVX2 gave the expected 2× speedup, but the AVX512 version produced artifacts. I later found out that I somehow missed the correct rounding option when reading the SIMD documentation and used the wrong one, resulting in NaN output. But it didn’t matter. I was not happy with the performance of that version.</p><p>At this point, Aras chimed in again and recommended <a href=\"https://github.com/AcademySoftwareFoundation/OpenImageIO/blob/main/src/include/OpenImageIO/fmath.h#L2013\" target=\"_blank\" rel=\"noopener\">another implementation</a>. I wrote a bit of code based on it, and somehow managed to make the same rounding option mistake again, which produced a rather psychedelic image. But I did not know that at that time.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/pow.avif\" alt=\"“Can you tell I took something” meme\" width=\"50%\"><figcaption><p>“Can you tell I took something” meme</p></figcaption></figure><p>At this point, I reluctantly decided that maybe it’s time to stop trying random code from the web, and maybe I should actually understand how this should work. This is what I found out before I realized what the real problem was.</p><p>First, the power function can be expressed as a combination of  and  functions. This has been already clear from prior research and from reading the SIMD implementations.</p>$$\nx^{y} = e^{y * \\ln x}\n$$<p>The first implementation I talked about above used this formula and stuck to using $e$ as the base. This unfortunate decision required some unnecessary back and forth transformations of the numbers. The exponential function and the logarithm can actually be combined together using any base, so the equation can be rewritten as:</p>$$\nx^{y} = 2^{y * \\log_2 x}\n$$<p>This is important when we consider how IEEE 754 floating-point numbers are encoded. To recap, a 32-bit float consists of (counting from the oldest bit):</p><ul></ul><p>And the number value is calculated as follows.</p>$$\n-1^S * 1.M * 2^{E-127}\n$$<p>Now let’s try to make some sense of this. The sign bit is irrelevant for us, the color values are never negative. Writing the exponent as $E-127$ is only necessary to decode the binary encoding of the number, which we will simplify to just $E$, assuming it is properly biased. The mantissa value written as $1.M$ is actually always in the 1 to 2 range. When this 1 to 2 range is multiplied by $2^E$, the range changes to, for example, 0.5 to 1, or 2 to 4, and so on.</p><p>With this knowledge in hand, let’s take a look at another mathematical identity.</p>$$\n\\log (a*b) = \\log a + \\log b\n$$<p>And guess what, the floating point numbers follow the $a*b$ formula. Let’s put it into the equation.</p>$$\n\\log_2 (1.M * 2^E) = \\log_2 1.M + \\log_2 2^E\n$$<p>Which can be simplified as follows.</p>$$\n\\log_2 (1.M * 2^E) = \\log_2 1.M + E\n$$<p>As it turns out, with base 2, all we have to do is calculate the logarithm of the mantissa and add the value of the exponent to get the logarithm of the whole floating-point number. And since the mantissa will always be in the range 1 to 2 (or 0.5 to 1 if the exponent is biased), we can approximate its logarithm quite accurately with a polynomial function.</p><p>The exponential function is also approximated by a polynomial and some similar tricks, but I have not had a close look at the implementation details.</p><p>Once the power function was taken care of, implementing the PQ transform as a SIMD function was fairly straightforward. The scalar version of the code took 1.56 s to run. The SSE 4.1 + FMA version took only 528 ms, which is about a 3× speedup. Nice!</p><p>Widening the SIMD code was trivial because there is no crosstalk between lanes, which would otherwise require costly shuffles and permutes. The AVX2 version ran at 276 ms, while the AVX512 version required only 145 ms.</p><p>Finally, by enabling parallelization, which we have already discussed, the run time was further reduced to a mere 31 ms. That’s only 2% of the original 1.56 seconds!</p><p>Both the PBR Neutral operator and the linear to sRGB transfer function use conditional execution. Since both functions operate on packs of 4, 8, or even 16 values, I simply compute both sides of the condition and then merge the results depending on the outcome of the pipelineable comparison operation.</p><p>Before the optimizations the test image took more than 8 seconds to load. With the SIMD code paths and multithreading in place, the same image can be loaded in just 0.8 seconds.</p><figure><img src=\"https://wolf.nereid.pl/images/image-viewer/profiler.avif\" alt=\"Trace of program execution\"><figcaption><p>Trace of program execution</p></figcaption></figure><p>Decoding of YCbCr planes with libheif is 248 ms. The YCbCr packing, normalization, color management, PQ transfer, and tone mapping pipeline is 355 ms. Resizing the image is 111 ms. Compression with zlib is 19 ms. Writing to the terminal is 127 ms.</p><p>It turns out that viewing a random cat picture from the internet is quite an involved process. The amount of things you have to cover makes you quite interdisciplinary.</p><p>On the other hand, it’s not  hard. The original implementation of the loaders for my compositor took me maybe two or three days. It took me about 5 days to release the first version of vv, basically starting from scratch, with only the image loader library available, which I even extended with support for OpenEXR, TIFF and RAW images, along with tone mapping during those 5 days.</p><p>And yet I still can’t open a BC or ETC texture in virtually any other image viewer, somehow.</p><p>The next steps for vv would be to check the color management pipeline for correctness. Support for color profiles will need to be added to a lot of formats that still do not have it. I may have already announced that the JPEG XL loader supports it correctly, but it turns out that the code paths are not executed. It may need a similar treatment to libheif. Oh well.</p><p>Loading animated GIFs would be nice, but the libgif interface looks like it was designed for 16-bit computers and their limitations. And it probably was. Mouse cursors need to be ported to the general image loader interface. Adding support for Windows .ico icons would also be nice.</p><p>I will most likely want to add support for a few more tone mapping operators.</p><p>Then maybe do some work on the graphical viewer to make it usable and reliable enough. After all, vv is just a thin wrapper over the image loading library, which does all the heavy lifting.</p>","contentLength":59306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42730512"},{"title":"Divers recover Phoenician shipwreck that sank 2.6k years ago off coast of Spain","url":"https://www.smithsonianmag.com/smart-news/divers-recover-ancient-shipwreck-that-sank-2600-years-ago-off-the-coast-of-spain-180985778/","date":1737059136,"author":"bookofjoe","guid":157,"unread":true,"content":"<p>Archaeologists have raised a 2,600-year-old shipwreck from the waters near southeastern Spain.</p><p>Divers initially located the vessel in 1994, according to <a href=\"https://www.cbsnews.com/news/ancient-shipwreck-mazarron-ii-2600-years-old-spain/\">CBS News</a>’ Emily Mae Czachor. The 27-foot-long wreck, which had been carrying a load of lead ingots when it sank, was discovered near the town of Mazarrón.</p><p>Known as the , the ship was located just six feet beneath the surface of the Mediterranean, roughly 200 feet away from a beach called Playa de la Isla. It was covered in sand, which helped keep it hidden for centuries.</p><p>More recently, changes along the coastline—including coastal construction and shifting sea currents—made the wreck site more vulnerable.</p><p>“The wreckage can no longer remain where it is because its sand protection is now disappearing,” said Carlos de Juan, an archaeologist at the University of Valencia who led the excavation project, in a July 2024 <a href=\"https://www.uv.es/uvweb/college/en/profile/uv-leads-extraction-a-phoenician-boat-mazarron-murcia-1285950309813/Novetat.html?id=1286391497880&amp;plantilla=UV_Noticies/Page/TPGDetaillNews\">statement</a>. “The wreckage has survived for centuries, but now it is time to roll up our sleeves and ensure that we can continue to enjoy this asset of cultural interest.”</p><p>For many years, the wreck had been covered by a protective metal box. But a group of experts who studied the wreck site between 2017 and 2019 found that the metal box<a href=\"https://www.cultura.gob.es/mnarqua/en/dam/jcr:c87a9a94-dc0e-4522-ad4e-a0305ad36e9a/dossier-mazarron-2-maquetado-eng.pdf\"> was sinking</a> and threatened to crush the shipwreck.</p><p>In the summer of 2023, archaeologists began to formulate a plan to raise the shipwreck from the seafloor. They spent <a href=\"https://archaeologymag.com/2023/07/ancient-phoenician-shipwreck-in-spain/\">560 hours</a> diving at the wreck site to make detailed diagrams of its many cracks and fissures.</p><p>“It is more reasonable to rescue the ship, treat it and exhibit it in a museum for people to enjoy it, rather than worrying every time a big storm arrives,” de Juan told&nbsp;<a href=\"https://www.reuters.com/lifestyle/spanish-archaeologists-plan-rescue-2500-year-old-phoenician-shipwreck-2023-06-30/\">Reuters</a>’ Emma Pinedo in June 2023.</p><p>Between September and November 2024, a team of 14 divers carefully brought the wooden shipwreck to the surface, piece by piece. Now, those fragments are <a href=\"https://www.uv.es/uvweb/uv-news/en/news/concludes-extraction-phoenician-wreckage-mazarron-ii-1285973304159/Novetat.html?id=1286408791600&amp;plantilla=UV_Noticies/Page/TPGDetaillNews\">going to a lab</a>&nbsp;at the Museum of Underwater Archaeology in Cartagena for conservation and reconstruction. That work is expected to take at least four years, reports&nbsp;<a href=\"https://elpais.com/espana/2024-11-11/culmina-con-exito-la-extraccion-del-mar-del-barco-de-epoca-fenicia-hundido-en-mazarron.html\"></a>’ Virginia Vadillo.</p><p>To preserve the ship, experts will start by removing the salt from each of the pieces, per . Next, they will apply resins to help fill in some of the places where the wood has rotted away. They will then freeze-dry the pieces before reassembling the ship</p><p>Archaeologists think the ship belonged to the&nbsp;<a href=\"https://www.britannica.com/topic/Phoenician\">Phoenicians</a>, a group of maritime traders and merchants who inhabited the eastern Mediterranean coast from around 1500 to 300 B.C.E.</p><p>is one of the few Phoenician-era shipwrecks that’s still largely intact, and it could offer new insights into Phoenician shipbuilding techniques and culture, the archaeologists say.</p><p>“It will tell us what types of wood were used to build the boat, where it was built, what navigation was like at the time, the degradation processes of the wood, the contamination that may have occurred in shallow waters,” said&nbsp;<a href=\"https://www.uv.es/uvweb/college/en/profile-1285950309813.html?p2=adiez&amp;idA=true\">Agustín Díez</a>, a historian at the University of Valencia who also worked on the project, in the statement.</p><p>Another shipwreck, called the&nbsp;<a href=\"https://www.cultura.gob.es/mnarqua/en/colecciones/yacimientos/mazarron.html\"></a>was discovered in the same area off the coast of southeastern Spain in 1993. Archaeologists raised it from the water two years later. After many years of conservation work, the ship went on display at the National Museum of Underwater Archaeology in 2005.</p>","contentLength":3275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42730449"},{"title":"Build a Database in 3000 Lines with 0 Dependencies","url":"https://build-your-own.org/blog/20251015_db_in_3000/","date":1737035974,"author":"not_a_boat","guid":156,"unread":true,"content":"<p>Complex software like databases, compilers, and browsers are treated\nlike black boxes. You use them every day as a , but you\nprobably don’t understand them as a , even though\nthey are nothing but code. Why?</p><ul><li>They have little in common with programmers’ daily task.</li><li>Their code bases are so large, so discouraging.</li></ul><p>But that doesn’t mean you can’t learn these things. People have built\nsmall, toy versions of these things for learning purposes, such as <a href=\"https://github.com/rswier/c4\">C in 4 Functions</a>, <a href=\"http://craftinginterpreters.com/\">Crafting Interpreters</a>, <a href=\"https://browser.engineering/\">Web Browser Engineering</a>. It\nturns out that if you minimize uninteresting details and focus on the\nessentials, you can recreate seemingly complex software and learn its\ncore ideas.</p><p>For compilers, some of the core ideas include:</p><ul><li>Parsing stuff with recursion.</li><li>Representing a program as a tree and simulate it.</li><li>Representing a program as a list of instructions with gotos.</li><li>Stack machine or register machine.</li></ul><p>None of these ideas are complicated. So a compiler is a viable and\nuseful exercise for programmers.</p><h2>02. What are the core\nideas of databases?</h2><p>I’ve built a <a href=\"https://build-your-own.org/database/\">small database in 3000 lines</a>\nfrom scratch in Go to learn the core ideas of databases. It’s not that\ncomplicated if you approach it in a certain way.</p><p>A database stores data on disk, but why not just use files? Why is\nthe filesystem not a database? Because databases eliminate a common\nsource of data corruption: partially written files caused by a crash or\npower loss.</p><p>Video games warn you not to turn off the power while saving, because\na partially written save file can destroy all your progress. Building a\ndatabase will teach you how to eliminate this concern. The solution is\nan <strong>append-only log with a checksum for each item</strong>.</p><ul><li>Append-only means that writes will not destroy existing data.</li><li>Checksum means that partial writes will be detected and discarded,\nmaking appends power-loss atomic.</li></ul><pre><code>╔═══╤════╦═══╤════╦══╌╌╌══╦═══╤════╗\n║sum│data║sum│data║more...║sum│data║\n╚═══╧════╩═══╧════╩══╌╌╌══╩═══╧════╝\n                          ╰───┬────╯\n                    possible partial write</code></pre><p>These are the first 2 ideas. But they are not enough to build a\ndatabase.</p><h3>2.2 Index data with data\nstructures</h3><p>Logs have uses for systems like Kafka, but you cannot just put\neverything in a log and forget about it. You have to find stuff in it,\nso you need data structures like B+trees, LSM-trees, hashtables. That’s\nwhere I started, I built a B+tree in 366 lines.</p><p>But if you just put a data structure on disk, you have the partial\nwrite problem. Is it possible to apply the log-based ideas to data\nstructures? There are 2 ideas:</p><p>The first idea is to make the data structure append-only, like a log.\nThere are ways to make a B+tree append-only, also called a\n. A copy-on-write tree does not\noverwrite existing nodes, it creates new nodes from leaf to root for the\nentire path. New nodes can be appended like a log.</p><pre><code>    d           d         D*\n   / \\         / \\       / \\\n  b   e  ──►  b   e  +  B*  e\n / \\         / \\       / \\\na   c       a   c     a   C*\n            original  updated</code></pre><p>The other idea is to use both a log and a main data structure:</p><ol type=\"1\"><li>Before updating the main data structure, store the intended updates\nin the log. Each log record contains a list of “write this data to that\nposition”.</li><li>Then apply the updates to the main data structure.</li><li>When the databases start, it can always apply the last log record to\nfix possible partial writes in the main data structure, regardless of\nits status.</li></ol><p>I chose the copy-on-write B+tree because it doesn’t require a log,\nwhich reduces the LoC. Moving the B+tree to disk involves interfacing\nwith the filesystem, including the . In 601 lines, I\nhave an append-only KV store on disk.</p><h3>2.3 Recycle and reuse unused\nspace</h3><p>I have to handle the consequence of append-only because storage is\nnot infinite. I simply added a free list to recycle unused B+tree nodes.\nIn 731 lines, I have a practical KV store.</p><pre><code>                     first_item\n                         ↓\nlist_head ─► [ next |    xxxxx ]\n                ↓\n             [ next | xxxxxxxx ]\n                ↓\nlist_tail ─► [ NULL | xxxx     ]\n                         ↑\n                     last_item</code></pre><p>A SQL query can be arbitrarily complex, but it boils down to 2 data\nstructure operations: point query and range query. That’s why I started\nwith B+tree data structures. Databases are built on data structures, not\non some theories about relations.</p><p>I added a layer on top of KV to store records with primary keys and\ncolumns. Each record is encoded as KV pair. The LoC increased to 1107.\nBut it’s still just a KV. So I added 2 more features:</p><ul><li>Secondary index, 1438 lines.</li></ul><p>I didn’t bother with SQL at this point because SQL is just a user\ninterface. I built APIs for databases which is called a .</p><p>I can ignore concurrency and make everything serialized. But using a\ncopy-on-write B+tree means that readers get snapshot isolation for free,\nbecause updates do not destroy old versions. So readers will not be\nblocked by the writer.</p><p>I went a step further to allow write transactions to run\nconcurrently, and then merge the updates at the end of the transaction.\nThis requires detecting and rejecting conflicting updates. In 1702\nlines, I have a transactional interface.</p><h3>2.6 SQL-like query language</h3><p>My user interface is just a set of APIs to operate the database. The\nnext step is a query language. I chose a SQL-like syntax. I only\nimplemented simple queries with a single point/range query, so there is\nno query planner.</p><p>Parsing SQL is not a database topic, but a compiler topic. It’s done\nby applying recursion in a specific way, which is hardly an idea once\nyou’ve learned it.</p><p>A query language is not just about data, it can do computations, like\n. So I need an interpreter.</p><p>Then I need lots of glue code between the query language and the\ndatabase. The lines of code increased from 1702 to 2795, most of them\nuninteresting.</p><h2>03. Database in 3000\nlines, incrementally</h2><p>This is my minimalist attempt to build a database from scratch. Each\nstep adds an essential part while minimizing uninteresting details.</p><table><tbody><tr></tr><tr><td>Practical KV with a free list.</td></tr><tr><td>Transactional interfaces.</td></tr><tr></tr></tbody></table><p>I’ve turned this into a <a href=\"https://build-your-own.org/database/\">book</a> so that you can\nfollow my steps. The first half of the book is free online and contains\nthe most interesting parts of the storage engine, which is usable as a\nKV on its own.</p><p>3000 lines is not much, but it’s an effective exercise. The first\nedition took me only 2 months of free time. The book is updated whenever\nI learn something new, so this is the second edition.</p>","contentLength":6675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=42725163"}]}